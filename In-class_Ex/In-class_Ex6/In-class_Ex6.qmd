---
title: "In-class Exercise 6"
subtitle: "Global and Local Measures of Spatial Autocorrelation: sfdep methods"
author: "Foo Jia Yi Samantha"
date-modified: "September 23, 2024"
date: "September 23, 2024"
execute: 
  eval: true
  echo: true
  freeze: true
---

```{=html}
<button>In-class Exercise</button> <button>R</button> <button>sfdep</button>
```
# 1. Let's Set Up!

## 1.1 Install Required Libraries

We will first want to install the GWModel package from CRAN

```{r}
#| eval: false
install.packages("GWmodel")
```

## 1.2 Importing Libraries into R

In this in-class exercise, sf, **sfdep**, tmap, and tidyverse will be used.

1.  `sf` provides a standardized way to work with spatial data in R. It allows for the manipulation and analysis of geospatial data in simple feature format
2.  `sfdep` is designed for spatial dependency and autocorrelation analysis. It specifically integrates with `sf` to calculate spatial autocorrelation statistics such as Moran’s I, Geary’s C, and other local or global spatial measures.
3.  `tmap` is a powerful package for visualizing spatial data through thematic maps. It supports both static and interactive mapping, making it ideal for displaying spatial patterns, clusters, and the results of autocorrelation analysis.
4.  `tidyverse` is a collection of packages (e.g., `dplyr`, `ggplot2`, `purrr`, `tibble`) that are designed for data manipulation, visualization, and functional programming in R. It is not specific to spatial analysis but is essential for general data wrangling.

```{r}
pacman::p_load(sf, sfdep, tmap, tidyverse)
```

## 1.3 Preparing the Datasets

I will be using the Hunan dataset used in the Hands-on Exercise 5 spatial weights and applications.

### 1.3.1 Importing Geospatial Data

Firstly, we will import the Hunan county boundary layer. This is a geospatial data set in ESRI shapefile format. The code chunk below uses `st_read()` of **sf** package.

```{r}
#| code-summary: Import the geospatial data
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

### 1.3.2 Importing Aspatial Data

Next, I will import the aspatial data set. This data is a csv file containing selected Hunan’s local development indicators in 2012.

```{r}
#| code-summary: Import the aspatial data
hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

### 1.3.3 Performing relational join

The code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using `left_join()` of dplyr package.

```{r}
hunan_GDPPC <- left_join(hunan, hunan2012) %>%
  select(1:4, 7, 15)
```

### 1.4 Plotting A Choropleth Map

Next, let's plot a choropleth map showing the distribution of GDPPC of Hunan Province.

```{r}

tmap_mode("plot")
tm_shape(hunan_GDPPC) +
  tm_fill("GDPPC", 
          style = "quantile", 
          palette = "Blues",
          title = "GDPPC") +
  tm_layout(main.title = "Distribution of GDP per capita by county, Hunan Province",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_borders(alpha = 0.5) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```

# 2. Global Measures of Spatial Association

## 2.1 Deriving Queen's contiguity weights: sfdep methods

Notice that st_weights() provides tree arguments,

-   **nb:** a neighbour list object as created by st_neighbors().

-   **style**: Default "W" for row standardized weights. This value can also be "B", "C", "U", "minmax", and "S". B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).

-   **allow_zero**: If TRUE, assigns zero as lagged value to zone without neighbors. 8

```{r}
wm_q <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb,
                         style = "W"),
         .before = 1) 
```

```{r}
# Inspect the dataframe
wm_q
```

## 2.2 Computing Global Moran's I

In the code chunk below, [`global_moran()`](https://sfdep.josiahparry.com/reference/global_moran) function is used to compute the Moran’s I value. Different from spdep package, the output is a tibble data.frame.

```{r}
moranI <- global_moran(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)
glimpse(moranI)
```

## 2.3 Performing Global Moran’sI test

In general, Moran’s I test will be performed instead of just computing the Moran’s I statistics. With sfdep package, Moran’s I test can be performed by using [`global_moran_test()`](https://sfdep.josiahparry.com/reference/global_moran_test.html) as shown in the code chunk below.

```{r}
global_moran_test(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)
```

::: callout-tip
-   The default for `alternative` argument is “two.sided”. Other supported arguments are “greater” or “less”. randomization, and

-   By default the `randomization` argument is **TRUE**. If FALSE, under the assumption of normality.
:::

## 2.4 Performing Global Moran's I Permutation Test

In practice, Monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by **globel\_ moran_perm().**

**Step 1:**

It's always good practice to to use **set.seed()** before performing simulation. This is to ensure that the computation is reproducible.

```{r}
set.seed(1234)
```

**Step 2**

Next, **global_moral_perm()** is used to perform Monte Carlo simulation.

```{r}
global_moran_perm(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt,
                  nsim = 99)
```

**Step 3: Analyse!**

::: callout-note
The statistical report on previous tab shows that the **p-value is smaller than alpha value of 0.05**. Hence, we have **enough** **statistical evidence** to **reject the null hypothesis** that the spatial distribution of GPD per capita are **resemble random distribution** (i.e. independent from spatial).

Since the **Moran’s I statistics is greater than 0**, we can infer that the spatial distribution shows sign of **clustering**. **i.e.** it indicates **positive spatial autocorrelation**, which means that similar values (either high or low) tend to cluster together in space.
:::

# 3. Local Indicators of Spatial Autocorrelation (LISA)

LISA map is a categorical map showing **outliers** and **clusters**.

-   There are two types of outliers namely: **High-Low** and **Low-High** **outliers**.

-   Likewise, there are two type of clusters namely: **High-High** and **Low-Low cluaters**.

In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values.

## 3.1 Computing local Moran’s I

In this section, I will learn how to compute Local Moran’s I of GDPPC at county level by using [`local_moran()`](https://sfdep.josiahparry.com/reference/local_moran.html) of sfdep package.

```{r}
lisa <- wm_q %>% 
  mutate(local_moran = local_moran(
    GDPPC, nb, wt, nsim = 99),
         .before = 1) %>%
  unnest(local_moran)
```

## 3.2 Visualising local Moran’s I

In this code chunk below, tmap functions are used prepare a choropleth map by using value in the *ii* field.

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(
    main.title = "local Moran's I of GDPPC",
    main.title.size = 2)
```

## 3.3 Visualising p-value of local Moran’s I

In the code chunk below, tmap functions are used prepare a choropleth map by using value in the *p_ii_sim* field.

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("p_ii_sim") + 
  tm_borders(alpha = 0.5) +
   tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 2)
```

::: callout-note
For p-values, the appropriate classification should be **0.001, 0.01, 0.05** and not significant instead of using default classification scheme.

**Suggested Classification for p-values:**

-   **p \< 0.001**: Highly significant clustering

-   **p \< 0.01**: Very significant clustering

-   **p \< 0.05**: Significant clustering

-   **Not significant (p \>= 0.05)**: No significant clustering
:::

## 3.4 Visualising local Moran’s I and p-value

For effective comparison, it will be better for us to plot both maps next to each other.

```{r}
tmap_mode("plot")
map1 <- tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of GDPPC",
            main.title.size = 0.8)

map2 <- tm_shape(lisa) +
  tm_fill("p_ii",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 2)
```

## 3.5 Plotting a LISA Map

In lisa sf data.frame, we can find three fields contain the LISA categories. They are *mean*, *median* and *pysal*. In general, classification in *mean* will be used as shown in the code chunk below.

```{r}
lisa_sig <- lisa  %>%
  filter(p_ii_sim < 0.05)
tmap_mode("plot")
tm_shape(lisa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(lisa_sig) +
  tm_fill("mean") + 
  tm_borders(alpha = 0.4)
```

# 4. Hot Spot and Cold Spot Area Analysis (HCSA)

HCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure

## 4.1 Computing local Gi\* statistics

As usual, we will need to derive a spatial weight matrix before we can compute local Gi\* statistics. Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.

```{r}
wm_idw <- hunan_GDPPC %>%
  mutate(nb = include_self(
    st_contiguity(geometry)),
    wts = st_inverse_distance(nb, 
                              geometry, 
                              scale = 1,
                              alpha = 1),
         .before = 1)
```

::: callout-note
-   Gi\* and local Gi\* are distance-based spatial statistics. Hence, distance methods instead of contiguity methods should be used to derive the spatial weight matrix.

-   Since we are going to compute Gi\* statistics, `include_self()`is used.
:::

Now, we will compute the local Gi\* by using the code chunk below.

```{r}
HCSA <- wm_idw %>% 
  mutate(local_Gi = local_gstar_perm(
    GDPPC, nb, wts, nsim = 99),
         .before = 1) %>%
  unnest(local_Gi)
HCSA
```

## 4.2 Visualising Gi\*

In the code chunk below, tmap functions are used to plot the local Gi\* (i.e. gi_star) at the province level.

```{r}
tmap_mode("plot")
tm_shape(HCSA) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8))
```

## 4.3 Visualising p-value of HCSA

In the code chunk below, tmap functions are used to plot the p-values of local Gi\* (i.e. p_sim) at the province level

```{r}
tmap_mode("plot")
tm_shape(HCSA) +
  tm_fill("p_sim") + 
  tm_borders(alpha = 0.5)
```

## 4.4 Visuaising local HCSA

For effective comparison, you can plot both maps next to each other as shown below.

```{r}
tmap_mode("plot")
map1 <- tm_shape(HCSA) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "Gi* of GDPPC",
            main.title.size = 0.8)

map2 <- tm_shape(HCSA) +
  tm_fill("p_value",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "p-value of Gi*",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 2)
```

## 4.5 Visualising hot spot and cold spot areas

Now, we are ready to plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.

```{r}
HCSA_sig <- HCSA  %>%
  filter(p_sim < 0.05)
tmap_mode("plot")
tm_shape(HCSA) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(HCSA_sig) +
  tm_fill("cluster") + 
  tm_borders(alpha = 0.4)
```

::: callout-note
Figure above reveals that there is one hot spot area and two cold spot areas. Interestingly, the hot spot areas coincide with the High-high cluster identifies by using local Moran’s I method in the earlier sub-section.
:::
