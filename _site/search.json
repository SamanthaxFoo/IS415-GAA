[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "Hey there! üëã I‚Äôm Samantha Foo, a Year 3 SMU student pursuing a Bachelor in Information Systems (Business Analytics) with a 2nd Major in Data Science & Analytics. I‚Äôm a huge data enthusiast, a FOOdie and I enjoy exploring new places/cities!\nJoin me on this data odyssey as I conquer Geospatial Analytics! üå±\n\n\n\n\n\nSep 22, 2024\nFoo Jia Yi Samantha\n\nTake Home Exercise 1\nSpatial & Spatio-Temporal Point Pattern Analysis of Myanmar‚Äôs Armed Conflicts\n\n\n\n\nOct 13, 2024\nFoo Jia Yi Samantha\n\nTake Home Exercise 2\nHarnessing Geospatial Methods to Analyse Drug Abuse Patterns in Thailand\n\n\n\n\nNov 3, 2024\nFoo Jia Yi Samantha\n\nTake Home Exercise 3\nNavigating Jakarta‚Äôs Traffic Network Using Grab Posisi Data\n\n\n\n\nAug 16, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 1\nGeospatial Wrangling with R\n\n\n\n\n\n\nAug 22, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 2\nChoropleth Mapping and GeoVisualisation with R\n\n\n\n\nAug 27, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 3\n1st and 2nd Order Spatial Point Patterns Analysis Methods\n\n\n\n\nSep 10, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 5\nSpatial Weights and Applications\n\n\n\n\nSep 18, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 6\nGlobal and Local Measures of Spatial Autocorrelation\n\n\n\n\nOct 13, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 8\nGeographical Segmentation with Spatially Constrained Clustering Techniques (Part 1)\n\n\n\n\nOct 18, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 9\nGeographical Segmentation with Spatially Constrained Clustering Techniques (Part 2)\n\n\n\n\nOct 22, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 10\nCalibrating Hedonic Pricing Model for Private Highrise Property with GWR Method\n\n\n\n\nOct 30, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 11\nCalibrating Hedonic Pricing Model for Private Highrise Property with GWR Method\n\n\n\n\nOct 30, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 12\nGeographically Weighted Predictive Models\n\n\n\n\nAug 19, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 1\nSet up RStudio and Create a Quarto Document\n\n\n\n\nAug 26, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 2\nImporting Libraries and Data into R, and Performing Data Wrangling\n\n\n\n\nSep 2, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 3\nCovering Errors in Hands-on Exercise 3 and Importing ACLED Datasets\n\n\n\n\nSep 10, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 4\nExploring Spatio-Temporal Kernel Density Estimation\n\n\n\n\nSep 18, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 5\nExploring Geographically Weighted Models\n\n\n\n\nSep 18, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 6\nGlobal and Local Measures of Spatial Autocorrelation: sfdep methods\n\n\n\n\nSep 30, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 7\nBuilding interactive Shiny Dashboard Applications\n\n\n\n\nOct 21, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 9\nGeographical Segmentation with Spatially Constrained Clustering Techniques\n\n\n\n\nOct 30, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 10\nCalibrating Hedonic Pricing Model for Private Highrise Property with MLR methods"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "Hey there! üëã I‚Äôm Samantha Foo, a Year 3 SMU student pursuing a Bachelor in Information Systems (Business Analytics) with a 2nd Major in Data Science & Analytics. I‚Äôm a huge data enthusiast, a FOOdie and I enjoy exploring new places/cities!\nJoin me on this data odyssey as I conquer Geospatial Analytics! üå±"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "University of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St.¬†Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nIn this hands-on exercise, I will be performing geospatial data science tasks in R by using the sf and tidyverse R packages. By the end of this hands-on exercise, I would have acquired the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#lets-set-up",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#lets-set-up",
    "title": "Hands-on Exercise 1",
    "section": "2. Let‚Äôs Set Up!",
    "text": "2. Let‚Äôs Set Up!\n\n2.1 Data Acquisition\nData are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data. With that said, I have extracted the data sets from the following four sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\nI will be tapping on these vastly available, public data from the government and private sectors for future exercises ahead!\n\n\n\n2.2 Set Up the Folders\nThis is the file structure for containing the data files that I have extracted in the previous step. The Hands-on_Ex1 folder consists of a data sub-folder, and is further separated by the geospatial and aspatial folders.\n\n\n\n\n\n\n\n2.3 Installing R Packages\nIn this exercise, I will be using these two R packages\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nWith that said, I installed the required packages using the code chunk below.\n\npacman::p_load(sf, tidyverse)\n\n\np_load is a function of the pacman package that is used to install and load sf and tidyverse packages into our R environment."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data-into-r",
    "title": "Hands-on Exercise 1",
    "section": "3. Importing Geospatial Data into R",
    "text": "3. Importing Geospatial Data into R\nIn this section, I will import the following geospatial data into R by using st_read() of the sf package:\n\nMP14_SUBZONE_WEB_PL: a polygon feature layer in ESRI shapefile format,\nCyclingPath: a line feature layer in ESRI shapefile format, and\nPreSchool: a point feature layer in kml file format.\n\n\n3.1 Importing Polygon Feature Data in .shp Format\nThe code chunk below uses¬†st_read()¬†function of¬†sf¬†package to import¬†MP14_SUBZONE_WEB_PL¬†shapefile (.shp) into R as a polygon feature data frame.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nüîé Observations: the mpsz simple feature data frame contains 323 multipolygon features, 15 fields and is in the¬†SVY21 projected coordinates system.\n\n\nüí° Note: dsn defines folder path and layer defines file name (AKA a shapefile, no need any extension like .shp)\n\n\n\n3.2 Importing Polyline Feature Data in .shp Format\nThe code chunk below imports¬†CyclingPath¬†shapefile (.shp) into R as a polyline feature data frame.\n\ncyclingpath = st_read(dsn = \"data/geospatial\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3138 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42644.17 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\nüîé Observations: the cyclingpath linestring feature data frame contains 3138 features and 2 fields and it is in the¬†SVY21 projected coordinates system.\n\n\n\n3.3 Importing GIS Point Feature Data in .kml Format\nThe code chunk below imports¬†PreSchoolsLocation.kml kml format into R as a point feature data frame.\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nüîé Observations: the PreSchoolsLocation.kml point feature data frame contains 2290 point features, 2 fields and is in the¬†WGS84¬†projected coordinates"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-aspatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-aspatial-data-into-r",
    "title": "Hands-on Exercise 1",
    "section": "4. Importing Aspatial Data into R",
    "text": "4. Importing Aspatial Data into R\n\n4.1 Importing Aspatial Data\nNotice that the¬†listings¬†data set is in csv file format. Instead of st_read(), we‚Äôll use read_csv()¬†from the readr¬†package to import¬†listings.csv.\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nRows: 3540 Columns: 18\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr   (6): name, host_name, neighbourhood_group, neighbourhood, room_type, l...\ndbl  (11): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nThis outputs an R object called¬†listings¬†which is a¬†tibble data frame.\n\nLet‚Äôs take a peak into our listings tibble data frame.\n\nglimpse(listings)\n\nRows: 3,540\nColumns: 18\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28‚Ä¶\n$ name                           &lt;chr&gt; \"Ensuite Room (Room 1 & 2) near EXPO\", ‚Ä¶\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925‚Ä¶\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",‚Ä¶\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg‚Ä¶\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu‚Ä¶\n$ latitude                       &lt;dbl&gt; 1.34537, 1.34754, 1.34531, 1.29015, 1.2‚Ä¶\n$ longitude                      &lt;dbl&gt; 103.9589, 103.9596, 103.9610, 103.8081,‚Ä¶\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat‚Ä¶\n$ price                          &lt;dbl&gt; NA, 80, 80, 50, 50, NA, 85, 65, 45, 54,‚Ä¶\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 180, 180, 92, 92, 180, 180,‚Ä¶\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 60,‚Ä¶\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20‚Ä¶\n$ reviews_per_month              &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0.11, 0.08, 0.8‚Ä¶\n$ calculated_host_listings_count &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49, 49, 6, 7, 7,‚Ä¶\n$ availability_365               &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 365, 0, 0, 365,‚Ä¶\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, ‚Ä¶\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"‚Ä¶\n\n\n\nüîé Observations: there are 3540 rows and 18 columns (not features and fields like in our simple data feature frame!)\n\n\nüí° Note: we‚Äôll be using the¬†latitude¬†and¬†longitude¬†fields in the next phase. These fields appear to be adopting the WGS84 geographic coordinate system.\n\n\n\n4.2 Converting Aspatial Data\nNext, we‚Äôll convert listing (a non-geospatial tabular data frame) into a simple feature data frame by using st_as_sf() from the sf package.\n\nüí° Note: a non-simple feature data frame will simply not have a ‚Äúgeometry‚Äù column. Use class(listings) as a simple test - if it outputs data.frame, tbl_df, tbl, etc and no sf, then it‚Äôs not a simple feature data frame!\n\n\nlistings_sf &lt;- st_as_sf(listings, coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;% st_transform(crs = 3414)\n\n\ncoordscrs%&gt;%\n\n\nIndicates the column name of the x-coordinates, followed by that of the y-coordinates.\n\n\nIndicates the coordinates system in epsg format (more info: epsg.io)\n\nEPSG: 4326 is WGS84 Geographic Coordinate System\nEPSG: 3414 is Singapore SVY21 Projected Coordinate System\n\n\n\nTo nest st_transform() and transform the newly created simple feature data frame into SVY21 Projected Coordinate System\n\n\n\nThis gives us the new simple feature data frame,¬†listings_sf:\n\nglimpse(listings_sf)\n\nRows: 3,540\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28‚Ä¶\n$ name                           &lt;chr&gt; \"Ensuite Room (Room 1 & 2) near EXPO\", ‚Ä¶\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925‚Ä¶\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",‚Ä¶\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg‚Ä¶\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu‚Ä¶\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat‚Ä¶\n$ price                          &lt;dbl&gt; NA, 80, 80, 50, 50, NA, 85, 65, 45, 54,‚Ä¶\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 180, 180, 92, 92, 180, 180,‚Ä¶\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 60,‚Ä¶\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20‚Ä¶\n$ reviews_per_month              &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0.11, 0.08, 0.8‚Ä¶\n$ calculated_host_listings_count &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49, 49, 6, 7, 7,‚Ä¶\n$ availability_365               &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 365, 0, 0, 365,‚Ä¶\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, ‚Ä¶\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"‚Ä¶\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (‚Ä¶\n\n\n\nüîé Observations:\nNotice that a new column called¬†geometry¬†has been added into the data frame. On the other hand, the¬†longitude¬†and¬†latitude¬†columns have been removed from the data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#exploring-contents-of-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#exploring-contents-of-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 1",
    "section": "5. Exploring Contents of a Simple Feature Data Frame",
    "text": "5. Exploring Contents of a Simple Feature Data Frame\nThere are 3 ways to explore the contents of a simple feature data frame like mpsz!\n\n5.1 Using st_geometry()\nThe sf data.frame contains a geometry column, that is a list of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses¬†st_geometry().\n\n# Retrieve geometry column\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\nüîé Observations: This MP14_SUBZONE_WEB_PL file consists of 323 features, consisting of multipolygon features.\n\n\n\n5.2 Using glimpse()\nWe use glimpse() from the dplyr package to understand the data type of each fields.\nE.g.¬†FMEL-UPD_D¬†field is in¬†date¬†data type and¬†X_ADDR,¬†Y_ADDR,¬†SHAPE_L¬†and¬†SHAPE_AREA¬†fields are in double-precision values.\n\n# Get data types\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ‚Ä¶\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, ‚Ä¶\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL‚Ä¶\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",‚Ä¶\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",‚Ä¶\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",‚Ä¶\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",‚Ä¶\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT‚Ä¶\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",‚Ä¶\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",‚Ä¶\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05‚Ä¶\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,‚Ä¶\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,‚Ä¶\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,‚Ä¶\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103‚Ä¶\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (‚Ä¶\n\n\n\nüîé Observations: This MP14_SUBZONE_WEB_PL file consists of 323 rows and 16 columns with datatypes ranging from integers, characters, date and doubles.\n\n\n\n5.3 Using head()\nWe use head() from the base R package to get the full information of the feature object mpsz. The n value indicates the no. of rows.\n\n# Display top 5 rows of the feature object\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nüîé Observations: We can notice that the MP14_SUBZONE_WEB_PL file consists of 5 features and 15 fields, with the top 5 rows per column."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1",
    "section": "6. Plotting the Geospatial Data",
    "text": "6. Plotting the Geospatial Data\nIn geospatial data science, looking at feature information is not sufficient. We are also interested in visualising the geospatial features of the sf object, in which plot() will help with that.\n\n# Plot multi-plot of all attributes\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above.\n\nWe can, however, choose to plot the geometry only as such:\n\n# Plot the geometry only\nplot(st_geometry(mpsz))\n\n\n\n\n\n\n\n\nOr, plot the sf object using a specific attribute\n\n# Plot a specific attribute\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\n\nüí° Note:¬†plot()¬†is meant for plotting the geospatial object at a high level. For high cartographic quality plot, other R package such as tmap should be used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#working-with-projection",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#working-with-projection",
    "title": "Hands-on Exercise 1",
    "section": "7. Working with Projection",
    "text": "7. Working with Projection\nWhat is ‚Äúmap projection‚Äù?: it is an important property of geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, I project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n7.1 Assigning EPSG code to a simple feature data frame\nDefine ‚ÄúESPG code‚Äù: a unique identifier to represent coordinate systems.\nCommon issues when importing geospatial data into R : the coordinate system of the source data are either‚Ä¶\n\nMissing (such as due to missing .proj for ESRI shapefile)\nWrongly assigned\n\nTo check the coordinate system of mpsz simple feature data frame, I‚Äôll use st_crs() from the sf package.\n* crs = Coordinate Reference System\n\n# Check coordinate system\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nüîé Observations Notice the mpsz data is a SVY21 projected coordinate system. However, the ESPG code is wrongly indicated as 9001 in the last few lines. The correct ESPG code for SVY21 should be 3114. Thus, we‚Äôll assign the correct code as such.\n\n\n# Assign new ESPG code\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n# Check that crs has been updated to 3414\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n7.2 Converting Data from Geographic to Projected Coordinate System\nRecall that the geographic coordinate system (e.g., WGS84) is not appropriate for analyses that involve distance/area. Hence, it‚Äôs common for us to transform the original data to a projected coordinate system.\nLet‚Äôs take a look at the preschool simple feature data frame. It shows that it is in the WGS84 coordinate system, i.e., geographic coordinate system.\n\n# Transform projection\nst_geometry(preschool)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT Z (103.8072 1.299333 0)\n\n\nPOINT Z (103.826 1.312839 0)\n\n\nPOINT Z (103.8409 1.348843 0)\n\n\nPOINT Z (103.8048 1.435024 0)\n\n\nPOINT Z (103.839 1.33315 0)\n\n\nNow, we‚Äôll transform preschool‚Äôs coordinate system from geographic (WGS84) to projected (SVY21).\n\npreschool3414 &lt;- st_transform(preschool, crs = 3414)\n\n\nüîé Observations: Notice that the last row shows ‚ÄúProjected CRS‚Äù now"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 1",
    "section": "8. Geoprocessing with sf Package",
    "text": "8. Geoprocessing with sf Package\nBesides providing functions to handling geospatial data (i.e.¬†importing, exporting, assigning projection, transforming projection etc), sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, I perform two commonly-used geoprocessing functions, namely buffering and point in polygon count.\n\n8.1 Buffering\nüìùThe scenario: The authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on both sides of the current cycling path. You are tasked to determine the extend of the land needed to be acquired and their total area.\nüí°The solution:\nFirstly, st_buffer() of the sf package is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, dist = 5, nQuadSegs = 30)\n\n\nA higher nQuadSegs value results in a smoother and more accurate circular buffer. The default is 30.\n\nThis is followed by calculating the area of the buffers\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n2218855 [m^2]\n\n\n\n\n8.2 Point-in-polygon count\nüìùThe scenario: A pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nüí°The solution:\nFirstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate the no. of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, top_n() of the dplyr package is used.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nNext, I calculate the density of pre-school by planning subzone. I used st_area() of the sf package to derive the area of each planning subzone.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%   st_area()\n\nNext, I used mutate()of the dplyr package to compute the density by using the code chunk below.\n\nmpsz3414 &lt;- mpsz3414 %&gt;% mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 1",
    "section": "9. Exploratory Data Analysis (EDA)",
    "text": "9. Exploratory Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, I will tap on ggplot2() functions to create functional yet transparent statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\n\n\nAlthough the syntax is very easy to use, the output is currently far from meeting publication quality. Furthermore, hist() function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2() functions will be used.\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n\n\nUsing ggplot2 method, I plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "title": "Hands-on Exercise 2",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nThematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices.\nGeovisualisation works by providing graphical ideation to render a place, phenomenon or a process.\nIn this hands-on exercise, I will learn how to plot functional and truthful chloropleth maps by using the tmap R package. The output of this exercise should look like thisL"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#lets-set-up",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#lets-set-up",
    "title": "Hands-on Exercise 2",
    "section": "2. Let‚Äôs Set Up!",
    "text": "2. Let‚Äôs Set Up!\n\n2.1 Importing Libraries into R\nIn this hands-on exercise, the key R package use is tmap package in R, alongside these four other R packages:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package. Hence, we will only need to install the tidyverse package.\n\nNow, let‚Äôs install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n2.2 Download Data and Set Up Folders\nWe will be using two data sets to create the choropleth maps\n1) Master Plan 2014 Subzone Boundary (Web): geospatial data consisting of the geographical boundary of Singapore at the planning subzone level.\nüìÖ The data is based on URA Master Plan 2014.\nüìÅ ESRI shapefile format (i.e.¬†MP14_SUBZONE_WEB_PL)\nüîó Can be downloaded at data.gov.sg\n2) Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling: aspatial data file. Although it does not contain any coordinates values, the PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\nüìÖ June 2011-2020\nüìÅ csv format (i.e.¬†respopagesextod2011to2020.csv)\nüîó Can be downloaded at Department of Statistics, Singapore\nThis is the file structure for containing the data files that I have extracted.\n\n\n\n2.3 Importing Data into R\n\n2.3.1 Importing Geospatial Data into R\nNow, we‚Äôll use the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n# Import shapefile\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n# Inspect shapefile\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\nüîé Observations: The MP14_SUBZONE_WEB_PL data set consists of 323 features and 15 fields made up of multipolygon features.\n\n\n\n2.3.2 Importing Aspatial (Attribute) Data into R\nFor aspatial datasets like respopagsex2011to2020.csv, we will import into Rstudio using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nüîé Observations: The respopagsex2011to2020.csv data follows the SVY21 projected coordinate which contains 984656 rows and 7 columns"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#data-preparation-and-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#data-preparation-and-wrangling",
    "title": "Hands-on Exercise 2",
    "section": "3. Data Preparation and Wrangling",
    "text": "3. Data Preparation and Wrangling\nBefore a thematic map can be prepared, we will need to prepare a data table with values from 2020 which includes these variables:\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n3.1 Data Wrangling\nIn order to carry out necessary data wrangling and transformation, the following functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\nüîé Observations: Notice that we have filtered our population data from 2020 and successfully grouped them by PA, SZ and AG which sums up the population within each category. I‚Äôve also summed up the rows for ECONOMY ACTIVE, AGED and TOTAL, and created a new DEPENDENCY column which takes the sum of YOUNG and AGED, and then divide that sum by the value of ECONOMY ACTIVE.\n\n\n\n3.2 Joining Geospatial Data and Attribute Data\nBefore we can perform the georelational join, we are required to convert the values in PA and SZ fields to uppercase to ensure consistency with the uppercase values in SUBZONE_N and PLN_AREA_N.\nHence, we will standardise the data values in these two fields.\n\n# Convert to uppercase\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext,¬†left_join()¬†of¬†dplyr¬†is used to join the geographical data and attribute table using planning subzone name e.g.¬†SUBZONE_N¬†and¬†SZ¬†as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 2",
    "section": "4. Choropleth Mapping Geospatial Data Using tmap",
    "text": "4. Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors.\nüìñ Scenario: A social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements, i.e.¬†tm_shape()\n\n\n4.1 Method 1: Plotting a Choropleth Map quickly using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\n# Plot choropleth map using qtm()\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\nüí° Note:\n\ntmap_mode() with ‚Äúplot‚Äù option is used to produce a static map. For interactive mode, ‚Äúview‚Äù option should be used.\nfill argument is used to map the attribute (i.e.¬†DEPENDENCY)\n\n\n\n\n4.2 Method 2: Plotting a Choropleth Map quickly using tm_shape()\n\n# Plot choropleth map using tmap's elements\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nStep 1: Drawing a Base Map Using tm_shape()\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\n\ntm_shape(mpsz_pop2020) + tm_polygons()\n\n\n\n\n\n\n\n\n\nüí° Note: tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\n\nStep 2: Drawing a Choropleth Map Using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020) + tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nStep 3: Drawing a Choropleth Map Using tm_fill() and tm_border()\nFirstly, we will try to draw a choropleth map by using tm_fill() alone.\n\nüí° Note: tm_polygons() is a wrapper of tm_fill() and tm_border()\ntm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\n\n\n# Add fill\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown below.\n\n# Add boundary\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\nüí° Note: Notice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). Default alpha value is 1.\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width (default is 1)\nlty = border line type (default is ‚Äúsolid‚Äù)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#data-classification-methods-of-tmap",
    "title": "Hands-on Exercise 2",
    "section": "5. Data Classification Methods of tmap",
    "text": "5. Data Classification Methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\n\ntmap provides a total ten data classification methods, namely:\n\nfixed,\nsd,\nequal,\npretty (default),\nquantile,\nkmeans,\nhclust,\nbclust,\nfisher,\njenks.\n\n\n\n5.1 Plotting Choropleth Maps with Built-in Classification Methods\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used. The code chunks below uses 5 classes where, n = 5.\n\nüí° There are 10 types of styles: jenks, equal, fixed, sd, pretty (default), quantile, kmeans, fisher, hclust and bclust\n\n\n1) jenks\nFirstly, we‚Äôll use the¬†jenks style method. It is known as natural breaks and is based on natural groupings inherent in the data. Data is clustered into groups that minimise the within-group variance and maximise the between-group variance.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n2) equal\nNext, we will try equal data classification method. This creates a more even distribution as shown.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nüí° Note: not compatible for data that are highly-skewed or with one or two large outliers\n\n\n\n3) sd\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"sd\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\nüí° Note: Should only use if the distribution resembles a normal distribution (bell-curve)!\n\n\n\n4) kmeans\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"kmeans\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n5) fisher\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"fisher\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\nüí° Note: At a glance, using Fisher and KMeans lead to similar visualisations.\n\n\n\n4) hclust\nhclust is hierarchical clustering used to create a hierarchy of clusters based on their similarity. Each data point starts as an individual cluster and then progressively merges or splits clusters until a stopping criterion is met.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"hclust\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n5) bclust\nbclust is bagged clustering which creates multiple subsets of the original dataset through resampling. Each subset is then used to train an individual clustering model, and the final cluster assignments are obtained by combining the results from all models.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"bclust\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\n\n\n\n5.2 Plotting Choropleth Maps with Custom Breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument in tm_fill().\n\nüí° Note: in tmap, the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\n\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nLooking at the summary statistics, the break point can be set to 0.60, 0.70, 0.80, and 0.90. The minimum and maximum breaks must also be included, which are 0 and 100 respectively.\n\n# Using this information, we will now proceed to plot the choropleth map.\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          palette=\"plasma\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\n\n\nüí° Observations: the legend has now been categorised according to the breaks vector,¬†c(0, 0.60, 0.70, 0.80, 0.90, 1.00).\n\n\n\n5.3 Customising Colour Schemes\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\nTo change the colour, we assign the preferred colour to the palette argument of tm_fill() as shown below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"plasma\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nAdd a ‚Äú-‚Äù prefix to reverse the colour shading.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#controlling-and-customising-map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#controlling-and-customising-map-layouts",
    "title": "Hands-on Exercise 2",
    "section": "6. Controlling and Customising Map Layouts",
    "text": "6. Controlling and Customising Map Layouts\n\n6.1 Map Legend\nIn¬†tmap, several¬†legend¬†options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            #legend.height = 0.45, \n            #legend.width = 0.35,\n            legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n6.2 Map Style\nThe layout of the map can also be adjusted using¬†tmap_style(). E.g. Classic\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\nüí° Observations: the classic tmap style creates a border with double lines, the colours used are more muted and neutral, and the font has been changed to something more elegant\n\n\n\n6.3 Cartographic Furniture\ntmap also provides arguments to draw other important map elements like compass, scale bar and grid lines.\nTo add compass, scale and gridlines, pay attention to how tm_compass(),¬†tm_scale_bar()¬†and¬†tm_grid() are used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n6.4 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred as facet maps, comprise of many adjacent maps. These facets enable easier visualisation of how spatial relationships change with respect to another variable. Such as, time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nMethod 1: By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\nüí° Observations: two choropleth maps been generated to represent the Young and Aged demographics respectively.\n\nAdditionally, the style and palette arguments can be adjusted accordingly.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nMethod 2: By defining a group-by variable in tm_facets()\ntm_facets() can help to group categorical data like regions and subzone areas such that the generated facet maps will zoom in to the specified variable.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated\n\n\n\n\n\n\n\n\n\n\nüí° Observations: we have generated 5 different choropleth maps that represent the 5 unique regions found in the REGION_N data variable!\n\n\n\nMethod 3: By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"viridis\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"plasma\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\nüí° Observations: as compared to the charts generated in Method 1, writing two tm_shape() functions allows us to create two separate choropleth maps produced as seen above."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#mappping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#mappping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-on Exercise 2",
    "section": "7. Mappping Spatial Object Meeting a Selection Criterion",
    "text": "7. Mappping Spatial Object Meeting a Selection Criterion\nMap outputs can also be targeted by using selection functions to meet the selection criterion. For example, we have selected the central region and¬†DEPENDENCY column to plot.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nüí° Note: In order to only display data from the Central Region, we need to filter the mpsz_pop2020 data frame via mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ]"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex1/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‚Äòhttp://mrcc.com/qgis.dtd‚Äô ‚ÄòSYSTEM‚Äô&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#getting-started",
    "title": "Hands-on Exercise 2",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nThematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices.\nGeovisualisation works by providing graphical ideation to render a place, phenomenon or a process.\nIn this hands-on exercise, I will learn how to plot functional and truthful chloropleth maps by using the tmap R package. The output of this exercise should look like thisL"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#lets-set-up",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#lets-set-up",
    "title": "Hands-on Exercise 2",
    "section": "2. Let‚Äôs Set Up!",
    "text": "2. Let‚Äôs Set Up!\n\n2.1 Importing Libraries into R\nIn this hands-on exercise, the key R package use is tmap package in R, alongside these four other R packages:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package. Hence, we will only need to install the tidyverse package.\n\nNow, let‚Äôs install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n2.2 Download Data and Set Up Folders\nWe will be using two data sets to create the choropleth maps\n1) Master Plan 2014 Subzone Boundary (Web): geospatial data consisting of the geographical boundary of Singapore at the planning subzone level.\nüìÖ The data is based on URA Master Plan 2014.\nüìÅ ESRI shapefile format (i.e.¬†MP14_SUBZONE_WEB_PL)\nüîó Can be downloaded at data.gov.sg\n2) Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling: aspatial data file. Although it does not contain any coordinates values, the PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\nüìÖ June 2011-2020\nüìÅ csv format (i.e.¬†respopagesextod2011to2020.csv)\nüîó Can be downloaded at Department of Statistics, Singapore\nThis is the file structure for containing the data files that I have extracted in the previous step.\n\n\n\n2.3 Importing Data into R\n\n2.3.1 Importing Geospatial Data into R\nNow, we‚Äôll use the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n# Import shapefile\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n# Inspect shapefile\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\nüîé Observations: The MP14_SUBZONE_WEB_PL data set consists of 323 features and 15 fields made up of multipolygon features.\n\n\n\n2.3.2 Importing Aspatial (Attribute) Data into R\nFor aspatial datasets like respopagsex2011to2020.csv, we will import into Rstudio using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nüîé Observations: The respopagsex2011to2020.csv data follows the SVY21 projected coordinate which contains 984656 rows and 7 columns"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#data-preparation-and-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#data-preparation-and-wrangling",
    "title": "Hands-on Exercise 2",
    "section": "3. Data Preparation and Wrangling",
    "text": "3. Data Preparation and Wrangling\nBefore a thematic map can be prepared, we will need to prepare a data table with values from 2020 which includes these variables:\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n3.1 Data Wrangling\nIn order to carry out necessary data wrangling and transformation, the following functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\nüîé Observations: Notice that we have filtered our population data from 2020 and successfully grouped them by PA, SZ and AG which sums up the population within each category. I‚Äôve also summed up the rows for ECONOMY ACTIVE, AGED and TOTAL, and created a new DEPENDENCY column which takes the sum of YOUNG and AGED, and then divide that sum by the value of ECONOMY ACTIVE.\n\n\n\n3.2 Joining Geospatial Data and Attribute Data\nBefore we can perform the georelational join, we are required to convert the values in PA and SZ fields to uppercase to ensure consistency with the uppercase values in SUBZONE_N and PLN_AREA_N.\nHence, we will standardise the data values in these two fields.\n\n# Convert to uppercase\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext,¬†left_join()¬†of¬†dplyr¬†is used to join the geographical data and attribute table using planning subzone name e.g.¬†SUBZONE_N¬†and¬†SZ¬†as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 2",
    "section": "4. Choropleth Mapping Geospatial Data Using tmap",
    "text": "4. Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors.\nüìñ Scenario: A social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n4.1 Method 1: Plotting a Choropleth Map quickly using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\nüí° Note:\n\ntmap_mode() with ‚Äúplot‚Äù option is used to produce a static map. For interactive mode, ‚Äúview‚Äù option should be used.\nfill argument is used to map the attribute (i.e.¬†DEPENDENCY)\n\n\n\n\n4.2 Method 2: Plotting a Choropleth Map quickly using tmap‚Äôs elements\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n4.3 Drawing a Base Map Using tm_shape()\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\n\ntm_shape(mpsz_pop2020) + tm_polygons()\n\n\n\n\n\n\n\n\n\nüí° Note: tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\n\n4.4 Drawing a Choropleth Map Using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020) + tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n4.5 Drawing a Choropleth Map Using tm_fill() and tm_border()\nFirstly, we will try to draw a choropleth map by using tm_fill() alone.\n\nüí° Note: tm_polygons() is a wrapper of tm_fill() and tm_border()\ntm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\n\n\n# Add fill\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown below.\n\n# Add boundary\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\nüí° Note: Notice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). Default alpha value is 1.\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width (default is 1)\nlty = border line type (default is ‚Äúsolid‚Äù)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#data-classification-methods-of-tmap",
    "title": "Hands-on Exercise 2",
    "section": "5. Data Classification Methods of tmap",
    "text": "5. Data Classification Methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\n\ntmap provides a total ten data classification methods, namely:\n\nfixed,\nsd,\nequal,\npretty (default),\nquantile,\nkmeans,\nhclust,\nbclust,\nfisher,\njenks.\n\n\n\n5.1 Plotting Choropleth Maps with Built-in Classification Methods\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used. The code chunks below uses 5 classes where, n = 5.\n\nüí° There are 10 types of styles: jenks, equal, fixed, sd, pretty (default), quantile, kmeans, fisher, hclust and bclust\n\n\n1) jenks\nFirstly, we‚Äôll use the¬†jenks style method. It is known as natural breaks and is based on natural groupings inherent in the data. Data is clustered into groups that minimise the within-group variance and maximises the between-group variance.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n2) equal\nNext, we will try equal data classification method. This creates a more even distribution as shown.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n3) sd\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"sd\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n4) kmeans\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"kmeans\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n5) fisher\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"fisher\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\nüí° Note: At a glance, using Fisher and KMeans lead to similar visualisations.\n\n\n\n4) hclust\nhclust is hierarchical clustering used to create a hierarchy of clusters based on their similarity. Each data point starts as an individual cluster and then progressively merges or splits clusters until a stopping criterion is met.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"hclust\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n5) bclust\nbclust is bagged clustering which creates multiple subsets of the original dataset through resampling. Each subset is then used to train an individual clustering model, and the final cluster assignments are obtained by combining the results from all models.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"bclust\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\n\n\n\n5.2 Plotting Choropleth Maps with Custom Breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nLooking at the summary statistics, the break point can be set to 0.60, 0.70, 0.80, and 0.90. The minimum and maximum breaks must also be included, which are 0 adn 100 respectively. These would translate to the¬†breaks¬†vector,¬†c(0, 0.60, 0.70, 0.80, 0.90, 1.00).\n\n# Using this information, we will now proceed to plot the choropleth map.\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          palette=\"plasma\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\n\n\n\n5.3 Customising Colour Schemes\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\nTo change the colour, we assign the preferred colour to the palette argument of tm_fill() as shown below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"plasma\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nAdd a ‚Äú-‚Äù prefix to reverse the colour shading.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#controlling-and-customising-map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#controlling-and-customising-map-layouts",
    "title": "Hands-on Exercise 2",
    "section": "6. Controlling and Customising Map Layouts",
    "text": "6. Controlling and Customising Map Layouts\n\n6.1 Map Legend\nIn¬†tmap, several¬†legend¬†options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            #legend.height = 0.45, \n            #legend.width = 0.35,\n            legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n6.2 Map Style\nThe layout of the map can also be adjusted using¬†tmap_style(). E.g. Classic\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n6.3 Cartographic Furniture\ntmap also provides arguments to draw other important map elements like compass, scale bar and grid lines.\nTo add compass, scale and gridlines, pay attention to how tm_compass(),¬†tm_scale_bar()¬†and¬†tm_grid() are used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n6.4 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred as facet maps, comprise of many adjacent maps. These facets enable easier visualisation of how spatial relationships change with respect to another variable. Such as, time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nMethod 1: By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\nAdditionally, the style and palette arguments can be adjusted accordingly.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nMethod 2: By defining a group-by variable in tm_facets()\ntm_facets() can help to group categorical data like regions and subzone areas such that the generated facet maps will zoom in to the specified variable.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated\n\n\n\n\n\n\n\n\n\n\n\nMethod 3: By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"viridis\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"plasma\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#mappping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#mappping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-on Exercise 2",
    "section": "7. Mappping Spatial Object Meeting a Selection Criterion",
    "text": "7. Mappping Spatial Object Meeting a Selection Criterion\nMap outputs can also be targeted by using selection functions to meet the selection criterion. For example, we have selected the central region and¬†DEPENDENCY column to plot.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex1/data/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‚Äòhttp://mrcc.com/qgis.dtd‚Äô ‚ÄòSYSTEM‚Äô&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-libraries-into-r",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-libraries-into-r",
    "title": "In-class Exercise 2",
    "section": "1. Importing Libraries into R",
    "text": "1. Importing Libraries into R\nIn this exercise, we will use the following R packages:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\ntidyverse for tidying data (https://tidyr.tidyverse.org/)\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-data-sets-into-r",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-data-sets-into-r",
    "title": "In-class Exercise 2",
    "section": "2. Importing Data Sets into R",
    "text": "2. Importing Data Sets into R\nWe will first import the three geospatial data sets into R using¬†st_read()¬†of the¬†sf¬†package.\n\n2.1 Importing Polygon Feature Data in .shp Format\n\n# Import shapefile\nmpsz14_shp &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n# Retrieve geometry column\nst_geometry(mpsz14_shp)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\n# Check class\nclass(mpsz14_shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\n\n\n2.2 Importing Polygon Feature Data in .kml Format\n\n# Import KML file\n#mpsz14_kml &lt;- st_read(\"data/geospatial/MP14_SUBZONE_WEB_PL.kml\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-polygon-feature-data-in-.shp-format",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-polygon-feature-data-in-.shp-format",
    "title": "In-class Exercise 2",
    "section": "2.1 Importing Polygon Feature Data in .shp Format",
    "text": "2.1 Importing Polygon Feature Data in .shp Format\n\n# Import shapefile\nmpsz14_shp &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n# Import KML file\nst_write(mpsz14_shp, \n         \"data/geospatial/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)\n\nWarning in CPL_write_ogr(obj, dsn, layer, driver,\nas.character(dataset_options), : GDAL Error 4: Unable to open\ndata/geospatial/MP14_SUBZONE_WEB_PL.kml to obtain file list.\n\n\nDeleting source `data/geospatial/MP14_SUBZONE_WEB_PL.kml' failed\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/geospatial/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n\nThere are 3 ways to explore the contents of a simple feature data frame like mpsz!\n\n1) Using st_geometry()\nThe sf data.frame contains a geometry column, that is a list of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses¬†st_geometry().\n\n# Retrieve geometry column \nst_geometry(mpsz14_shp)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\nüîé Observations: This MP14_SUBZONE_WEB_PL file consists of 323 features, consisting of multipolygon features.\n\n\n\n2) Using glimpse()\nWe use glimpse() from the dplyr package to understand the data type of each fields.\nE.g.¬†FMEL-UPD_D¬†field is in¬†date¬†data type and¬†X_ADDR,¬†Y_ADDR,¬†SHAPE_L¬†and¬†SHAPE_AREA¬†fields are in double-precision values.\n\n# Get data types \nglimpse(mpsz14_shp)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ‚Ä¶\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, ‚Ä¶\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL‚Ä¶\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",‚Ä¶\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",‚Ä¶\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",‚Ä¶\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",‚Ä¶\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT‚Ä¶\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",‚Ä¶\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",‚Ä¶\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05‚Ä¶\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,‚Ä¶\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,‚Ä¶\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,‚Ä¶\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103‚Ä¶\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (‚Ä¶\n\n\n\nüîé Observations: This MP14_SUBZONE_WEB_PL file consists of 323 rows and 16 columns with datatypes ranging from integers, characters, date and doubles.\n\n\n\n3) Using head()\nWe use head() from the base R package to get the full information of the feature object mpsz. The n value indicates the no. of rows.\n\n# Display top 5 rows of the feature object \nhead(mpsz14_shp, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nüîé Observations: We can notice that the MP14_SUBZONE_WEB_PL file consists of 5 features and 15 fields, with the top 5 rows per column."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#using-glimpse",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#using-glimpse",
    "title": "In-class Exercise 2",
    "section": "2) Using glimpse()",
    "text": "2) Using glimpse()\nWe use glimpse() from the dplyr package to understand the data type of each fields.\nE.g.¬†FMEL-UPD_D¬†field is in¬†date¬†data type and¬†X_ADDR,¬†Y_ADDR,¬†SHAPE_L¬†and¬†SHAPE_AREA¬†fields are in double-precision values.\n\n# Get data types \nglimpse(mpsz14_shp)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ‚Ä¶\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, ‚Ä¶\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL‚Ä¶\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",‚Ä¶\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",‚Ä¶\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",‚Ä¶\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",‚Ä¶\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT‚Ä¶\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",‚Ä¶\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",‚Ä¶\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05‚Ä¶\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,‚Ä¶\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,‚Ä¶\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,‚Ä¶\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103‚Ä¶\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (‚Ä¶\n\n\n\nüîé Observations: This MP14_SUBZONE_WEB_PL file consists of 323 rows and 16 columns with datatypes ranging from integers, characters, date and doubles.\n\n\n3) Using head()\nWe use head() from the base R package to get the full information of the feature object mpsz. The n value indicates the no. of rows.\n\n# Display top 5 rows of the feature object \nhead(mpsz14_shp, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nüîé Observations: We can notice that the MP14_SUBZONE_WEB_PL file consists of 5 features and 15 fields, with the top 5 rows per column.\n\n\n\n2.2 Importing Polygon Feature Data in .kml Format\n\n# Import KML file\n#mpsz14_kml &lt;- st_read(\"data/geospatial/MP14_SUBZONE_WEB_PL.kml\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-polygon-feature-data-in-.kml-format",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-polygon-feature-data-in-.kml-format",
    "title": "In-class Exercise 2",
    "section": "2.2 Importing Polygon Feature Data in .kml Format",
    "text": "2.2 Importing Polygon Feature Data in .kml Format\n\nüí° Note: delete_dsn = TRUE will help delete the original data before rendering it"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-polygon-data-in-.shp-format",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-polygon-data-in-.shp-format",
    "title": "In-class Exercise 2",
    "section": "2.1 Importing MP14_SUBZONE_WEB_PL (Web) Polygon Data in .shp Format",
    "text": "2.1 Importing MP14_SUBZONE_WEB_PL (Web) Polygon Data in .shp Format\n\n# Import shapefile\nmpsz14_shp &lt;- st_read(dsn = \"data\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThere are 3 ways to explore the contents of a simple feature data frame like mpsz!\n\n1) Using st_geometry()\nThe sf data.frame contains a geometry column, that is a list of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses¬†st_geometry().\n\n# Retrieve geometry column \nst_geometry(mpsz14_shp)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\nüîé Observations: This MP14_SUBZONE_WEB_PL file consists of 323 features, consisting of multipolygon features.\n\n\n\n2) Using glimpse()\nWe use glimpse() from the dplyr package to understand the data type of each fields.\nE.g.¬†FMEL-UPD_D¬†field is in¬†date¬†data type and¬†X_ADDR,¬†Y_ADDR,¬†SHAPE_L¬†and¬†SHAPE_AREA¬†fields are in double-precision values.\n\n# Get data types \nglimpse(mpsz14_shp)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ‚Ä¶\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, ‚Ä¶\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL‚Ä¶\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",‚Ä¶\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",‚Ä¶\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",‚Ä¶\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",‚Ä¶\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT‚Ä¶\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",‚Ä¶\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",‚Ä¶\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05‚Ä¶\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,‚Ä¶\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,‚Ä¶\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,‚Ä¶\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103‚Ä¶\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (‚Ä¶\n\n\n\nüîé Observations: This MP14_SUBZONE_WEB_PL file consists of 323 rows and 16 columns with datatypes ranging from integers, characters, date and doubles.\n\n\n\n3) Using head()\nWe use head() from the base R package to get the full information of the feature object mpsz. The n value indicates the no. of rows.\n\n# Display top 5 rows of the feature object \nhead(mpsz14_shp, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nüîé Observations: We can notice that the MP14_SUBZONE_WEB_PL file consists of 5 features and 15 fields, with the top 5 rows per column."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-polygon-data-in-.kml-format",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-polygon-data-in-.kml-format",
    "title": "In-class Exercise 2",
    "section": "2.2 Importing MP14_SUBZONE_WEB_PL (Web) Polygon Data in .kml Format",
    "text": "2.2 Importing MP14_SUBZONE_WEB_PL (Web) Polygon Data in .kml Format\nWe use the below code chunk to export mpsz14_shp sf data.frame into kml file which saves the file into our data folder.\n\n# Convert .shp file into .kml\nst_write(mpsz14_shp, \n         \"data/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)\n\nDeleting source `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n# Import KML file\nmpsz14_kml = st_read(\"data/MP14_SUBZONE_WEB_PL.kml\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data\\MP14_SUBZONE_WEB_PL.kml' \n  using driver `KML'\nSimple feature collection with 323 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n# Display top 5 rows of the feature object \nhead(mpsz14_kml, n=5)  \n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.8142 ymin: 1.272838 xmax: 103.8725 ymax: 1.291523\nGeodetic CRS:  WGS 84\n  Name Description                       geometry\n1                  MULTIPOLYGON (((103.8647 1....\n2                  MULTIPOLYGON (((103.8431 1....\n3                  MULTIPOLYGON (((103.8507 1....\n4                  MULTIPOLYGON (((103.8255 1....\n5                  MULTIPOLYGON (((103.8194 1....\n\n\n\nüí° Note: delete_dsn = TRUE will help delete the original data before rendering it"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp19_subzone_web_pl-no-sea-.shp-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp19_subzone_web_pl-no-sea-.shp-data",
    "title": "In-class Exercise 2",
    "section": "2.3 Importing MP19_SUBZONE_WEB_PL (No Sea) .shp Data",
    "text": "2.3 Importing MP19_SUBZONE_WEB_PL (No Sea) .shp Data\n\n# Import shapefile\nmpsz19_shp &lt;- st_read(dsn = \"data\", layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nüîé Observations: We can notice that the data file consists of 332 features and 6 fields, and follows the WGS64 coordinate system. Here we can notice it uses the `ESRI Shapefile‚Äô driver."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp19_subzone_web_pl-no-sea-.kml-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp19_subzone_web_pl-no-sea-.kml-data",
    "title": "In-class Exercise 2",
    "section": "2.4 Importing MP19_SUBZONE_WEB_PL (No Sea) .kml Data",
    "text": "2.4 Importing MP19_SUBZONE_WEB_PL (No Sea) .kml Data\n\n# Convert .shp file into .kml\nst_write(mpsz19_shp, \n         \"data/MP19_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)\n\nDeleting source `data/MP19_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP19_SUBZONE_WEB_PL' to data source \n  `data/MP19_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 332 features with 6 fields and geometry type Multi Polygon.\n\n# Import KML file\nmpsz19_kml = st_read(\"data/MP19_SUBZONE_WEB_PL.kml\")\n\nReading layer `MP19_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data\\MP19_SUBZONE_WEB_PL.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n# Display top 5 rows of the feature object \nhead(mpsz19_kml, n=5)  \n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6537 ymin: 1.216215 xmax: 103.8811 ymax: 1.29742\nGeodetic CRS:  WGS 84\n  Name Description                       geometry\n1                  MULTIPOLYGON (((103.8802 1....\n2                  MULTIPOLYGON (((103.8376 1....\n3                  MULTIPOLYGON (((103.8341 1....\n4                  MULTIPOLYGON (((103.7125 1....\n5                  MULTIPOLYGON (((103.8472 1....\n\n\n\nüîé Observations: We can notice that the datafile also consists of 332 features and 6 fields, and follows the WGS64 coordinate system, but it uses the kml driver accordingly."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-.shp-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-.shp-data",
    "title": "In-class Exercise 2",
    "section": "2.1 Importing MP14_SUBZONE_WEB_PL (Web) .shp Data",
    "text": "2.1 Importing MP14_SUBZONE_WEB_PL (Web) .shp Data\n\n# Import shapefile\nmpsz14_shp &lt;- st_read(dsn = \"data\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThere are 3 ways to explore the contents of a simple feature data frame like mpsz!\n\n1) Using st_geometry()\nThe sf data.frame contains a geometry column, that is a list of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses¬†st_geometry().\n\n# Retrieve geometry column \nst_geometry(mpsz14_shp)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\nüîé Observations: This MP14_SUBZONE_WEB_PL file consists of 323 features, consisting of multipolygon features.\n\n\n\n2) Using glimpse()\nWe use glimpse() from the dplyr package to understand the data type of each fields.\nE.g.¬†FMEL-UPD_D¬†field is in¬†date¬†data type and¬†X_ADDR,¬†Y_ADDR,¬†SHAPE_L¬†and¬†SHAPE_AREA¬†fields are in double-precision values.\n\n# Get data types \nglimpse(mpsz14_shp)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ‚Ä¶\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, ‚Ä¶\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL‚Ä¶\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",‚Ä¶\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",‚Ä¶\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",‚Ä¶\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",‚Ä¶\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT‚Ä¶\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",‚Ä¶\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",‚Ä¶\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05‚Ä¶\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,‚Ä¶\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,‚Ä¶\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,‚Ä¶\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103‚Ä¶\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (‚Ä¶\n\n\n\nüîé Observations: This MP14_SUBZONE_WEB_PL file consists of 323 rows and 16 columns with datatypes ranging from integers, characters, date and doubles.\n\n\n\n3) Using head()\nWe use head() from the base R package to get the full information of the feature object mpsz. The n value indicates the no. of rows. I will use thiis method for this in-class exercise.\n\n# Display top 5 rows of the feature object \nhead(mpsz14_shp, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nüîé Observations: We can notice that the MP14_SUBZONE_WEB_PL file consists of 5 features and 15 fields, with the top 5 rows per column."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-.kml-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-.kml-data",
    "title": "In-class Exercise 2",
    "section": "2.2 Importing MP14_SUBZONE_WEB_PL (Web) .kml Data",
    "text": "2.2 Importing MP14_SUBZONE_WEB_PL (Web) .kml Data\nWe use the below code chunk to export mpsz14_shp sf data.frame into kml file which saves the file into our data folder.\n\n# Convert .shp file into .kml\nst_write(mpsz14_shp, \n         \"data/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)\n\nDeleting source `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n# Import KML file\nmpsz14_kml = st_read(\"data/MP14_SUBZONE_WEB_PL.kml\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data\\MP14_SUBZONE_WEB_PL.kml' \n  using driver `KML'\nSimple feature collection with 323 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n# Display top 5 rows of the feature object \nhead(mpsz14_kml, n=5)  \n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.8142 ymin: 1.272838 xmax: 103.8725 ymax: 1.291523\nGeodetic CRS:  WGS 84\n  Name Description                       geometry\n1                  MULTIPOLYGON (((103.8647 1....\n2                  MULTIPOLYGON (((103.8431 1....\n3                  MULTIPOLYGON (((103.8507 1....\n4                  MULTIPOLYGON (((103.8255 1....\n5                  MULTIPOLYGON (((103.8194 1....\n\n\n\nüí° Note: delete_dsn = TRUE will help delete the original data before rendering it"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#in-class-exercise-objectives",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#in-class-exercise-objectives",
    "title": "In-class Exercise 1",
    "section": "In-class Exercise Objectives",
    "text": "In-class Exercise Objectives\nIn this week‚Äôs in-class Exercise 1, I explored the setting up of our RStudio and installed all R tools required for this IS314 Geospatial Analytics and Application module."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#steps-taken-in-this-exercise",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#steps-taken-in-this-exercise",
    "title": "In-class Exercise 1",
    "section": "Steps Taken in this Exercise",
    "text": "Steps Taken in this Exercise\nTo create this exercise file, I first created a sub-folder under the In-class_Ex folder and named it In-class_Ex1 and created this Quarto document by executing the following steps:\n\nClick ‚ÄúFile‚Äù tab\nClick ‚ÄúNew File‚Äù\nUnder ‚ÄúNew File‚Äù, select ‚ÄúQuarto Document‚Äù\n\n\nüîé Observations: A new file is added in File Explorer with a .qmd file format. You will need to save the file and label it a name for the file to render and load.\n\nNext, I created the header of this document by adding YAML code that can be indicated using back-ticks ``` &lt;add code here&gt; ```. Within the YAML code, I indicated the document title, writer, publishing date and modifcation date as displayed accordingly in this webpage."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex2/data/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‚Äòhttp://mrcc.com/qgis.dtd‚Äô ‚ÄòSYSTEM‚Äô&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "title": "Hands-on Exercise 3",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nIn this exercise, I will be exploring the basic methods of spatial point pattern analysis - split into two parts.\n\nPart 1: [1st Order Spatial Point Patterns Analysis]\nPart 2: [2nd Order Spatial Point Patterns Analysis]\n\nIn particular, I will be using the spatstat package for this exercise.\n\nüí° What‚Äôs spatstat? the spatstat package is a comprehensive package for the analysis of spatial point patterns. It is a very powerful package, but it is also very complex. We will only be using a small subset of the functionality of the package. (More info can be found on this spatstat website)\n\nThe goal of this exercise is to discover the spatial point processes of childecare centres in Singapore by answering the following questions:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nIf no, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#lets-set-up",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#lets-set-up",
    "title": "Hands-on Exercise 3",
    "section": "2. Let‚Äôs Set Up!",
    "text": "2. Let‚Äôs Set Up!\n\n2.1 Importing Libraries into R\nIn this exercise, we will use the following R packages:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e.¬†raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nNow, let‚Äôs install and load these packages in RStudio.\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse)\n\n\n\n2.2 Download Data and Set Up Folders\nWe will use 3 data sets for this exercise:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\nThis is the file structure for containing the data files that I have extracted."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#import-data-sets-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#import-data-sets-into-r",
    "title": "Hands-on Exercise 3",
    "section": "3. Import Data Sets into R",
    "text": "3. Import Data Sets into R\nWe will first import the three geospatial data sets into R using¬†st_read()¬†of the¬†sf¬†package.\n\nchildcare_sf &lt;- st_read(\"data/aspatial/child-care-services-geojson.geojson\") %&gt;% st_transform(crs = 3414) \n\nReading layer `child-care-services-geojson' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex3\\data\\aspatial\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nsg_sf &lt;- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\")  \n\nReading layer `CostalOutline' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 3",
    "section": "4. Geospatial Data Wrangling",
    "text": "4. Geospatial Data Wrangling\n\n4.1 Standardising Coordinate Systems\nBefore we proceed, let‚Äôs check if the geospatial data sets are projected in the same projection system.\n\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nüí° Observations: Notice that sg_sf and mpsz_sf is in the SVY21 coordinate system format, but their EPSG code is wrongly indicated as 9001, instead of 3414.\n\nLet‚Äôs assign the correct ESPG code to mpsz_sf and sg_sf simple feature data frames:\n\nsg_sf &lt;- st_transform(sg_sf, 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, 3414)\n\n\n\n4.2 Mapping the Geospatial Data Sets\nNext, let‚Äôs map the geospatial data sets to show their spatial patterns.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_sf) +\n  qtm(childcare_sf)\n\n\n\n\n\n\n\n\n\nüí° Observations: We can see that all the geospatial layers are within the same map extend. This shows that their referencing system and coordinate values are referred to similar spatial context. This is very important in any geospatial analysis.\n\nAlternatively, we can also prepare a pin map by using the code chunk below.\n\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\nüí° Note: remember to switch back to plot mode after the interactive map as each interactive mode will consume a connection. It is also advised to avoid displaying ecessive numbers of interactive maps (i.e.¬†not more than 10) in one RMarkdown document when publish on Netlify.\n\n\ntmap_mode('plot')\n\ntmap mode set to plotting\n\n\n\n\n4.3 Converting the Simple Features to sp‚Äôs Spatial* Class\nWhen we convert childcare_sf geojson data to a Spatial class, we can observe below that the childcare_sf data is still stored in the¬†Description¬†attribute and has not been fully utilised. In particular, it is in a HTML format\n\nchildcare &lt;- as_Spatial(childcare_sf)\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\nAs such, let us transform this data to make it more meaningful and easier for us to read.\n\nlibrary(xml2)\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nchildcare_validity &lt;- st_is_valid(childcare_sf)\nchildcare_invalid &lt;- which(!childcare_validity)\nif (length(childcare_invalid) &gt; 0) {\n  print(\"ChildCare Invalid!\")\n  print(childcare_sf[childcare_invalid, ])\n} else {\n  print(\"it's valid!\")\n}\n\n[1] \"it's valid!\"\n\n# Ensure the geometry column is preserved\ngeometry_column &lt;- st_geometry(childcare_sf)\nparse_description &lt;- function(html_string) {\n  html &lt;- read_html(html_string)\n  html &lt;- html %&gt;% html_nodes(\"tr\") %&gt;% .[!grepl(\"Attributes\", .)]\n  headers &lt;- html %&gt;% html_nodes(\"th\") %&gt;% html_text(trim = TRUE)\n  values &lt;- html %&gt;% html_nodes(\"td\") %&gt;% html_text(trim = TRUE)\n  \n  # Handle cases where the number of headers and values don't match\n  if (length(headers) != length(values)) {\n    max_length &lt;- max(length(headers), length(values))\n    headers &lt;- c(headers, rep(\"ExtraHeader\", max_length - length(headers)))\n    values &lt;- c(values, rep(\"NULL\", max_length - length(values)))\n  }\n  \n  setNames(values, headers)\n}\n\n# Apply parsing function, unnest the description fields, and remove the original 'Description' column\nchildcare_sf &lt;- childcare_sf %&gt;% \n  mutate(Description_parsed = map(Description, parse_description)) %&gt;%\n  unnest_wider(Description_parsed) %&gt;%\n  select(-Description)  # Remove the original 'Description' column\n\n# Overwrite the 'Name' column with the 'LANDYADDRESSPOINT' column values\nchildcare_sf &lt;- childcare_sf %&gt;%\n  mutate(Name = NAME)  # Overwrite 'Name' with 'LANDYADDRESSPOINT'\n\n# Replace empty strings or NA across all columns with \"NULL\"\nchildcare_sf &lt;- childcare_sf %&gt;%\n  mutate(across(!geometry, ~ ifelse(is.na(.) | . == \"\", \"NULL\", .)))\n\n# Reassign the geometry to the dataframe\nst_geometry(childcare_sf) &lt;- geometry_column\n# Ensure it's still an sf object\nclass(childcare_sf)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe will now convert the sf geospatial data frames to sp Spatial* class and display the information of these three Spatial* classes.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 16\nnames       :                    Name, ADDRESSBLOCKHOUSENUMBER, ADDRESSBUILDINGNAME, ADDRESSPOSTALCODE,                                                                       ADDRESSSTREETNAME, ADDRESSTYPE,         DESCRIPTION, HYPERLINK, LANDXADDRESSPOINT, LANDYADDRESSPOINT,                    NAME, PHOTOURL, ADDRESSFLOORNUMBER,          INC_CRC,     FMEL_UPD_D, ... \nmin values  :    3-IN-1 FAMILY CENTRE,                    NULL,                NULL,            018989,                                                  1 & 3, Stratton Road, SINGAPORE 806787,        NULL, Child Care Services,      NULL,                 0,                 0,    3-IN-1 FAMILY CENTRE,     NULL,               NULL, 00A958622500BF89, 20200812221033, ... \nmax values  : ZEE SCHOOLHOUSE PTE LTD,                    NULL,                NULL,            829646, UPPER BASEMENT LEVEL, WEST WING, TERMINAL 1, SINGAPORE CHANGI AIRPORT, SINGAPORE 819642,        NULL,                NULL,      NULL,                 0,                 0, ZEE SCHOOLHOUSE PTE LTD,     NULL,               NULL, FFCFA88A8CE5665A, 20200826094036, ... \n\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\n\nüí° Observations: each data frame has been converted into their respective Spatial Points and Spatial Polygons data frames.\n\n\n\n4.4 Converting the Spatial* Class Into Generic sp Format, then ppp Object Format\nThe spatstat package requires analytical data in planar point pattern (ppp) object format. As there is no direct way to convert a Spatial* classes into ppp object, we will need to convert the Spatial* classes into a Spatial object first.\n\nStep 1: Convert Spatial* classes into generic Spatial objects\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nHere is a display of the sp objects properties as shown below.\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nüí° Observations: However, notice that the sp objects do not contain information such as, variables, names, min values and max values.\n\n\n\nStep 2: Converting the sp objects into ppp objects\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat‚Äôs ppp object format.\n\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf))\n\nWarning: data contain duplicated points\n\n\nLet‚Äôs plot the ppp object to see what it looks like.\n\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\nWe can also take a quick look at the ppp object properties by using the code chunk below.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\n\nüí° Observations: Notice the warning message about duplicates. In spatial point patterns analysis, the presence of duplicates is a significant issue as the statistical methodology used is based largely on the assumption that points represent a unique location.\n\n\n\n\n4.5 Handling the duplicates\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nTo count the number of coincidence point, we will use the multiplicity() function as shown.\n\nmultiplicity(childcare_ppp)\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 128\n\n\n\nüí° Observations: The output shows that there are 338 duplicated point events.\n\nTo view the locations of these duplicate point events, we will plot childcare data accordingly.\n\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\ntmap_mode('plot')\n\ntmap mode set to plotting\n\n\n\n\n\n\n\n\nNote\n\n\n\nüí° How to identify duplicated points? duplicated points can be discovered by looking at the darker spots.\nThree ways to handle the duplicates:\n\nRemove the duplicates: This is the easiest way to handle the duplicates. However, it is not recommended because it will result in loss of information.\nJittering: Add a small amount of random noise to the duplicated points so they do not occupy the exact same space.\nMake each point unique by adding a unique identifier to each point as marks. This is the most recommended way to handle the duplicates. However, it is also the most tedious way to handle the duplicates.\n\n\n\nWith that said, we will use the second method to handle the duplicates. We will use the jitter() function to add a small amount of random noise to the duplicated points.\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n# Check for duplicate points in the data\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n4.6 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to convert the sg SpatialPolygon object into owinobject of spatstat.\n\nsg_owin &lt;- as.owin(sg_sf)\n\nThe output can be displayed using the plot() function\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\nand summary() function of base R\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n4.7 Combining Point Events Object and owin Object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the codes below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe ppp object outputted from combining both the point and polygon feature is shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\nNext, I plot the newly created childcareSG_ppp object as shown.\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#kernel-density-estimation",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#kernel-density-estimation",
    "title": "Hands-on Exercise 3",
    "section": "5. Kernel Density Estimation",
    "text": "5. Kernel Density Estimation\nIn this section, I will be computing the kernel density estimation (KDE) of childcare services in Singapore by using density() of the spatstat package.\nHere are the following configurations of density():\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is Gaussian, which is the default. Other smoothing methods are: ‚Äúepanechnikov‚Äù, ‚Äúquartic‚Äù or ‚Äúdisc‚Äù.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n5.1 Compute a Kernel Density\nThe code chunk below computes a kernel density by using the\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                              kernel=\"gaussian\")\n\nThe plot() function of Base R is then used to display the kernel density derived.\n\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\nüí° Observations: The density values of the output range from 0 to 0.000035 which is way too small to comprehend!\nüí° Why? It is worth noting that the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in ‚Äúnumber of points per square meter‚Äù.\n\nAs a side note, one can retrieve the bandwidth used to compute the KDE layer by using the code chunk below.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n298.4095 \n\n\n\n\n5.2 Re-scalling KDE values\nTo make the density values more comprehensible, we will rescale the density values from meter to kilometer using rescale().\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\nNow, we can re-run the density() function to compute the KDE map.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\nüí° Observations: Notice the output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n5.3 Working with Different Automatic Bandwidth Methods\nBesides bw.diggle(), there are other automatic bandwidth selection methods that can be used to determine the bandwidth. Such as bw.CvL(), bw.scott(), and bw.ppl().\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods\n\n bw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\n bw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.224898 1.450966 \n\n bw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\n bw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.2984095 \n\n\n\n\n\n\n\n\nNote\n\n\n\nTo use bw.diggle() or bw.ppl()?\nBaddeley et. (2016) suggested to use bw.ppl() when the pattern consists predominantly of tight clusters. While the bw.diggle() method works better when detecting a single tight cluster in the midst of random noise.\n\n\n\n# Let's compare the outputs!\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\n5.4 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is Gaussian. Nonetheless, there are 3 other options: Epanechnikov, Quartic and Dics.\nLet‚Äôs compute these three other kernel density estimations by indicating the kernel method as such.\n\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\n\n\n\n\n\n\n\n\n\n\n5.5. Fixed and Adaptive KDE\n\n5.5.1 Computing KDE by using fixed bandwidth\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\n\n\n\n5.5.2 Computing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n5.5.3 Converting KDE output into grid object\nThe result is the same, we just convert it so that it is suitable for mapping purposes\n\nkde_raster &lt;- raster(kde_childcareSG.bw)\ngridded_kde_childcareSG_bw &lt;- as(kde_raster, \"SpatialGridDataFrame\")\n\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\nStep 1) Converting gridded output into raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\n\nüí° Observations: Notice that the CRS property is NA.\n\n\n\nStep 2) Assigning projection systems\nThe code chunk below will be used to include the CRS information on kde_childcareSG_bw_raster RasterLayer.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\n\nüí° Observations: Notice that the CRS property is now completed.\n\n\n\n\n5.5.4 Visualising KDE Layer output in tmap\nFinally, we will display the KDE raster layer in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"layer\", palette=\"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\n\n\nüí° Observations: Notice that the raster values are encoded explicitly onto the raster pixel using the values in ‚Äúlayer‚Äù field.\n\n\n\n5.5.5 Comparing Spatial Point Patterns using KDE\nNext, I will compare the KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\nStep 1) Extracting Study Area\nThe code chunk below will be used to extract the target planning areas.\n\n# Extracting the planning areas\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nNext, let‚Äôs plot the target planning areas.\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(tm, main = \"Tampines\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\nWarning: plotting the first 10 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(jw, main = \"Jurong West\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\nStep 2) Creating owin Object\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\nStep 3) Combining Childcare Points and Study Area\nNext, we run these codes to extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale.ppp() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\nStep 4) Computing KDE\nThe code chunk below will be used to compute the KDE of these four planning area. The bw.diggle() method is used to derive the bandwidth of each planning area.\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\nStep 5) Computing Fixed Bandwidth KDE\nFor comparison purposes with fixed bandwidth KDE, we will use 250m as the bandwidth.\n\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#fixed-and-adaptive-kde",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#fixed-and-adaptive-kde",
    "title": "Hands-on Exercise 3",
    "section": "5.5. Fixed and Adaptive KDE",
    "text": "5.5. Fixed and Adaptive KDE\n\n5.5.1 Computing KDE by using fixed bandwidth\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\n\n\n\n5.5.2 Computing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n5.5.3 Converting KDE output into grid object\nThe result is the same, we just convert it so that it is suitable for mapping purposes\n\nkde_raster &lt;- raster(kde_childcareSG.bw)\ngridded_kde_childcareSG_bw &lt;- as(kde_raster, \"SpatialGridDataFrame\")\n\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\nStep 1) Converting gridded output into raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\n\nüí° Observations: Notice that the CRS property is NA.\n\n\n\nStep 2) Assigning projection systems\nThe code chunk below will be used to include the CRS information on kde_childcareSG_bw_raster RasterLayer.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\n\nüí° Observations: Notice that the CRS property is now completed.\n\n\n\n\n5.5.4 Visualising KDE Layer output in tmap\nFinally, we will display the KDE raster layer in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"layer\", palette=\"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\n\n\nüí° Observations: Notice that the raster values are encoded explicitly onto the raster pixel using the values in ‚Äúlayer‚Äù field.\n\n\n\n5.5.5 Comparing Spatial Point Patterns using KDE\nNext, I will compare the KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\nStep 1) Extracting Study Area\nThe code chunk below will be used to extract the target planning areas.\n\n# Extracting the planning areas\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nNext, let‚Äôs plot the target planning areas.\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(tm, main = \"Tampines\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\nWarning: plotting the first 10 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(jw, main = \"Jurong West\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\nStep 2) Creating owin Object\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\nStep 3) Combining Childcare Points and Study Area\nNext, we run these codes to extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale.ppp() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\nStep 4) Computing KDE\nThe code chunk below will be used to compute the KDE of these four planning area. The bw.diggle() method is used to derive the bandwidth of each planning area.\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\nStep 5) Computing Fixed Bandwidth KDE\nFor comparison purposes with fixed bandwidth KDE, we will use 250m as the bandwidth.\n\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#nearest-neighbour-analysis",
    "title": "Hands-on Exercise 3",
    "section": "6. Nearest Neighbour Analysis",
    "text": "6. Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\n\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n6.1 Testing spatial point patterns using Clark and Evans Test\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.55631, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\nüí° Observations: The output shows that the p-value is less than 0.05. Therefore, we reject the null hypothesis and conclude that the distribution of childcare services are not randomly distributed. We can also see that the R value is less than 1. This means that the distribution of childcare services are clustered.\n\n\n\n6.2 Clark and Evans Test: Choa Chu Kang planning area\nIn the code chunk below,¬†clarkevans.test()¬†of¬†spatstat¬†is used to performs Clark-Evans test of aggregation for childcare centre in Choa Chu Kang planning area.\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.95348, p-value = 0.487\nalternative hypothesis: two-sided\n\n\n\n\n6.3 Clark and Evans Test: Tampines planning area\nIn the code chunk below, the similar test is used to analyse the spatial point patterns of childcare centre in Tampines planning area.\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.79145, p-value = 0.0001673\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-g-function",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-g-function",
    "title": "Hands-on Exercise 3",
    "section": "7. Analysing Spatial Point Process Using G-Function",
    "text": "7. Analysing Spatial Point Process Using G-Function\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n7.1 Choa Chu Kang planning area\n\n7.1.1 Computing G-function Estimation\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n7.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with G-function\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n# Plot\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n7.2 Tampines planning area\n\n7.2.1 Computing G-function Estimation\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n\n\n\n\n7.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with G-function\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n# Plot\nplot(G_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-f-function",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-f-function",
    "title": "Hands-on Exercise 3",
    "section": "8. Analysing Spatial Point Process Using F-Function",
    "text": "8. Analysing Spatial Point Process Using F-Function\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n8.1 Choa Chu Kang planning area\n\n8.1.1 Computing F-function Estimation\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\n# Computing F-function estimation \nF_CK = Fest(childcare_ck_ppp) \n\n# Plot\nplot(F_CK)\n\n\n\n\n\n\n\n\n\n\n8.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with F-function\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n# Plot\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n8.2 Tampines planning area\n\n8.2.1 Computing F-function Estimation\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\n\n\n\n8.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with F-function\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n# Plot\nplot(F_CK.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-k-function",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-k-function",
    "title": "Hands-on Exercise 3",
    "section": "9. Analysing Spatial Point Process Using K-Function",
    "text": "9. Analysing Spatial Point Process Using K-Function\nK-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n9.1 Choa Chu Kang planning area\n\n9.1.1 Computing K-function Estimation\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n9.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with F-function\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n# Plot\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n9.2 Tampines planning area\n\n9.2.1 Computing K-function Estimation\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n9.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with F-function\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n# Plot\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-l-function",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-l-function",
    "title": "Hands-on Exercise 3",
    "section": "10. Analysing Spatial Point Process Using L-Function",
    "text": "10. Analysing Spatial Point Process Using L-Function\nIn this section, I will be computing L-function estimation by using Lest() of spatstat package. I will also perform monta carlo simulation test using envelope() of the spatstat package.\n\n10.1 Choa Chu Kang planning area\n\n10.1.1 Computing L-Function Estimation\nFirstly, let‚Äôs compute the L-function estimation for Choa Chu Kang.\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n10.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with L-function\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n# Plot\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n10.2 Tampines planning area\n\n10.2.1 Computing L-Function Estimation\nNext, let‚Äôs compute the L-function estimation for Tampines.\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n10.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below will be used to perform the hypothesis testing.\n\n# Monte Carlo test with L-function\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n# Plot\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-1-installing-maptools",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-1-installing-maptools",
    "title": "In-class Exercise 3",
    "section": "Issue #1: Installing maptools",
    "text": "Issue #1: Installing maptools\nmaptools have been retired and binary have been removed from CRAN. However, we can download from Posit Public Package Manager snapshots by using this code chunk below.\n\n# You can use this but it's not encouraged since maptools has depreciated!\ninstall.packages(\"maptools\",\n                 repos = \"https://packagemanager.posit.co/cran/2023-10-13\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-2-creating-coastal-outline-data",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-2-creating-coastal-outline-data",
    "title": "In-class Exercise 3",
    "section": "Issue #2: Creating Coastal Outline Data",
    "text": "Issue #2: Creating Coastal Outline Data\nIn sf package, there are two functions that allow us to combine multiple simple features into one simple features. They are st_ combine() and st_union().\n\nst_combine() returns a single, combined geometry, with no resolved boundaries; returned geometries may well be invalid.\nIf y is missing, st_union(x) returns a single geometry with resolved boundaries, else the geometries for all unioned pairs of xi] and yfil.\n\n\n# Impmort dataset into R\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n# Derive costal outline sf tibble data.frame\nsg_sf &lt;- mpsz_sf %&gt;%\n  st_union()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-3-converting-data-to-spatialgriddataframe",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-3-converting-data-to-spatialgriddataframe",
    "title": "In-class Exercise 3",
    "section": "Issue #3: Converting Data to SpatialGridDataFrame",
    "text": "Issue #3: Converting Data to SpatialGridDataFrame\nSince maptools isn‚Äôt installed in the Hands-on Exercise 3, we will need to use another method for converting the results of kde_childcareSG.bw to a Spatial Grid Data Frame.\n\n## This code won't work anymore\n# gridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\n\n## This code should work instead\n# kde_raster &lt;- raster(kde_childcareSG.bw)\n# gridded_kde_childcareSG_bw &lt;- as(kde_raster, \"SpatialGridDataFrame\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take-home Exercise 1 - Part 1",
    "section": "",
    "text": "The conflict in Myanmar is not just a result of the coup but is deeply rooted in the country‚Äôs decades-old complex ethnic and political landscape, characterised by tensions between the central government and various ethnic minority groups, each with its own armed forces. The post-coup violence has exacerbated these long-standing conflicts, leading to a severe humanitarian crisis, with thousands killed, hundreds of thousands displaced, and widespread human rights abuses reported.\n\n\n\nAs such, Geospatial analytics has become a valuable tool for evaluating and comprehending the intricacies of increasing conflicts. This exercise aims to reveal the spatial and spatio-temporal distribution of armed conflict in Myanmar by leveraging spatial point pattern analysis. Additionally, it aims to gain clearer insights into the geographical and logistical patterns of violence throughout the nation.\nBy the end of this take-home exercise, I aim to complete these steps in my spatial point pattern analysis in uncovering the distribution of armed conflict in Myanmar.\n\nUsing appropriate function of sf and tidyverse packages, import and transform the downloaded armed conflict data and administrative boundary data into sf tibble data.frames.\nUsing the geospatial data sets prepared, derive quarterly KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatial Point Patterns Analysis.\nUsing the geospatial data sets prepared, derive quarterly spatio-temporal KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatio-temporal Point Patterns Analysis.\nUsing appropriate tmap functions, display the KDE and Spatio-temporal KDE layers on openstreetmap of Myanmar.\nDescribe the spatial patterns revealed by the KDE and Spatio-temporal KDE maps.\n\n\n\n\n\n\nThis Armed Conflict Location & Event Data (ACLED) is an independent, impartial, international non-profit organisation which owns an extensive database of violent conflict and protest in countries and territories around the world.\n\nFor the purpose of this exercise, I have downloaded ACLED‚Äôs data on Myanmar which includes a series of conflict events, particularly between 1 January 2021 to 30 June 2024.\nüîó Source: ACLED\nüìÅ Format: comma separated values (CSV)\nAs the dataset is rather extensive, I will be performing my analysis on armed conflict events in a quarterly basis to streamline my tasks. The data included in this dataset are as follows:\n\n\n\nEvent Type\nACLED categorises events into various types. I will mainly be focusing on these four event types: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\n\nevent_id_cnty: unique ID for each conflict\nevent_type: category of event e.g.¬†Battle, Violence Against Civilians, Protests, Explosions/Remote Violence, Strategic Developments\nsub_event_type: a more detailed classification within event type\ndisorder_type: classifies the event based on the nature of the disorder e.g.¬†political violence, demonstrations, strategic developments[A1]\ncivilian_targeting: yes/no value, whether event involves specifically targeting civilians\nNote: when ‚Äústrategic developments‚Äù are used in Event Type, it is also used in the disorder type (vice-versa)\n\n\n\nLocation and Geospatial Data\nThe database provides detailed geographic information, pinpointing the exact or approximate locations of conflict events across Myanmar. This includes cities, towns, and rural areas.\n\niso: the country code for Myanmar which uses 104 in this case\nregion: region of conflict within Myanmar\ncountry: indicates Myanmar\nadmin1, admin2, admin3: 1st, 2nd and 3rd level administration division within Myanmar e.g.¬†states, division, sub-division\nlocation: specific geographic location or name of the place where the conflict event occurred\nlatitude: latitude of the conflict event\nlongitude: longitude of the conflict event\ngeo_precision: indicates the level of precision for the geographic coordinates provided\n\n\n\nDate and Time\nACLED records the specific dates and, where possible, times of conflict events.\n\nevent_date: date of conflict\nyear: year of conflict\ntime_precision: accuracy of the date and time information provided\n\n\n\nActors\n\nIndicate the actors involved in the conflict, such as the Tatmadaw (Myanmar‚Äôs military), ethnic armed organizations, local militias, civilian protestors, and other groups.\nactor1: primary actor involved in the conflict event. E.g. a government force, rebel group, militia, or any organised entity\nassoc_actor_1: a secondary group that is aligned with or supports the primary actor (Actor1) in the event\ninter1: an interaction code that categorises actor1, could be a government force, rebel group, military force, rioter, civilian, or other entities\ninteraction:¬†combined description of actor1 and actor2 (no particular order of aggression)\n\n\n\nFatalities\n\nfatalities:¬†tracks the number of reported fatalities associated with each conflict event\n\n\n\nOthers\n\nsource: source of information for the conflict event\nsource_scale: scale of the source e.g.¬†local, national, international\nnotes : additional comments\ntags: keywords associated with the conflict event\ntimestamp: date and time when conflict event was entered/updated in the database\n\n\n\n\n\n\n\nI will also be using a geospatial dataset from the Myanmar Information Management Unit (MIMU) in shapefile (.shp) format, specifically of the Myanmar state at the 2nd administrative level with district boundaries.\nüîó Source: MIMU\nüìÅ Format: shapefile (.shp)\nMy reason for choosing the district boundary dataset is that we do not want to select a boundary dataset that is too generalised when analysing conflict events since it might not provide sufficient insights to trends where conflict events happen. Neither do we want to analyse a geography that is too detailed (e.g.¬†Admin 3 - townships) since it can be computationally inefficient as seen in the types of boundary data below.\n\n\n\n\n\nI have donwloaded the two data sets and organised them into my folder as follows."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#lets-set-up",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#lets-set-up",
    "title": "Take-home Exercise 1 - Part 1",
    "section": "2. Let‚Äôs Set Up!",
    "text": "2. Let‚Äôs Set Up!\n\n2.1 Importing Libraries into R\nTo carry out this exercise, I will be using the following R packages:\n\nsf: a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat: has a wide range of useful functions for point pattern analysis. In this take-home exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster: reads, writes, manipulates, analyses and model of gridded spatial data (i.e.¬†raster). In this take-home exercise, it will be used to convert image output generate by spatstat into raster format.\ntmap / sparr: provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\ntidyverse / dplyr: for transforming and organising data for analysis\nmagick: used for plotting animated map plots into GIFs for the spatio-temporal point pattern analysis\n\nNow, let‚Äôs install and load these packages in RStudio.\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, tidyverse, magick, dplyr)\n\n\n\n2.2 Importing Data Sets into R\n\n1) Armed Conflicts Data\nNext, I will import the downloaded armed conflict data. For aspatial datasets like this, we will import into Rstudio using read_csv() function of the readr package.\n\n# Import armed conflict data\nconflict_data &lt;- read_csv(\"data/aspatial/2021-01-01-2024-06-30-Myanmar.csv\")\n\nRows: 87746 Columns: 28\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (18): event_id_cnty, event_date, disorder_type, event_type, sub_event_ty...\ndbl (10): year, time_precision, inter1, interaction, iso, latitude, longitud...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe 2021-01-01-2024-06-30-Myanmar.csv dataset contains 87746 rows and 28 columns which indicates the presence of 87746 unique armed conflict events in Myanmar.\n\n\nAfter importing the dataset, we can inspect the dataset using the glimpse() function.\n\n# Inspect the conflict data\nglimpse(conflict_data)\n\nRows: 87,746\nColumns: 28\n$ event_id_cnty      &lt;chr&gt; \"MMR64313\", \"MMR64313\", \"MMR64320\", \"MMR64320\", \"MM‚Ä¶\n$ event_date         &lt;chr&gt; \"30 June 2024\", \"30 June 2024\", \"30 June 2024\", \"30‚Ä¶\n$ year               &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 202‚Ä¶\n$ time_precision     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ disorder_type      &lt;chr&gt; \"Political violence\", \"Political violence\", \"Politi‚Ä¶\n$ event_type         &lt;chr&gt; \"Battles\", \"Battles\", \"Battles\", \"Battles\", \"Battle‚Ä¶\n$ sub_event_type     &lt;chr&gt; \"Armed clash\", \"Armed clash\", \"Armed clash\", \"Armed‚Ä¶\n$ actor1             &lt;chr&gt; \"People's Defense Force - Mandalay\", \"Military Forc‚Ä¶\n$ assoc_actor_1      &lt;chr&gt; \"MDA - AGF: Madaya - The Authentic Genes Force; SST‚Ä¶\n$ inter1             &lt;dbl&gt; 3, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 2, 1, ‚Ä¶\n$ interaction        &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 10, 13, 13, 10, 12, 12, 12,‚Ä¶\n$ civilian_targeting &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ iso                &lt;dbl&gt; 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 1‚Ä¶\n$ region             &lt;chr&gt; \"Southeast Asia\", \"Southeast Asia\", \"Southeast Asia‚Ä¶\n$ country            &lt;chr&gt; \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanma‚Ä¶\n$ admin1             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Ma‚Ä¶\n$ admin2             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Pyinoolwin\", \"Pyinoolwin\",‚Ä¶\n$ admin3             &lt;chr&gt; \"Patheingyi\", \"Patheingyi\", \"Singu\", \"Singu\", \"Thab‚Ä¶\n$ location           &lt;chr&gt; \"Aung Tha Pyay\", \"Aung Tha Pyay\", \"Pin Lel Gyi\", \"P‚Ä¶\n$ latitude           &lt;dbl&gt; 22.1504, 22.1504, 22.5752, 22.5752, 22.8800, 22.880‚Ä¶\n$ longitude          &lt;dbl&gt; 96.2364, 96.2364, 96.0661, 96.0661, 95.9700, 95.970‚Ä¶\n$ geo_precision      &lt;dbl&gt; 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, ‚Ä¶\n$ source             &lt;chr&gt; \"Democratic Voice of Burma; Irrawaddy\", \"Democratic‚Ä¶\n$ source_scale       &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Na‚Ä¶\n$ notes              &lt;chr&gt; \"On 30 June 2024, near Aung Tha Pyay village (Pathe‚Ä¶\n$ fatalities         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, ‚Ä¶\n$ tags               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ timestamp          &lt;dbl&gt; 1720552468, 1720552468, 1720552468, 1720552468, 172‚Ä¶\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe event_date field shows that it uses a character datatype instead of date - we will fix this later. Also, we can observe that thelongitude and langitude fields appear to be adopting the WGS84 geographic coordinate system since they are in the -180/180 and -90/90 range respectively.\n\n\n\n\n2) Myanmar Boundary Data\n\n\n\n\n\n\nObservations\n\n\n\nWhen working with Myanmar‚Äôs boundary, we need to assign the appropriate coordinate reference system. However, since Myanmar is split into two UTM - West Myanmar (crs: 32646) and East Myanmar (crs: 32647).\n\n\nHence, I will also import the administrative boundary data into a simple features tibble data.frame using¬†st_read()¬†of the sf package and check the number of rows returned for both CRS 32646 and 32647. This function reads the shapefile data and returns an¬†sf¬†object that can be used for further analysis.\n\n\nFind out conflicts count by CRS\nconflict_crs &lt;- st_as_sf(conflict_data, coords = c(\"longitude\", \"latitude\"), crs = 4326) \n\n# Count number of conflicts for CRS 32646\nconflict_data_32646 &lt;- st_transform(conflict_crs, crs = 32646)\ncount_32646 &lt;- nrow(conflict_data_32646)\n# Count number of conflicts for CRS 32647\nconflict_data_32647 &lt;- st_transform(conflict_crs, crs = 32647)\ncount_32647 &lt;- nrow(conflict_data_32647)\n\ncrs_counts &lt;- data.frame(\n  CRS = c(\"EPSG: 32646\", \"EPSG: 32647\"),\n  Conflicts_Count = c(count_32646, count_32647)\n)\n\nprint(crs_counts)\n\n\n          CRS Conflicts_Count\n1 EPSG: 32646           87746\n2 EPSG: 32647           87746\n\n\nSince there is no difference in the count, I will decide to focus on UTM zone 47N (EPSG:32647), east of Myanmar, for the purpose of this exercise. The st_transform() function below converts the CRS of the sf object to EPSG:32647.\n\n# Import boundary data\nboundary_sf &lt;- st_read(dsn = \"data/geospatial\",layer = \"mmr_polbnda_adm2_250k_mimu\") %&gt;% st_transform(crs = 32647)\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Take-home_Ex\\Take-home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nIn the code below, we can notice that the ESPG code has been updated to 32647.\n\n# Check for changes\nst_crs(boundary_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96¬∞E and 102¬∞E, northern hemisphere between equator and 84¬∞N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nHere, I will use the¬†plot()¬†function which plots the geometry of the¬†sf¬†object. The¬†st_geometry()¬†function is used to extract the geometry of the¬†mpsz_sf¬†object which includes the districts of Myanmar as shown below.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(boundary_sf))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-wrangling",
    "title": "Take-home Exercise 1 - Part 1",
    "section": "3. Data Wrangling",
    "text": "3. Data Wrangling\n\n3.1 Fixing Incorrect Datatypes\nRecall that the earlier inspection of the conflict_data tibble data frame revealed that the datatype indicated for event date is wrongly labelled as a character instead of a date format.\nAs such, let‚Äôs convert the datatype to the correct ‚Äòdate‚Äô format as shown below.\n\n# Convert the datatype for event_date\nconflict_data$event_date &lt;- as.Date(conflict_data$event_date, format = \"%d %B %Y\")\n\n# Check for changes\nhead(conflict_data)\n\n# A tibble: 6 √ó 28\n  event_id_cnty event_date  year time_precision disorder_type      event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence Battles   \n2 MMR64313      2024-06-30  2024              1 Political violence Battles   \n3 MMR64320      2024-06-30  2024              1 Political violence Battles   \n4 MMR64320      2024-06-30  2024              1 Political violence Battles   \n5 MMR64321      2024-06-30  2024              1 Political violence Battles   \n6 MMR64321      2024-06-30  2024              1 Political violence Battles   \n# ‚Ñπ 22 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;\n\n\n\n\n3.2 Adding new year_quarter column\nWe will want to create a new column to indicate the specific year and quarter for each conflict event since the spatial analysis will be done later in a quarterly manner.\n\n\nExtract year and quarter\nconflict_data$year_quarter &lt;- paste0(\n  year(conflict_data$event_date), \n  \" Q\", \n  quarter(conflict_data$event_date)\n)\n\n# View the new data column\nunique(conflict_data$year_quarter)\n\n\n [1] \"2024 Q2\" \"2024 Q1\" \"2023 Q4\" \"2023 Q3\" \"2023 Q2\" \"2023 Q1\" \"2022 Q4\"\n [8] \"2022 Q3\" \"2022 Q2\" \"2022 Q1\" \"2021 Q4\" \"2021 Q3\" \"2021 Q2\" \"2021 Q1\"\n\n\n\n\n3.3 Fixing Duplicated Event ID in conflict_data Dataframe\nAs shown, there are presence of duplicates in our dataframe returned by the duplicated() function.\n\n# Check for duplicates\nany(duplicated(conflict_data))\n\n[1] TRUE\n\n\nBased on the duplicated event ID: MMR64313 for instance. We can observe the two records are of the same political violence event happening between two actors on 30/6/2024, between the People‚Äôs Defense Force and Military Forces of Myanmar. Upon further research, these two actors are opposing political parties of Myanmar‚Äôs ongoing conflict.\n\n# Inspect an instance of the duplciated event IDs\nhead(conflict_data,2)\n\n# A tibble: 2 √ó 29\n  event_id_cnty event_date  year time_precision disorder_type      event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence Battles   \n2 MMR64313      2024-06-30  2024              1 Political violence Battles   \n# ‚Ñπ 23 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;, year_quarter &lt;chr&gt;\n\n\n\n\n\n\n\n\nReflection\n\n\n\nShould duplicated data be removed in this analysis?\nA single event (e.g.¬†MMR64313) can have duplicated rows with different actor1 values, typically due to counterattacks from opposing sides, leading to different data entries into the conflict_data dataset.\nHence, I will remove duplicated events found in the conflict_data dataframe as long as the rows have the same event ID indicated.\n\n\nHere, I did another check to ensure there is not more than 2 possible repeated event IDs in the first 20 rows of conflict_data.\n\n\nCheck duplicated events for first 20 rows\nduplicate_counts_first_20 &lt;- conflict_data %&gt;%\n  slice(1:20) %&gt;%            \n  group_by(event_id_cnty) %&gt;% \n  summarize(count = n()) %&gt;%  \n  filter(count &gt; 1)         \n\n# View the result\nprint(duplicate_counts_first_20)\n\n\n# A tibble: 9 √ó 2\n  event_id_cnty count\n  &lt;chr&gt;         &lt;int&gt;\n1 MMR64313          2\n2 MMR64320          2\n3 MMR64321          2\n4 MMR64323          2\n5 MMR64325          2\n6 MMR64326          2\n7 MMR64328          2\n8 MMR64330          2\n9 MMR64331          2\n\n\nWith that checked, I‚Äôll remove the duplicated rows with a repeated Event ID but will retain the actor1 value and append it to a new actor2 column, assuming two actors are involved in every conflict.\n\n\nRemove duplicated rows\n# Load necessary library\nlibrary(dplyr)\n\n# Retrieve data of duplicated rows and summarize actor2, assoc_actor_2, and inter2\nmerged_duplicates &lt;- conflict_data %&gt;%\n  filter(duplicated(event_id_cnty) | duplicated(event_id_cnty, fromLast = TRUE)) %&gt;%\n  arrange(event_id_cnty) %&gt;%\n  group_by(event_id_cnty) %&gt;%\n  summarize(\n    actor2 = last(actor1)\n  )\n\n# Keep rows without duplicates\nconflict_data_no_duplicates &lt;- conflict_data %&gt;%\n  filter(!duplicated(event_id_cnty))\n\n# Update conflict_data dataframe with new columns from merged_duplicates\nconflict_data &lt;- conflict_data_no_duplicates %&gt;%\n  left_join(merged_duplicates, by = \"event_id_cnty\") %&gt;%\n  mutate(\n    # Ensure actor2 exists after the join, if not fill with actor1\n    actor2 = coalesce(actor2, actor1)\n  )\n\n# View dataframe\nhead(conflict_data[c('event_id_cnty','actor1','actor2')])\n\n\n# A tibble: 6 √ó 3\n  event_id_cnty actor1                                           actor2         \n  &lt;chr&gt;         &lt;chr&gt;                                            &lt;chr&gt;          \n1 MMR64313      People's Defense Force - Mandalay                Military Force‚Ä¶\n2 MMR64320      People's Defense Force - Mandalay                Military Force‚Ä¶\n3 MMR64321      People's Defense Force - Mandalay                Military Force‚Ä¶\n4 MMR64322      Military Forces of Myanmar (2021-)               Military Force‚Ä¶\n5 MMR64323      PKDF MMU: People's Knight Defense Force - Myinmu Military Force‚Ä¶\n6 MMR64324      Military Forces of Myanmar (2021-)               Military Force‚Ä¶\n\n\nWe can observe that there are no longer any duplicated event IDs in our conflict_data data frame.\n\nany(duplicated(conflict_data))\n\n[1] FALSE\n\n\n\n\n3.4 Converting Aspatial Data to Simple Feature Format\nFor the purpose of this exercise, we will want to integrate and analyse aspatial data in a geographic context. I‚Äôll do a check if conflict_data needs to be converted to a sf data frame - if it outputs anything else but sf, then it‚Äôs not a simple feature data frame!\n\nclass(conflict_data)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can see that conflict_data is not a sf data frame. Since a non-simple feature data frame does not have a ‚Äúgeometry‚Äù column, we‚Äôll need to convert conflict_data into a simple feature data frame\n\n\nWe can convert conflict_data into a simple feature data frame by using¬†st_as_sf()¬†from the¬†sf¬†package. Addiitionally, we will also need to transform coordinate system from geographic (ESPG: 4326) to projected (ESPG: 32647) using st_transform().\n\n# Convert to simple feature format\nconflict_data_sf &lt;- st_as_sf(conflict_data, coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;% st_transform(crs = 32647)\n\n# Inspect the changes\nglimpse(conflict_data_sf)\n\nRows: 51,553\nColumns: 29\n$ event_id_cnty      &lt;chr&gt; \"MMR64313\", \"MMR64320\", \"MMR64321\", \"MMR64322\", \"MM‚Ä¶\n$ event_date         &lt;date&gt; 2024-06-30, 2024-06-30, 2024-06-30, 2024-06-30, 20‚Ä¶\n$ year               &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 202‚Ä¶\n$ time_precision     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ disorder_type      &lt;chr&gt; \"Political violence\", \"Political violence\", \"Politi‚Ä¶\n$ event_type         &lt;chr&gt; \"Battles\", \"Battles\", \"Battles\", \"Strategic develop‚Ä¶\n$ sub_event_type     &lt;chr&gt; \"Armed clash\", \"Armed clash\", \"Armed clash\", \"Chang‚Ä¶\n$ actor1             &lt;chr&gt; \"People's Defense Force - Mandalay\", \"People's Defe‚Ä¶\n$ assoc_actor_1      &lt;chr&gt; \"MDA - AGF: Madaya - The Authentic Genes Force; SST‚Ä¶\n$ inter1             &lt;dbl&gt; 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 3, 3, 3, 7, 1, ‚Ä¶\n$ interaction        &lt;dbl&gt; 13, 13, 13, 10, 13, 10, 12, 12, 12, 12, 12, 13, 13,‚Ä¶\n$ civilian_targeting &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ iso                &lt;dbl&gt; 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 1‚Ä¶\n$ region             &lt;chr&gt; \"Southeast Asia\", \"Southeast Asia\", \"Southeast Asia‚Ä¶\n$ country            &lt;chr&gt; \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanma‚Ä¶\n$ admin1             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Sagaing\", \"Sag‚Ä¶\n$ admin2             &lt;chr&gt; \"Mandalay\", \"Pyinoolwin\", \"Pyinoolwin\", \"Shwebo\", \"‚Ä¶\n$ admin3             &lt;chr&gt; \"Patheingyi\", \"Singu\", \"Thabeikkyin\", \"Khin-U\", \"My‚Ä¶\n$ location           &lt;chr&gt; \"Aung Tha Pyay\", \"Pin Lel Gyi\", \"Thabeikkyin\", \"Khi‚Ä¶\n$ geo_precision      &lt;dbl&gt; 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, ‚Ä¶\n$ source             &lt;chr&gt; \"Democratic Voice of Burma; Irrawaddy\", \"Irrawaddy\"‚Ä¶\n$ source_scale       &lt;chr&gt; \"National\", \"National\", \"National\", \"Subnational-Na‚Ä¶\n$ notes              &lt;chr&gt; \"On 30 June 2024, near Aung Tha Pyay village (Pathe‚Ä¶\n$ fatalities         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ tags               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ timestamp          &lt;dbl&gt; 1720552468, 1720552468, 1720552468, 1720552468, 172‚Ä¶\n$ year_quarter       &lt;chr&gt; \"2024 Q2\", \"2024 Q2\", \"2024 Q2\", \"2024 Q2\", \"2024 Q‚Ä¶\n$ actor2             &lt;chr&gt; \"Military Forces of Myanmar (2021-)\", \"Military For‚Ä¶\n$ geometry           &lt;POINT [m]&gt; POINT (214961 2452068), POINT (198303.2 24994‚Ä¶\n\n\n\n\n\n\n\n\nObservations\n\n\n\nNotice that a new column called¬†geometry¬†has been added into the data frame. On the other hand, the¬†longitude¬†and¬†latitude¬†columns have been removed from the data frame.\n\n\nWe can further inspect the newly created ‚Äògeometry‚Äô column of conflict_data_sf\n\n# Retrieve geometry column\nst_geometry(conflict_data_sf)\n\nGeometry set for 51553 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -208804.4 ymin: 1103500 xmax: 640934.5 ymax: 3042960\nProjected CRS: WGS 84 / UTM zone 47N\nFirst 5 geometries:\n\n\nPOINT (214961 2452068)\n\n\nPOINT (198303.2 2499463)\n\n\nPOINT (189105.4 2533434)\n\n\nPOINT (160913.9 2522331)\n\n\nPOINT (146213 2428487)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt consists of 51,533 features consisting of point geometric features where the underlying datum is in WGS 84 format.\n\n\nTo ensure that the coordinate system is correctly updated, we can use the st_crs() function where we observe that the ESPG code is correctly indicated as 32647.\n\n# Check CRS format\nst_crs(conflict_data_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96¬∞E and 102¬∞E, northern hemisphere between equator and 84¬∞N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\n3.5 Reduce Data File Size\nIn this section, I will reduce the current Myanmar armed conflict dataset as the time taken for computing the kernel density estimates can take up to 30 minutes long which is not computationally efficient.\n\n1) Remove ‚ÄòProtests‚Äô and ‚ÄòRiots‚Äô Event Types\nI will remove rows in the conflicts_data_sf dataset that don‚Äôt focus on the four main event types (Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians), as mentioned in the exercise brief.\n\nconflict_data_sf &lt;- conflict_data_sf %&gt;%\n  filter(!(event_type %in% c(\"Protests\", \"Riots\")))\n\nunique(conflict_data_sf$event_type)\n\n[1] \"Battles\"                    \"Strategic developments\"    \n[3] \"Violence against civilians\" \"Explosions/Remote violence\"\n\n\n\n\n2) Remove unused columns in boundary_sf\nAs seen, there are 8 columns in the simple feature data frame of boundary_sf.\n\n# Inspect first rows of data in boundary_sf\nhead(boundary_sf)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -14915.04 ymin: 1736124 xmax: 187961.7 ymax: 2051144\nProjected CRS: WGS 84 / UTM zone 47N\n  OBJECTID         ST ST_PCODE        DT   DT_PCODE      DT_MMR PCode_V\n1        1 Ayeyarwady   MMR017  Hinthada MMR017D002    ·Äü·ÄÑ·Ä∫·Äπ·Äû·Ä¨·Äê·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n2        2 Ayeyarwady   MMR017   Labutta MMR017D004    ·Äú·Äï·ÄΩ·Äê·Äπ·Äê·Ä¨·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n3        3 Ayeyarwady   MMR017    Maubin MMR017D005     ·Äô·Ä°·Ä∞·Äï·ÄÑ·Ä∫·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n4        4 Ayeyarwady   MMR017 Myaungmya MMR017D003 ·Äô·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äº·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n5        5 Ayeyarwady   MMR017   Pathein MMR017D001      ·Äï·ÄØ·Äû·Ä≠·Äô·Ä∫·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n6        6 Ayeyarwady   MMR017    Pyapon MMR017D006     ·Äñ·Äª·Ä¨·Äï·ÄØ·Ä∂·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n                        geometry\n1 MULTIPOLYGON (((90859.89 20...\n2 MULTIPOLYGON (((75991.51 17...\n3 MULTIPOLYGON (((115559 1928...\n4 MULTIPOLYGON (((39919.39 18...\n5 MULTIPOLYGON (((-6302.348 1...\n6 MULTIPOLYGON (((93411.72 17...\n\n\nI will remove ‚ÄôDT_MMR‚Äù column as we already have the District Name in English in DT and won‚Äôt require the district names in Myanmar Language. Next, we will remove the coded versions of ST (state/region) and DT (district) columns, namely ST_PCODE and DT_PCODE. Additionally, we won‚Äôt need the PCode_V column since we will be dropping the PCODE column too.\n\nboundary_sf &lt;- boundary_sf %&gt;% dplyr::select('OBJECTID', 'ST', 'DT','geometry')\nsummary(boundary_sf)\n\n    OBJECTID          ST                 DT                     geometry \n Min.   : 1.00   Length:80          Length:80          MULTIPOLYGON :80  \n 1st Qu.:20.75   Class :character   Class :character   epsg:32647   : 0  \n Median :40.50   Mode  :character   Mode  :character   +proj=utm ...: 0  \n Mean   :40.50                                                           \n 3rd Qu.:60.25                                                           \n Max.   :80.00                                                           \n\n\n\n\n3) Remove unused columns in conflict_data\nI will also remove unnecessary columns of the conflict_data data frame that won‚Äôt be used in our spatial analysis later. I‚Äôll rename admin1 to ST (states) and admin2 to DT (districts) for easier reference.\n\n\nRemove unnecessary columns\nconflict_data_sf &lt;- conflict_data_sf %&gt;%\n  select(event_id_cnty, event_date, year_quarter, disorder_type, \n         event_type, location, admin1, admin2, geometry, actor1, actor2,\n         interaction, fatalities) %&gt;%\n  rename(\n    ST = admin1,\n    DT = admin2\n  )\n\nsummary(conflict_data_sf)\n\n\n event_id_cnty        event_date         year_quarter       disorder_type     \n Length:42608       Min.   :2021-01-01   Length:42608       Length:42608      \n Class :character   1st Qu.:2022-01-10   Class :character   Class :character  \n Mode  :character   Median :2022-10-13   Mode  :character   Mode  :character  \n                    Mean   :2022-10-29                                        \n                    3rd Qu.:2023-08-29                                        \n                    Max.   :2024-06-30                                        \n  event_type          location              ST                 DT           \n Length:42608       Length:42608       Length:42608       Length:42608      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n          geometry        actor1             actor2           interaction   \n POINT        :42608   Length:42608       Length:42608       Min.   :10.00  \n epsg:32647   :    0   Class :character   Class :character   1st Qu.:13.00  \n +proj=utm ...:    0   Mode  :character   Mode  :character   Median :17.00  \n                                                             Mean   :18.86  \n                                                             3rd Qu.:17.00  \n                                                             Max.   :80.00  \n   fatalities    \n Min.   :  0.00  \n 1st Qu.:  0.00  \n Median :  0.00  \n Mean   :  1.27  \n 3rd Qu.:  1.00  \n Max.   :201.00  \n\n\n\n\n\n3.6 Converting Simple Features Data Frame into ppp Object\nIt is important that we convert conflict_data_sf (a simple feature data frame) into a planer point pattern (ppp) object format, since the spatstat package that we‚Äôll be using for the Spatial Point Pattern Analysis later is specifically designed for working with ppp-formated data. Additionally, I will begin with categorising the ppp objects into their unique year_quarter category.\n\n\nCreate ppp objects based on year_quarter category\n# Create an empty list to store the ppp objects\nppp_list &lt;- list()\n\n# Loop through each unique year_quarter category\nfor (yq in unique(conflict_data_sf$year_quarter)) {\n  # Subset the data for the current year_quarter\n  subset_data_sf &lt;- conflict_data_sf %&gt;% filter(year_quarter == yq)\n  \n  # Convert the subset to a ppp object\n  subset_ppp &lt;- as.ppp(subset_data_sf$geometry)\n  \n  # Add the ppp object to the list\n  ppp_list[[yq]] &lt;- subset_ppp\n}\n\n# Check list\nppp_list\n\n\n$`2024 Q2`\nPlanar point pattern: 2788 points\nwindow: rectangle = [-208804.4, 597543.7] x [1103500.1, 3026504.9] units\n\n$`2024 Q1`\nPlanar point pattern: 3186 points\nwindow: rectangle = [-207135, 591875.9] x [1245380, 3026504.9] units\n\n$`2023 Q4`\nPlanar point pattern: 3627 points\nwindow: rectangle = [-206931.7, 604775.1] x [1103500.1, 3020772.2] units\n\n$`2023 Q3`\nPlanar point pattern: 3010 points\nwindow: rectangle = [-197883.4, 518300.4] x [1103500.1, 3027041.8] units\n\n$`2023 Q2`\nPlanar point pattern: 2745 points\nwindow: rectangle = [-191261.5, 518300.4] x [1103500.1, 3006372.9] units\n\n$`2023 Q1`\nPlanar point pattern: 3101 points\nwindow: rectangle = [-199243.8, 591875.9] x [1103500.1, 3026504.9] units\n\n$`2022 Q4`\nPlanar point pattern: 3296 points\nwindow: rectangle = [-206531.5, 518300.4] x [1103500.1, 2931517.1] units\n\n$`2022 Q3`\nPlanar point pattern: 3486 points\nwindow: rectangle = [-206196.6, 568361.5] x [1103500.1, 3026504.9] units\n\n$`2022 Q2`\nPlanar point pattern: 3580 points\nwindow: rectangle = [-206931.7, 640934.5] x [1103500.1, 3026504.9] units\n\n$`2022 Q1`\nPlanar point pattern: 3563 points\nwindow: rectangle = [-204784, 591875.9] x [1103500.1, 3026504.9] units\n\n$`2021 Q4`\nPlanar point pattern: 3844 points\nwindow: rectangle = [-200024.3, 591875.9] x [1103500.1, 3042960.3] units\n\n$`2021 Q3`\nPlanar point pattern: 2754 points\nwindow: rectangle = [-193181.1, 591875.9] x [1103500.1, 3042960.3] units\n\n$`2021 Q2`\nPlanar point pattern: 2916 points\nwindow: rectangle = [-191409.1, 640934.5] x [1132472.1, 3042960.3] units\n\n$`2021 Q1`\nPlanar point pattern: 712 points\nwindow: rectangle = [-203795.3, 591875.9] x [1375186.1, 3026504.9] units\n\n\nWe can visualise the spread of conflict events across each quarter from January 2021 to June 2024 using the plot() function as shown below.\n\n\nVisualise the spread of conflicts by year_quarter\n# Ensure 'year_quarter' is a factor\nconflict_data_sf$year_quarter &lt;- as.factor(conflict_data_sf$year_quarter)\n\n# Loop through each unique year_quarter and create separate plots\nyear_quarters &lt;- unique(conflict_data_sf$year_quarter)\n\n# Set up a grid layout for multiple plots (adjust 'mfrow' as needed)\npar(mfrow = c(4,4))\npar(mar = c(0,0,1,0))\n\n# Loop through each year_quarter and plot\nfor (yq in year_quarters) {\n  subset_data_sf &lt;- conflict_data_sf[conflict_data_sf$year_quarter == yq, ]\n  conflict_data_ppp &lt;- as.ppp(subset_data_sf$geometry)\n  \n  # Plot each subset ppp object\n  plot(conflict_data_ppp, main = paste(yq))\n}\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt is noticeable that there conflict events have occured more frequently since 2021 as points plotted on the graph have gotten darker across 2021 to 2024. We can also observe the possibility of duplicated events occurring from the darker spots in the plot, in which it appears more intense in Myanmar‚Äôs central and west regions.\n\n\n\n\n3.7 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area, that is Myanmar‚Äôs boundary in this case. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to convert the boundary_data_sf simple feature data frame into an owin object of spatstat.\n\n# Convert to owin object\nmyanmar_owin &lt;- as.owin(boundary_sf)\n\n# Visualise the owin object\nplot(myanmar_owin)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom my observations, the as.owin() function converts the boundary_data_sf spatial boundary into a window object that represents the outer boundary of the spatial region and does not handle internal structures or districts we previously saw from the plot of boundary_data_sf.\n\n\nWe can also take a quick look at the owin object properties as shown. I will be converting it to a data frame for the purposes of getting a quick glimpse of the object.\n\n# Summary info of owin object\nowin_df &lt;- as.data.frame(myanmar_owin)\nprint(head(owin_df))\n\n         x       y id sign\n1 56519.39 2741919  1   -1\n2 56917.28 2741947  1   -1\n3 57000.15 2741973  1   -1\n4 57068.51 2741994  1   -1\n5 57221.44 2742142  1   -1\n6 57068.51 2741994  1   -1\n\n\n\n\n3.8 Combining ppp Object and owin Object\nIn this last step of geospatial data wrangling, I will mask all ppp object with the owin object I created earlier to put in place all conflict events within the boundary of Myanmar. Doing so can also optimise the memory usage for large datasets. I‚Äôll also make the density values more comprehensible by re-scaling the density values from metres to kilometres using¬†rescale().\n\nCombine ppp and owin objectCheck results of (E.g. 2024 Q2)\n\n\n\n# Initialize an empty list to store masked ppp objects\nmasked_ppp_list_km &lt;- list()\n\n# Iterate over each ppp object in the list\nfor (quarter in names(ppp_list)) {\n  ppp_obj &lt;- ppp_list[[quarter]]\n  # Mask the ppp object with the owin object\n  masked_ppp &lt;- ppp_obj[myanmar_owin]\n  masked_ppp_km &lt;- rescale(masked_ppp, 1000, \"km\")\n  # Store the masked ppp object in the new list\n  masked_ppp_list_km[[quarter]] &lt;- masked_ppp_km\n}\n\n\n\n\n# Inspect 2024 Q2 masked ppp object\nsummary(masked_ppp_list_km$`2024 Q2`)\n\nPlanar point pattern:  2788 points\nAverage intensity 0.004162974 points per square km\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 16 decimal places\n\nWindow: polygonal boundary\n1077 separate polygons (510 holes)\n                    vertices         area relative.area\npolygon 1 (hole)           9 -4.40155e-08     -6.57e-14\npolygon 2 (hole)           3 -4.82818e-08     -7.21e-14\npolygon 3 (hole)           3 -2.78345e-09     -4.16e-15\npolygon 4 (hole)           3 -4.99415e-09     -7.46e-15\npolygon 5 (hole)           3 -6.70916e-10     -1.00e-15\npolygon 6 (hole)           3 -4.99758e-09     -7.46e-15\npolygon 7 (hole)           6 -3.24429e-09     -4.84e-15\npolygon 8 (hole)          11 -5.35507e-08     -8.00e-14\npolygon 9 (hole)           3 -3.06875e-09     -4.58e-15\npolygon 10 (hole)          3 -7.63672e-09     -1.14e-14\npolygon 11 (hole)          3 -8.61828e-11     -1.29e-16\npolygon 12 (hole)          3 -2.44104e-08     -3.64e-14\npolygon 13 (hole)          3 -4.88141e-11     -7.29e-17\npolygon 14 (hole)          4 -1.74466e-10     -2.61e-16\npolygon 15 (hole)          4 -2.30890e-11     -3.45e-17\npolygon 16 (hole)          4 -6.25346e-14     -9.34e-20\npolygon 17 (hole)          4 -3.68040e-08     -5.50e-14\npolygon 18 (hole)          3 -1.27228e-12     -1.90e-18\npolygon 19 (hole)          4 -1.25504e-09     -1.87e-15\npolygon 20 (hole)          3 -1.56980e-08     -2.34e-14\npolygon 21 (hole)          4 -1.13609e-12     -1.70e-18\npolygon 22 (hole)          4 -8.02868e-08     -1.20e-13\npolygon 23 (hole)          4 -3.98057e-11     -5.94e-17\npolygon 24 (hole)          4 -2.89739e-08     -4.33e-14\npolygon 25 (hole)          5 -7.39370e-08     -1.10e-13\npolygon 26 (hole)          6 -1.08246e-07     -1.62e-13\npolygon 27 (hole)         13 -3.27066e-07     -4.88e-13\npolygon 28 (hole)          3 -3.55050e-09     -5.30e-15\npolygon 29 (hole)          3 -2.84064e-08     -4.24e-14\npolygon 30 (hole)          3 -8.07641e-10     -1.21e-15\npolygon 31 (hole)          4 -5.20948e-09     -7.78e-15\npolygon 32 (hole)          4 -3.21399e-10     -4.80e-16\npolygon 33 (hole)          4 -2.54955e-11     -3.81e-17\npolygon 34 (hole)          3 -4.29638e-10     -6.42e-16\npolygon 35 (hole)          6 -6.02383e-10     -8.99e-16\npolygon 36 (hole)          4 -4.91852e-10     -7.34e-16\npolygon 37                26  2.85778e+00      4.27e-06\npolygon 38 (hole)          3 -6.80850e-13     -1.02e-18\npolygon 39 (hole)          4 -3.09085e-13     -4.62e-19\npolygon 40 (hole)          6 -3.61049e-12     -5.39e-18\npolygon 41 (hole)          7 -5.33310e-12     -7.96e-18\npolygon 42 (hole)          4 -3.62663e-12     -5.42e-18\npolygon 43 (hole)          3 -2.00302e-14     -2.99e-20\npolygon 44 (hole)          3 -2.85255e-12     -4.26e-18\npolygon 45 (hole)          4 -1.15535e-12     -1.73e-18\npolygon 46 (hole)         17 -6.25657e-13     -9.34e-19\npolygon 47 (hole)         20 -1.53263e-11     -2.29e-17\npolygon 48 (hole)          3 -1.42406e-14     -2.13e-20\npolygon 49                 3  0.00000e+00      0.00e+00\npolygon 50 (hole)          4 -2.46048e-12     -3.67e-18\npolygon 51 (hole)          4 -2.93672e-13     -4.39e-19\npolygon 52 (hole)          3 -1.35167e-16     -2.02e-22\npolygon 53 (hole)          3 -3.66243e-13     -5.47e-19\npolygon 54 (hole)         13 -3.22774e-12     -4.82e-18\npolygon 55 (hole)          7 -3.79713e-12     -5.67e-18\npolygon 56 (hole)          4 -6.06083e-13     -9.05e-19\npolygon 57 (hole)          6 -5.94835e-12     -8.88e-18\npolygon 58 (hole)          4 -4.11305e-13     -6.14e-19\npolygon 59 (hole)         12 -7.16981e-12     -1.07e-17\npolygon 60 (hole)          4 -1.96241e-14     -2.93e-20\npolygon 61 (hole)          4 -6.16304e-12     -9.20e-18\npolygon 62 (hole)          4 -2.13366e-13     -3.19e-19\npolygon 63 (hole)          4 -2.21852e-12     -3.31e-18\npolygon 64 (hole)          3 -4.22166e-15     -6.30e-21\npolygon 65 (hole)          5 -6.21282e-13     -9.28e-19\npolygon 66 (hole)          4 -5.02821e-16     -7.51e-22\npolygon 67 (hole)          4 -3.30895e-13     -4.94e-19\npolygon 68 (hole)         17 -1.19462e-11     -1.78e-17\npolygon 69 (hole)          3 -1.27084e-13     -1.90e-19\npolygon 70 (hole)          4 -1.80406e-13     -2.69e-19\npolygon 71 (hole)          8 -3.87178e-12     -5.78e-18\npolygon 72 (hole)          4 -7.14736e-13     -1.07e-18\npolygon 73 (hole)          5 -3.43923e-12     -5.14e-18\npolygon 74                 3  0.00000e+00      0.00e+00\npolygon 75 (hole)          8 -2.57732e-12     -3.85e-18\npolygon 76 (hole)          4 -1.36629e-12     -2.04e-18\npolygon 77 (hole)         13 -1.46870e-11     -2.19e-17\npolygon 78 (hole)          3 -3.08778e-14     -4.61e-20\npolygon 79 (hole)         18 -1.78311e-12     -2.66e-18\npolygon 80 (hole)         28 -1.67874e-11     -2.51e-17\npolygon 81 (hole)          4 -4.46113e-13     -6.66e-19\npolygon 82 (hole)          3 -8.98160e-15     -1.34e-20\npolygon 83 (hole)          4 -1.43028e-12     -2.14e-18\npolygon 84 (hole)          4 -6.38438e-14     -9.53e-20\npolygon 85 (hole)         19 -1.89999e-11     -2.84e-17\npolygon 86 (hole)          4 -1.49124e-13     -2.23e-19\npolygon 87 (hole)          3 -3.68533e-14     -5.50e-20\npolygon 88 (hole)          4 -4.45374e-13     -6.65e-19\npolygon 89 (hole)          4 -4.83479e-13     -7.22e-19\npolygon 90 (hole)         12 -9.32407e-12     -1.39e-17\npolygon 91 (hole)          3 -8.84897e-13     -1.32e-18\npolygon 92 (hole)         12 -8.41711e-12     -1.26e-17\npolygon 93 (hole)          3 -3.73405e-14     -5.58e-20\npolygon 94 (hole)          3 -1.40773e-12     -2.10e-18\npolygon 95 (hole)          4 -7.84182e-13     -1.17e-18\npolygon 96 (hole)          5 -4.10716e-13     -6.13e-19\npolygon 97 (hole)         10 -5.89467e-12     -8.80e-18\npolygon 98 (hole)          3 -3.54370e-15     -5.29e-21\npolygon 99 (hole)         11 -6.90903e-12     -1.03e-17\npolygon 100 (hole)         3 -1.89866e-13     -2.84e-19\npolygon 101 (hole)         3 -3.09480e-15     -4.62e-21\npolygon 102 (hole)         3 -3.95637e-17     -5.91e-23\npolygon 103 (hole)         4 -1.88731e-16     -2.82e-22\npolygon 104 (hole)         3 -5.93215e-17     -8.86e-23\npolygon 105 (hole)         6 -1.10389e-12     -1.65e-18\npolygon 106 (hole)         5 -1.62107e-13     -2.42e-19\npolygon 107 (hole)         4 -6.07269e-13     -9.07e-19\npolygon 108 (hole)         3 -4.17062e-14     -6.23e-20\npolygon 109 (hole)         7 -4.10998e-12     -6.14e-18\npolygon 110 (hole)         7 -2.99640e-12     -4.47e-18\npolygon 111 (hole)         4 -8.08268e-13     -1.21e-18\npolygon 112 (hole)         8 -1.72022e-12     -2.57e-18\npolygon 113 (hole)         3 -5.46853e-13     -8.17e-19\npolygon 114 (hole)         3 -9.21233e-15     -1.38e-20\npolygon 115 (hole)         3 -2.34233e-14     -3.50e-20\npolygon 116 (hole)         9 -1.27183e-12     -1.90e-18\npolygon 117 (hole)         4 -8.58065e-13     -1.28e-18\npolygon 118 (hole)         6 -4.85387e-12     -7.25e-18\npolygon 119 (hole)         6 -4.38024e-12     -6.54e-18\npolygon 120 (hole)         6 -1.18459e-12     -1.77e-18\npolygon 121 (hole)        18 -5.24919e-12     -7.84e-18\npolygon 122                3  0.00000e+00      0.00e+00\npolygon 123 (hole)        10 -3.29610e-12     -4.92e-18\npolygon 124 (hole)         4 -8.31786e-15     -1.24e-20\npolygon 125 (hole)         8 -5.92159e-12     -8.84e-18\npolygon 126 (hole)         4 -2.92352e-15     -4.37e-21\npolygon 127 (hole)         3 -2.22606e-13     -3.32e-19\npolygon 128 (hole)        16 -1.60557e-11     -2.40e-17\npolygon 129 (hole)        10 -4.89387e-12     -7.31e-18\npolygon 130 (hole)         4 -1.48133e-13     -2.21e-19\npolygon 131 (hole)         4 -2.04528e-12     -3.05e-18\npolygon 132 (hole)         4 -4.07107e-13     -6.08e-19\npolygon 133 (hole)         4 -2.13923e-12     -3.19e-18\npolygon 134 (hole)         4 -2.58593e-17     -3.86e-23\npolygon 135 (hole)         3 -8.40799e-14     -1.26e-19\npolygon 136 (hole)         4 -5.03212e-13     -7.51e-19\npolygon 137 (hole)         4 -1.10455e-12     -1.65e-18\npolygon 138 (hole)         4 -3.74743e-14     -5.60e-20\npolygon 139 (hole)         6 -1.42384e-12     -2.13e-18\npolygon 140 (hole)         4 -1.96713e-14     -2.94e-20\npolygon 141 (hole)         4 -7.29770e-13     -1.09e-18\npolygon 142 (hole)         3 -7.99677e-16     -1.19e-21\npolygon 143 (hole)         3 -1.00370e-15     -1.50e-21\npolygon 144 (hole)         3 -9.07286e-16     -1.35e-21\npolygon 145 (hole)         4 -2.63668e-14     -3.94e-20\npolygon 146 (hole)         5 -8.51993e-13     -1.27e-18\npolygon 147 (hole)         4 -3.15631e-13     -4.71e-19\npolygon 148 (hole)         4 -7.89181e-13     -1.18e-18\npolygon 149 (hole)         6 -4.24179e-13     -6.33e-19\npolygon 150 (hole)        16 -1.00987e-11     -1.51e-17\npolygon 151 (hole)         3 -6.53241e-18     -9.75e-24\npolygon 152 (hole)         6 -2.87035e-12     -4.29e-18\npolygon 153 (hole)         3 -2.64530e-15     -3.95e-21\npolygon 154 (hole)         8 -1.99962e-12     -2.99e-18\npolygon 155 (hole)         3 -2.45345e-13     -3.66e-19\npolygon 156 (hole)         3 -3.99767e-14     -5.97e-20\npolygon 157 (hole)         6 -1.08025e-12     -1.61e-18\npolygon 158 (hole)         3 -1.22938e-13     -1.84e-19\npolygon 159 (hole)         5 -3.07870e-13     -4.60e-19\npolygon 160 (hole)        12 -4.81197e-12     -7.19e-18\npolygon 161 (hole)         4 -8.16819e-14     -1.22e-19\npolygon 162 (hole)         3 -1.28714e-12     -1.92e-18\npolygon 163 (hole)         4 -1.87938e-13     -2.81e-19\npolygon 164 (hole)        11 -1.79968e-12     -2.69e-18\npolygon 165 (hole)         3 -5.93827e-15     -8.87e-21\npolygon 166 (hole)         3 -4.50189e-13     -6.72e-19\npolygon 167 (hole)         4 -4.87377e-13     -7.28e-19\npolygon 168 (hole)        13 -1.01705e-11     -1.52e-17\npolygon 169 (hole)         3 -7.37542e-13     -1.10e-18\npolygon 170 (hole)         4 -3.62179e-13     -5.41e-19\npolygon 171                3  9.13135e-19      1.36e-24\npolygon 172 (hole)         8 -1.74082e-12     -2.60e-18\npolygon 173 (hole)         3 -9.31514e-14     -1.39e-19\npolygon 174               43  7.32477e+00      1.09e-05\npolygon 175 (hole)         3 -6.79111e-14     -1.01e-19\npolygon 176 (hole)        19 -1.27551e-11     -1.90e-17\npolygon 177 (hole)         3 -7.65776e-15     -1.14e-20\npolygon 178 (hole)         9 -4.34371e-12     -6.49e-18\npolygon 179 (hole)         4 -7.09328e-15     -1.06e-20\npolygon 180 (hole)         6 -8.43021e-12     -1.26e-17\npolygon 181 (hole)        30 -5.76461e-12     -8.61e-18\npolygon 182 (hole)        12 -5.45083e-12     -8.14e-18\npolygon 183 (hole)        16 -1.90750e-11     -2.85e-17\npolygon 184 (hole)        15 -2.10315e-11     -3.14e-17\npolygon 185 (hole)        52 -3.02970e-11     -4.52e-17\npolygon 186              103  1.86991e+01      2.79e-05\npolygon 187 (hole)        10 -7.40529e-12     -1.11e-17\npolygon 188 (hole)         6 -2.03062e-12     -3.03e-18\npolygon 189 (hole)         7 -1.02421e-12     -1.53e-18\npolygon 190 (hole)         3 -1.32600e-19     -1.98e-25\npolygon 191 (hole)         3 -5.60223e-13     -8.37e-19\npolygon 192 (hole)        10 -2.37610e-12     -3.55e-18\npolygon 193 (hole)         3 -1.90831e-16     -2.85e-22\npolygon 194 (hole)         4 -1.76514e-12     -2.64e-18\npolygon 195 (hole)         3 -1.37598e-16     -2.05e-22\npolygon 196 (hole)         4 -5.28260e-13     -7.89e-19\npolygon 197               37  9.32316e+00      1.39e-05\npolygon 198 (hole)         4 -2.33117e-13     -3.48e-19\npolygon 199 (hole)         4 -6.59209e-13     -9.84e-19\npolygon 200 (hole)         4 -5.12582e-13     -7.65e-19\npolygon 201 (hole)         4 -4.70065e-13     -7.02e-19\npolygon 202 (hole)         3 -2.10303e-13     -3.14e-19\npolygon 203 (hole)         6 -1.36386e-12     -2.04e-18\npolygon 204 (hole)         4 -2.18244e-13     -3.26e-19\npolygon 205 (hole)         4 -3.72475e-13     -5.56e-19\npolygon 206 (hole)         4 -8.32022e-13     -1.24e-18\npolygon 207 (hole)         4 -1.08153e-12     -1.61e-18\npolygon 208 (hole)         3 -5.92333e-17     -8.84e-23\npolygon 209 (hole)         7 -6.03242e-12     -9.01e-18\npolygon 210 (hole)         6 -2.27756e-12     -3.40e-18\npolygon 211 (hole)         4 -1.53368e-12     -2.29e-18\npolygon 212 (hole)         3 -2.33223e-14     -3.48e-20\npolygon 213 (hole)         6 -8.61964e-12     -1.29e-17\npolygon 214              371  2.43869e+02      3.64e-04\npolygon 215 (hole)         4 -8.45587e-13     -1.26e-18\npolygon 216              297  2.84905e+02      4.25e-04\npolygon 217 (hole)         4 -3.40250e-13     -5.08e-19\npolygon 218 (hole)         6 -1.39288e-12     -2.08e-18\npolygon 219 (hole)         4 -1.27223e-13     -1.90e-19\npolygon 220 (hole)         4 -9.59107e-13     -1.43e-18\npolygon 221 (hole)         8 -1.39768e-11     -2.09e-17\npolygon 222 (hole)         3 -3.34368e-14     -4.99e-20\npolygon 223 (hole)         4 -9.58519e-13     -1.43e-18\npolygon 224 (hole)         5 -1.32345e-13     -1.98e-19\npolygon 225 (hole)         4 -5.98296e-13     -8.93e-19\npolygon 226 (hole)         7 -3.05539e-12     -4.56e-18\npolygon 227 (hole)         3 -1.80098e-15     -2.69e-21\npolygon 228 (hole)         9 -6.38785e-12     -9.54e-18\npolygon 229               33  1.68222e+01      2.51e-05\npolygon 230 (hole)         8 -1.04272e-11     -1.56e-17\npolygon 231               33  4.47665e-01      6.68e-07\npolygon 232               19  1.34593e-01      2.01e-07\npolygon 233               39  1.36327e+00      2.04e-06\npolygon 234              137  1.55547e+02      2.32e-04\npolygon 235               36  8.76479e+00      1.31e-05\npolygon 236               79  3.08116e+01      4.60e-05\npolygon 237              388  2.25271e+02      3.36e-04\npolygon 238              316  7.78512e+01      1.16e-04\npolygon 239               13  1.09564e-01      1.64e-07\npolygon 240               18  3.49727e-01      5.22e-07\npolygon 241               31  1.23017e+00      1.84e-06\npolygon 242               16  6.55537e-01      9.79e-07\npolygon 243               24  8.49487e-01      1.27e-06\npolygon 244               30  2.54436e+00      3.80e-06\npolygon 245              336  4.15806e+01      6.21e-05\npolygon 246              330  1.69190e+02      2.53e-04\npolygon 247               47  1.08035e+01      1.61e-05\npolygon 248               39  4.94369e+00      7.38e-06\npolygon 249               23  2.72438e+00      4.07e-06\npolygon 250               33  5.70263e+00      8.52e-06\npolygon 251               90  4.20329e+01      6.28e-05\npolygon 252               28  1.35341e+00      2.02e-06\npolygon 253              225  1.08816e+02      1.62e-04\npolygon 254               33  9.16670e+00      1.37e-05\npolygon 255              192  7.02655e+01      1.05e-04\npolygon 256               49  1.49245e+01      2.23e-05\npolygon 257               98  1.79076e+01      2.67e-05\npolygon 258                6  6.37552e-01      9.52e-07\npolygon 259               49  1.01233e+01      1.51e-05\npolygon 260              141  3.43053e+01      5.12e-05\npolygon 261              195  3.24345e+01      4.84e-05\npolygon 262               51  3.38313e+00      5.05e-06\npolygon 263               34  2.01400e+00      3.01e-06\npolygon 264               13  2.50435e-01      3.74e-07\npolygon 265                9  9.04824e-02      1.35e-07\npolygon 266               34  4.61794e+00      6.90e-06\npolygon 267               17  4.58200e-01      6.84e-07\npolygon 268               15  2.74776e-01      4.10e-07\npolygon 269               21  5.34978e-01      7.99e-07\npolygon 270               19  4.55347e-01      6.80e-07\npolygon 271               71  3.42557e+00      5.11e-06\npolygon 272               24  1.32420e+00      1.98e-06\npolygon 273               15  3.26247e-01      4.87e-07\npolygon 274               39  8.65790e-01      1.29e-06\npolygon 275               43  1.41627e+00      2.11e-06\npolygon 276               24  7.52068e-01      1.12e-06\npolygon 277               96  1.32101e+01      1.97e-05\npolygon 278               38  1.18003e+00      1.76e-06\npolygon 279              429  5.99087e+02      8.95e-04\npolygon 280               13  1.74105e-01      2.60e-07\npolygon 281               19  2.52336e-01      3.77e-07\npolygon 282               16  3.11495e-01      4.65e-07\npolygon 283               11  9.11047e-02      1.36e-07\npolygon 284               12  2.13470e-01      3.19e-07\npolygon 285               17  5.82663e-01      8.70e-07\npolygon 286               56  2.60440e+01      3.89e-05\npolygon 287              107  4.91389e+00      7.34e-06\npolygon 288               51  2.79076e+00      4.17e-06\npolygon 289               89  1.61156e+01      2.41e-05\npolygon 290               28  1.30499e+00      1.95e-06\npolygon 291               11  1.27616e-01      1.91e-07\npolygon 292               34  2.54199e+00      3.80e-06\npolygon 293               27  1.72476e+00      2.58e-06\npolygon 294               37  2.01882e+00      3.01e-06\npolygon 295               23  1.65571e+00      2.47e-06\npolygon 296               33  3.05816e+00      4.57e-06\npolygon 297               14  3.23153e-01      4.83e-07\npolygon 298               91  1.51209e+01      2.26e-05\npolygon 299               12  2.42901e-01      3.63e-07\npolygon 300               11  1.37889e-01      2.06e-07\npolygon 301               58  2.29751e+01      3.43e-05\npolygon 302               48  5.10265e+00      7.62e-06\npolygon 303               22  1.30706e+00      1.95e-06\npolygon 304               15  3.49480e-01      5.22e-07\npolygon 305               17  1.57570e+00      2.35e-06\npolygon 306               34  3.68725e+00      5.51e-06\npolygon 307               34  5.21904e+00      7.79e-06\npolygon 308               24  5.42734e+00      8.10e-06\npolygon 309              422  4.66497e+02      6.97e-04\npolygon 310              142  2.98767e+01      4.46e-05\npolygon 311              132  2.18707e+01      3.27e-05\npolygon 312               19  5.88230e-01      8.78e-07\npolygon 313               22  1.77611e+00      2.65e-06\npolygon 314 (hole)         4 -6.42926e-10     -9.60e-16\npolygon 315               40  4.09952e+00      6.12e-06\npolygon 316 (hole)         3 -2.52076e-09     -3.76e-15\npolygon 317 (hole)        24 -5.61620e-07     -8.39e-13\npolygon 318               28  1.47685e+00      2.21e-06\npolygon 319 (hole)         8 -8.93461e-08     -1.33e-13\npolygon 320 (hole)         8 -1.34123e-07     -2.00e-13\npolygon 321               67  9.99685e+00      1.49e-05\npolygon 322 (hole)         8 -6.36345e-08     -9.50e-14\npolygon 323 (hole)         6 -7.57978e-08     -1.13e-13\npolygon 324 (hole)         3 -4.32022e-12     -6.45e-18\npolygon 325 (hole)         4 -2.24224e-08     -3.35e-14\npolygon 326 (hole)         5 -9.35663e-08     -1.40e-13\npolygon 327 (hole)        13 -1.39251e-07     -2.08e-13\npolygon 328 (hole)         7 -1.46533e-07     -2.19e-13\npolygon 329 (hole)         6 -7.59071e-08     -1.13e-13\npolygon 330 (hole)         7 -5.15915e-09     -7.70e-15\npolygon 331 (hole)        10 -4.09465e-08     -6.11e-14\npolygon 332 (hole)        21 -1.86624e-07     -2.79e-13\npolygon 333 (hole)         3 -1.69684e-08     -2.53e-14\npolygon 334 (hole)         8 -4.08363e-08     -6.10e-14\npolygon 335 (hole)         3 -1.73631e-08     -2.59e-14\npolygon 336 (hole)         5 -1.41820e-08     -2.12e-14\npolygon 337 (hole)        10 -6.15226e-08     -9.19e-14\npolygon 338 (hole)         8 -2.90782e-08     -4.34e-14\npolygon 339 (hole)         8 -6.15579e-08     -9.19e-14\npolygon 340 (hole)         4 -7.46371e-10     -1.11e-15\npolygon 341 (hole)         4 -1.05432e-08     -1.57e-14\npolygon 342 (hole)         4 -8.25148e-09     -1.23e-14\npolygon 343 (hole)         3 -2.59991e-08     -3.88e-14\npolygon 344 (hole)         3 -9.09758e-09     -1.36e-14\npolygon 345 (hole)         3 -1.96869e-08     -2.94e-14\npolygon 346 (hole)         3 -4.01481e-11     -5.99e-17\npolygon 347 (hole)         5 -1.49421e-08     -2.23e-14\npolygon 348 (hole)         4 -9.23861e-09     -1.38e-14\npolygon 349                3  4.91147e-16      7.33e-22\npolygon 350 (hole)         7 -5.73304e-08     -8.56e-14\npolygon 351 (hole)         4 -4.26735e-08     -6.37e-14\npolygon 352 (hole)         7 -1.69748e-08     -2.53e-14\npolygon 353 (hole)         3 -1.05617e-08     -1.58e-14\npolygon 354               25  4.82266e-01      7.20e-07\npolygon 355 (hole)         6 -3.72405e-08     -5.56e-14\npolygon 356 (hole)         7 -3.08332e-08     -4.60e-14\npolygon 357 (hole)         4 -6.69857e-09     -1.00e-14\npolygon 358 (hole)         4 -1.15937e-08     -1.73e-14\npolygon 359 (hole)         4 -3.11046e-09     -4.64e-15\npolygon 360 (hole)        12 -1.45689e-07     -2.18e-13\npolygon 361 (hole)         4 -5.64800e-08     -8.43e-14\npolygon 362 (hole)         3 -1.11188e-08     -1.66e-14\npolygon 363 (hole)         4 -2.60006e-08     -3.88e-14\npolygon 364 (hole)        28 -1.79082e-07     -2.67e-13\npolygon 365 (hole)         4 -1.58213e-10     -2.36e-16\npolygon 366 (hole)         4 -9.07739e-12     -1.36e-17\npolygon 367 (hole)         4 -6.73612e-09     -1.01e-14\npolygon 368               16  4.14093e-01      6.18e-07\npolygon 369 (hole)         4 -5.36782e-09     -8.02e-15\npolygon 370 (hole)         6 -1.51450e-08     -2.26e-14\npolygon 371 (hole)         4 -4.38078e-09     -6.54e-15\npolygon 372 (hole)         4 -1.36409e-08     -2.04e-14\npolygon 373 (hole)         4 -3.84271e-08     -5.74e-14\npolygon 374 (hole)         3 -7.56649e-09     -1.13e-14\npolygon 375 (hole)         4 -8.04108e-09     -1.20e-14\npolygon 376 (hole)         6 -2.53481e-08     -3.78e-14\npolygon 377 (hole)         4 -1.15603e-08     -1.73e-14\npolygon 378 (hole)         3 -2.16577e-09     -3.23e-15\npolygon 379 (hole)         3 -2.87541e-09     -4.29e-15\npolygon 380 (hole)         4 -5.42479e-08     -8.10e-14\npolygon 381 (hole)         3 -4.75090e-09     -7.09e-15\npolygon 382 (hole)         4 -3.45047e-08     -5.15e-14\npolygon 383 (hole)         6 -7.20472e-09     -1.08e-14\npolygon 384 (hole)         4 -2.01300e-08     -3.01e-14\npolygon 385 (hole)         9 -7.99880e-08     -1.19e-13\npolygon 386 (hole)         4 -1.30938e-09     -1.96e-15\npolygon 387 (hole)         9 -1.92423e-08     -2.87e-14\npolygon 388 (hole)         3 -2.44412e-13     -3.65e-19\npolygon 389 (hole)         4 -2.73889e-08     -4.09e-14\npolygon 390 (hole)         3 -1.15109e-08     -1.72e-14\npolygon 391 (hole)         4 -8.77810e-08     -1.31e-13\npolygon 392               26  3.03928e+00      4.54e-06\npolygon 393 (hole)         6 -8.99139e-08     -1.34e-13\npolygon 394 (hole)         7 -1.69870e-07     -2.54e-13\npolygon 395 (hole)        11 -1.85737e-07     -2.77e-13\npolygon 396 (hole)        32 -8.16597e-07     -1.22e-12\npolygon 397 (hole)         7 -6.59191e-08     -9.84e-14\npolygon 398 (hole)         3 -1.48932e-08     -2.22e-14\npolygon 399 (hole)        13 -3.56626e-07     -5.33e-13\npolygon 400 (hole)         3 -1.64295e-12     -2.45e-18\npolygon 401 (hole)         3 -2.03106e-09     -3.03e-15\npolygon 402               14  1.53563e-01      2.29e-07\npolygon 403 (hole)         4 -1.79041e-08     -2.67e-14\npolygon 404 (hole)        15 -2.31620e-07     -3.46e-13\npolygon 405 (hole)         7 -3.46670e-08     -5.18e-14\npolygon 406 (hole)         6 -1.90172e-08     -2.84e-14\npolygon 407 (hole)         6 -3.15828e-08     -4.72e-14\npolygon 408 (hole)        13 -1.40633e-08     -2.10e-14\npolygon 409 (hole)         6 -2.76240e-08     -4.12e-14\npolygon 410 (hole)         3 -3.74153e-08     -5.59e-14\npolygon 411 (hole)         3 -1.16808e-08     -1.74e-14\npolygon 412 (hole)        10 -4.10705e-09     -6.13e-15\npolygon 413 (hole)         4 -6.52777e-09     -9.75e-15\npolygon 414 (hole)         6 -5.03054e-08     -7.51e-14\npolygon 415 (hole)         4 -2.54303e-09     -3.80e-15\npolygon 416 (hole)         3 -1.76691e-10     -2.64e-16\npolygon 417 (hole)         4 -7.90017e-08     -1.18e-13\npolygon 418 (hole)         3 -1.07902e-09     -1.61e-15\npolygon 419 (hole)         3 -8.74115e-10     -1.31e-15\npolygon 420 (hole)        12 -6.41926e-08     -9.59e-14\npolygon 421 (hole)         4 -4.36895e-09     -6.52e-15\npolygon 422 (hole)         4 -1.85182e-08     -2.77e-14\npolygon 423 (hole)        10 -1.21248e-07     -1.81e-13\npolygon 424 (hole)        10 -5.59537e-08     -8.35e-14\npolygon 425 (hole)         4 -2.20079e-08     -3.29e-14\npolygon 426               19  5.08538e-01      7.59e-07\npolygon 427 (hole)         4 -1.45573e-08     -2.17e-14\npolygon 428 (hole)         4 -4.47181e-09     -6.68e-15\npolygon 429               16  1.80565e-01      2.70e-07\npolygon 430 (hole)         3 -9.03958e-14     -1.35e-19\npolygon 431               26  9.75091e-01      1.46e-06\npolygon 432 (hole)         4 -1.90718e-09     -2.85e-15\npolygon 433 (hole)         5 -1.60264e-08     -2.39e-14\npolygon 434 (hole)         4 -1.82919e-08     -2.73e-14\npolygon 435 (hole)         3 -7.26182e-09     -1.08e-14\npolygon 436 (hole)         4 -2.12571e-08     -3.17e-14\npolygon 437 (hole)         4 -2.41922e-09     -3.61e-15\npolygon 438 (hole)         3 -2.80771e-08     -4.19e-14\npolygon 439 (hole)         7 -5.31797e-09     -7.94e-15\npolygon 440 (hole)        16 -1.29582e-07     -1.93e-13\npolygon 441 (hole)         4 -1.88745e-08     -2.82e-14\npolygon 442 (hole)         3 -1.45605e-08     -2.17e-14\npolygon 443 (hole)         3 -2.28810e-08     -3.42e-14\npolygon 444 (hole)         4 -1.59810e-08     -2.39e-14\npolygon 445 (hole)         5 -1.72617e-08     -2.58e-14\npolygon 446 (hole)         3 -4.96859e-09     -7.42e-15\npolygon 447 (hole)        14 -8.85135e-08     -1.32e-13\npolygon 448 (hole)         4 -1.85807e-08     -2.77e-14\npolygon 449 (hole)         4 -8.61250e-09     -1.29e-14\npolygon 450 (hole)         4 -2.06032e-08     -3.08e-14\npolygon 451 (hole)         3 -1.66645e-09     -2.49e-15\npolygon 452 (hole)         7 -1.66808e-08     -2.49e-14\npolygon 453 (hole)         4 -1.67544e-09     -2.50e-15\npolygon 454 (hole)        12 -3.97402e-08     -5.93e-14\npolygon 455 (hole)         4 -4.18178e-08     -6.24e-14\npolygon 456 (hole)         3 -2.93135e-09     -4.38e-15\npolygon 457 (hole)         4 -6.31983e-10     -9.44e-16\npolygon 458 (hole)         3 -8.50840e-13     -1.27e-18\npolygon 459 (hole)         7 -8.42406e-09     -1.26e-14\npolygon 460 (hole)         3 -1.34855e-12     -2.01e-18\npolygon 461 (hole)         6 -1.60663e-08     -2.40e-14\npolygon 462 (hole)         4 -5.66656e-11     -8.46e-17\npolygon 463 (hole)         5 -1.17874e-08     -1.76e-14\npolygon 464 (hole)         3 -1.75180e-14     -2.62e-20\npolygon 465 (hole)         5 -2.62273e-08     -3.92e-14\npolygon 466 (hole)         7 -2.12912e-08     -3.18e-14\npolygon 467 (hole)        11 -4.47347e-08     -6.68e-14\npolygon 468 (hole)         6 -5.00912e-09     -7.48e-15\npolygon 469 (hole)         5 -3.58831e-08     -5.36e-14\npolygon 470 (hole)         3 -1.06446e-09     -1.59e-15\npolygon 471 (hole)         4 -1.50038e-08     -2.24e-14\npolygon 472 (hole)         3 -8.74496e-10     -1.31e-15\npolygon 473 (hole)         7 -3.75991e-08     -5.61e-14\npolygon 474               19  2.15295e-01      3.21e-07\npolygon 475 (hole)         6 -3.71337e-08     -5.54e-14\npolygon 476 (hole)         3 -4.33900e-09     -6.48e-15\npolygon 477 (hole)         7 -8.65067e-08     -1.29e-13\npolygon 478 (hole)         8 -7.76951e-08     -1.16e-13\npolygon 479 (hole)         6 -9.01907e-08     -1.35e-13\npolygon 480 (hole)         8 -1.10404e-07     -1.65e-13\npolygon 481 (hole)         8 -7.38263e-08     -1.10e-13\npolygon 482 (hole)         3 -1.71517e-08     -2.56e-14\npolygon 483               16  3.50061e-01      5.23e-07\npolygon 484 (hole)         3 -1.50971e-14     -2.25e-20\npolygon 485 (hole)         3 -1.78834e-04     -2.67e-10\npolygon 486                3  0.00000e+00      0.00e+00\npolygon 487 (hole)         4 -4.21570e-08     -6.29e-14\npolygon 488 (hole)         7 -1.74178e-07     -2.60e-13\npolygon 489               14  2.11571e-01      3.16e-07\npolygon 490 (hole)         3 -1.05578e-08     -1.58e-14\npolygon 491 (hole)         4 -4.32122e-08     -6.45e-14\npolygon 492               11  1.59536e-01      2.38e-07\npolygon 493 (hole)         4 -3.72418e-08     -5.56e-14\npolygon 494 (hole)         6 -2.87377e-08     -4.29e-14\npolygon 495 (hole)         3 -3.87472e-09     -5.79e-15\npolygon 496 (hole)         6 -1.05664e-08     -1.58e-14\npolygon 497 (hole)         4 -7.04519e-09     -1.05e-14\npolygon 498              111  7.35028e+01      1.10e-04\npolygon 499 (hole)         3 -5.99345e-09     -8.95e-15\npolygon 500 (hole)         3 -2.33491e-08     -3.49e-14\npolygon 501 (hole)         5 -1.14503e-07     -1.71e-13\npolygon 502               19  6.34649e-01      9.48e-07\npolygon 503 (hole)         3 -2.26548e-09     -3.38e-15\npolygon 504 (hole)         4 -5.35016e-08     -7.99e-14\npolygon 505               10  3.74827e-02      5.60e-08\npolygon 506               13  1.37100e-01      2.05e-07\npolygon 507               31  4.47193e+00      6.68e-06\npolygon 508               19  1.23742e+00      1.85e-06\npolygon 509               20  1.79201e+00      2.68e-06\npolygon 510 (hole)         3 -1.64235e-08     -2.45e-14\npolygon 511               16  2.19464e-01      3.28e-07\npolygon 512               11  7.94092e-02      1.19e-07\npolygon 513 (hole)         3 -2.99572e-08     -4.47e-14\npolygon 514 (hole)         3 -8.18639e-09     -1.22e-14\npolygon 515 (hole)         3 -2.81986e-08     -4.21e-14\npolygon 516               14  1.45779e-01      2.18e-07\npolygon 517               31  6.24672e-01      9.33e-07\npolygon 518               11  2.29288e-02      3.42e-08\npolygon 519               18  2.10998e-01      3.15e-07\npolygon 520               26  2.52571e+00      3.77e-06\npolygon 521               16  5.77783e-01      8.63e-07\npolygon 522               26  8.87985e-01      1.33e-06\npolygon 523               13  1.10557e-01      1.65e-07\npolygon 524               21  9.55338e-01      1.43e-06\npolygon 525               32  2.18189e+00      3.26e-06\npolygon 526               18  7.64830e-01      1.14e-06\npolygon 527               34  1.71336e+00      2.56e-06\npolygon 528              115  9.29594e-01      1.39e-06\npolygon 529               68  2.06761e-01      3.09e-07\npolygon 530               40  4.87851e+00      7.28e-06\npolygon 531               10  7.43137e-02      1.11e-07\npolygon 532               17  5.95020e-01      8.88e-07\npolygon 533               21  4.02132e-01      6.00e-07\npolygon 534               21  1.09635e+00      1.64e-06\npolygon 535               14  1.15356e-01      1.72e-07\npolygon 536 (hole)        18 -1.39516e-07     -2.08e-13\npolygon 537               20  3.13610e-01      4.68e-07\npolygon 538              275  3.92648e+02      5.86e-04\npolygon 539               16  2.19612e-01      3.28e-07\npolygon 540               13  2.64858e-01      3.95e-07\npolygon 541               23  1.25549e+00      1.87e-06\npolygon 542                9  1.79312e-01      2.68e-07\npolygon 543               15  2.93132e-01      4.38e-07\npolygon 544               15  5.29892e-01      7.91e-07\npolygon 545               23  4.95351e-01      7.40e-07\npolygon 546 (hole)        10 -2.23148e-07     -3.33e-13\npolygon 547               14  4.41976e-01      6.60e-07\npolygon 548               19  3.32318e-01      4.96e-07\npolygon 549 (hole)         6 -6.70431e-08     -1.00e-13\npolygon 550               16  4.15914e-01      6.21e-07\npolygon 551               22  5.10526e-01      7.62e-07\npolygon 552               48  1.87148e+01      2.79e-05\npolygon 553               18  1.62882e+00      2.43e-06\npolygon 554 (hole)         9 -1.96623e-07     -2.94e-13\npolygon 555               71  2.43615e+01      3.64e-05\npolygon 556              116  5.16444e+01      7.71e-05\npolygon 557               19  1.54144e+00      2.30e-06\npolygon 558               11  2.01013e-01      3.00e-07\npolygon 559 (hole)         4 -1.87899e-08     -2.81e-14\npolygon 560               20  2.31201e+00      3.45e-06\npolygon 561               15  5.94674e-01      8.88e-07\npolygon 562              218  1.56252e+02      2.33e-04\npolygon 563              162  8.80247e+01      1.31e-04\npolygon 564               16  3.69968e-01      5.52e-07\npolygon 565               14  1.61451e-01      2.41e-07\npolygon 566               18  5.54566e-01      8.28e-07\npolygon 567               19  5.07078e-01      7.57e-07\npolygon 568 (hole)         7 -1.85276e-07     -2.77e-13\npolygon 569               57  6.86475e+00      1.03e-05\npolygon 570               15  6.56018e-01      9.80e-07\npolygon 571               21  6.65092e-01      9.93e-07\npolygon 572               15  5.97784e-01      8.93e-07\npolygon 573               37  2.56010e+00      3.82e-06\npolygon 574               13  1.66996e-01      2.49e-07\npolygon 575               20  8.18381e-01      1.22e-06\npolygon 576               21  2.89704e+00      4.33e-06\npolygon 577               15  4.08213e-01      6.10e-07\npolygon 578 (hole)         3 -1.98618e-09     -2.97e-15\npolygon 579              104  4.71547e+01      7.04e-05\npolygon 580               28  3.80443e+00      5.68e-06\npolygon 581               67  3.17343e+01      4.74e-05\npolygon 582              730  9.20735e+02      1.37e-03\npolygon 583               16  2.01576e-01      3.01e-07\npolygon 584               12  1.41506e-01      2.11e-07\npolygon 585               15  4.92125e-01      7.35e-07\npolygon 586               18  1.58128e+00      2.36e-06\npolygon 587 (hole)         7 -6.67005e-08     -9.96e-14\npolygon 588               25  1.94322e+00      2.90e-06\npolygon 589               12  2.51373e-01      3.75e-07\npolygon 590              412  4.47936e+02      6.69e-04\npolygon 591               22  7.57105e-01      1.13e-06\npolygon 592               16  6.97434e-01      1.04e-06\npolygon 593 (hole)         3 -2.86672e-08     -4.28e-14\npolygon 594               39  4.45130e+00      6.65e-06\npolygon 595 (hole)         4 -2.17002e-08     -3.24e-14\npolygon 596 (hole)         3 -1.55741e-08     -2.33e-14\npolygon 597               13  8.83786e-01      1.32e-06\npolygon 598               10  2.06200e-01      3.08e-07\npolygon 599 (hole)         3 -3.50430e-08     -5.23e-14\npolygon 600               76  3.60497e+01      5.38e-05\npolygon 601 (hole)         5 -5.41615e-08     -8.09e-14\npolygon 602               13  4.56433e-01      6.82e-07\npolygon 603               28  2.30613e+00      3.44e-06\npolygon 604 (hole)         4 -1.59313e-08     -2.38e-14\npolygon 605               27  2.50338e+00      3.74e-06\npolygon 606               14  4.69238e-01      7.01e-07\npolygon 607               10  1.69886e-01      2.54e-07\npolygon 608 (hole)         8 -1.09145e-07     -1.63e-13\npolygon 609               16  5.42822e-01      8.11e-07\npolygon 610               10  1.17633e-01      1.76e-07\npolygon 611                8  7.08579e-02      1.06e-07\npolygon 612              144  7.85300e+01      1.17e-04\npolygon 613               37  1.93477e+00      2.89e-06\npolygon 614 (hole)         3 -1.47049e-09     -2.20e-15\npolygon 615               25  5.59996e-01      8.36e-07\npolygon 616 (hole)         8 -1.20535e-07     -1.80e-13\npolygon 617               44  2.86031e+00      4.27e-06\npolygon 618               26  1.26276e+00      1.89e-06\npolygon 619              149  1.37840e+02      2.06e-04\npolygon 620               18  4.84958e-01      7.24e-07\npolygon 621               73  2.93195e+01      4.38e-05\npolygon 622 (hole)         5 -1.99458e-08     -2.98e-14\npolygon 623               45  6.87481e+00      1.03e-05\npolygon 624 (hole)         4 -1.81549e-08     -2.71e-14\npolygon 625               64  1.80880e+01      2.70e-05\npolygon 626               12  8.76879e-01      1.31e-06\npolygon 627               26  2.78381e+00      4.16e-06\npolygon 628               35  8.72326e+00      1.30e-05\npolygon 629               53  1.06237e+01      1.59e-05\npolygon 630               26  5.40467e+00      8.07e-06\npolygon 631              148  1.05037e+02      1.57e-04\npolygon 632               21  8.32798e-01      1.24e-06\npolygon 633               21  2.23023e+00      3.33e-06\npolygon 634                8  6.30805e-01      9.42e-07\npolygon 635               78  3.67603e+01      5.49e-05\npolygon 636               18  1.64745e+00      2.46e-06\npolygon 637               58  1.31747e+01      1.97e-05\npolygon 638               94  1.16837e+01      1.74e-05\npolygon 639               53  3.17801e+00      4.75e-06\npolygon 640              137  8.46454e+00      1.26e-05\npolygon 641               23  4.36507e-01      6.52e-07\npolygon 642               27  9.72136e-01      1.45e-06\npolygon 643               55  1.98854e+00      2.97e-06\npolygon 644               48  1.02651e+01      1.53e-05\npolygon 645               15  3.16153e-01      4.72e-07\npolygon 646               22  8.93201e-01      1.33e-06\npolygon 647               18  2.91538e-01      4.35e-07\npolygon 648               59  3.70736e+00      5.54e-06\npolygon 649               11  1.55205e-01      2.32e-07\npolygon 650               14  1.87401e-01      2.80e-07\npolygon 651               11  6.54128e-02      9.77e-08\npolygon 652               11  8.64659e-02      1.29e-07\npolygon 653               12  2.25129e-01      3.36e-07\npolygon 654               30  2.72178e+00      4.06e-06\npolygon 655 (hole)         3 -2.26252e-06     -3.38e-12\npolygon 656              180  1.80314e+01      2.69e-05\npolygon 657               14  4.01942e-01      6.00e-07\npolygon 658              103  1.33467e+01      1.99e-05\npolygon 659               14  2.07708e-01      3.10e-07\npolygon 660               55  4.38623e+00      6.55e-06\npolygon 661               41  4.69733e+00      7.01e-06\npolygon 662               74  3.92633e+00      5.86e-06\npolygon 663               31  2.79477e+00      4.17e-06\npolygon 664               17  8.74607e-01      1.31e-06\npolygon 665               26  4.50962e-01      6.73e-07\npolygon 666               18  1.15065e+00      1.72e-06\npolygon 667              164  1.48037e+01      2.21e-05\npolygon 668               12  3.17290e-01      4.74e-07\npolygon 669               69  2.59551e+01      3.88e-05\npolygon 670               37  7.85963e-01      1.17e-06\npolygon 671               14  3.37873e-01      5.05e-07\npolygon 672               12  1.77980e-01      2.66e-07\npolygon 673               21  6.46195e-01      9.65e-07\npolygon 674               24  2.69998e+00      4.03e-06\npolygon 675               30  1.00675e+00      1.50e-06\npolygon 676               57  4.22747e+00      6.31e-06\npolygon 677               23  8.63543e-01      1.29e-06\npolygon 678               11  2.01925e-01      3.02e-07\npolygon 679               95  2.88232e+01      4.30e-05\npolygon 680              452  9.64208e+01      1.44e-04\npolygon 681               77  1.72297e+01      2.57e-05\npolygon 682               16  3.33522e-01      4.98e-07\npolygon 683               19  6.82895e-01      1.02e-06\npolygon 684               43  3.95062e+00      5.90e-06\npolygon 685               29  1.00709e+00      1.50e-06\npolygon 686               21  8.90014e-01      1.33e-06\npolygon 687               22  7.93453e-01      1.18e-06\npolygon 688               13  4.52813e-01      6.76e-07\npolygon 689               30  1.49817e+00      2.24e-06\npolygon 690               48  4.23996e+00      6.33e-06\npolygon 691               37  1.20290e+00      1.80e-06\npolygon 692               19  4.85805e-01      7.25e-07\npolygon 693               46  2.18001e+00      3.26e-06\npolygon 694                7  1.33723e-01      2.00e-07\npolygon 695               54  1.82059e+01      2.72e-05\npolygon 696               10  8.57866e-01      1.28e-06\npolygon 697               14  1.98445e-01      2.96e-07\npolygon 698               19  4.68357e-01      6.99e-07\npolygon 699               52  8.57765e+00      1.28e-05\npolygon 700               73  7.91998e+00      1.18e-05\npolygon 701               23  8.19561e+00      1.22e-05\npolygon 702               10  1.06594e-01      1.59e-07\npolygon 703              169  1.39462e+01      2.08e-05\npolygon 704               17  2.55915e-01      3.82e-07\npolygon 705               12  1.81516e-01      2.71e-07\npolygon 706               16  3.68509e-01      5.50e-07\npolygon 707               12  1.24954e-01      1.87e-07\npolygon 708             1012  4.61886e+02      6.90e-04\npolygon 709               15  5.42647e-01      8.10e-07\npolygon 710               12  5.53088e-01      8.26e-07\npolygon 711               24  1.06455e+00      1.59e-06\npolygon 712               14  3.17038e-01      4.73e-07\npolygon 713               17  1.25559e+00      1.87e-06\npolygon 714               16  1.92407e+00      2.87e-06\npolygon 715               80  1.86306e+01      2.78e-05\npolygon 716               11  6.83880e-02      1.02e-07\npolygon 717              166  4.21470e+01      6.29e-05\npolygon 718               22  1.36174e+00      2.03e-06\npolygon 719               31  6.07363e-01      9.07e-07\npolygon 720               10  1.27067e-01      1.90e-07\npolygon 721              164  5.02951e+01      7.51e-05\npolygon 722               17  1.20798e+00      1.80e-06\npolygon 723               59  2.41720e+00      3.61e-06\npolygon 724               12  2.52952e-01      3.78e-07\npolygon 725               14  2.44407e-01      3.65e-07\npolygon 726               43  4.89177e+00      7.30e-06\npolygon 727               40  3.86642e+00      5.77e-06\npolygon 728               23  4.59203e-01      6.86e-07\npolygon 729               14  1.79442e-01      2.68e-07\npolygon 730               13  1.61191e-01      2.41e-07\npolygon 731               43  1.33365e+00      1.99e-06\npolygon 732               75  1.20247e+01      1.80e-05\npolygon 733               17  9.96466e-02      1.49e-07\npolygon 734               22  1.60660e+00      2.40e-06\npolygon 735              730  1.16983e+02      1.75e-04\npolygon 736              129  7.87539e+00      1.18e-05\npolygon 737               22  4.66256e-01      6.96e-07\npolygon 738               19  5.44519e-01      8.13e-07\npolygon 739               16  3.73911e-01      5.58e-07\npolygon 740               88  3.47678e+01      5.19e-05\npolygon 741               46  3.23711e+00      4.83e-06\npolygon 742               47  9.17482e-01      1.37e-06\npolygon 743               14  4.44869e-01      6.64e-07\npolygon 744               43  1.29527e+00      1.93e-06\npolygon 745               65  3.20974e+00      4.79e-06\npolygon 746              306  1.83695e+01      2.74e-05\npolygon 747               17  4.25486e-01      6.35e-07\npolygon 748               29  1.08467e+00      1.62e-06\npolygon 749               26  1.16602e+00      1.74e-06\npolygon 750              135  4.90655e+00      7.33e-06\npolygon 751               23  1.87723e+00      2.80e-06\npolygon 752               21  7.02338e-01      1.05e-06\npolygon 753               30  4.99970e+00      7.47e-06\npolygon 754               21  7.71571e-01      1.15e-06\npolygon 755               13  1.47832e-01      2.21e-07\npolygon 756               26  3.17307e+00      4.74e-06\npolygon 757               17  1.03642e+00      1.55e-06\npolygon 758              100  8.74537e+00      1.31e-05\npolygon 759               11  1.13950e-01      1.70e-07\npolygon 760               26  8.27840e-01      1.24e-06\npolygon 761               17  4.10882e-01      6.14e-07\npolygon 762               13  1.49250e-01      2.23e-07\npolygon 763               35  1.76408e+00      2.63e-06\npolygon 764               15  2.03390e-01      3.04e-07\npolygon 765               18  1.70982e-01      2.55e-07\npolygon 766               32  7.98599e+00      1.19e-05\npolygon 767               27  3.20839e-01      4.79e-07\npolygon 768              539  1.17139e+02      1.75e-04\npolygon 769               51  1.57614e+00      2.35e-06\npolygon 770               19  3.60772e-01      5.39e-07\npolygon 771               58  2.10060e+00      3.14e-06\npolygon 772               13  1.70224e-01      2.54e-07\npolygon 773               11  6.11736e-01      9.13e-07\npolygon 774               35  5.26152e+00      7.86e-06\npolygon 775               16  1.99371e-01      2.98e-07\npolygon 776               14  4.48979e-01      6.70e-07\npolygon 777               25  1.92913e+00      2.88e-06\npolygon 778               16  4.26157e-01      6.36e-07\npolygon 779               46  1.88229e+00      2.81e-06\npolygon 780               17  2.29456e-01      3.43e-07\npolygon 781               36  4.01418e+00      5.99e-06\npolygon 782               57  1.77153e+01      2.65e-05\npolygon 783               20  5.72298e-01      8.55e-07\npolygon 784               14  2.46782e-01      3.68e-07\npolygon 785               29  1.01300e+00      1.51e-06\npolygon 786               90  8.54955e+00      1.28e-05\npolygon 787               21  3.99098e-01      5.96e-07\npolygon 788               12  2.81650e-01      4.21e-07\npolygon 789               52  3.19337e+00      4.77e-06\npolygon 790               13  3.40300e-01      5.08e-07\npolygon 791               29  9.08533e-01      1.36e-06\npolygon 792               20  3.07393e-01      4.59e-07\npolygon 793               19  1.15906e+00      1.73e-06\npolygon 794               17  1.16182e+00      1.73e-06\npolygon 795               21  2.60877e+00      3.90e-06\npolygon 796               15  1.09001e-01      1.63e-07\npolygon 797               37  1.31921e+00      1.97e-06\npolygon 798               53  1.46854e+00      2.19e-06\npolygon 799               59  8.92822e+00      1.33e-05\npolygon 800                9  7.44981e-02      1.11e-07\npolygon 801               14  2.41462e-01      3.61e-07\npolygon 802               96  7.25940e+00      1.08e-05\npolygon 803               11  1.06055e-01      1.58e-07\npolygon 804               49  1.87834e+00      2.80e-06\npolygon 805               23  6.08310e-01      9.08e-07\npolygon 806               50  6.76488e-01      1.01e-06\npolygon 807               22  4.83089e-01      7.21e-07\npolygon 808               17  1.17278e-01      1.75e-07\npolygon 809               13  8.98786e-01      1.34e-06\npolygon 810               43  1.01757e+00      1.52e-06\npolygon 811               52  1.68377e+00      2.51e-06\npolygon 812              348  2.50314e+02      3.74e-04\npolygon 813               43  1.29120e+00      1.93e-06\npolygon 814               71  2.02836e+00      3.03e-06\npolygon 815 (hole)         4 -4.12791e-06     -6.16e-12\npolygon 816              141  4.53240e+00      6.77e-06\npolygon 817               52  3.62008e+00      5.41e-06\npolygon 818               20  7.69539e-01      1.15e-06\npolygon 819               88  8.88904e+00      1.33e-05\npolygon 820               12  1.44668e-01      2.16e-07\npolygon 821               46  2.28026e+00      3.40e-06\npolygon 822               39  4.79165e+00      7.15e-06\npolygon 823               53  2.86736e+00      4.28e-06\npolygon 824               38  3.22508e+00      4.82e-06\npolygon 825               18  4.44863e-01      6.64e-07\npolygon 826               45  1.45134e+00      2.17e-06\npolygon 827               86  2.56400e+01      3.83e-05\npolygon 828              158  1.74510e+01      2.61e-05\npolygon 829               14  2.50383e-01      3.74e-07\npolygon 830              120  7.17019e+00      1.07e-05\npolygon 831              146  2.28448e+01      3.41e-05\npolygon 832              143  2.34472e+01      3.50e-05\npolygon 833               20  3.47415e-01      5.19e-07\npolygon 834 (hole)         4 -4.70080e-12     -7.02e-18\npolygon 835               84  3.86387e+00      5.77e-06\npolygon 836               68  1.10850e+01      1.66e-05\npolygon 837              863  7.47702e+01      1.12e-04\npolygon 838               53  7.54742e+00      1.13e-05\npolygon 839 (hole)         5 -5.07266e-12     -7.57e-18\npolygon 840               64  2.11016e+00      3.15e-06\npolygon 841              103  1.47525e+01      2.20e-05\npolygon 842               26  3.40345e+00      5.08e-06\npolygon 843 (hole)         6 -3.63322e-12     -5.43e-18\npolygon 844 (hole)         8 -7.62695e-12     -1.14e-17\npolygon 845              103  8.40445e+00      1.25e-05\npolygon 846               23  4.92041e-01      7.35e-07\npolygon 847               27  5.53625e-01      8.27e-07\npolygon 848               60  3.56102e+00      5.32e-06\npolygon 849              151  1.51314e+01      2.26e-05\npolygon 850 (hole)         3 -7.39478e-13     -1.10e-18\npolygon 851              708  4.18129e+02      6.24e-04\npolygon 852              120  1.32857e+01      1.98e-05\npolygon 853              856  2.53898e+02      3.79e-04\npolygon 854 (hole)         3 -1.51167e-12     -2.26e-18\npolygon 855 (hole)         3 -4.16327e-13     -6.22e-19\npolygon 856 (hole)         3 -1.81968e-13     -2.72e-19\npolygon 857 (hole)         5 -8.69542e-12     -1.30e-17\npolygon 858               13  2.47298e-01      3.69e-07\npolygon 859 (hole)         5 -7.23071e-12     -1.08e-17\npolygon 860               16  7.35754e-01      1.10e-06\npolygon 861 (hole)         4 -5.36614e-12     -8.01e-18\npolygon 862               21  5.31633e-01      7.94e-07\npolygon 863 (hole)        11 -4.03802e-12     -6.03e-18\npolygon 864 (hole)         8 -8.21058e-12     -1.23e-17\npolygon 865 (hole)         3 -8.94235e-13     -1.34e-18\npolygon 866 (hole)         5 -8.77891e-13     -1.31e-18\npolygon 867               72  1.03185e+01      1.54e-05\npolygon 868 (hole)         4 -1.46473e-12     -2.19e-18\npolygon 869 (hole)         3 -9.57738e-14     -1.43e-19\npolygon 870               38  1.34734e+00      2.01e-06\npolygon 871                8  4.09489e-01      6.11e-07\npolygon 872 (hole)         4 -2.72557e-12     -4.07e-18\npolygon 873               17  2.46995e-01      3.69e-07\npolygon 874 (hole)         3 -4.56587e-13     -6.82e-19\npolygon 875               14  1.42285e-01      2.12e-07\npolygon 876 (hole)         4 -1.74558e-12     -2.61e-18\npolygon 877 (hole)         6 -5.91253e-12     -8.83e-18\npolygon 878 (hole)         4 -1.34510e-13     -2.01e-19\npolygon 879 (hole)         3 -6.12765e-17     -9.15e-23\npolygon 880                9  1.56346e-01      2.33e-07\npolygon 881                4  2.90958e-17      4.34e-23\npolygon 882               42  5.49581e-01      8.21e-07\npolygon 883 (hole)         5 -2.12744e-12     -3.18e-18\npolygon 884 (hole)         3 -5.23582e-12     -7.82e-18\npolygon 885 (hole)         3 -2.43443e-13     -3.64e-19\npolygon 886 (hole)         3 -8.02710e-13     -1.20e-18\npolygon 887 (hole)         6 -1.09890e-11     -1.64e-17\npolygon 888 (hole)         4 -1.53858e-13     -2.30e-19\npolygon 889               59  1.15813e+00      1.73e-06\npolygon 890 (hole)         4 -2.30355e-12     -3.44e-18\npolygon 891               12  1.16845e-01      1.74e-07\npolygon 892               11  6.00122e-02      8.96e-08\npolygon 893               14  1.63465e-01      2.44e-07\npolygon 894                8  4.73674e-02      7.07e-08\npolygon 895               23  6.69330e-01      9.99e-07\npolygon 896 (hole)         3 -1.13772e-13     -1.70e-19\npolygon 897              144  6.48346e+00      9.68e-06\npolygon 898               74  4.79923e+00      7.17e-06\npolygon 899              116  1.02706e+01      1.53e-05\npolygon 900               16  2.74005e-01      4.09e-07\npolygon 901               97  4.72599e+00      7.06e-06\npolygon 902               18  6.15583e-01      9.19e-07\npolygon 903              146  1.38979e+01      2.08e-05\npolygon 904 (hole)         5 -4.08205e-12     -6.10e-18\npolygon 905               32  1.94516e+00      2.90e-06\npolygon 906               26  9.81356e-01      1.47e-06\npolygon 907 (hole)         3 -2.01834e-12     -3.01e-18\npolygon 908 (hole)         4 -9.94985e-13     -1.49e-18\npolygon 909               82  4.39797e+00      6.57e-06\npolygon 910               33  9.36113e-01      1.40e-06\npolygon 911 (hole)         3 -1.40697e-13     -2.10e-19\npolygon 912               47  1.51844e+00      2.27e-06\npolygon 913 (hole)         3 -6.88122e-12     -1.03e-17\npolygon 914              196  2.32689e+01      3.47e-05\npolygon 915 (hole)         3 -2.40592e-12     -3.59e-18\npolygon 916 (hole)         5 -6.19998e-12     -9.26e-18\npolygon 917               53  1.30069e+00      1.94e-06\npolygon 918               70  1.00802e+01      1.51e-05\npolygon 919 (hole)         3 -3.17405e-13     -4.74e-19\npolygon 920               10  5.33870e-01      7.97e-07\npolygon 921              211  2.73026e+01      4.08e-05\npolygon 922               41  1.44141e+00      2.15e-06\npolygon 923                9  5.20220e-01      7.77e-07\npolygon 924              316  6.66808e+01      9.96e-05\npolygon 925               17  7.48505e-01      1.12e-06\npolygon 926               15  1.49799e-01      2.24e-07\npolygon 927               24  4.99500e-01      7.46e-07\npolygon 928               12  2.24463e-01      3.35e-07\npolygon 929              100  1.06028e+01      1.58e-05\npolygon 930               11  1.24267e-01      1.86e-07\npolygon 931              143  1.89581e+01      2.83e-05\npolygon 932               60  7.07802e+00      1.06e-05\npolygon 933              124  1.23476e+01      1.84e-05\npolygon 934               34  5.67625e+00      8.48e-06\npolygon 935              797  1.86955e+02      2.79e-04\npolygon 936               15  7.15891e-01      1.07e-06\npolygon 937               20  1.61624e+00      2.41e-06\npolygon 938               13  1.73955e-01      2.60e-07\npolygon 939               60  2.42590e+00      3.62e-06\npolygon 940               20  3.86365e-01      5.77e-07\npolygon 941              213  3.69421e+01      5.52e-05\npolygon 942               15  4.55574e-01      6.80e-07\npolygon 943               35  1.56058e+00      2.33e-06\npolygon 944               32  1.42419e+00      2.13e-06\npolygon 945               68  2.30711e+00      3.44e-06\npolygon 946              240  5.93935e+01      8.87e-05\npolygon 947              145  1.58875e+01      2.37e-05\npolygon 948               39  3.67239e+00      5.48e-06\npolygon 949               54  3.84847e+00      5.75e-06\npolygon 950               32  1.14073e+00      1.70e-06\npolygon 951               18  3.58549e-01      5.35e-07\npolygon 952               25  8.06470e-01      1.20e-06\npolygon 953               15  2.67818e-01      4.00e-07\npolygon 954               26  1.43681e+00      2.15e-06\npolygon 955               18  3.10061e-01      4.63e-07\npolygon 956               18  4.71644e-01      7.04e-07\npolygon 957               16  5.04146e-01      7.53e-07\npolygon 958               72  7.33720e+00      1.10e-05\npolygon 959               27  1.37772e+00      2.06e-06\npolygon 960               15  3.17217e-01      4.74e-07\npolygon 961               37  1.17498e+00      1.75e-06\npolygon 962               21  7.05388e-01      1.05e-06\npolygon 963              216  2.05399e+01      3.07e-05\npolygon 964               16  2.40093e-01      3.59e-07\npolygon 965               29  1.71282e+00      2.56e-06\npolygon 966               30  1.28695e+00      1.92e-06\npolygon 967               36  2.29670e+00      3.43e-06\npolygon 968              129  1.97698e+01      2.95e-05\npolygon 969               65  2.65969e+00      3.97e-06\npolygon 970              283  3.30575e+01      4.94e-05\npolygon 971               28  7.06271e-01      1.05e-06\npolygon 972               28  5.50158e-01      8.21e-07\npolygon 973               49  2.25015e+00      3.36e-06\npolygon 974               26  1.24280e+00      1.86e-06\npolygon 975               28  9.02794e-01      1.35e-06\npolygon 976               18  4.84064e-01      7.23e-07\npolygon 977               47  2.33442e+00      3.49e-06\npolygon 978               17  2.23835e-01      3.34e-07\npolygon 979               34  7.32504e-01      1.09e-06\npolygon 980               41  7.15538e-01      1.07e-06\npolygon 981               21  5.81687e-01      8.69e-07\npolygon 982               63  3.83819e+00      5.73e-06\npolygon 983               27  1.33192e+00      1.99e-06\npolygon 984               46  2.95028e+00      4.41e-06\npolygon 985               10  1.34210e-01      2.00e-07\npolygon 986               16  2.38274e-01      3.56e-07\npolygon 987               18  4.18536e-01      6.25e-07\npolygon 988               46  1.26584e+00      1.89e-06\npolygon 989               14  2.14679e-01      3.21e-07\npolygon 990               76  3.63371e+00      5.43e-06\npolygon 991              339  4.44685e+01      6.64e-05\npolygon 992              109  6.29786e+00      9.40e-06\npolygon 993               14  2.89570e-01      4.32e-07\npolygon 994               37  9.07704e-01      1.36e-06\npolygon 995               68  3.86104e+00      5.77e-06\npolygon 996              183  1.45458e+01      2.17e-05\npolygon 997               44  1.64355e+00      2.45e-06\npolygon 998               26  1.21667e+00      1.82e-06\npolygon 999               13  1.95710e-01      2.92e-07\npolygon 1000              38  1.44402e+00      2.16e-06\npolygon 1001              14  3.29394e-01      4.92e-07\npolygon 1002              14  1.79828e-01      2.69e-07\npolygon 1003              44  2.11118e+00      3.15e-06\npolygon 1004             643  1.79215e+02      2.68e-04\npolygon 1005              24  1.10979e+00      1.66e-06\npolygon 1006              30  1.57106e+00      2.35e-06\npolygon 1007              46  3.61716e+00      5.40e-06\npolygon 1008              93  7.56340e-01      1.13e-06\npolygon 1009              57  3.07732e+00      4.59e-06\npolygon 1010              18  2.16224e-01      3.23e-07\npolygon 1011              17  5.12388e-01      7.65e-07\npolygon 1012               9  7.44780e-02      1.11e-07\npolygon 1013              57  8.09122e+00      1.21e-05\npolygon 1014              16  4.01323e-01      5.99e-07\npolygon 1015             118  2.90304e+01      4.33e-05\npolygon 1016              22  4.44693e-01      6.64e-07\npolygon 1017              43  2.28763e+00      3.42e-06\npolygon 1018              27  1.15444e+00      1.72e-06\npolygon 1019              25  1.21695e+00      1.82e-06\npolygon 1020              53  4.49228e+00      6.71e-06\npolygon 1021 (hole)        8 -5.14959e-08     -7.69e-14\npolygon 1022 (hole)        3 -1.87101e-12     -2.79e-18\npolygon 1023 (hole)       31 -2.12239e-07     -3.17e-13\npolygon 1024 (hole)        6 -4.99400e-08     -7.46e-14\npolygon 1025 (hole)        8 -1.14003e-07     -1.70e-13\npolygon 1026 (hole)        4 -5.05161e-08     -7.54e-14\npolygon 1027 (hole)        4 -6.87492e-09     -1.03e-14\npolygon 1028 (hole)        6 -5.81106e-12     -8.68e-18\npolygon 1029 (hole)        7 -1.10596e-07     -1.65e-13\npolygon 1030 (hole)       14 -3.52603e-07     -5.26e-13\npolygon 1031 (hole)       14 -1.57740e-07     -2.36e-13\npolygon 1032 (hole)        6 -6.42551e-08     -9.59e-14\npolygon 1033 (hole)        4 -3.16857e-08     -4.73e-14\npolygon 1034 (hole)        8 -7.25950e-08     -1.08e-13\npolygon 1035 (hole)        4 -1.10823e-08     -1.65e-14\npolygon 1036 (hole)        8 -1.54074e-07     -2.30e-13\npolygon 1037 (hole)        4 -5.15531e-09     -7.70e-15\npolygon 1038 (hole)        7 -4.08919e-08     -6.11e-14\npolygon 1039 (hole)        9 -6.45354e-08     -9.64e-14\npolygon 1040 (hole)       13 -6.64876e-08     -9.93e-14\npolygon 1041 (hole)        5 -5.31928e-08     -7.94e-14\npolygon 1042 (hole)        6 -4.67379e-08     -6.98e-14\npolygon 1043 (hole)        4 -2.58840e-08     -3.86e-14\npolygon 1044 (hole)       10 -9.35297e-08     -1.40e-13\npolygon 1045 (hole)        4 -1.76462e-08     -2.63e-14\npolygon 1046 (hole)        4 -5.14884e-08     -7.69e-14\npolygon 1047 (hole)        3 -1.43941e-09     -2.15e-15\npolygon 1048 (hole)        8 -2.11864e-08     -3.16e-14\npolygon 1049 (hole)        4 -3.97950e-09     -5.94e-15\npolygon 1050 (hole)        3 -2.72931e-08     -4.08e-14\npolygon 1051 (hole)        4 -9.91931e-08     -1.48e-13\npolygon 1052 (hole)        9 -4.30090e-08     -6.42e-14\npolygon 1053 (hole)        3 -2.69442e-08     -4.02e-14\npolygon 1054 (hole)        6 -3.65336e-08     -5.46e-14\npolygon 1055 (hole)       10 -5.24779e-08     -7.84e-14\npolygon 1056 (hole)       12 -5.88191e-08     -8.78e-14\npolygon 1057 (hole)        8 -3.34285e-08     -4.99e-14\npolygon 1058 (hole)       18 -1.75177e-07     -2.62e-13\npolygon 1059 (hole)        6 -6.63687e-08     -9.91e-14\npolygon 1060 (hole)        3 -1.24000e-08     -1.85e-14\npolygon 1061 (hole)       10 -1.14560e-07     -1.71e-13\npolygon 1062 (hole)        5 -5.13944e-08     -7.67e-14\npolygon 1063 (hole)       10 -8.30360e-08     -1.24e-13\npolygon 1064 (hole)        4 -1.89513e-08     -2.83e-14\npolygon 1065 (hole)        6 -1.07333e-07     -1.60e-13\npolygon 1066 (hole)        3 -5.80811e-09     -8.67e-15\npolygon 1067              79  1.47390e+01      2.20e-05\npolygon 1068              54  9.35305e+00      1.40e-05\npolygon 1069 (hole)        3 -1.94488e-09     -2.90e-15\npolygon 1070 (hole)       10 -6.90517e-08     -1.03e-13\npolygon 1071 (hole)        4 -1.17368e-08     -1.75e-14\npolygon 1072 (hole)        9 -5.79029e-08     -8.65e-14\npolygon 1073 (hole)        3 -4.78872e-08     -7.15e-14\npolygon 1074 (hole)        3 -5.90058e-09     -8.81e-15\npolygon 1075 (hole)        5 -6.11108e-08     -9.12e-14\npolygon 1076           37606  6.60254e+05      9.86e-01\npolygon 1077 (hole)        5 -2.16281e-12     -3.23e-18\nenclosing rectangle: [-210.0086, 724.6476] x [1072.0263, 3158.4671] km\n                     (934.7 x 2086 km)\nWindow area = 669714 square km\nUnit of length: 1 km\nFraction of frame area: 0.343\n\n\n\n\n\nThe¬†ppp¬†object outputted from combining both the point and polygon feature results in the boundary of Myanmar outlining the plot of conflict events as shown.\n\n# Plot each masked ppp object\npar(mfrow = c(2,3), mar = c(0,0,1,0))\nfor (quarter in names(masked_ppp_list_km)) {\n  plot(masked_ppp_list_km[[quarter]], main = paste(\"Year Quarter:\", quarter))\n}\n\n\n\n\nLet‚Äôs also create combined ppp and owin objects by the districts I am interested in this study. We‚Äôll prepare the objects for the districts: Yinmarbin, Shwebo, Pakokku and Mandalay.\n\nYinmarbinShweboPakokkuMandalay\n\n\n\n# Prepare Dataset for Yinmarbin District\nconflict_yinmarbin = filter(conflict_data_sf, DT == \"Yinmarbin\")\nboundary_yinmarbin &lt;- filter(boundary_sf, DT == \"Yinmarbin\")\n\n# Create a combined ppp and owin object\nyinmarbin_owin &lt;- as.owin(boundary_yinmarbin)\nppp_obj &lt;- as.ppp(conflict_yinmarbin$geometry)\n\nppp_obj &lt;- as.ppp(st_geometry(conflict_yinmarbin))\n\n# Handle duplicates\nppp_obj &lt;- rjitter(ppp_obj, retry=TRUE, nsim=1, drop=TRUE)\n\nyinmarbin_ppp_owin &lt;- ppp_obj[yinmarbin_owin]\nyinmarbin_ppp_owin &lt;- rescale(yinmarbin_ppp_owin, 1000, \"km\")\n\n\n\n\n# Prepare Dataset for Shwebo District\nconflict_shwebo = filter(conflict_data_sf, DT == \"Shwebo\")\nboundary_shwebo &lt;- filter(boundary_sf, DT == \"Shwebo\")\n\n# Create a combined ppp and owin object\nshwebo_owin &lt;- as.owin(boundary_shwebo)\nppp_obj &lt;- as.ppp(conflict_shwebo$geometry)\n\n# Handle duplicates\nppp_obj &lt;- rjitter(ppp_obj, retry=TRUE, nsim=1, drop=TRUE)\n\nshwebo_ppp_owin &lt;- ppp_obj[shwebo_owin]\nshwebo_ppp_owin &lt;- rescale(shwebo_ppp_owin, 1000, \"km\")\n\n\n\n\n# Prepare Dataset for Pakokku District\nconflict_pakokku = filter(conflict_data_sf, DT == \"Pakokku\")\nboundary_pakokku &lt;- filter(boundary_sf, DT == \"Pakokku\")\n\n# Create a combined ppp and owin object\npakokku_owin &lt;- as.owin(boundary_pakokku)\nppp_obj &lt;- as.ppp(conflict_pakokku$geometry)\n\n# Handle duplicates\nppp_obj &lt;- rjitter(ppp_obj, retry=TRUE, nsim=1, drop=TRUE)\n\npakokku_ppp_owin &lt;- ppp_obj[pakokku_owin]\npakokku_ppp_owin &lt;- rescale(pakokku_ppp_owin, 1000, \"km\")\n\n\n\n\n# Prepare Dataset for Mandalay District\nconflict_mandalay = filter(conflict_data_sf, DT == \"Mandalay\")\nboundary_mandalay &lt;- filter(boundary_sf, DT == \"Mandalay\")\n\n# Create a combined ppp and owin object\nmandalay_owin &lt;- as.owin(boundary_mandalay)\nppp_obj &lt;- as.ppp(conflict_mandalay$geometry)\n\n# Handle duplicates\nppp_obj &lt;- rjitter(ppp_obj, retry=TRUE, nsim=1, drop=TRUE)\n\nmandalay_ppp_owin &lt;- ppp_obj[mandalay_owin]\nmandalay_ppp_owin &lt;- rescale(mandalay_ppp_owin, 1000, \"km\")\n\n\n\n\n\npar(mfrow = c(2,2), mar = c(0,0,1,0))\nplot(yinmarbin_ppp_owin, main = paste(\"Yinmarbin District\"))\nplot(shwebo_ppp_owin, main = paste(\"Shwebo District\"))\nplot(pakokku_ppp_owin, main = paste(\"Pakokku District\"))\nplot(mandalay_ppp_owin, main = paste(\"Mandalay District\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nHere I plot the districts with the highest concentration of armed conflicts. We can observe localised clustering of armed conflicts in certain towns of each district (darker spots on the map) which also indicates high frequency of conflicts occurring in these areas. On the other hand, there are also parts of each district with smaller frequency of conflicts but clusters of conflicts formed."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#st-order-spatial-point-patterns-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#st-order-spatial-point-patterns-analysis",
    "title": "Take-home Exercise 1 - Part 1",
    "section": "5. 1st Order Spatial Point Patterns Analysis",
    "text": "5. 1st Order Spatial Point Patterns Analysis\n\n5.1 Kernel Density Estimation\n\n5.1.1 Working with Fixed Bandwidth Methods\nUsing the geospatial data sets prepared, I will now perform 1st order spatial point pattern analysis by leveraging kernel density estimation (KDE) to understand the intensity of conflicts in different regions.\nI will be using a variety of fixed bandwidth methods via density() of the spatstat package, to determine the most optimal method for this analysis. Namely using bw.diggle(), bw.ppl(), bw.CvL() and bw.scott().\n\nSteps taken to calculate the KDE:\n\nExtract the masked ppp object for the current quarter.\nCompute the kernel density estimate by setting the signma parameters.\nPlot the kernel density estimate using plot() where ‚ÄúBw‚Äù represents the optimal bandwidth\n\n\nFor the purposes of identifying the most optimal bandwidth method, I will create a ppp_obj using the 2021 Q1 conflict events first to assist my decision-making.\n\n# Set Up\nppp_obj = masked_ppp_list_km$`2021 Q1`\ncolours &lt;- colorRampPalette(c(\"midnightblue\", \"skyblue\"))(100)\n\n\nbw.diggle()bw.ppl()bw.CvL()bw.scott()\n\n\nThe bw.diggle() bandwidth is referred to as Diggle‚Äôs cross-validation bandwidth which minimises the mean-squared error (MSE) to balance between under and over-smoothing.\n\n# bw.diggle()\nkde_conflict_bw_diggle &lt;- density(ppp_obj,\n                           sigma=bw.diggle,\n                           edge=TRUE,\n                           kernel=\"gaussian\")\noptimal_bw_d = floor(bw.diggle(ppp_obj)[[1]]*10)/10\nplot(kde_conflict_bw_diggle, main = paste(\"BW: diggle\", \"(\",optimal_bw_d,\"km)\"), col = colours)\n\n\n\n\nThe second bandwidth method I attempted using is bw.ppl(), This method chooses the bandwidth that minimises the likelihood cross-validation score and improving the prediction accuracy of the kernel density estimate.\n\n# bw.ppl()\nkde_conflict_bw_ppl &lt;- density(ppp_obj,\n                           sigma=bw.ppl,\n                           edge=TRUE,\n                           kernel=\"gaussian\")\noptimal_bw_p = floor(bw.ppl(ppp_obj)[[1]]*10)/10\nplot(kde_conflict_bw_ppl, main = paste(\"Bw: ppl\", \"(\",optimal_bw_p,\"km)\"), col = colours)\n\n\n\n\nThirdly, let‚Äôs explore the bandwidth method bw.CvL(), also known as Cronie and Van Lieshout cross-validation, designed to provide an optimal, adaptive bandwidth for inhomogeneous point patterns.. Similar to bw.ppl(), it aims to reduce the error measure but also aims to balance over and under-fitting based on the spatial structure of the data.\n\n# bw.CvL()\nkde_conflict_bw_CvL &lt;- density(ppp_obj,\n                           sigma=bw.CvL,\n                           edge=TRUE,\n                           kernel=\"gaussian\")\noptimal_bw_c = floor(bw.CvL(ppp_obj)[[1]]*10)/10\nplot(kde_conflict_bw_CvL, main = paste(\"Bw: CvL (\",optimal_bw_c,\"km)\"), col = colours)\n\n\n\n\nLastly, I will explore the bw.scott() bandwidth method. This method returns separate bandwidths for the x- and y-axes which is ideal for our spatial data that contains both x and y components. I will combine these bandwidths into a single value for isotropic kernel density estimation by taking the taking the geometric mean as shown in the value returned by sigma_combined.\n\n# bw.scott()\nbw_values &lt;- bw.scott(ppp_obj)\nsigma_x &lt;- bw_values[1]\nsigma_y &lt;- bw_values[2]\nsigma_combined &lt;- sqrt(sigma_x * sigma_y)\n\nkde_conflict_bw_scott &lt;- density(ppp_obj,\n                           sigma = sigma_combined,\n                           edge = TRUE,\n                           kernel = \"gaussian\")\n\noptimal_bw_s = floor(sigma_combined*10)/10\nplot(kde_conflict_bw_scott, main = paste(\"Bw: scott\", \"(\",optimal_bw_s,\"km)\"), col = colours)\n\n\n\n\n\n\nSelecting a Bandwidth Method\nPramanik N. (2019) proposes that choosing an optimal bandwidth method is crucial in fitting the data appropriately to balance between bias and variance. As such, the four methods listed below cater to different types of data depending on how varied the densities are spread across and the granularity of conflict events. Let‚Äôs explore which bandwidth method is optimal for our dataset.\n\nbw.diggle(): appears to be effective for homogeneous data in seeing general conflict hotspots and uses the smallest bandwidth size\nbw.ppl(): tends to choose slightly larger bandwidths, it provide more localised density estimates which highlights finer spatial details. As such, we can see more variability and finer details in the density distribution, with more variation between high- and low-density areas.\nbw.CvL(): the kernel density plot shows that CvL makes a good attempt in balancing between detail and smoothness, making it more suitable for capturing the overall density trends in spatial data with some local structures highlighted.\nbw.scott(): as shown, the geometric mean ensures equal smoothing in both x and y directions, and is largely similar to bw.Cvl(), making it a good choice for a balanced and general and fast overview of the spatial data distribution.\n\n\n\nPlot all bandwidth methods\npar(mfrow = c(2,2), mar = c(0,0,1,0)) \nplot(kde_conflict_bw_diggle, main = paste(\"diggle (\",optimal_bw_d,\"km)\"), col = colours)\nplot(kde_conflict_bw_ppl, main = paste(\"ppl (\",optimal_bw_p,\"km)\"), col = colours)\nplot(kde_conflict_bw_CvL, main = paste(\"CvL (\",optimal_bw_c,\"km)\"), col = colours)\nplot(kde_conflict_bw_scott, main = paste(\"scott (\",optimal_bw_s,\"km)\"), col = colours)\n\n\n\n\n# Plot histogram to compare KDE\npar(mar = c(2,2,2,2),mfrow = c(2,2))\nhist(kde_conflict_bw_diggle, main = \"diggle\")\nhist(kde_conflict_bw_ppl, main = \"ppl\")\nhist(kde_conflict_bw_CvL, main = \"scott\")\nhist(kde_conflict_bw_scott, main = \"CvL\")\n\n\n\nüí° Decision: I decided to use bw.CvL() for computing the KDE of the masked ppp objects based on each quarter. The KDE function outputs a relatively smooth density estimate that isn‚Äôt too detailed like bw.ppl() and not as generalised as bw.scott().\nThis coincides with wider range of spatial concentrations captured by the histogram, doing a good job with both capturing the non-homogeneous data spread of our Myanmar conflict data. Moreover, it isn‚Äôt as computationally heavy as bw.ppl().\n\n\n\nPutting Together our Fixed KDE using bw.CvL()\nNow, let us perform the KDE computation for the conflict events across all quarters using bw.CvL().\n\n# Calculate density using bw.CvL()\npar(mfrow = c(2,3), mar = c(0,0,1,0)) \nfor (quarter in names(masked_ppp_list_km)) {\n  ppp_obj = masked_ppp_list_km[[quarter]]\n  kde_conflict_bw &lt;- density(ppp_obj,\n                             sigma=bw.CvL,\n                             edge=TRUE,\n                             kernel=\"gaussian\")\n  optimal_bw = floor(bw.CvL(ppp_obj)[[1]]*10)/10\n  plot(kde_conflict_bw, main = paste(quarter, \"(Bw:\",optimal_bw,\"m)\"), col = colours)\n}\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nBandwidth Size: A bandwidth of around 60,000 to 100,000 is considered relatively large as compared to the bandwidth returned from using bw.diggle() and bw.ppl(). Hence, this results in a smoother density estimate with less emphasis on local clusters as indicated in the generalised spatial trends.\nNote: Using a fixed KDE is beneficial for our quarterly analysis but the bandwidth returned is different for each quarter. Later on, I‚Äôll take the average bandwidth across different time periods so we can focus on spatial distribution over time.\n\n\n\n\n\n5.1.2 Working with Different Kernel Methods\n\n\n\n\n\nBias is an inevitable issue when it comes to choosing the type of kernel function. Rajagopalan B., et al (1997) states that kernels such as Epanechnikov, Quartic and Disc suffer a ‚Äúleakage problem‚Äù where some of the probability mass is truncated at the boundary, leading to boundary bias. At the same time, the Gaussian kernel has no truncation, but small ‚Äúleakage‚Äù beyond data range of the boundary inadvertantly causes bias to exist still. Shen et al.¬†(2020) further proposes that the bandwidth is a more of an influential factor in kernel estimation than the selection of different kernel functions.\nLet us evaluate if this is true by experimenting with a variety of kernel methods as they ultimately control how we weight points within the bandwidth radius. The default kernel in density.ppp() is the gaussian. alternatives such as epanechnikov, quartic, and disc are also available. I will use the 2021 Q1 conflict data to assist in identifying the most optimal kernel method.\n\n# Set Up\npar(mfrow = c(2,2), mar = c(0,0,1,0)) \nppp_obj = masked_ppp_list_km$`2021 Q1`\n\n# Using the gaussian kernel\nkde_conflict_g &lt;- density(ppp_obj,\n                           sigma=bw.CvL,\n                           edge=TRUE,\n                           kernel=\"gaussian\")\nplot(kde_conflict_g, main=\"Gaussian Kernel\", col = colours)\n\n# Using the epanechniko kernel\nkde_conflict_e &lt;- density(ppp_obj,\n                           sigma=bw.CvL,\n                           edge=TRUE,\n                           kernel=\"epanechnikov\")\nplot(kde_conflict_e, main=\"Epanechnikov Kernel\", col = colours)\n\n# Using the quartic kernel\nkde_conflict_q &lt;- density(ppp_obj,\n                           sigma=bw.CvL,\n                           edge=TRUE,\n                           kernel=\"quartic\")\nplot(kde_conflict_q, main=\"Quartic Kernel\", col = colours)\n\n# Using the disc kernel\nkde_conflict_d &lt;- density(ppp_obj,\n                           sigma=bw.CvL,\n                           edge=TRUE,\n                           kernel=\"disc\")\nplot(kde_conflict_d, main=\"Disc Kernel\", col = colours)\n\n\n\n# Plot histogram to compare KDE\npar(mar = c(2,2,2,2),mfrow = c(2,2))\nhist(kde_conflict_g, main = \"Gaussian\")\nhist(kde_conflict_e, main = \"Epanechnikov\")\nhist(kde_conflict_q, main = \"Quartic\")\nhist(kde_conflict_d, main = \"Disc\")\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAs expected, we don‚Äôt see major variations in the smoothness and spread of KDE across a range of distances, and bias continues to be present across all kernel methods as seen from the long-tails. However, there are slight differences in the sharpness of the bandwidth radius in terms of how localised or widespread our data points are being captured.\n\nGaussian: provides a localised density estimate over the entire spatial extent as compared to epanechnikov and quartic. It is good at highlighting variance and opposing ends of conflict intensities as shown by the wider range used in the legend.\nEpanechnikov: It is more efficient than the gaussian in terms of variance but produces a slightly rougher surface. It is also more localised than the quartic kernel, focusing on areas near each point, with a sharper boundary at the bandwidth limit.\nQuartic: Results in a good balance between smoothness and localised influence, smoother than epanechnikov but with similar properties. It appears suitable for moderate smoothing and sharper focus on local patterns.\nDisc: results in the sharpest density estimate as compared to the other three kernels as all points within a certain distance are made to have equal influence and zero influence beyond that distance.\n\nDecision: Hence, I will use the quartic kernel method to ensure a relatively smooth density estimate with emphasis on local points over distant ones.\n\n\n\n5.1.3.1 Using kernel = ‚Äòquartic‚Äô\nAs such, I run the density estimate computation using kernel = 'quartic'.\n\n# Using 'quartic' kernel\npar(mfrow = c(2,3), mar = c(0,0,1,0)) \n\nfor (quarter in names(masked_ppp_list_km)) {\n  ppp_obj = masked_ppp_list_km[[quarter]]\n  kde_conflict_bw &lt;- density(ppp_obj,\n                             sigma=bw.CvL,\n                             edge=TRUE,\n                             kernel=\"quartic\")\n  optimal_bw = floor(bw.CvL(ppp_obj)[[1]]*1000)/1000\n  plot(kde_conflict_bw, main = paste(quarter, \"(Bw:\",optimal_bw,\"km)\"), col = colours)\n}\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can see high densities of armed conflict in the central and southern regions of Myanmar but more can be uncovered from conflict data. Let‚Äôs proceed to the next section.\n\n\n\n\n5.1.3.2 Using sigma = 71.831\nFor all subsequent fixed KDE computations, I will assign sigma using the average of the CvL bandwidth returned from each quarter. Here‚Äôs the calculations based on the plots returned:\n\nAverage bandwidth size = 61.649 + 64.386 + 74.501 + 114.08 + 103.863 + 103.863 + 95.567 + 56.757 + 57.752 + 66.968 + 49.649 + 48.135 + 48.135 + 60.323) / 14 = 71.831\n\nLet‚Äôs recompute the Fixed KDE based on the newly calculated average bandwidth such that sigma = 71.831. I‚Äôll store the quarterly KDE outputs into a list called kde_conflict_bw_list.\n\n# Add KDE into this list\nkde_conflict_bw_list &lt;- list()\nfor (quarter in names(masked_ppp_list_km)) {\n  ppp_obj = masked_ppp_list_km[[quarter]]\n  kde_conflict_bw &lt;- density(ppp_obj,\n                             sigma=71.831,\n                             edge=TRUE,\n                             kernel=\"quartic\")\n  kde_conflict_bw_list[[quarter]] &lt;- kde_conflict_bw\n}\n\n\n# Plot graph\npar(mfrow = c(2,3), mar = c(0,0,1,0)) \n\nfor (quarter in names(kde_conflict_bw_list)){\n  kde_conflict_bw &lt;- kde_conflict_bw_list[quarter]\n  plot(kde_conflict_bw, main = paste(quarter, \"(Bw: 71.831 km)\"), col = colours)\n}\n\n\n\n\n\n\n\n5.1.3 Working with Adaptive KDE\n\n\n\n\n\nAs seen above, fixed bandwidths tend to oversmooth the mode of the distribution. On the contrary, the adaptive kernel estimate has the ability to reduce variability of estimates in areas with low density and increases it in areas with higher density (The Stata Journal, 2003).\nOnce again, let us use the 2021 Q1 conflict data to illustrate the difference in outputs of all three adaptive methods.\n\nVoronoi Adaptive KDEAdaptive KDENearest Neighbour Adaptive KDE\n\n\n\n# Set Up\nppp_obj = masked_ppp_list_km$`2021 Q1`\n\n# Using Voronoi adaptive KDE\nvd_adaptive_kde &lt;- adaptive.density(ppp_obj, f=1, method=\"voronoi\")\n\n# Plot\npar(mar = c(0,1,1,1))\nplot(vd_adaptive_kde, main = \"Voronoi-Dirichlet Adaptive KDE\", col = colours)\n\n\n\n\n\n# Using adaptive KDE\nadaptive_kde &lt;- adaptive.density(ppp_obj, method=\"kernel\")\n\n# Plot\npar(mar = c(0,1,1,1))\nplot(adaptive_kde, main = \"Adaptive KDE\", col = colours)\n\n\n\n\nI‚Äôve used a relatively larger number of neighbours (i.e.¬†k = 10) to provide a smoother, more general density estimate to capture broader trends and may smooth out details.\n\n# Using nearest neighbour adaptive KDE\nnn_kde &lt;- nndensity(ppp_obj, k=10)\n\n# Plot\npar(mar = c(0,1,1,1))\nplot(nn_kde, main = \"Nearest Neighbour Adaptive KDE\", col = colours)\n\n\n\n\n\nWe can also compare the performance of each method based on the top 4 states with highest proportion of conflicts as highlighted earlier.\n\nVoronoi Adaptive KDEAdaptive KDENearest Neighbour Adaptive KDE\n\n\n\n\n\n\n\n\n\n\n\n\nComparing the three Adaptive KDE Types\nFrom the outputs above, it appears that there is no major differences between the distribution of KDE values returned across the three methods, where there is high concentration of points in a specific area. Hence, we will choose to go with¬†Adapative Kernel¬†method.\n\n\n\n\n\nLet‚Äôs compare the results of my two selected fixed and adaptive KDEs (E.g. Magway District)\n\n\nWe can observe how adaptive kernels provides a more detailed picture of conflict spatial distribution but since it‚Äôs largely localised, conflict spots require more effort in identifying and can be computationally heavy for this exercise.\nAdditionally, varying bandwidth makes comparisons across regions or time periods (like quarters) more difficult because the scale of smoothing is not constant across space and time.\n\n\n\n5.1.4 Converting Gridded KDE Output into Raster\nNext, we need to convert the KDE output to KDE raster layers before it can be viewed using tmap.\nStep 1) Converting KDE to Spatial Grid Data Frame\n\nlibrary(grid)\nplot_list &lt;- list()\nfor (quarter in names(kde_conflict_bw_list)) {\n  ppp_obj &lt;- kde_conflict_bw_list[[quarter]]\n  gridded_ppp_obj &lt;- as(ppp_obj, \"SpatialGridDataFrame\")\n  plot_list[[quarter]] &lt;- spplot(gridded_ppp_obj, main = paste(quarter), col.regions = colours)\n}\n\nlibrary(gridExtra)\nplot_list_subset1 &lt;- plot_list[1:6]\nplot_list_subset2 &lt;- plot_list[7:12]\nplot_list_subset3 &lt;- plot_list[13:14]\ngrid.newpage()\ngrid.arrange(grobs = plot_list_subset1, ncol = 3, nrow = 2)\ngrid.newpage() \ngrid.arrange(grobs = plot_list_subset2, ncol = 3, nrow = 2)\ngrid.newpage()\ngrid.arrange(grobs = plot_list_subset3, ncol = 3, nrow = 2)\n\n\n\n\nStep 2) Rasterisation of Grid Outputs & Assigning Projection Systems\n\ngridded_ppp_obj_raster_list &lt;- list()\nfor (quarter in names(kde_conflict_bw_list)) {\n  gridded_ppp_obj = kde_conflict_bw_list[[quarter]]\n  gridded_ppp_obj_raster &lt;- raster(gridded_ppp_obj)\n  projection(gridded_ppp_obj_raster) &lt;- CRS(\"+init=EPSG:32647\")\n  gridded_ppp_obj_raster_list[[quarter]] &lt;- gridded_ppp_obj_raster\n}\n\n# Inspect for 2024 Q2\ngridded_ppp_obj_raster_list$`2024 Q2`\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 7.302001, 16.30032  (x, y)\nextent     : -210.0086, 724.6476, 1072.026, 3158.467  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=47 +datum=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : 7.061861e-06, 0.01847887  (min, max)\n\n\nStep 3) Plot Maps\n\ntmap_mode(\"plot\")\nplots_by_quarter &lt;- list()\nfor (quarter in names(gridded_ppp_obj_raster_list)){\n  gridded_ppp_obj_raster = gridded_ppp_obj_raster_list[[quarter]]\n  raster_plot &lt;- tm_shape(gridded_ppp_obj_raster) +\n    tm_raster(\"layer\", title=\"Density\", palette=\"Blues\") +\n    tm_layout(legend.position = c(\"left\",\"bottom\"),frame=FALSE, main.title = quarter,\n            main.title.size=1, main.title.position = \"center\", legend.text.size = 0.5,legend.title.size = 0.7) \n  plots_by_quarter[[quarter]] &lt;- raster_plot\n}\n\ntmap_arrange(plots_by_quarter[1:14], ncol=5, nrow=3)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nPlotting raster grid versions of KDE outputs uses discrete colour ranges which does a good job in highlighting gradual changes in conflict events across an area. Since 2021 Q2, more conflicts are seen in Southern parts of Myanmar. The density range differs for each quarter but we can see an increase in no. of armed conflicts per kilometre from 2021 Q1 to 2022 Q2, which stagnates in density and increases again in 2023 Q4.\n\n\n\n\n\n5.2 Nearest Neighbour Analysis\nOur current analyses does not reveal patterns of clustering or dispersion, to which Michael J. Crawley proposes to employ Clark-Evans test spatial randomness for its simplicity and applicability for first-order spatial analysis, which means checking for overall spatial randomness based on nearest-neighbor distances. (Crawley M. J. , 2007)\n\n\n\n\n\nClark-Evans Test\nThe test checks whether the observed point pattern of armed conflicts in Myanmar shows clustering (points are closer than expected under randomness), dispersion (points are more spread out), or randomness.\n\n5.2.1 Clark-Evans Test (Myanmar)\nThe test hypotheses are:\n\nHo = The distribution of armed conflicts in Myanmar are randomly distributed.\nH1= The distribution of armed conflicts in Myanmar are not randomly distributed.\nThe 95% confident interval will be used.\n\nWe will conduct the test using clarkevans.test() of statspat.\n\nfor (quarter in names(masked_ppp_list_km)) {\n  ppp_obj = masked_ppp_list_km[[quarter]]\n  print(quarter)\n  print(clarkevans.test(ppp_obj,\n                  correction=\"none\",\n                  clipregion=\"boundary_sf\",\n                  alternative=c(\"clustered\"),\n                  nsim=99))\n}\n\n[1] \"2024 Q2\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.26663, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2024 Q1\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.23563, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2023 Q4\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.21795, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2023 Q3\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.22002, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2023 Q2\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.24485, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2023 Q1\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.24365, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2022 Q4\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.22139, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2022 Q3\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.23974, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2022 Q2\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.22989, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2022 Q1\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.21976, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2021 Q4\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.21341, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2021 Q3\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.21808, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2021 Q2\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.17458, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2021 Q1\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.24696, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFor a 95% confidence level, If the p-value &lt; 0.05, I will reject the null hypothesis of complete spatial randomness and check if data is uniform (R &gt; 1) or clustered (R &lt; 1).\nWith that said, all tests conducted across each quarter rejects the null hypothesis as p &lt; 0.05 and spatial points are found to be clustered since R &lt; 1.\n\n\n\n\n5.2.2 Clark-Evans Test (Top 4 Districts)\n\n# Yinmarbin District\nclarkevans.test(yinmarbin_ppp_owin,\n                  correction=\"none\",\n                  clipregion=\"boundary_yinmarbin\",\n                  alternative=c(\"clustered\"),\n                  nsim=39)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  yinmarbin_ppp_owin\nR = 0.13718, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n# Shwebo District\nclarkevans.test(shwebo_ppp_owin,\n                  correction=\"none\",\n                  clipregion=\"boundary_shwebo\",\n                  alternative=c(\"clustered\"),\n                  nsim=39)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  shwebo_ppp_owin\nR = 0.20141, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n# Pakokku District\nclarkevans.test(pakokku_ppp_owin,\n                  correction=\"none\",\n                  clipregion=\"boundary_pakokku\",\n                  alternative=c(\"clustered\"),\n                  nsim=39)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  pakokku_ppp_owin\nR = 0.20698, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n# Mandalay District\nclarkevans.test(mandalay_ppp_owin,\n                  correction=\"none\",\n                  clipregion=\"boundary_mandalay\",\n                  alternative=c(\"clustered\"),\n                  nsim=39)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  mandalay_ppp_owin\nR = 0.16014, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nI will reject the null hypothesis of complete spatial randomness since the p values of each district is smaller than 0.05. Additionally, we can observe clustering of spatial points since R &lt; 1 is returned for all districts. However, this test isn‚Äôt sufficient in highlighting the statistical significance of the spatial patterns - I‚Äôll use the Monte Carlo Simulation later to handle this.\n\n\n\n\n\n5.3 Further Data Exploration\nBy using the fixed KDE with CvL bandwidth and quartic kernel, let‚Äôs see what insights can we glean from the density of conflicts in Myanmar.\n\n5.3.1 KDE by Event Type\nFirst, let‚Äôs identify the unique event types in this dataset.\n\n# Check unique events\nunique(conflict_data_sf$event_type)\n\n[1] \"Battles\"                    \"Strategic developments\"    \n[3] \"Violence against civilians\" \"Explosions/Remote violence\"\n\n\nNow, let us analyse the kernel density estimate of each unique event type found in conflict_data_sf to identify hot and cold spots across Myanmar.\n\n\nPlot the KDE based on Event Type\n# Set Up\npar(mfrow = c(2,2), mar = c(0,0,1,0)) \n\nconflict_data_sf %&gt;%\n  group_by(event_type) %&gt;%\n  group_split() -&gt; conflict_by_event_type\n\n# Convert the sf object to owin\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_sf))\n\nkde_list &lt;- lapply(seq_along(conflict_by_event_type), function(i) {\n  data &lt;- conflict_by_event_type[[i]]\n  event_type &lt;- unique(data$event_type)\n  ppp_obj &lt;- as.ppp(st_geometry(data), W = district_boundary)\n  ppp_obj &lt;- rescale(ppp_obj, 1000, \"km\")\n  kde &lt;- density(ppp_obj,\n                 sigma=71.831,\n                 edge=TRUE,\n                 kernel=\"quartic\")\n  plot(kde, main = paste(event_type), col=colours)\n  return(kde)\n})\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can almost see an equal spread of all four event types, with explosions and strategic violence being more dominantly found in Central-Western Myanmar, in the Sagaing and Mandalay states, followed by battles and violence against citizens. All events seem to plaque the Southern states (e.g.¬†Yangon) with the exception of battles.\n\n\n\n\n5.3.2 KDE Across Top 4 States With Most Conflicts\nPreviously, we identified the top 4 states with the highest proportions of conflicts as Sagaing, Mandalay, Magway and Yangon. We can delve deeper into each state by analysing the intensity of conflicts across these states using density().\n\n\nPlot the KDE of Top 4 States\n# Set Up\npar(mfrow = c(2,2), mar = c(0,0,1,0))\n\n# Sagaing\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_sagaing))\nppp_obj &lt;- as.ppp(st_geometry(conflict_sagaing), W = district_boundary)\nppp_obj_sagaing &lt;- rescale(ppp_obj, 1000, \"km\")\nkde &lt;- density(ppp_obj_sagaing,\n               sigma=71.831,\n               edge=TRUE,\n               kernel=\"quartic\")\nplot(kde, main = paste(\"Sagaing\"), col=colours)\n\n# Mandalay\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_mandalay))\nppp_obj &lt;- as.ppp(st_geometry(conflict_mandalay), W = district_boundary)\nppp_obj_mandalay &lt;- rescale(ppp_obj, 1000, \"km\")\nkde &lt;- density(ppp_obj_mandalay,\n               sigma=71.831,\n               edge=TRUE,\n               kernel=\"quartic\")\nplot(kde, main = paste(\"Mandalay\"), col=colours)\n\n# Magway\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_magway))\nppp_obj &lt;- as.ppp(st_geometry(conflict_magway), W = district_boundary)\nppp_obj_magway &lt;- rescale(ppp_obj, 1000, \"km\")\nkde &lt;- density(ppp_obj_magway,\n               sigma=71.831,\n               edge=TRUE,\n               kernel=\"quartic\")\nplot(kde, main = paste(\"Magway\"), col=colours)\n\n# Yangon\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_yangon))\nppp_obj &lt;- as.ppp(st_geometry(conflict_yangon), W = district_boundary)\nppp_obj_yangon &lt;- rescale(ppp_obj, 1000, \"km\")\nkde &lt;- density(ppp_obj_yangon,\n               sigma=71.831,\n               edge=TRUE,\n               kernel=\"quartic\")\nplot(kde, main = paste(\"Yangon\"), col=colours)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt‚Äôs interesting that armed conflict isn‚Äôt evenly distributed across the states though it does seem that armed conflict has inflicted the entire state of Yangon. Nonetheless, it is worth noting that Yangon is relatively smaller in size than the other three states and that will increase the density of conflict quite significantly.\n\n\n\n\n5.3.3 KDE of Top 4 States by Event Type\nIt‚Äôll also be interesting to breakdown each top 4 state by the event type category as shown.\n\nSagaingMandalayMagwayYangon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe kernel density of violence against civillians is generally found to be the lowest amongst all conflict events. Additionally, all types of armed conflicts tend to occur repeatedly in the same parts of each state. E.g. conflicts regarding strategic development tend to happen in Southern part of the Sagaing state, just as it is for explosions/remote violence.\n\n\n\n\n5.5.4 KDE by Interaction Type\n\n# Convert Interaction Code to Text\nunique(conflict_data_sf$interaction)\n\n [1] 13 10 12 37 70 17 22 11 33 30 27 20 23 60 47 28 38 80 78 18 24 14\n\n\n\nlibrary(dplyr)\n\n# Create a named vector for mapping interaction values\ninteraction_map &lt;- c(\n  \"10\" = \"SOLE STATE FORCES ACTION\",\n  \"11\" = \"STATE FORCES VERSUS STATE FORCES\",\n  \"12\" = \"STATE FORCES VERSUS REBELS\",\n  \"13\" = \"STATE FORCES VERSUS POLITICAL MILITIA\",\n  \"14\" = \"STATE FORCES VERSUS IDENTITY MILITIA\",\n  \"17\" = \"STATE FORCES VERSUS CIVILIANS\",\n  \"18\" = \"STATE FORCES VERSUS EXTERNAL/OTHER FORCES\",\n  \"20\" = \"SOLE REBEL ACTION\",\n  \"22\" = \"REBELS VERSUS REBELS\",\n  \"23\" = \"REBELS VERSUS POLITICAL MILITIA\",\n  \"24\" = \"REBELS VERSUS IDENTITY MILITIA\",\n  \"27\" = \"REBELS VERSUS CIVILIANS\",\n  \"28\" = \"REBELS VERSUS OTHERS\",\n  \"30\" = \"SOLE POLITICAL MILITIA ACTION\",\n  \"33\" = \"POLITICAL MILITIA VERSUS POLITICAL MILITIA\",\n  \"34\" = \"POLITICAL MILITIA VERSUS IDENTITY MILITIA\",\n  \"37\" = \"POLITICAL MILITIA VERSUS CIVILIANS\",\n  \"38\" = \"POLITICAL MILITIA VERSUS OTHERS\",\n  \"40\" = \"SOLE IDENTITY MILITIA ACTION\",\n  \"44\" = \"IDENTITY MILITIA VERSUS IDENTITY MILITIA\",\n  \"47\" = \"IDENTITY MILITIA VERSUS CIVILIANS\",\n  \"48\" = \"IDENTITY MILITIA VERSUS OTHER\",\n  \"70\" = \"SOLE CIVILIAN ACTION\",\n  \"77\" = \"CIVILIANS VERSUS CIVILIANS\",\n  \"78\" = \"OTHER ACTOR VERSUS CIVILIANS\",\n  \"80\" = \"SOLE OTHER ACTION\",\n  \"88\" = \"OTHER VERSUS OTHER\"\n)\n\nconflict_data_sf &lt;- conflict_data_sf %&gt;%\n  rename(interaction_code = interaction) %&gt;%\n  mutate(interaction = interaction_map[as.character(interaction_code)])  \n\n# View the updated dataframe\nhead(conflict_data_sf[c(\"interaction_code\",\"interaction\")])\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 76997.72 ymin: 2428487 xmax: 214961 ymax: 2533434\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 6 √ó 3\n  interaction_code interaction                                   geometry\n             &lt;dbl&gt; &lt;chr&gt;                                      &lt;POINT [m]&gt;\n1               13 STATE FORCES VERSUS POLITICAL MILI‚Ä¶   (214961 2452068)\n2               13 STATE FORCES VERSUS POLITICAL MILI‚Ä¶ (198303.2 2499463)\n3               13 STATE FORCES VERSUS POLITICAL MILI‚Ä¶ (189105.4 2533434)\n4               10 SOLE STATE FORCES ACTION            (160913.9 2522331)\n5               13 STATE FORCES VERSUS POLITICAL MILI‚Ä¶   (146213 2428487)\n6               10 SOLE STATE FORCES ACTION            (76997.72 2447719)\n\n\n\n1. Involvement of Political Militia\nThe conflict landscape in Myanmar is complex and fragmented, with political militias focused on advancing political agendas or controlling governance structures. Let‚Äôs discover how the density of armed conflicts differ for each interaction of the political militia. They can be associated as either of these groups.\n\nPro-Government Militias: supporting the central government or Tatmadaw, such as Pyithu Sit and BGFs.\nPro-Democracy Militias: emerged as part of the resistance movement against the military junta. E.g. the People‚Äôs Defence Forces (PDF)\n\n\n\nPlot conflicts involving political militia\ncodes &lt;- c('13','23','33')\npar(mfrow = c(3,1), mar = c(0,0,1,0)) \n\nfor(code in codes) {\n  # Filter for the specific interaction type\n  filtered_data &lt;- conflict_data_sf %&gt;%\n    filter(interaction_code == code)\n  \n  # Convert to ppp object\n  boundary_owin &lt;- as.owin(boundary_sf)\n  ppp_owin_obj &lt;- as.ppp(filtered_data$geometry, W = boundary_owin)\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  \n  # Compute KDE\n  kde &lt;- density(ppp_owin_obj_km,\n                 sigma=71.831,\n                 edge=TRUE,\n                 kernel=\"quartic\")\n  \n  # Plot the KDE\n  plot(kde, main = paste(interaction_map[code]), col = colours)\n}\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe political army or organisations tend to be less involved with unarmed civilians but state forces, rebels and other policial militia.\n\nAgainst State Forces\n\n\nHigh density of conflicts with state forces (e.g.¬†police) in central and western Myanmar\nThese regions have significant ethnic diversity and political grievances. E.g. Rakhine State in the west is home to ethnic Rakhine groups who have historically sought greater autonomy and have been involved in conflicts with state forces.\nCentral regions, such as Sagaing and Magway, have also seen intense resistance against state forces, particularly after the 2021 military coup.\n\n\nAgainst Rebels\n\n\nRebels and political militias operate throughout Myanmar with varying objectives but of a much smaller intensity than with other actors (seen from its low density scale).\nClashes between rebels and political militias can stem from differing political goals, territorial disputes, or strategic interests.\nFor instance, a rebel group might view a political militia as a rival in a contested region or as a competitor for local support.\n\n\nAgainst other Political Miliitia\n\n\nIn central and western Myanmar, where political militias are more active, there is often competition among different resistance groups. These militias may have different leadership, ideologies, or goals, leading to clashes.\n\n\n\n\n\n2. Involvement of Identity Militia\nIdentity militias are typically organised around ethnic, religious, or cultural identities. heir main goal is to protect the interests, rights, and autonomy of specific identity-based groups within Myanmar.\n\n\nPlot conflicts involving identity militia\ncodes &lt;- c('14','24')\npar(mfrow = c(2,1), mar = c(0,0,1,0)) \n\nfor(code in codes) {\n  # Filter for the specific interaction type\n  filtered_data &lt;- conflict_data_sf %&gt;%\n    filter(interaction_code == code)\n  \n  # Convert to ppp object\n  boundary_owin &lt;- as.owin(boundary_sf)\n  ppp_owin_obj &lt;- as.ppp(filtered_data$geometry, W = boundary_owin)\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  \n  # Compute KDE\n  kde &lt;- density(ppp_owin_obj_km,\n                 sigma=71.831,\n                 edge=TRUE,\n                 kernel=\"quartic\")\n  \n  # Plot the KDE\n  plot(kde, main = paste(interaction_map[code]), col = colours)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nArmed conflicts involving the identity militia is of a much smaller intensity than political militia with about 5e^-0.5 or 3 conflict events per kilometre.\n\nState Forces vs Identity Militia\n\n\nConflicts between these two parties are concerntrated in the Western region which has a history of conflict, including uprisings against colonial and post-colonial governments.\nAdditionally, the region‚Äôs proximity to Bangladesh has some influence on local conflicts as ethnic groups in Myanmar may have connections or support from across the border.\nIn the Rakhine State of western Myanmar, historical grievances and demands for autonomy has also fueled tensions between these groups and the central government (Crisis Group, 2024).\n\n\nRebels vs identity militia\n\n\nHigher intensity of armed conflicts are seen in North Myanmar\nNorthern Myanmar hosts various ethnic armed groups and rebel factions that seek autonomy or independence. (E.g. Kachin, Shan and Chin)\nNorthern Myanmar is rich in natural resources, e.g.¬†jade and timber. Control over these resources can be a significant driver of conflict, as various armed groups vie for control and when there‚Äôs a lack of economic opportunities. (Fishbein & Lusan, 2022)\nUnresolved conflicts from colonial and post-colonial periods continue to influence current tensions between different ethnic groups and armed factions in the Northen region.\n\n\n\n\n\n3. Involvement of Civilian Actors\n\n\nPlot conflicts involving civilians\ncodes &lt;- c('17', '27', '37', '47', '78')\npar(mfrow = c(3,2), mar = c(0,0,1,0)) \n\nfor(code in codes) {\n  # Filter for the specific interaction type\n  filtered_data &lt;- conflict_data_sf %&gt;%\n    filter(interaction_code == code)\n  \n  # Convert to ppp object with boundary window\n  boundary_owin &lt;- as.owin(boundary_sf)\n  ppp_obj &lt;- as.ppp(filtered_data$geometry, W = boundary_owin)\n  \n  # Rescale the ppp object\n  ppp_obj_km &lt;- rescale(ppp_obj, 1000, \"km\")\n  \n  # Compute KDE\n  kde &lt;- density(ppp_obj_km,\n                 sigma = 71.831,\n                 edge = TRUE,\n                 kernel = \"quartic\")\n  \n  # Plot the KDE\n  plot(kde, main = paste(interaction_map[code]), col = colours)\n}\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAcross all actors, we see that perhaps civilians are involved with the biggest range of actors.\nNotably, we see the lowest intensity of clashes with identity militia which could stem from how civilians might be part of the same social/ethnic networks as the identity militia and these militia may seek the support of civilian populations too.\nIn contrast, state forces and civilians have the highest intensity of armed conflicts\n\nIn Western Rakhine State, Buddhist-majority and Muslim-minority communities have experienced significant tension causing state forces to act in response to these tensions which has led to severe clashes with civilian populations.\nReports of human rights abuses, including arbitrary arrests, torture, and extrajudicial killings by state forces, has led to large-scale displacement and humanitarian crises, further exacerbating tensions between state forces and civilians. The struggle for resources and safety often intensifies the conflicts. (Farge E. & Mantovani C., 2024)\n\n\n\n\n\n4. Civilians Involvement by Event Type\n\n\nCompute KDE of Civilians by Event Type\n# Set Up\npar(mfrow = c(2,2), mar = c(0,0,1,0)) \ncodes &lt;- c('17', '27', '37', '47', '78')\n\nconflict_data_sf %&gt;%\n  filter(interaction_code %in% codes) %&gt;%\n  group_by(event_type) %&gt;%\n  group_split() -&gt; conflict_by_event_type\n\n# Convert the sf object to owin\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_sf))\n\nkde_list &lt;- lapply(seq_along(conflict_by_event_type), function(i) {\n  data &lt;- conflict_by_event_type[[i]]\n  event_type &lt;- unique(data$event_type)\n  ppp_obj &lt;- as.ppp(st_geometry(data), W = district_boundary)\n  ppp_obj &lt;- rescale(ppp_obj, 1000, \"km\")\n  kde &lt;- density(ppp_obj,\n                 sigma=71.831,\n                 edge=TRUE,\n                 kernel=\"quartic\")\n  plot(kde, main = paste(event_type), col=colours)\n  return(kde)\n})\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nCivilians are seen to be embroiled mostly in conflicts resulting from strategic development and violence against civilians events, primarily in Central and Western Myanmar. Fortunately, conflicts involving explosions or remote violence is not as intense against civilians but this raises a great concern on the humanitarian crisis faced by civilians in Myanmar.\n\n\n\n\n\n5.5.5 OpenStreetMap Myanmar - Spatial Points\nUsing tmap functions, I will display an interactive view of te KDE layers on openstreetmap of Myanmar to observe intensity of conflicts at the district level. We can see higher intensities of conflict in central districts (e.g.¬†Yinmarbin, Pakokku and Shwebo) and western districts (e.g.¬†Yangon).\n\nppp_obj&lt;- as.ppp(conflict_data_sf$geometry)\nppp_owin_obj &lt;- ppp_obj[myanmar_owin]\nppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n\nkde_fixed &lt;- density(ppp_owin_obj_km, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nraster_kde_fixed &lt;- raster(kde_fixed)\n\nprojection(raster_kde_fixed) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\n\n\n# Plot KDE Map on OpenStreetMap\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\nkde_fixed_output &lt;- tm_basemap(server = \"OpenStreetMap.HOT\") +\n  tm_basemap(server = \"Esri.WorldImagery\") +\n  tm_shape(raster_kde_fixed) +\n  tm_raster(\"layer\",\n            n = 10,\n            title = \"KDE Fixed CvL\",\n            alpha = 0.6,\n            palette = c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\")) +\n  tm_shape(boundary_sf)+\n  tm_polygons(alpha=0.1,id=\"DT\")+\n  tmap_options(check.and.fix = TRUE)\n\nkde_fixed_output\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#overview",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#overview",
    "title": "Take-home Exercise 1 - Part 1",
    "section": "",
    "text": "The conflict in Myanmar is not just a result of the coup but is deeply rooted in the country‚Äôs decades-old complex ethnic and political landscape, characterised by tensions between the central government and various ethnic minority groups, each with its own armed forces. The post-coup violence has exacerbated these long-standing conflicts, leading to a severe humanitarian crisis, with thousands killed, hundreds of thousands displaced, and widespread human rights abuses reported.\n\n\n\nAs such, Geospatial analytics has become a valuable tool for evaluating and comprehending the intricacies of increasing conflicts. This exercise aims to reveal the spatial and spatio-temporal distribution of armed conflict in Myanmar by leveraging spatial point pattern analysis. Additionally, it aims to gain clearer insights into the geographical and logistical patterns of violence throughout the nation.\nBy the end of this take-home exercise, I aim to complete these steps in my spatial point pattern analysis in uncovering the distribution of armed conflict in Myanmar.\n\nUsing appropriate function of sf and tidyverse packages, import and transform the downloaded armed conflict data and administrative boundary data into sf tibble data.frames.\nUsing the geospatial data sets prepared, derive quarterly KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatial Point Patterns Analysis.\nUsing the geospatial data sets prepared, derive quarterly spatio-temporal KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatio-temporal Point Patterns Analysis.\nUsing appropriate tmap functions, display the KDE and Spatio-temporal KDE layers on openstreetmap of Myanmar.\nDescribe the spatial patterns revealed by the KDE and Spatio-temporal KDE maps.\n\n\n\n\n\n\nThis Armed Conflict Location & Event Data (ACLED) is an independent, impartial, international non-profit organisation which owns an extensive database of violent conflict and protest in countries and territories around the world.\n\nFor the purpose of this exercise, I have downloaded ACLED‚Äôs data on Myanmar which includes a series of conflict events, particularly between 1 January 2021 to 30 June 2024.\nüîó Source: ACLED\nüìÅ Format: comma separated values (CSV)\nAs the dataset is rather extensive, I will be performing my analysis on armed conflict events in a quarterly basis to streamline my tasks. The data included in this dataset are as follows:\n\n\n\nEvent Type\nACLED categorises events into various types. I will mainly be focusing on these four event types: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\n\nevent_id_cnty: unique ID for each conflict\nevent_type: category of event e.g.¬†Battle, Violence Against Civilians, Protests, Explosions/Remote Violence, Strategic Developments\nsub_event_type: a more detailed classification within event type\ndisorder_type: classifies the event based on the nature of the disorder e.g.¬†political violence, demonstrations, strategic developments[A1]\ncivilian_targeting: yes/no value, whether event involves specifically targeting civilians\nNote: when ‚Äústrategic developments‚Äù are used in Event Type, it is also used in the disorder type (vice-versa)\n\n\n\nLocation and Geospatial Data\nThe database provides detailed geographic information, pinpointing the exact or approximate locations of conflict events across Myanmar. This includes cities, towns, and rural areas.\n\niso: the country code for Myanmar which uses 104 in this case\nregion: region of conflict within Myanmar\ncountry: indicates Myanmar\nadmin1, admin2, admin3: 1st, 2nd and 3rd level administration division within Myanmar e.g.¬†states, division, sub-division\nlocation: specific geographic location or name of the place where the conflict event occurred\nlatitude: latitude of the conflict event\nlongitude: longitude of the conflict event\ngeo_precision: indicates the level of precision for the geographic coordinates provided\n\n\n\nDate and Time\nACLED records the specific dates and, where possible, times of conflict events.\n\nevent_date: date of conflict\nyear: year of conflict\ntime_precision: accuracy of the date and time information provided\n\n\n\nActors\n\nIndicate the actors involved in the conflict, such as the Tatmadaw (Myanmar‚Äôs military), ethnic armed organizations, local militias, civilian protestors, and other groups.\nactor1: primary actor involved in the conflict event. E.g. a government force, rebel group, militia, or any organised entity\nassoc_actor_1: a secondary group that is aligned with or supports the primary actor (Actor1) in the event\ninter1: an interaction code that categorises actor1, could be a government force, rebel group, military force, rioter, civilian, or other entities\ninteraction:¬†combined description of actor1 and actor2 (no particular order of aggression)\n\n\n\nFatalities\n\nfatalities:¬†tracks the number of reported fatalities associated with each conflict event\n\n\n\nOthers\n\nsource: source of information for the conflict event\nsource_scale: scale of the source e.g.¬†local, national, international\nnotes : additional comments\ntags: keywords associated with the conflict event\ntimestamp: date and time when conflict event was entered/updated in the database\n\n\n\n\n\n\n\nI will also be using a geospatial dataset from the Myanmar Information Management Unit (MIMU) in shapefile (.shp) format, specifically of the Myanmar state at the 2nd administrative level with district boundaries.\nüîó Source: MIMU\nüìÅ Format: shapefile (.shp)\nMy reason for choosing the district boundary dataset is that we do not want to select a boundary dataset that is too generalised when analysing conflict events since it might not provide sufficient insights to trends where conflict events happen. Neither do we want to analyse a geography that is too detailed (e.g.¬†Admin 3 - townships) since it can be computationally inefficient as seen in the types of boundary data below.\n\n\n\n\n\nI have donwloaded the two data sets and organised them into my folder as follows."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#spatio-tempmoral-kde",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#spatio-tempmoral-kde",
    "title": "Take-home Exercise 1",
    "section": "7. Spatio-Tempmoral KDE",
    "text": "7. Spatio-Tempmoral KDE\nWe focus on the continuous time"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#overall",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#overall",
    "title": "In-class Exercise 3",
    "section": "1. Overall",
    "text": "1. Overall\n\n1.1 The research questions\nThe specific question we would like to answer is:\n\nare the locations of forest fire in Kepulauan Bangka Belitung spatial and spatio-temporally independent?\nif the answer is NO, where and when the observed forest fire locations tend to cluster?\n\n\n\n1.2 The data\nFor the purpose of this exercise, two data sets are used, they are:\n\nforestfires, a csv file provides locations of forest fire detected from the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor data. The data are downloaded from Fire Information for Resource Management System. For the purpose of this exercise, only forest fires within Kepulauan Bangka Belitung will be used.\nKepulauan_Bangka_Belitung, an ESRI shapefile showing the sub-district (i.e.¬†kelurahan) boundary of Kepulauan Bangka Belitung. The data set was downloaded from Indonesia Geospatial portal. The original data covers the whole Indonesia. For the purpose of this exercise, only sub-districts within Kepulauan Bangka Belitung are extracted."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#installing-and-loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#installing-and-loading-the-r-packages",
    "title": "In-class Exercise 3",
    "section": "2. Installing and Loading the R packages",
    "text": "2. Installing and Loading the R packages\nFor the purpose of this study, I will be using these five R packages. They are:\n\nrgdal for importing geospatial data in GIS file format such as shapefile into R and save them as Spatial*DataFrame,\nmaptools for converting Spatial* object into ppp object,\nraster for handling raster data in R,\nspatstat for performing Spatial Point Patterns Analysis such as kcross, Lcross, etc., and\ntmap for producing cartographic quality thematic maps.\n\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#importing-and-preparing-study-area",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#importing-and-preparing-study-area",
    "title": "In-class Exercise 3",
    "section": "3. Importing and Preparing Study Area",
    "text": "3. Importing and Preparing Study Area\n\n3.1 Importing Study Area\nLet us first import the data using the st_read() function.\n\nkbb &lt;- st_read(dsn=\"data/rawdata\",\n               layer = \"Kepulauan_Bangka_Belitung\") \n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex4\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nkbb\n\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   OBJECTID         NAMOBJ      FCODE REMARK\n1     26195        Airbara BA03070040   &lt;NA&gt;\n2     26196       Airgegas BA03070040   &lt;NA&gt;\n3     26197    Arung Dalam BA03070040   &lt;NA&gt;\n4     26202 Batu Betumpang BA03070040   &lt;NA&gt;\n5     26205      Bedengung BA03070040   &lt;NA&gt;\n6     26206      Belimbing BA03070040   &lt;NA&gt;\n7     26207         Bencah BA03070040   &lt;NA&gt;\n8     26209          Berok BA03070040   &lt;NA&gt;\n9     26210         Bikang BA03070040   &lt;NA&gt;\n10    26212    Bukit Terap BA03070040   &lt;NA&gt;\n                                       METADATA SRS_ID KDBBPS KDCBPS   KDCPUM\n1  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.03\n2  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.03\n3  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.04.01\n4  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.07\n5  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.05\n6  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.04.06\n7  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.03\n8  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.04.01\n9  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.01\n10 TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.06\n   KDEBPS        KDEPUM KDPBPS KDPKAB KDPPUM LUASWH TIPADM      WADMKC\n1    &lt;NA&gt; 19.03.03.2008   &lt;NA&gt;  19.03     19      0      1   Air Gegas\n2    &lt;NA&gt; 19.03.03.2001   &lt;NA&gt;  19.03     19      0      1   Air Gegas\n3    &lt;NA&gt; 19.04.01.1002   &lt;NA&gt;  19.04     19      0      2        Koba\n4    &lt;NA&gt; 19.03.07.2001   &lt;NA&gt;  19.03     19      0      1  Pulaubesar\n5    &lt;NA&gt; 19.03.05.2006   &lt;NA&gt;  19.03     19      0      1      Payung\n6    &lt;NA&gt; 19.04.06.2009   &lt;NA&gt;  19.04     19      0      1 Lubuk Besar\n7    &lt;NA&gt; 19.03.03.2004   &lt;NA&gt;  19.03     19      0      1   Air Gegas\n8    &lt;NA&gt; 19.04.01.1017   &lt;NA&gt;  19.04     19      0      2        Koba\n9    &lt;NA&gt; 19.03.01.2006   &lt;NA&gt;  19.03     19      0      1     Toboali\n10   &lt;NA&gt; 19.03.06.2005   &lt;NA&gt;  19.03     19      0      1 Tukak Sadai\n           WADMKD         WADMKK                    WADMPR WIADKC WIADKK WIADPR\n1         Airbara Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n2        Airgegas Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n3     Arung Dalam  Bangka Tengah Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n4  Batu Betumpang Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n5       Bedengung Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n6       Belimbing  Bangka Tengah Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n7          Bencah Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n8           Berok  Bangka Tengah Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n9          Bikang Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n10    Bukit Terap Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n   WIADKD                            UUPP       LUAS      AREA\n1    &lt;NA&gt; Hasil Delineasi Batas Desa 2019  77.160034  77160034\n2    &lt;NA&gt; Hasil Delineasi Batas Desa 2019  68.445344  68445426\n3       0 Hasil Delineasi Batas Desa 2019  20.759893  20759893\n4    &lt;NA&gt; Hasil Delineasi Batas Desa 2019 138.255656 138247711\n5    &lt;NA&gt; Hasil Delineasi Batas Desa 2019  96.103135  96102987\n6    &lt;NA&gt; Hasil Delineasi Batas Desa 2019  21.356034  21356035\n7    &lt;NA&gt; Hasil Delineasi Batas Desa 2019 133.589935 133590216\n8       0 Hasil Delineasi Batas Desa 2019   3.196318   3196318\n9    &lt;NA&gt; Hasil Delineasi Batas Desa 2019  53.235589  53235592\n10   &lt;NA&gt; Hasil Delineasi Batas Desa 2019  18.038894  18038895\n                         geometry\n1  POLYGON Z ((106.4285 -2.562...\n2  POLYGON Z ((106.4589 -2.692...\n3  POLYGON Z ((106.3998 -2.478...\n4  POLYGON Z ((106.0563 -2.778...\n5  POLYGON Z ((106.2187 -2.679...\n6  POLYGON Z ((106.4636 -2.568...\n7  POLYGON Z ((106.5133 -2.724...\n8  POLYGON Z ((106.4047 -2.477...\n9  POLYGON Z ((106.522 -2.8827...\n10 POLYGON Z ((106.6278 -2.968...\n\n\nWe will need to drop the ‚Äòz‚Äô dimension value from the dataset as we are only working with x,y dimensions, not with height data. Hence, let‚Äôs re-read the data and perform some wrangling.\n\nkbb_sf &lt;- st_read(dsn=\"data/rawdata\", layer=\"Kepulauan_Bangka_Belitung\") %&gt;%\n  st_union() %&gt;%\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;%\n  st_transform(crs = 32748)\n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex4\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nst_as_s2(): dropping Z and/or M coordinate\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_read() reads the spatial data from the specified file.\nst_union() performs a spatial union, combining all separate geometries (e.g., polygons) into one single geometry object. This is useful if you want to treat the entire area as a single entity, rather than as individual geometries (e.g., islands or districts).\nst_zm(drop = TRUE, what = \"ZM\") removes the Z (elevation) and M (measure) dimensions, simplifying the geometry to 2D.\nst_transform(crs = 32748) reprojects the geometry to the specified coordinate reference system (CRS), EPSG:32748 (UTM zone 48S, often used for areas around Southeast Asia).\n\n\n\nLet‚Äôs inspect the newly created dataframe.\n\nkbb_sf\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 512066.8 ymin: 9655398 xmax: 705559.4 ymax: 9834006\nProjected CRS: WGS 84 / UTM zone 48S\n\n\nMULTIPOLYGON (((590979.6 9741359, 590966.1 9741...\n\n\n\n\n3.2 Converting to OWIN Layer\nNext, as.owin() is used to convert the kbb data into an own object.\n\nkbb_owin &lt;- as.owin(kbb_sf)\nkbb_owin\n\nwindow: polygonal boundary\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n\n\nNext, class() is used to confirm if the output is indeed an owin object.\n\nclass(kbb_owin)\n\n[1] \"owin\""
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#importing-and-preparing-forest-fire-data.",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#importing-and-preparing-forest-fire-data.",
    "title": "In-class Exercise 3",
    "section": "4. Importing and Preparing Forest Fire Data.",
    "text": "4. Importing and Preparing Forest Fire Data.\nNext, we will import the forest fire data (i.e.¬†forestfires.csv) into the R environment.\n\nfire_sf &lt;- read_csv(\"data/rawdata/forestfires.csv\") %&gt;%\n  st_as_sf(coords = c(\"longitude\",\"latitude\"),\n           crs = 4326) %&gt;%\n  st_transform(crs = 32748)\n\nRows: 741 Columns: 15\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr   (3): satellite, instrument, daynight\ndbl  (11): latitude, longitude, brightness, scan, track, acq_time, confidenc...\ndate  (1): acq_date\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSince ppp object only acce[ts a numerical or character as mark, we will use the codes below to convert the data type of acq_dae to numeric.\n\nfire_sf &lt;- fire_sf %&gt;%\n  mutate(DayofYear = yday(acq_date)) %&gt;%\n  mutate(Month_num = month(acq_date)) %&gt;%\n  mutate(Month_fac = month(acq_date, \n                         label = TRUE,\n                         abbr = FALSE))\n\nfire_sf\n\nSimple feature collection with 741 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 521564.1 ymin: 9658137 xmax: 695791 ymax: 9828767\nProjected CRS: WGS 84 / UTM zone 48S\n# A tibble: 741 √ó 17\n   brightness  scan track acq_date   acq_time satellite instrument confidence\n *      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1       312.   1.3   1.1 2023-01-10      629 Aqua      MODIS              67\n 2       314.   1.2   1.1 2023-01-10      629 Aqua      MODIS              70\n 3       315.   1.2   1.1 2023-01-10      629 Aqua      MODIS              71\n 4       309.   1.2   1.1 2023-01-10      629 Aqua      MODIS              54\n 5       308.   1.2   1.1 2023-01-10      629 Aqua      MODIS              33\n 6       322.   1.3   1.1 2023-01-10      629 Aqua      MODIS              72\n 7       318.   1.2   1.1 2023-01-10      629 Aqua      MODIS              71\n 8       318.   1.2   1.1 2023-01-10      629 Aqua      MODIS              75\n 9       327.   2     1.4 2023-01-12      616 Aqua      MODIS              73\n10       321.   2     1.4 2023-01-12      616 Aqua      MODIS              75\n# ‚Ñπ 731 more rows\n# ‚Ñπ 9 more variables: version &lt;dbl&gt;, bright_t31 &lt;dbl&gt;, frp &lt;dbl&gt;,\n#   daynight &lt;chr&gt;, type &lt;dbl&gt;, geometry &lt;POINT [m]&gt;, DayofYear &lt;dbl&gt;,\n#   Month_num &lt;dbl&gt;, Month_fac &lt;ord&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#visualise-the-plot",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#visualise-the-plot",
    "title": "In-class Exercise 3",
    "section": "5. Visualise the Plot",
    "text": "5. Visualise the Plot\n\n5.1 Overall Plot\nNow, I will prepare a point symbol map showing the distribution of fire points.\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n5.2 Visuaising geographic distribution of forest fires by month\nNext, I will prepare a point symbol map showing the monthly geographic distribution of forest fires in 2023.\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf)+\n  tm_dots(size = 0.1) +\n  tm_facets(by = \"Month_fac\",\n            free.coords = FALSE,\n            drop.units = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#computing-stkde-by-month",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#computing-stkde-by-month",
    "title": "In-class Exercise 3",
    "section": "6. Computing STKDE by Month",
    "text": "6. Computing STKDE by Month\nIn this section, I will learn how to compute STKDE by using¬†spattemp.density()¬†of¬†sparr¬†package.\n\n6.1 Extracting Forest Fires by Month\nThe code below is used to remove the unwanted fields from the fire_sf simple feature data frame. This is because as.ppp() only needs the mark field and geometry field from the input of the data frame.\n\nfire_month &lt;- fire_sf %&gt;%\n  select(Month_num)\n\nhead(fire_month)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 606178.8 ymin: 9682757 xmax: 669933.6 ymax: 9703062\nProjected CRS: WGS 84 / UTM zone 48S\n# A tibble: 6 √ó 2\n  Month_num           geometry\n      &lt;dbl&gt;        &lt;POINT [m]&gt;\n1         1 (606178.8 9703062)\n2         1 (661410.6 9683536)\n3         1 (637808.8 9682757)\n4         1 (654882.2 9690665)\n5         1 (669933.6 9697468)\n6         1 (609133.5 9700119)\n\n\n\n\n6.2 Creating ppp objects\nThe code below is used to derive a ppp object called the fire_month from fire_month of data.frame.\n\nfire_month_ppp &lt;- as.ppp(fire_month)\nfire_month_ppp\n\nMarked planar point pattern: 741 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n\n\nThe code below is used to check the output is in the correct object class\n\nsummary(fire_month_ppp)\n\nMarked planar point pattern:  741 points\nAverage intensity 2.49258e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n                    (174200 x 170600 units)\nWindow area = 29728200000 square units\n\n\nWe can check the duplication in a¬†ppp¬†object by using the code chunk below.\n\nany(duplicated(fire_month_ppp))\n\n[1] FALSE\n\n\n\n\n6.3 Including Owin object\nHere we combine fire_month_ppp object with the kkb_owin object into one.\n\nfire_month_owin &lt;- fire_month_ppp[kbb_owin]\nsummary(fire_month_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\nAs a good practice,¬†plot()¬†is used to plot¬†ff_owin¬†so that we can examine the correctness of the output object.\n\nplot(fire_month_owin)\n\n\n\n\n\n\n\n\n\n\n6.4 Computing Spatio-temporal KDE\nNext,¬†spattemp.density()¬†of sparr package is used to compute the STKDE.\n\nst_kde &lt;- spattemp.density(fire_month_owin)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 0.0304 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [1, 12]\n\nEvaluation\n  128 x 128 x 12 trivariate lattice\n  Density range: [1.233458e-27, 8.202976e-10]\n\n\n\n\n6.5 Plotting the spatio-temporal KDE object\nWe‚Äôll use the plot() function of R base to plot the KDE between July 2023 to December 2023.\n\ntims &lt;- c(7,8,9,10,11,12)\npar(mfcol=c(1,2))\nfor(i in tims){ \n  plot(st_kde, i, \n       override.par=FALSE, \n       fix.range=TRUE, \n       main=paste(\"KDE at month\",i))\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#computing-stkde-by-day-of-year",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#computing-stkde-by-day-of-year",
    "title": "In-class Exercise 3",
    "section": "7. Computing STKDE by Day of Year",
    "text": "7. Computing STKDE by Day of Year\nNow, I will compute the STKDE of forest fires by day of year.\n\n7.1 Creating ppp object\nIn the code chunk below, DayofYear from the fire_sf data frame is selected and is included in the output ppp object.\n\nfire_yday_ppp &lt;- fire_sf %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\n\n\n\n7.2 Including Owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\nfire_yday_owin &lt;- fire_yday_ppp[kbb_owin]\nsummary(fire_yday_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   10.0   213.0   258.0   245.9   287.0   352.0 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\n\n\n7.3 Performing Spatio-Temporal KDE\nNow, I will perform a spatio-temporal kernel density estimate on the fire_yday_owin object which gives us insights into where and when fire occurrences are concentrated within the specified observation window.\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 6.3198 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [10, 352]\n\nEvaluation\n  128 x 128 x 343 trivariate lattice\n  Density range: [3.959516e-27, 2.751287e-12]\n\n\nPlotting the graph by days of the year will produce 365/366 charts.\n\n#plot(kde_yday)\n\nInstead, let us plot an animated plot to show the change in KDE across each day of the year.\n\nkde_yday$z$'10'\n\nreal-valued pixel image\n128 x 128 pixel array (ny, nx)\nenclosing rectangle: [512070, 705560] x [9655400, 9834000] units\n\n\n\nplot(kde_yday$z$'10')\n\n\n\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(spatstat)\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.98\nEnabled features: cairo, freetype, fftw, ghostscript, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fontconfig, x11\n\nlibrary(viridis)  # For color mapping\n\nLoading required package: viridisLite\n\n# Create a directory to store PNG frames\nif (!dir.exists(\"frames\")) {\n  dir.create(\"frames\")\n}\n\n# Get the unique day values from kde_yday\ndays &lt;- names(kde_yday$z)  # Assuming 'kde_yday$z' contains KDE results for each day\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- kde_yday$z[[day]]  # Access KDE result for the day\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Load magick library\nlibrary(magick)\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"frames\", full.names = TRUE, pattern = \"*.png\"))\n\n# Create animated GIF\nanimation &lt;- image_animate(image_join(frames), fps = 10)  # Adjust fps as needed\n\n# Save the animation\noutput_path &lt;- \"animated_kde_yday.gif\"\nimage_write(animation, path = output_path)\n\n# Display the GIF (optional)\nprint(animation)\n\n# A tibble: 344 √ó 7\n   format width height colorspace matte filesize density\n   &lt;chr&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n 1 gif      800    800 sRGB       FALSE        0 72x72  \n 2 gif      800    800 sRGB       TRUE         0 72x72  \n 3 gif      800    800 sRGB       TRUE         0 72x72  \n 4 gif      800    800 sRGB       TRUE         0 72x72  \n 5 gif      800    800 sRGB       TRUE         0 72x72  \n 6 gif      800    800 sRGB       TRUE         0 72x72  \n 7 gif      800    800 sRGB       TRUE         0 72x72  \n 8 gif      800    800 sRGB       TRUE         0 72x72  \n 9 gif      800    800 sRGB       TRUE         0 72x72  \n10 gif      800    800 sRGB       TRUE         0 72x72  \n# ‚Ñπ 334 more rows"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#exploratory-data-analysis",
    "title": "Take-home Exercise 1 - Part 1",
    "section": "4. Exploratory Data Analysis",
    "text": "4. Exploratory Data Analysis\n\n4.1 Identifying Districts with Highest Proportion of Conflicts\nIt‚Äôll also be interesting to find out specific districts with the highest concentration of armed conflicts. I will first calculate the total occurrences of conflict events per district and add the column to boundary_sf.\n\n\nCount number of conflicts by districts\nconflict_count &lt;- conflict_data_sf %&gt;%\n  group_by(DT) %&gt;%\n  summarise(total_count_DT = n()) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(DT, total_count_DT)\n\n# Perform the join\nboundary_sf &lt;- boundary_sf %&gt;%\n  left_join(conflict_count, by = \"DT\")\n\n\nNext, let‚Äôs calculate the proportion of total conflicts and add it as a column into the boundary_sf dataset as proportion_DT.\n\n# Create new 'proportion_DT' column\nboundary_sf &lt;- boundary_sf %&gt;%\n  mutate(proportion_DT = total_count_DT / sum(total_count_DT))\nhead(boundary_sf[c('DT','total_count_DT','proportion_DT')])\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -14915.04 ymin: 1736124 xmax: 187961.7 ymax: 2051144\nProjected CRS: WGS 84 / UTM zone 47N\n         DT total_count_DT proportion_DT                       geometry\n1  Hinthada            160            NA MULTIPOLYGON (((90859.89 20...\n2   Labutta             51            NA MULTIPOLYGON (((75991.51 17...\n3    Maubin            118            NA MULTIPOLYGON (((115559 1928...\n4 Myaungmya             59            NA MULTIPOLYGON (((39919.39 18...\n5   Pathein            334            NA MULTIPOLYGON (((-6302.348 1...\n6    Pyapon            131            NA MULTIPOLYGON (((93411.72 17...\n\n\nAt a quick glance, we can see that central and southern parts of Myanmar have the highest proportions of armed conflict events occurring.\n\n\nSet up the points map\ndistricts_choropleth &lt;-\ntm_shape(boundary_sf) +\n  tm_fill(\"proportion_DT\",\n          n=10,\n          title=\"Proportion\",\n          style=\"equal\",\n          palette=\"Blues\") +\n  tm_borders(lwd=0.2,\n             alpha=1) +\n  tm_text(text = \"DT\", \n          size = 0.2, \n          col = \"black\",\n          fontface = \"bold\") +\n  tm_layout(main.title = \"Distribution of Conflict Points Across Districts\",\n            legend.outside=FALSE,\n            main.title.size=1)\n\n\n\n# Plot the map\ntmap_mode(\"plot\")\ntmap_arrange(districts_choropleth)\n\n\nMore specifically, we can observe that conflict hotspots are mainly found in the districts of Yinmarbin, Shwebo, Pakokku and Mandalay which lies in the central regions of Myanmar.\n\n\n4.2 Identifying States with Highest Proportion of Conflicts\nInstead, let us also explore the top 10 states with the highest proportions of armed conflict events.\n\n\nCount number of conflicts by states\nconflict_count &lt;- conflict_data_sf %&gt;%\n  group_by(ST) %&gt;%\n  summarise(total_count_ST = n()) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(ST, total_count_ST)\n\n# Perform the join\nboundary_sf &lt;- boundary_sf %&gt;%\n  left_join(conflict_count, by = \"ST\")\n\n\nLikewise, I‚Äôll add a new column called proportion_ST to represent the proportion based on each Myanmar state.\n\nboundary_sf &lt;- boundary_sf %&gt;%\n  mutate(proportion_ST = total_count_ST / sum(total_count_ST))\n\nhead(boundary_sf[c('ST','total_count_ST','proportion_ST')])\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -14915.04 ymin: 1736124 xmax: 187961.7 ymax: 2051144\nProjected CRS: WGS 84 / UTM zone 47N\n          ST total_count_ST proportion_ST                       geometry\n1 Ayeyarwady            853            NA MULTIPOLYGON (((90859.89 20...\n2 Ayeyarwady            853            NA MULTIPOLYGON (((75991.51 17...\n3 Ayeyarwady            853            NA MULTIPOLYGON (((115559 1928...\n4 Ayeyarwady            853            NA MULTIPOLYGON (((39919.39 18...\n5 Ayeyarwady            853            NA MULTIPOLYGON (((-6302.348 1...\n6 Ayeyarwady            853            NA MULTIPOLYGON (((93411.72 17...\n\n\nAt a quick glance, we can see that central and southern parts of Myanmar have the highest proportions of armed conflict events occurring, namely in Sagaing, Mandalay, Magway and Yangon states as indicated in the map below (darkest shade of blue).\n\n\nCreate the points map\nstates_choropleth &lt;-\ntm_shape(boundary_sf) +\n  tm_fill(\"proportion_ST\",\n          n=10,\n          title=\"Proportion\",\n          style=\"equal\",\n          palette=\"Blues\") +\n  tm_borders(lwd=0.2,\n             alpha=1) +\n  tm_text(text = \"ST\", \n          size = 0.2, \n          col = \"black\",\n          fontface = \"bold\") +\n  tm_layout(main.title = \"Distribution of Conflict Points Across States\",\n            legend.outside=FALSE,\n            main.title.size=1)\n\n\n\n# Plot the map\ntmap_mode(\"plot\")\ntmap_arrange(states_choropleth)\n\n\n\n\n4.3 Standard Distances of Top 4 Districts\nNext, I‚Äôve plot the standard distances for the four districts that I‚Äôm most interested in. The codes here will calculate the average location of the conflict points within the district (i.e.¬†the mean centre) and measure the dispersion of points around the mean centre.\nThe circle drawn around the mean centre shows us the standard distance to which we can observe that Mandalay has the smallest standard distance and hence, has the most conflict points closely clustered around the mean centre.\n\n\nFind hot spot of Yinmarbin District\n# Define the districts\ndistricts &lt;- c(\"Yinmarbin\", \"Shwebo\", \"Pakokku\", \"Mandalay\")\n\nfor (district in districts) {\n  # Filter the conflict data for the current district\n  conflict_district &lt;- filter(conflict_data_sf, DT == district)\n  boundary_district &lt;- filter(boundary_sf, DT == district)\n\n  # Create a combined ppp and owin object\n  district_owin &lt;- as.owin(boundary_district)\n  ppp_obj &lt;- as.ppp(st_geometry(conflict_district))\n\n  # Handle duplicates\n  ppp_obj &lt;- rjitter(ppp_obj, retry = TRUE, nsim = 1, drop = TRUE)\n\n  # Mask ppp object with the boundary\n  district_ppp_owin &lt;- ppp_obj[district_owin]\n  district_ppp_owin &lt;- rescale(district_ppp_owin, 1000, \"km\")\n\n  # Calculate the mean center\n  mean_coords &lt;- c(mean(district_ppp_owin$x), mean(district_ppp_owin$y))\n\n  # Calculate the standard distance\n  sd_x &lt;- sd(district_ppp_owin$x)\n  sd_y &lt;- sd(district_ppp_owin$y)\n  standard_distance &lt;- sqrt(sd_x^2 + sd_y^2)\n\n  # Prepare to plot for the current district\n  plot(district_ppp_owin, \n       main = paste(\"Standard Distance in\", district), \n       pch = 19, \n       col = \"lightgrey\",\n       xlim = range(district_ppp_owin$x) + c(-5, 5),  # Expand limits\n       ylim = range(district_ppp_owin$y) + c(-5, 5))  # Expand limits\n\n  # Highlight the mean center with an 'X'\n  points(mean_coords[1], mean_coords[2], pch = 4, col = \"red\", cex = 1)  # Smaller 'X'\n\n  # Create a circle around the mean center for the standard distance\n  bearing &lt;- seq(0, 2 * pi, length.out = 360)\n  circle_x &lt;- mean_coords[1] + standard_distance * cos(bearing)\n  circle_y &lt;- mean_coords[2] + standard_distance * sin(bearing)\n\n  # Draw the circle\n  lines(circle_x, circle_y, col = 'red', lwd = 2)\n}\n\n# Reset the plot layout\npar(mfrow = c(1,1), mar = c(0,0,1,0))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html",
    "title": "Hands-on Exercise 5",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#overview",
    "title": "Hands-on Exercise 5",
    "section": "1. Overview",
    "text": "1. Overview\nIn this hands-on exercise, I will be computing spatial weights by executing the following:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#lets-set-up",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#lets-set-up",
    "title": "Hands-on Exercise 5",
    "section": "2. Let‚Äôs Set Up!",
    "text": "2. Let‚Äôs Set Up!\n\n2.1 Importing Libraries into R\nIn this hands-on exercise, we will we need to ensure that¬†spdep,¬†sf,¬†tmap¬†and¬†tidyverse¬†packages of R are currently installed in R.\n\n\nLoad the packages\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\n\n\n\n2.2 Download Data and Set Up Folders\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan‚Äôs local development indicators in 2012.\n\nThis is the file structure for containing the data files that I have extracted."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#import-data-sets-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#import-data-sets-into-r",
    "title": "Hands-on Exercise 5",
    "section": "3. Import Data Sets into R",
    "text": "3. Import Data Sets into R\n\n3.1 Importing Geospatial Data\nFirstly, we will import the Hunan county boundary layer. This is a geospatial data set in ESRI shapefile format. The code chunk below uses¬†st_read()¬†of¬†sf¬†package.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex5\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n3.2 Importing Aspatial Data\nNext, I will import the aspatial data set. This data is a csv file containing selected Hunan‚Äôs local development indicators in 2012.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan‚Äôs SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012) |&gt; \n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#visualizing-regional-development-indicator",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#visualizing-regional-development-indicator",
    "title": "Hands-on Exercise 5",
    "section": "4. Visualizing Regional Development Indicator",
    "text": "4. Visualizing Regional Development Indicator\nNow, we will use qtm() function of tmap package to create a basemap and a choropleth map showing the distribution of GDPPC 2012.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#computing-contiguity-spatial-weights",
    "title": "Hands-on Exercise 5",
    "section": "5. Computing Contiguity Spatial Weights",
    "text": "5. Computing Contiguity Spatial Weights\nIn this section, I will use the poly2nb() function of spdep package to compute contiguity spatial weights.\n\nüí° What does poly2nb() do?\nThis function builds a neighbours list based on regions with contiguous boundaries. A ‚Äúqueen‚Äù argument can take either TRUE or FALSE as options.\nNote: If you do not specify this argument the default is set to TRUE, that is, if you don‚Äôt specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n\n5.1 Computing Queen contiguity based neighbors\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nWe can observe that there are 88 regions in the data set and that the average number of neighbours is 5.1. The maximum number of neighbours is 11 and the minimum is 1.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen‚Äôs method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n5.2 Computing Rook contiguity based neighbors\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nWe can observe that there are 88 regions in the data set and that the average number of neighbours is 5. The maximum number of neighbours is 10 and the minimum is 1.\n\n\n5.3 Visualising the contiguity weights\nA connectivity graph takes a point in any polygon and draws a line to all its neighbors. The most common way to get the points is to use the coordinates of the centroids of the polygons.\nHowever, getting the points associated with each polygon is a little more complicated than just running st_centroid() on the sf object. We need the coordinates to be in a separate data frame for this to work. To do this, we need to use a mapping function map_dbl().\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nWe can check the first few observations to see if things are formatted correctly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\nNow, we can visualize the queen and rook spatial weights, using 3 ways.\n\nMethod 1: Plotting Queen contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n\n\n\n\nMethod 2: Plotting Rook contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nMethod 3: Plotting both Queen and Rook contiguity based neighbours maps\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#computing-distance-based-neighbours",
    "title": "Hands-on Exercise 5",
    "section": "6. Computing distance based neighbours",
    "text": "6. Computing distance based neighbours\nThis section will use the dnearneigh() function of spdep package to compute distance based neighbours.\n\nüí° What does dnearneigh() do?\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument.\nIf unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\n6.1 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n6.2 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nWe can observe that there are 88 regions, with an average of 3.681818 neighbours per region.\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine¬†table()¬†and¬†card()¬†of spdep.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\nn_comp &lt;- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\n\n6.2.1 Plotting fixed distance weight matrix\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n6.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\n\n6.3.1 Plotting distance based neighbours\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#weights-based-on-idw",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#weights-based-on-idw",
    "title": "Hands-on Exercise 5",
    "section": "7. Weights based on IDW",
    "text": "7. Weights based on IDW\nIn this section, I attempt to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#row-standardised-weights-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#row-standardised-weights-matrix",
    "title": "Hands-on Exercise 5",
    "section": "8. Row-standardised weights matrix",
    "text": "8. Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=‚ÄúW‚Äù).\nThis is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values.\nWhile this is the most intuitive way to summaries the neighbors‚Äô values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data.\nFor this example, we‚Äôll stick with the style=‚ÄúW‚Äù option for simplicity‚Äôs sake but note that other more robust options are available, notably style=‚ÄúB‚Äù.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon‚Äôs eight neighbors type:\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor‚Äôs income will be multiplied by 0.125 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#application-of-spatial-weight-matrix",
    "title": "Hands-on Exercise 5",
    "section": "9. Application of Spatial Weight Matrix",
    "text": "9. Application of Spatial Weight Matrix\nNow, I will create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n9.1 Spatial lag with row-standardised weights\nFinally, we‚Äôll compute the average neighbor GDPPC value for each polygon. These values are often referred to as¬†spatially lagged values.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecall that in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nFrom the output above, we can see that row-standardized weights are calculated by dividing each weight by the sum of the weights for each polygon. In other words, the weights are normalized so that they sum to 1. This is done to ensure that the spatial lag variable is on the same scale as the original variable.\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n9.2 Spatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nNow, let‚Äôs examine the results by using this code.\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nFrom the output above, we can see that the spatial lag GDPPC values are calculated by summing the GDPPC of the neighboring counties. We can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n9.3 Spatial window average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw().\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\n\nüí° Note: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\n\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nTo compare the values of lag GDPPC and Spatial window average,¬†kable()¬†of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523‚Ä¶\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684‚Ä¶\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,‚Ä¶\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649‚Ä¶\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288‚Ä¶\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675‚Ä¶\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,‚Ä¶\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299‚Ä¶\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688‚Ä¶\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734‚Ä¶\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117‚Ä¶\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358‚Ä¶\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143‚Ä¶\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578‚Ä¶\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394‚Ä¶\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843‚Ä¶\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259‚Ä¶\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198‚Ä¶\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007‚Ä¶\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149‚Ä¶\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, ‚Ä¶\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276‚Ä¶\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,‚Ä¶\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778‚Ä¶\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858‚Ä¶\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512‚Ä¶\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842‚Ä¶\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,‚Ä¶\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561‚Ä¶\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,‚Ä¶\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485‚Ä¶\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895‚Ä¶\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661‚Ä¶\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472‚Ä¶\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,‚Ä¶\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642‚Ä¶\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317‚Ä¶\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812‚Ä¶\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305‚Ä¶\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,‚Ä¶\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254‚Ä¶\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,‚Ä¶\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345‚Ä¶\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627‚Ä¶\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275‚Ä¶\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742‚Ä¶\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783‚Ä¶\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844‚Ä¶\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206‚Ä¶\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034‚Ä¶\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712‚Ä¶\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,‚Ä¶\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521‚Ä¶\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835‚Ä¶\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499‚Ä¶\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716‚Ä¶\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074‚Ä¶\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,‚Ä¶\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661‚Ä¶\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123‚Ä¶\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892‚Ä¶\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957‚Ä¶\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,‚Ä¶\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134‚Ä¶\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418‚Ä¶\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139‚Ä¶\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152‚Ä¶\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264‚Ä¶\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017‚Ä¶\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573‚Ä¶\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346‚Ä¶\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,‚Ä¶\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324‚Ä¶\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411‚Ä¶\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,‚Ä¶\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,‚Ä¶\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318‚Ä¶\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265‚Ä¶\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207‚Ä¶\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946‚Ä¶\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513‚Ä¶\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613‚Ä¶\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963‚Ä¶\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722‚Ä¶\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855‚Ä¶\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,‚Ä¶\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,‚Ä¶\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472‚Ä¶\n\n\n\n\n\nLastly,¬†qtm()¬†of¬†tmap¬†package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n9.4 Spatial window sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with¬†lag.listw().\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using¬†as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\n\nüí° Note: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\n\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nTo compare the values of lag GDPPC and Spatial window average,¬†kable()¬†of Knitr package is used to prepare a table using the code chunk below.\n\nhunan |&gt;\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523‚Ä¶\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684‚Ä¶\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,‚Ä¶\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649‚Ä¶\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288‚Ä¶\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675‚Ä¶\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,‚Ä¶\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299‚Ä¶\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688‚Ä¶\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734‚Ä¶\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117‚Ä¶\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358‚Ä¶\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143‚Ä¶\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578‚Ä¶\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394‚Ä¶\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843‚Ä¶\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259‚Ä¶\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198‚Ä¶\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007‚Ä¶\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149‚Ä¶\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, ‚Ä¶\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276‚Ä¶\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,‚Ä¶\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778‚Ä¶\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858‚Ä¶\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512‚Ä¶\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842‚Ä¶\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,‚Ä¶\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561‚Ä¶\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,‚Ä¶\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485‚Ä¶\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895‚Ä¶\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661‚Ä¶\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472‚Ä¶\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,‚Ä¶\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642‚Ä¶\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317‚Ä¶\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812‚Ä¶\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305‚Ä¶\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,‚Ä¶\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254‚Ä¶\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,‚Ä¶\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345‚Ä¶\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627‚Ä¶\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275‚Ä¶\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742‚Ä¶\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783‚Ä¶\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844‚Ä¶\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206‚Ä¶\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034‚Ä¶\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712‚Ä¶\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,‚Ä¶\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521‚Ä¶\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835‚Ä¶\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499‚Ä¶\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716‚Ä¶\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074‚Ä¶\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,‚Ä¶\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661‚Ä¶\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123‚Ä¶\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892‚Ä¶\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957‚Ä¶\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,‚Ä¶\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134‚Ä¶\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418‚Ä¶\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139‚Ä¶\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152‚Ä¶\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264‚Ä¶\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017‚Ä¶\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573‚Ä¶\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346‚Ä¶\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,‚Ä¶\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324‚Ä¶\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411‚Ä¶\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,‚Ä¶\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,‚Ä¶\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318‚Ä¶\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265‚Ä¶\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207‚Ä¶\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946‚Ä¶\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513‚Ä¶\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613‚Ä¶\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963‚Ä¶\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722‚Ä¶\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855‚Ä¶\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,‚Ä¶\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,‚Ä¶\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472‚Ä¶\n\n\n\n\n\nLastly, qtm() of¬†tmap¬†package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#nd-order-spatial-point-patterns-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#nd-order-spatial-point-patterns-analysis",
    "title": "Take-home Exercise 1 - Part 1",
    "section": "6. 2nd Order Spatial Point Patterns Analysis",
    "text": "6. 2nd Order Spatial Point Patterns Analysis\nUnlike 1st-order analysis, which studies the intensity of points (e.g., density), let‚Äôs also leverage 2nd-order analysis to examine how points are distributed relative to each other, which can offer deeper insights into the spatial interaction between events.\nI will use the K-function and L-function is to understand the spatial relationships between events, particularly focusing on whether the points exhibit clustering, uniformity, or randomness.\n\n6.1 Using K-Function Estimation\nK-function helps detect spatial patterns by comparing the observed distribution of points against a random pattern at different distances.\n\n6.1.1 Yinmarbin District\n1) Computing K-Function Estimation\nFor Yinmarbin district, let‚Äôs compute K-function estimates by using Kest() of the spatstat package by using the yinmarbin_ppp_owin masked ppp object we created previously.\n\nK_ck = Kest(yinmarbin_ppp_owin, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Yinmarbin District (K-Function)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nHow to interpret the plot:\n\nK-iso represents the observed or estimated K-function value calculated from the actual data\nK-pois is the theoretical K-function that represents the expected K-function\n\nWith that said‚Ä¶\nWe can observe how the observed line (K-iso) is found to lie above the theoretical line (K-pois) which suggests conflict points in Yinmarbin are highly clustered. Clustering is the strongest at a 20km distance which suggests large-scale clustering. In fact, it is more clustered together than expected by the null hypothesis.\nNote: Since I had used the default edge = TRUE settings, edge correction will account for missing neighbours outside the boundary which helps maintain an accurate estimate of the K-function. Hence, there is marginal difference in the actual and expected K-function.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monte Carlo simulation test) will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of conflict events in Myanmar are randomly distributed.\nH1= The distribution of conflict events in Myanmar are not randomly distributed.\nThe null hypothesis will be rejected if the observed K-function lies above/below the theoretical K-function and envelope.\n\nBy using envelope(), we can get a more robust interpretation by comparing the observed K-function against a simulation envelope of K-functions generated under the null hypothesis.\n\n\n\n\n\n\nNote\n\n\n\nTo achieve a 95% confidence envelope in a K-function test with Complete Spatial Randomness, I will need to exclude the upper 2.5% and lower 2.5% of the simulated K-functions., i.e.¬†I will need to generate at least 40 simulations where nsim = 39.\nWe‚Äôll set rank = 1 for a conservative setting, such that the envelope is based on the extreme values (highest and lowest) from the simulations.\n\n\n\n# Monte Carlo test with K-function\nK_yinmarbin_csr &lt;- envelope(yinmarbin_ppp_owin, Kest, \n                            nsim = 39, rank = 1, glocal=TRUE)\nplot(K_yinmarbin_csr, main = paste(\"Yinmarbin District (CSR)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe tight envelope suggests we can be confident that deviations between the observed and theoretical lines are meaningful, rather than due to random variation. It also indicates that simulations under CSR are producing homogeneous patterns.\nOn the other hand, the observed K-function line is constantly above the shaded region of the envelope and theoretical line, suggesting that observed spatial pattern are more clustered than expected under the null hypothesis of CSR\n\n\nJust to be sure that there is no improvement in a higher number of simulations, I ran it again with nsim = 99. As shown below, the k-function output is similar but with a somewhat smoother plot than before since we are working with less variability. Hence, I‚Äôll stick to nsim = 39.\n\n\n\n6.1.2 Shwebo District\n1) Computing K-Function Estimation\nFor Shwebo district, let‚Äôs compute K-function estimates by using Kest() of the spatstat package.\n\nK_ck = Kest(shwebo_ppp_owin, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Shwebo District (K-Function)\"))\n\nThe observed line is consistently above at all distances, implying that conflict events are clustered at both small and large areas, rather than being randomly distributed across the area.\n\n\n\n\n\n\n\nObservations\n\n\n\nThe observed line is consistently above at all distances, implying that conflict events are clustered at both small and large areas, rather than being randomly distributed across the area.\n\n\n2) Performing Complete Spatial Randomness Test\n\n# Monte Carlo test with K-function\nK_shwebo_csr &lt;- envelope(shwebo_ppp_owin, Kest, \n                     nsim = 39, rank = 1, glocal=TRUE)\nplot(K_shwebo_csr, main = paste(\"Shwebo District (CSR)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nSince the observed line lies above the envelope and the shaded region of the envelope is rather narrow, it strongly suggests clustering in Shwebo, rather than due to random variation.\n\n\n\n\n6.1.3 Pakokku District\n1) Computing K-Function Estimation\nFor Pakokku district, let‚Äôs compute K-function estimates by using Kest() of the spatstat package.\n\nK_ck = Kest(pakokku_ppp_owin, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Pakokku District (K-Function)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can observe how the observed line (K-iso) is found to be above the theoretical line (K-pois) which suggests conflict points in Pakokku are highly clustered. In fact, it is more clustered together than expected by the null hypothesis.\n\n\n2) Performing Complete Spatial Randomness Test\n\n# Monte Carlo test with K-function\nK_pakokku_csr &lt;- envelope(pakokku_ppp_owin, Kest, \n                     nsim = 39, rank = 1, glocal=TRUE)\nplot(K_pakokku_csr, main = paste(\"Pakokku District (CSR)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nLikewise, we can be certain about the clustering patterns in Pakokku since the observed K values deviates above the envelope as shown.\n\n\n\n\n6.1.4 Mandalay District\n1) Computing K-Function Estimation\nFor Mandalay district, let‚Äôs compute K-function estimates by using Kest() of the spatstat package.\n\nK_ck = Kest(mandalay_ppp_owin, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Mandalay District (K-Function)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nSignificant clustering is observed generally across multiple distances in Mandalay and clusterinng is particularly evident at longer spatial distances in Mandalay seen from how the deviations of observed K values increase by the distance.\n\n\n2) Performing Complete Spatial Randomness Test\n\n# Monte Carlo test with K-function\nK_mandalay_csr &lt;- envelope(mandalay_ppp_owin, Kest, \n                     nsim = 39, rank = 1, glocal=TRUE)\nplot(K_mandalay_csr, main = paste(\"Mandalay District (CSR)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe envelope size continues to be narrow with K values plotted above the envelope. This strongly suggests that the point pattern is not random, and the points are clustered.\n\n\n\n\n\n6.2 Using L-Function Estimation\nIn this section, I will be computing L-function via Lest() of spatstat package which is normalises the K-function to a linear scale for easier interpretation.\n\n6.2.1 Yinmarbin District\n1) Computing L-function Estimation\n\nL_yinmarbin = Lest(yinmarbin_ppp_owin, correction = \"Ripley\")\nplot(L_yinmarbin, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(km)\",\n     main = paste(\"Yinmarbin District (L-Function)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe theoretical L-function under CSR remains at 0, meaning that if the points were randomly distributed, the L-function would be expected to show no significant clustering or dispersion.\nThere is a consistent deviation of each observed L value from the theoretical value across all distances, particularly with the strongest deviation at &lt;5 km and &gt; 20km. This confirms that conflict points are both localised in towns of Yinmarbin and found across large areas in Yinmarbin.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of conflict events in Myanmar are randomly distributed.\nH1= The distribution of conflict events in Myanmar are not randomly distributed.\nThe null hypothesis will be rejected if the observed L-function lies above/below the theoretical L-function and envelope.\n\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n# Monte Carlo test with L-function\nL_yinmarbin_csr &lt;- envelope(yinmarbin_ppp_owin, Lest, \n                     nsim = 39, rank = 1, glocal=TRUE)\nplot(L_yinmarbin_csr, . - r ~ r, xlab=\"d(km)\", ylab=\"L(d)-r\",\n       main = paste(\"Yinmarbin District (CSR)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe envelope outputted is narrow which once again reflects lower variability in spatial distribution in Yinmarbin where a large number of points found in this district could have led to more stable results. There is also statistically significant clustering particularly in specific towns in Yinmarbin and across Yinmarbin itself, since we see larger deviations of observed L values from the envelope.\n\n\n\n\n6.2.2 Shwebo District\n1) Computing L-function Estimation\n\nL_shwebo = Lest(shwebo_ppp_owin, correction = \"Ripley\")\nplot(L_shwebo, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(km)\",\n     main = paste(\"Shwebo District (L-Function)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe observed L values obtained for Shwebo districct tend to show a smaller deviation above the theoretical L-function in smaller areas like towns, suggesting localised clustering is less pronounced. Conversely, larger deviations are seen at larger distances which shows broader patterns of clustering are present in Shwebo district.\n\n\n2) Performing Complete Spatial Randomness Test\n\n# Monte Carlo test with L-function\nL_shwebo_csr &lt;- envelope(shwebo_ppp_owin, Lest, \n                     nsim = 39, rank = 1, glocal=TRUE)\nplot(L_shwebo_csr, . - r ~ r, xlab=\"d(km)\", ylab=\"L(d)-r\",\n       main = paste(\"Shwebo District (CSR)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe envelope size continues to grow larger as distance increases, which indicates greater variability in spatial patterns of the Shwebo district but overall, we can confirm strong clustering of conflicts especially in bigger areas of the district which suggests wide-scale hotspots, as well as some localised clustering at smaller scales.\n\n\n\n\n6.2.3 Pakokku District\n1) Computing L-function Estimation\n\nL_pakokku = Lest(pakokku_ppp_owin, correction = \"Ripley\")\nplot(L_yinmarbin, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(km)\",\n     main = paste(\"Pakokku District (L-Function)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nLike the other districts, there is clustering seen in Pakokku across smaller areas like towns and the distribution across the district itself.\n\n\n2) Performing Complete Spatial Randomness Test\n\n# Monte Carlo test with L-function\nL_pakokku_csr &lt;- envelope(pakokku_ppp_owin, Lest, \n                     nsim = 39, rank = 1, glocal=TRUE)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nUnder the CSR test, the intensity of clustering weakens across larger distances (&gt;20km) than we would have assumed in the previous step. This happens as Monte Carlo simulation accounts for random fluctuations especially at larger distances (e.g., &gt;20 km) where random variability in the point pattern naturally increases.\nIn short, significant clustering is present at both small and large scales, but with diminishing intensity as we measure larger distances.\n\n\n\n\n6.2.4 Mandalay District\n1) Computing L-function Estimation\n\nL_mandalay = Lest(mandalay_ppp_owin, correction = \"Ripley\")\nplot(L_mandalay, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(km)\",\n     main = paste(\"Mandalay District (L-Function)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe distance scale for Mandalay is the smallest (0-10km) since the district itself has a smaller area. Nonetheless, we see almost a consistent spread of localised and widespread distribution of conflict points in Mandalay which indicates that conflicts occur across both specific towns and the across the districct.\n\n\n2) Performing Complete Spatial Randomness Test\n\n# Monte Carlo test with L-function\nL_mandalay_csr &lt;- envelope(mandalay_ppp_owin, Lest, \n                     nsim = 39, rank = 1, glocal=TRUE)\nplot(L_mandalay_csr, . - r ~ r, xlab=\"d(km)\", ylab=\"L(d)-r\",\n       main = paste(\"Mandalay District (CSR)\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe Monte Carlo simulation shows that there‚Äôs statistically significant clustering across all distances since the observed L values consistently lie above the envelope. There is also a marginal increase in variability of random points observed which is inherent in larger distance scales as seen from the slight widening of the envelope as distance increases.\n\n\nContinue to Part 2 &gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#references",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#references",
    "title": "Take-home Exercise 1",
    "section": "11. References",
    "text": "11. References\n\nCrawley, M. J. (2007). The R Book. Wiley.\nCrisis Group. (2024, August 27). Breaking Away: The Battle for Myanmar‚Äôs Rakhine State. https://www.crisisgroup.org/asia/south-east-asia/myanmar/339-breaking-away-battle-myanmars-rakhine-state¬†\nFarge, E., & Mantovani, C. (2024, September 17). Myanmar military stepping up civilian killings and arrests, says UN report. https://www.reuters.com/world/asia-pacific/myanmar-military-intensifies-civilian-killings-arrests-says-un-report-2024-09-17/#:~:text=The%20report%20by%20the%20United,the%20military%20since%20the%20coup.¬†\nFishbein, E., & Lusan, N. N. (2022, December 14). ‚ÄòAfraid of the gun‚Äô: Military coup fuels Myanmar resource grab. Al Jazeera. https://www.aljazeera.com/news/2022/12/14/afraid-of-the-gun-military-coup-fuels-myanmar-resource-grab¬†\nRajagopalan, B., Lall, U., & Tarboton, D. (1997). Evaluation of kernel density estimation methods for daily precipitation resampling. Springer-Verlag.\nShen, B., Xiang Xu, Plaza, A., & Huang, Q. (2020, November 15). Unfolding Spatial-Temporal Patterns of Taxi Trip based on an Improved Network Kernel Density Estimation. MDPI. Retrieved September 22, 2024, from https://www.mdpi.com/2220-9964/9/11/683¬†\nThe Stata Journal. (2003). Adaptive kernel density estimation. Sage Journals. https://journals.sagepub.com/doi/pdf/10.1177/1536867X0300300204"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "To carry out this exercise, I will be using the following R packages:\n\nsf: a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat: has a wide range of useful functions for point pattern analysis. In this take-home exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster: reads, writes, manipulates, analyses and model of gridded spatial data (i.e.¬†raster). In this take-home exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools: provides a set of tools for manipulating geographic data. We mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap: provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nNow, let‚Äôs install and load these packages in RStudio.\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, tidyverse)\n\n\n\n\nNext, I will import the downloaded armed conflict data. For aspatial datasets like this, we will import into Rstudio using read_csv() function of the readr package.\n\n# Import armed conflict data\nconflict_data &lt;- read_csv(\"data/aspatial/2021-01-01-2024-06-30-Myanmar.csv\")\n\nRows: 87746 Columns: 28\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (18): event_id_cnty, event_date, disorder_type, event_type, sub_event_ty...\ndbl (10): year, time_precision, inter1, interaction, iso, latitude, longitud...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe 2021-01-01-2024-06-30-Myanmar.csv dataset contains 87746 rows and 28 columns which indicates the presence of 87746 unique armed conflict events in Myanmar.\n\n\nAfter importing the dataset, we can inspect the dataset using the glimpse() function.\n\n# Inspect the conflict data\nglimpse(conflict_data)\n\nRows: 87,746\nColumns: 28\n$ event_id_cnty      &lt;chr&gt; \"MMR64313\", \"MMR64313\", \"MMR64320\", \"MMR64320\", \"MM‚Ä¶\n$ event_date         &lt;chr&gt; \"30 June 2024\", \"30 June 2024\", \"30 June 2024\", \"30‚Ä¶\n$ year               &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 202‚Ä¶\n$ time_precision     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ disorder_type      &lt;chr&gt; \"Political violence\", \"Political violence\", \"Politi‚Ä¶\n$ event_type         &lt;chr&gt; \"Battles\", \"Battles\", \"Battles\", \"Battles\", \"Battle‚Ä¶\n$ sub_event_type     &lt;chr&gt; \"Armed clash\", \"Armed clash\", \"Armed clash\", \"Armed‚Ä¶\n$ actor1             &lt;chr&gt; \"People's Defense Force - Mandalay\", \"Military Forc‚Ä¶\n$ assoc_actor_1      &lt;chr&gt; \"MDA - AGF: Madaya - The Authentic Genes Force; SST‚Ä¶\n$ inter1             &lt;dbl&gt; 3, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 2, 1, ‚Ä¶\n$ interaction        &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 10, 13, 13, 10, 12, 12, 12,‚Ä¶\n$ civilian_targeting &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ iso                &lt;dbl&gt; 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 1‚Ä¶\n$ region             &lt;chr&gt; \"Southeast Asia\", \"Southeast Asia\", \"Southeast Asia‚Ä¶\n$ country            &lt;chr&gt; \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanma‚Ä¶\n$ admin1             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Ma‚Ä¶\n$ admin2             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Pyinoolwin\", \"Pyinoolwin\",‚Ä¶\n$ admin3             &lt;chr&gt; \"Patheingyi\", \"Patheingyi\", \"Singu\", \"Singu\", \"Thab‚Ä¶\n$ location           &lt;chr&gt; \"Aung Tha Pyay\", \"Aung Tha Pyay\", \"Pin Lel Gyi\", \"P‚Ä¶\n$ latitude           &lt;dbl&gt; 22.1504, 22.1504, 22.5752, 22.5752, 22.8800, 22.880‚Ä¶\n$ longitude          &lt;dbl&gt; 96.2364, 96.2364, 96.0661, 96.0661, 95.9700, 95.970‚Ä¶\n$ geo_precision      &lt;dbl&gt; 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, ‚Ä¶\n$ source             &lt;chr&gt; \"Democratic Voice of Burma; Irrawaddy\", \"Democratic‚Ä¶\n$ source_scale       &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Na‚Ä¶\n$ notes              &lt;chr&gt; \"On 30 June 2024, near Aung Tha Pyay village (Pathe‚Ä¶\n$ fatalities         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, ‚Ä¶\n$ tags               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ timestamp          &lt;dbl&gt; 1720552468, 1720552468, 1720552468, 1720552468, 172‚Ä¶\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe event_date field shows that it uses a character datatype instead of date - we will fix this later. Also, we can observe that thelongitude and langitude fields appear to be adopting the WGS84 geographic coordinate system since they are in the -180/180 and -90/90 range respectively.\n\n\nI will also import the administrative boundary data into a simple features tibble data.frame using¬†st_read()¬†of the sf package. This function reads the shapefile data and returns an¬†sf¬†object that can be used for further analysis.\n\n# Import boundary data\nboundary_sf &lt;- st_read(dsn = \"data/geospatial\",layer = \"mmr_polbnda_adm2_250k_mimu\") %&gt;% st_transform(crs = 32647)\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Take-home_Ex\\Take-home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIn the code above, the %&gt;% operator is used to pass the output of st_read() directly to the st_transform() function. Since the dataset represents the Myanmar boundary, we need to assign the appropriate coordinate reference system, which is UTM zone 47N (EPSG:32647), east of Myanmar. The st_transform() function then converts the CRS of the sf object to EPSG:32647.\n\n\nIn the code below, we can notice that the ESPG code has been updated to 32647.\n\n# Check for changes\nst_crs(boundary_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96¬∞E and 102¬∞E, northern hemisphere between equator and 84¬∞N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nHere, I will use the¬†plot()¬†function which plots the geometry of the¬†sf¬†object. The¬†st_geometry()¬†function is used to extract the geometry of the¬†mpsz_sf¬†object which includes the districts of Myanmar as shown below.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(boundary_sf))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#lets-set-up",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#lets-set-up",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "To carry out this exercise, I will be using the following R packages:\n\nsf: a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat: has a wide range of useful functions for point pattern analysis. In this take-home exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster: reads, writes, manipulates, analyses and model of gridded spatial data (i.e.¬†raster). In this take-home exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools: provides a set of tools for manipulating geographic data. We mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap: provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nNow, let‚Äôs install and load these packages in RStudio.\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, tidyverse)\n\n\n\n\nNext, I will import the downloaded armed conflict data. For aspatial datasets like this, we will import into Rstudio using read_csv() function of the readr package.\n\n# Import armed conflict data\nconflict_data &lt;- read_csv(\"data/aspatial/2021-01-01-2024-06-30-Myanmar.csv\")\n\nRows: 87746 Columns: 28\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (18): event_id_cnty, event_date, disorder_type, event_type, sub_event_ty...\ndbl (10): year, time_precision, inter1, interaction, iso, latitude, longitud...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe 2021-01-01-2024-06-30-Myanmar.csv dataset contains 87746 rows and 28 columns which indicates the presence of 87746 unique armed conflict events in Myanmar.\n\n\nAfter importing the dataset, we can inspect the dataset using the glimpse() function.\n\n# Inspect the conflict data\nglimpse(conflict_data)\n\nRows: 87,746\nColumns: 28\n$ event_id_cnty      &lt;chr&gt; \"MMR64313\", \"MMR64313\", \"MMR64320\", \"MMR64320\", \"MM‚Ä¶\n$ event_date         &lt;chr&gt; \"30 June 2024\", \"30 June 2024\", \"30 June 2024\", \"30‚Ä¶\n$ year               &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 202‚Ä¶\n$ time_precision     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ disorder_type      &lt;chr&gt; \"Political violence\", \"Political violence\", \"Politi‚Ä¶\n$ event_type         &lt;chr&gt; \"Battles\", \"Battles\", \"Battles\", \"Battles\", \"Battle‚Ä¶\n$ sub_event_type     &lt;chr&gt; \"Armed clash\", \"Armed clash\", \"Armed clash\", \"Armed‚Ä¶\n$ actor1             &lt;chr&gt; \"People's Defense Force - Mandalay\", \"Military Forc‚Ä¶\n$ assoc_actor_1      &lt;chr&gt; \"MDA - AGF: Madaya - The Authentic Genes Force; SST‚Ä¶\n$ inter1             &lt;dbl&gt; 3, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 2, 1, ‚Ä¶\n$ interaction        &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 10, 13, 13, 10, 12, 12, 12,‚Ä¶\n$ civilian_targeting &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ iso                &lt;dbl&gt; 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 1‚Ä¶\n$ region             &lt;chr&gt; \"Southeast Asia\", \"Southeast Asia\", \"Southeast Asia‚Ä¶\n$ country            &lt;chr&gt; \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanma‚Ä¶\n$ admin1             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Ma‚Ä¶\n$ admin2             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Pyinoolwin\", \"Pyinoolwin\",‚Ä¶\n$ admin3             &lt;chr&gt; \"Patheingyi\", \"Patheingyi\", \"Singu\", \"Singu\", \"Thab‚Ä¶\n$ location           &lt;chr&gt; \"Aung Tha Pyay\", \"Aung Tha Pyay\", \"Pin Lel Gyi\", \"P‚Ä¶\n$ latitude           &lt;dbl&gt; 22.1504, 22.1504, 22.5752, 22.5752, 22.8800, 22.880‚Ä¶\n$ longitude          &lt;dbl&gt; 96.2364, 96.2364, 96.0661, 96.0661, 95.9700, 95.970‚Ä¶\n$ geo_precision      &lt;dbl&gt; 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, ‚Ä¶\n$ source             &lt;chr&gt; \"Democratic Voice of Burma; Irrawaddy\", \"Democratic‚Ä¶\n$ source_scale       &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Na‚Ä¶\n$ notes              &lt;chr&gt; \"On 30 June 2024, near Aung Tha Pyay village (Pathe‚Ä¶\n$ fatalities         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, ‚Ä¶\n$ tags               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ timestamp          &lt;dbl&gt; 1720552468, 1720552468, 1720552468, 1720552468, 172‚Ä¶\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe event_date field shows that it uses a character datatype instead of date - we will fix this later. Also, we can observe that thelongitude and langitude fields appear to be adopting the WGS84 geographic coordinate system since they are in the -180/180 and -90/90 range respectively.\n\n\nI will also import the administrative boundary data into a simple features tibble data.frame using¬†st_read()¬†of the sf package. This function reads the shapefile data and returns an¬†sf¬†object that can be used for further analysis.\n\n# Import boundary data\nboundary_sf &lt;- st_read(dsn = \"data/geospatial\",layer = \"mmr_polbnda_adm2_250k_mimu\") %&gt;% st_transform(crs = 32647)\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Take-home_Ex\\Take-home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIn the code above, the %&gt;% operator is used to pass the output of st_read() directly to the st_transform() function. Since the dataset represents the Myanmar boundary, we need to assign the appropriate coordinate reference system, which is UTM zone 47N (EPSG:32647), east of Myanmar. The st_transform() function then converts the CRS of the sf object to EPSG:32647.\n\n\nIn the code below, we can notice that the ESPG code has been updated to 32647.\n\n# Check for changes\nst_crs(boundary_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96¬∞E and 102¬∞E, northern hemisphere between equator and 84¬∞N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nHere, I will use the¬†plot()¬†function which plots the geometry of the¬†sf¬†object. The¬†st_geometry()¬†function is used to extract the geometry of the¬†mpsz_sf¬†object which includes the districts of Myanmar as shown below.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(boundary_sf))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#data-wrangling",
    "title": "Take-home Exercise 1",
    "section": "3. Data Wrangling",
    "text": "3. Data Wrangling\n\n3.1 Fixing Incorrect Datatypes\nRecall that the earlier inspection of the conflict_data tibble data frame revealed that the datatype indicated for event date is wrongly labelled as a character instead of a date format.\nAs such, let‚Äôs convert the datatype to the correct ‚Äòdate‚Äô format as shown below.\n\n# Convert the datatype for event_date\nconflict_data$event_date &lt;- as.Date(conflict_data$event_date, format = \"%d %B %Y\")\n\n# Check for changes\nhead(conflict_data)\n\n# A tibble: 6 √ó 28\n  event_id_cnty event_date  year time_precision disorder_type      event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence Battles   \n2 MMR64313      2024-06-30  2024              1 Political violence Battles   \n3 MMR64320      2024-06-30  2024              1 Political violence Battles   \n4 MMR64320      2024-06-30  2024              1 Political violence Battles   \n5 MMR64321      2024-06-30  2024              1 Political violence Battles   \n6 MMR64321      2024-06-30  2024              1 Political violence Battles   \n# ‚Ñπ 22 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;\n\n\n\n\n3.2 Adding new year_quarter column\nWe will want to create a new column to indicate the specific year and quarter for each conflict event since the spatial analysis will be done later in a quarterly manner.\n\n\nExtract year and quarter\nconflict_data$year_quarter &lt;- paste0(\n  year(conflict_data$event_date), \n  \" Q\", \n  quarter(conflict_data$event_date)\n)\n\n# View the new data column\nunique(conflict_data$year_quarter)\n\n\n [1] \"2024 Q2\" \"2024 Q1\" \"2023 Q4\" \"2023 Q3\" \"2023 Q2\" \"2023 Q1\" \"2022 Q4\"\n [8] \"2022 Q3\" \"2022 Q2\" \"2022 Q1\" \"2021 Q4\" \"2021 Q3\" \"2021 Q2\" \"2021 Q1\"\n\n\n\n\n3.3 Fixing Duplicated Event ID in conflict_data Dataframe\nAs shown, there are presence of duplicates in our dataframe returned by the duplicated() function.\n\n# Check for duplicates\nany(duplicated(conflict_data))\n\n[1] TRUE\n\n\nBased on the duplicated event ID: MMR64313 for instance. We can observe the two records are of the same political violence event happening between two actors on 30/6/2024, between the People‚Äôs Defense Force and Military Forces of Myanmar. Upon further research, these two actors are opposing political parties of Myanmar‚Äôs ongoing conflict.\n\n# Inspect an instance of the duplciated event IDs\nhead(conflict_data,2)\n\n# A tibble: 2 √ó 29\n  event_id_cnty event_date  year time_precision disorder_type      event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence Battles   \n2 MMR64313      2024-06-30  2024              1 Political violence Battles   \n# ‚Ñπ 23 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;, year_quarter &lt;chr&gt;\n\n\n\n\n\n\n\n\nReflection\n\n\n\nShould duplicated data be removed in this analysis?\nA single event (e.g.¬†MMR64313) can have duplicated rows with different actor1 values, typically due to counterattacks from opposing sides, leading to different data entries into the conflict_data dataset.\nHence, I will remove duplicated events found in the conflict_data dataframe as long as the rows have the same event ID indicated.\n\n\nHere, I did another check to ensure there is not more than 2 possible repeated event IDs in the first 20 rows of conflict_data.\n\n\nCheck duplicated events for first 20 rows\nduplicate_counts_first_20 &lt;- conflict_data %&gt;%\n  slice(1:20) %&gt;%            \n  group_by(event_id_cnty) %&gt;% \n  summarize(count = n()) %&gt;%  \n  filter(count &gt; 1)         \n\n# View the result\nprint(duplicate_counts_first_20)\n\n\n# A tibble: 9 √ó 2\n  event_id_cnty count\n  &lt;chr&gt;         &lt;int&gt;\n1 MMR64313          2\n2 MMR64320          2\n3 MMR64321          2\n4 MMR64323          2\n5 MMR64325          2\n6 MMR64326          2\n7 MMR64328          2\n8 MMR64330          2\n9 MMR64331          2\n\n\nWith that checked, I‚Äôll remove the duplicated rows with a repeated Event ID.\n\n\nRemove duplicated rows\n# Retrieve data of duplicated rows\nmerged_duplicates &lt;- conflict_data %&gt;%\n  filter(duplicated(event_id_cnty) | duplicated(event_id_cnty, fromLast = TRUE)) %&gt;%\n  arrange(event_id_cnty) %&gt;%\n  group_by(event_id_cnty) %&gt;%\n  summarize(\n    actor2 = last(actor1),\n    assoc_actor_2 = last(assoc_actor_1)\n  )\n\nconflict_data_no_duplicates &lt;- conflict_data %&gt;%\n  filter(!duplicated(event_id_cnty))\n\n# Update conflict_data dataframe with new columns\nconflict_data &lt;- conflict_data_no_duplicates %&gt;%\n  left_join(merged_duplicates, by = \"event_id_cnty\")\n\n# View dataframe\nprint(head(conflict_data))\n\n\n# A tibble: 6 √ó 31\n  event_id_cnty event_date  year time_precision disorder_type         event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence    Battles   \n2 MMR64320      2024-06-30  2024              1 Political violence    Battles   \n3 MMR64321      2024-06-30  2024              1 Political violence    Battles   \n4 MMR64322      2024-06-30  2024              1 Strategic developmen‚Ä¶ Strategic‚Ä¶\n5 MMR64323      2024-06-30  2024              1 Political violence    Battles   \n6 MMR64324      2024-06-30  2024              1 Strategic developmen‚Ä¶ Strategic‚Ä¶\n# ‚Ñπ 25 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;, year_quarter &lt;chr&gt;, actor2 &lt;chr&gt;,\n#   assoc_actor_2 &lt;chr&gt;\n\n\nWe can observe that there are no longer any duplicated event IDs in our conflict_data data frame.\n\nany(duplicated(conflict_data))\n\n[1] FALSE\n\n\n\n\n3.4 Converting Aspatial Data to Simple Feature Format\nFor the purpose of this exercise, we will want to integrate and analyse aspatial data in a geographic context. I‚Äôll do a check if conflict_data needs to be converted to a sf data frame - if it outputs anything else but sf, then it‚Äôs not a simple feature data frame!\n\nclass(conflict_data)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can see that conflict_data is not a sf data frame. Since a non-simple feature data frame does not have a ‚Äúgeometry‚Äù column, we‚Äôll need to convert conflict_data into a simple feature data frame\n\n\nWe can convert conflict_data into a simple feature data frame by using¬†st_as_sf()¬†from the¬†sf¬†package. Addiitionally, we will also need to transform coordinate system from geographic (ESPG: 4326) to projected (ESPG: 32647) using st_transform().\n\n# Convert to simple feature format\nconflict_data_sf &lt;- st_as_sf(conflict_data, coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;% st_transform(crs = 32647)\n\n# Inspect the changes\nglimpse(conflict_data_sf)\n\nRows: 51,553\nColumns: 30\n$ event_id_cnty      &lt;chr&gt; \"MMR64313\", \"MMR64320\", \"MMR64321\", \"MMR64322\", \"MM‚Ä¶\n$ event_date         &lt;date&gt; 2024-06-30, 2024-06-30, 2024-06-30, 2024-06-30, 20‚Ä¶\n$ year               &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 202‚Ä¶\n$ time_precision     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ disorder_type      &lt;chr&gt; \"Political violence\", \"Political violence\", \"Politi‚Ä¶\n$ event_type         &lt;chr&gt; \"Battles\", \"Battles\", \"Battles\", \"Strategic develop‚Ä¶\n$ sub_event_type     &lt;chr&gt; \"Armed clash\", \"Armed clash\", \"Armed clash\", \"Chang‚Ä¶\n$ actor1             &lt;chr&gt; \"People's Defense Force - Mandalay\", \"People's Defe‚Ä¶\n$ assoc_actor_1      &lt;chr&gt; \"MDA - AGF: Madaya - The Authentic Genes Force; SST‚Ä¶\n$ inter1             &lt;dbl&gt; 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 3, 3, 3, 7, 1, ‚Ä¶\n$ interaction        &lt;dbl&gt; 13, 13, 13, 10, 13, 10, 12, 12, 12, 12, 12, 13, 13,‚Ä¶\n$ civilian_targeting &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ iso                &lt;dbl&gt; 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 1‚Ä¶\n$ region             &lt;chr&gt; \"Southeast Asia\", \"Southeast Asia\", \"Southeast Asia‚Ä¶\n$ country            &lt;chr&gt; \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanma‚Ä¶\n$ admin1             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Sagaing\", \"Sag‚Ä¶\n$ admin2             &lt;chr&gt; \"Mandalay\", \"Pyinoolwin\", \"Pyinoolwin\", \"Shwebo\", \"‚Ä¶\n$ admin3             &lt;chr&gt; \"Patheingyi\", \"Singu\", \"Thabeikkyin\", \"Khin-U\", \"My‚Ä¶\n$ location           &lt;chr&gt; \"Aung Tha Pyay\", \"Pin Lel Gyi\", \"Thabeikkyin\", \"Khi‚Ä¶\n$ geo_precision      &lt;dbl&gt; 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, ‚Ä¶\n$ source             &lt;chr&gt; \"Democratic Voice of Burma; Irrawaddy\", \"Irrawaddy\"‚Ä¶\n$ source_scale       &lt;chr&gt; \"National\", \"National\", \"National\", \"Subnational-Na‚Ä¶\n$ notes              &lt;chr&gt; \"On 30 June 2024, near Aung Tha Pyay village (Pathe‚Ä¶\n$ fatalities         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ tags               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ timestamp          &lt;dbl&gt; 1720552468, 1720552468, 1720552468, 1720552468, 172‚Ä¶\n$ year_quarter       &lt;chr&gt; \"2024 Q2\", \"2024 Q2\", \"2024 Q2\", \"2024 Q2\", \"2024 Q‚Ä¶\n$ actor2             &lt;chr&gt; \"Military Forces of Myanmar (2021-)\", \"Military For‚Ä¶\n$ assoc_actor_2      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Uniden‚Ä¶\n$ geometry           &lt;POINT [m]&gt; POINT (214961 2452068), POINT (198303.2 24994‚Ä¶\n\n\n\n\n\n\n\n\nObservations\n\n\n\nNotice that a new column called¬†geometry¬†has been added into the data frame. On the other hand, the¬†longitude¬†and¬†latitude¬†columns have been removed from the data frame.\n\n\nWe can further inspect the newly created ‚Äògeometry‚Äô column of conflict_data_sf\n\n# Retrieve geometry column\nst_geometry(conflict_data_sf)\n\nGeometry set for 51553 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -208804.4 ymin: 1103500 xmax: 640934.5 ymax: 3042960\nProjected CRS: WGS 84 / UTM zone 47N\nFirst 5 geometries:\n\n\nPOINT (214961 2452068)\n\n\nPOINT (198303.2 2499463)\n\n\nPOINT (189105.4 2533434)\n\n\nPOINT (160913.9 2522331)\n\n\nPOINT (146213 2428487)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt consists of 51,533 features consisting of point geometric features where the underlying datum is in WGS 84 format.\n\n\nTo ensure that the coordinate system is correctly updated, we can use the st_crs() function where we observe that the ESPG code is correctly indicated as 32647.\n\n# Check CRS format\nst_crs(conflict_data_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96¬∞E and 102¬∞E, northern hemisphere between equator and 84¬∞N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\n3.5 Reduce Data File Size\nIn this section, I will reduce the current Myanmar armed conflict dataset as the time taken for computing the kernel density estimates can take up to 30 minutes long which is not computationally efficient.\n\n1) Remove ‚ÄòProtests‚Äô and ‚ÄòRiots‚Äô Event Types\nI will remove rows in the conflicts_data_sf dataset that don‚Äôt focus on the four main event types (Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians), as mentioned in the exercise brief.\n\nconflict_data_sf &lt;- conflict_data_sf %&gt;%\n  filter(!(event_type %in% c(\"Protests\", \"Riots\")))\n\nunique(conflict_data_sf$event_type)\n\n[1] \"Battles\"                    \"Strategic developments\"    \n[3] \"Violence against civilians\" \"Explosions/Remote violence\"\n\n\n\n\n2) Remove unused columns in boundary_sf\nAs seen, there are 8 columns in the simple feature data frame of boundary_sf.\n\n# Inspect first rows of data in boundary_sf\nhead(boundary_sf)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -14915.04 ymin: 1736124 xmax: 187961.7 ymax: 2051144\nProjected CRS: WGS 84 / UTM zone 47N\n  OBJECTID         ST ST_PCODE        DT   DT_PCODE      DT_MMR PCode_V\n1        1 Ayeyarwady   MMR017  Hinthada MMR017D002    ·Äü·ÄÑ·Ä∫·Äπ·Äû·Ä¨·Äê·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n2        2 Ayeyarwady   MMR017   Labutta MMR017D004    ·Äú·Äï·ÄΩ·Äê·Äπ·Äê·Ä¨·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n3        3 Ayeyarwady   MMR017    Maubin MMR017D005     ·Äô·Ä°·Ä∞·Äï·ÄÑ·Ä∫·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n4        4 Ayeyarwady   MMR017 Myaungmya MMR017D003 ·Äô·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äô·Äº·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n5        5 Ayeyarwady   MMR017   Pathein MMR017D001      ·Äï·ÄØ·Äû·Ä≠·Äô·Ä∫·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n6        6 Ayeyarwady   MMR017    Pyapon MMR017D006     ·Äñ·Äª·Ä¨·Äï·ÄØ·Ä∂·ÄÅ·Äõ·Ä≠·ÄØ·ÄÑ·Ä∫     9.4\n                        geometry\n1 MULTIPOLYGON (((90859.89 20...\n2 MULTIPOLYGON (((75991.51 17...\n3 MULTIPOLYGON (((115559 1928...\n4 MULTIPOLYGON (((39919.39 18...\n5 MULTIPOLYGON (((-6302.348 1...\n6 MULTIPOLYGON (((93411.72 17...\n\n\nI will remove ‚ÄôDT_MMR‚Äù column as we already have the District Name in English in DT and won‚Äôt require the district names in Myanmar Language. Next, we will remove the coded versions of ST (state/region) and DT (district) columns, namely ST_PCODE and DT_PCODE. Additionally, we won‚Äôt need the PCode_V column since we will be dropping the PCODE column too.\n\nboundary_sf &lt;- boundary_sf %&gt;% dplyr::select('OBJECTID', 'ST', 'DT','geometry')\nsummary(boundary_sf)\n\n    OBJECTID          ST                 DT                     geometry \n Min.   : 1.00   Length:80          Length:80          MULTIPOLYGON :80  \n 1st Qu.:20.75   Class :character   Class :character   epsg:32647   : 0  \n Median :40.50   Mode  :character   Mode  :character   +proj=utm ...: 0  \n Mean   :40.50                                                           \n 3rd Qu.:60.25                                                           \n Max.   :80.00                                                           \n\n\n\n\n3) Remove unused columns in conflict_data\nI will also remove unnecessary columns of the conflict_data data frame that won‚Äôt be used in our spatial analysis later.\n\n\nRemove unnecessary columns\nconflict_data_sf &lt;- conflict_data_sf %&gt;%\n  select(event_id_cnty, event_date, year_quarter, disorder_type, event_type, location, geometry, fatalities)\n\nsummary(conflict_data_sf)\n\n\n event_id_cnty        event_date         year_quarter       disorder_type     \n Length:42608       Min.   :2021-01-01   Length:42608       Length:42608      \n Class :character   1st Qu.:2022-01-10   Class :character   Class :character  \n Mode  :character   Median :2022-10-13   Mode  :character   Mode  :character  \n                    Mean   :2022-10-29                                        \n                    3rd Qu.:2023-08-29                                        \n                    Max.   :2024-06-30                                        \n  event_type          location                  geometry       fatalities    \n Length:42608       Length:42608       POINT        :42608   Min.   :  0.00  \n Class :character   Class :character   epsg:32647   :    0   1st Qu.:  0.00  \n Mode  :character   Mode  :character   +proj=utm ...:    0   Median :  0.00  \n                                                             Mean   :  1.27  \n                                                             3rd Qu.:  1.00  \n                                                             Max.   :201.00  \n\n\nLet‚Äôs append conflict_data_sf with the columns of boundary_sf to assist our analysis later.\n\n# Link conflict event to its district region\nconflict_data_sf &lt;- st_join(conflict_data_sf, boundary_sf, join = st_intersects)\n\n\n\n\n3.6 Converting Simple Features Data Frame into ppp Object\nIt is important that we convert conflict_data_sf (a simple feature data frame) into a planer point pattern (ppp) object format, since the spatstat package that we‚Äôll be using for the Spatial Point Pattern Analysis later is specifically designed for working with ppp-formated data. Additionally, I will begin with categorising the ppp objects into their unique year_quarter category.\n\n\nCreate ppp objects based on year_quarter category\n# Create an empty list to store the ppp objects\nppp_list &lt;- list()\n\n# Loop through each unique year_quarter category\nfor (yq in unique(conflict_data_sf$year_quarter)) {\n  # Subset the data for the current year_quarter\n  subset_data_sf &lt;- conflict_data_sf %&gt;% filter(year_quarter == yq)\n  \n  # Convert the subset to a ppp object\n  subset_ppp &lt;- as.ppp(subset_data_sf$geometry)\n  \n  # Add the ppp object to the list\n  ppp_list[[yq]] &lt;- subset_ppp\n}\n\n# Check list\nppp_list\n\n\n$`2024 Q2`\nPlanar point pattern: 2788 points\nwindow: rectangle = [-208804.4, 597543.7] x [1103500.1, 3026504.9] units\n\n$`2024 Q1`\nPlanar point pattern: 3186 points\nwindow: rectangle = [-207135, 591875.9] x [1245380, 3026504.9] units\n\n$`2023 Q4`\nPlanar point pattern: 3627 points\nwindow: rectangle = [-206931.7, 604775.1] x [1103500.1, 3020772.2] units\n\n$`2023 Q3`\nPlanar point pattern: 3010 points\nwindow: rectangle = [-197883.4, 518300.4] x [1103500.1, 3027041.8] units\n\n$`2023 Q2`\nPlanar point pattern: 2745 points\nwindow: rectangle = [-191261.5, 518300.4] x [1103500.1, 3006372.9] units\n\n$`2023 Q1`\nPlanar point pattern: 3101 points\nwindow: rectangle = [-199243.8, 591875.9] x [1103500.1, 3026504.9] units\n\n$`2022 Q4`\nPlanar point pattern: 3296 points\nwindow: rectangle = [-206531.5, 518300.4] x [1103500.1, 2931517.1] units\n\n$`2022 Q3`\nPlanar point pattern: 3486 points\nwindow: rectangle = [-206196.6, 568361.5] x [1103500.1, 3026504.9] units\n\n$`2022 Q2`\nPlanar point pattern: 3580 points\nwindow: rectangle = [-206931.7, 640934.5] x [1103500.1, 3026504.9] units\n\n$`2022 Q1`\nPlanar point pattern: 3563 points\nwindow: rectangle = [-204784, 591875.9] x [1103500.1, 3026504.9] units\n\n$`2021 Q4`\nPlanar point pattern: 3844 points\nwindow: rectangle = [-200024.3, 591875.9] x [1103500.1, 3042960.3] units\n\n$`2021 Q3`\nPlanar point pattern: 2754 points\nwindow: rectangle = [-193181.1, 591875.9] x [1103500.1, 3042960.3] units\n\n$`2021 Q2`\nPlanar point pattern: 2916 points\nwindow: rectangle = [-191409.1, 640934.5] x [1132472.1, 3042960.3] units\n\n$`2021 Q1`\nPlanar point pattern: 712 points\nwindow: rectangle = [-203795.3, 591875.9] x [1375186.1, 3026504.9] units\n\n\nWe can visualise the spread of conflict events across each quarter from January 2021 to June 2024 using the plot() function as shown below.\n\n\nVisualise the spread of conflicts by year_quarter\n# Ensure 'year_quarter' is a factor\nconflict_data_sf$year_quarter &lt;- as.factor(conflict_data_sf$year_quarter)\n\n# Loop through each unique year_quarter and create separate plots\nyear_quarters &lt;- unique(conflict_data_sf$year_quarter)\n\n# Set up a grid layout for multiple plots (adjust 'mfrow' as needed)\npar(mfrow = c(2,3))\npar(mar = c(0,0,1,0))\n\n# Loop through each year_quarter and plot\nfor (yq in year_quarters) {\n  subset_data_sf &lt;- conflict_data_sf[conflict_data_sf$year_quarter == yq, ]\n  conflict_data_ppp &lt;- as.ppp(subset_data_sf$geometry)\n  \n  # Plot each subset ppp object\n  plot(conflict_data_ppp, main = paste(\"Year-Quarter:\", yq))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt is noticeable that there conflict events have occured more frequently since 2021 as points plotted on the graph have gotten darker across 2021 to 2024. We can also observe the possibility of duplicated events occurring from the darker spots in the plot, in which it appears more intense in Myanmar‚Äôs central and west regions.\n\n\n\n\n3.7 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area, that is Myanmar‚Äôs boundary in this case. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to convert the boundary_data_sf simple feature data frame into an owin object of spatstat.\n\n# Convert to owin object\nmyanmar_owin &lt;- as.owin(boundary_sf)\n\n# Visualise the owin object\nplot(myanmar_owin)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom my observations, the as.owin() function converts the boundary_data_sf spatial boundary into a window object that represents the outer boundary of the spatial region and does not handle internal structures or districts we previously saw from the plot of boundary_data_sf.\n\n\nWe can also take a quick look at the owin object properties as shown. I will be converting it to a data frame for the purposes of getting a quick glimpse of the object.\n\n# Summary info of owin object\nowin_df &lt;- as.data.frame(myanmar_owin)\nprint(head(owin_df))\n\n         x       y id sign\n1 56519.39 2741919  1   -1\n2 56917.28 2741947  1   -1\n3 57000.15 2741973  1   -1\n4 57068.51 2741994  1   -1\n5 57221.44 2742142  1   -1\n6 57068.51 2741994  1   -1\n\n\n\n\n3.8 Combining ppp Object and owin Object\nIn this last step of geospatial data wrangling, I will mask all ppp object with the owin object I created earlier to put in place all conflict events within the boundary of Myanmar. Doing so can also optimise the memory usage for large datasets.\n\n\n\n\n\n\nThe¬†ppp¬†object outputted from combining both the point and polygon feature results in the boundary of Myanmar outlining the plot of conflict events as shown.\n\n# Set up plotting layout\nn &lt;- length(masked_ppp_list)\n\n# Plot each masked ppp object\npar(mfrow = c(2,3), mar = c(0,0,1,0))  # Adjust margins as needed\nfor (quarter in names(masked_ppp_list)) {\n  plot(masked_ppp_list[[quarter]], main = paste(\"Year Quarter:\", quarter))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmasked_ppp_list_km = list()\n\nfor (quarter in names(masked_ppp_list)) {\n  ppp_obj &lt;- masked_ppp_list[[quarter]]\n  ppp_obj_km &lt;- rescale(ppp_obj, 1000, \"km\")\n  masked_ppp_list_km[[quarter]] &lt;- ppp_obj_km\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#spatio-temporal-kde",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#spatio-temporal-kde",
    "title": "Take-home Exercise 1",
    "section": "7. Spatio-Temporal KDE",
    "text": "7. Spatio-Temporal KDE\n\n\nSet up DayofYear variable per quarter\nQ2_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2024 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2024 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\n\n\n7.1 Creating ppp object\nIn the code chunk below, DayofYear from the fire_sf data frame is selected and is included in the output ppp object.\n\n\nCreate ppp object per quarter\nQ2_2024_ppp &lt;- Q2_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2024_ppp &lt;- Q1_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2023_ppp &lt;- Q4_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2023_ppp &lt;- Q3_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2023_ppp &lt;- Q2_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2023_ppp &lt;- Q1_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2022_ppp &lt;- Q4_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2022_ppp &lt;- Q3_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2022_ppp &lt;- Q2_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2022_ppp &lt;- Q1_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2021_ppp &lt;- Q4_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2021_ppp &lt;- Q3_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2021_ppp &lt;- Q2_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2021_ppp &lt;- Q1_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\n\n7.2 Combining ppp with owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\n\nMask the ppp object with owin object\nQ2_2024_owin &lt;- Q2_2024_ppp[myanmar_owin]\n\nQ1_2024_owin &lt;- Q1_2024_ppp[myanmar_owin]\n\nQ4_2023_owin &lt;- Q4_2023_ppp[myanmar_owin]\n\nQ3_2023_owin &lt;- Q3_2023_ppp[myanmar_owin]\n\nQ2_2023_owin &lt;- Q2_2023_ppp[myanmar_owin]\n\nQ1_2023_owin &lt;- Q1_2023_ppp[myanmar_owin]\n\nQ4_2022_owin &lt;- Q4_2022_ppp[myanmar_owin]\n\nQ3_2022_owin &lt;- Q3_2022_ppp[myanmar_owin]\n\nQ2_2022_owin &lt;- Q2_2022_ppp[myanmar_owin]\n\nQ1_2022_owin &lt;- Q1_2022_ppp[myanmar_owin]\n\nQ4_2021_owin &lt;- Q4_2021_ppp[myanmar_owin]\n\nQ3_2021_owin &lt;- Q3_2021_ppp[myanmar_owin]\n\nQ2_2021_owin &lt;- Q2_2021_ppp[myanmar_owin]\n\nQ1_2021_owin &lt;- Q1_2021_ppp[myanmar_owin]\n\n\nNow, I will perform a spatio-temporal kernel density estimate on the owin object which gives us insights into where and when conflict event occurrences are concentrated within the specified observation window.\n\n\nPerform spatial temporal KDE per quarter\nQ2_2024_stkde &lt;- spattemp.density(Q2_2024_owin)\n\nQ1_2024_stkde &lt;- spattemp.density(Q1_2024_owin)\n\nQ4_2023_stkde &lt;- spattemp.density(Q4_2023_owin)\n\nQ3_2023_stkde &lt;- spattemp.density(Q3_2023_owin)\n\nQ2_2023_stkde &lt;- spattemp.density(Q2_2023_owin)\n\nQ1_2023_stkde &lt;- spattemp.density(Q1_2023_owin)\n\nQ4_2022_stkde &lt;- spattemp.density(Q4_2022_owin)\n\nQ3_2022_stkde &lt;- spattemp.density(Q3_2022_owin)\n\nQ2_2022_stkde &lt;- spattemp.density(Q2_2022_owin)\n\nQ1_2022_stkde &lt;- spattemp.density(Q1_2022_owin)\n\nQ4_2021_stkde &lt;- spattemp.density(Q4_2021_owin)\n\nQ3_2021_stkde &lt;- spattemp.density(Q3_2021_owin)\n\nQ2_2021_stkde &lt;- spattemp.density(Q2_2021_owin)\n\nQ1_2021_stkde &lt;- spattemp.density(Q1_2021_owin)\n\n\n\n\nSTKDE of 2024 Q2\n# Load necessary libraries\nlibrary(spatstat)\nlibrary(magick)\nlibrary(viridis)\n\n# Create a directory to store PNG frames\nif (!dir.exists(\"2024_Q2_frames\")) {\n  dir.create(\"2024_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2024_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2024_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2024_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2024 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2024_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2024_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2024 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"2024_Q1_frames\")) {\n  dir.create(\"2024_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2024_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2024_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2024_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2024 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2024_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2024_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q4\n# Create a directory to store PNG frames\nif (!dir.exists(\"2023_Q4_frames\")) {\n  dir.create(\"2023_Q4_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q4_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q4_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2023_Q4_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q4 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2023_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2023_Q4_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q3\n# Create a directory to store PNG frames\nif (!dir.exists(\"2023_Q3_frames\")) {\n  dir.create(\"2023_Q3_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q3_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q3_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2023_Q3_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q3 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2023_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2023_Q3_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q2\n# Create a directory to store PNG frames\nif (!dir.exists(\"2023_Q2_frames\")) {\n  dir.create(\"2023_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2023_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2023_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2023_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"2023_Q1_frames\")) {\n  dir.create(\"2023_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2023_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2023_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2023_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q4\n# Create a directory to store PNG frames\nif (!dir.exists(\"2022_Q4_frames\")) {\n  dir.create(\"2022_Q4_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q4_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q4_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2022_Q4_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q4 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2022_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2022_Q4_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q3\n# Create a directory to store PNG frames\nif (!dir.exists(\"2022_Q3_frames\")) {\n  dir.create(\"2022_Q3_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q3_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q3_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2022_Q3_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q3 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2022_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2022_Q3_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q2\n# Create a directory to store PNG frames\nif (!dir.exists(\"2022_Q2_frames\")) {\n  dir.create(\"2022_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2022_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2022_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2022_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"2022_Q1_frames\")) {\n  dir.create(\"2022_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2022_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2022_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2022_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q4\n# Create a directory to store PNG frames\nif (!dir.exists(\"2021_Q4_frames\")) {\n  dir.create(\"2021_Q4_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q4_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q4_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2021_Q4_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2021 Q4 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2021_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2021_Q4_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q3\n# Create a directory to store PNG frames\nif (!dir.exists(\"2021_Q3_frames\")) {\n  dir.create(\"2021_Q3_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q3_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q3_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2021_Q3_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2021 Q3 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2021_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2021_Q3_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q2\n# Create a directory to store PNG frames\nif (!dir.exists(\"2021_Q2_frames\")) {\n  dir.create(\"2021_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2021_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2021 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2021_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2021_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"2021_Q1_frames\")) {\n  dir.create(\"2021_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2021_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2021_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2021_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\nLet‚Äôs plot our animated spatio-temporal KDE outputs now.\n\nlibrary(spatstat)\nlibrary(magick)\nlibrary(viridis)\n\n# 2024 Q2\nframes &lt;- image_read(list.files(\"2024_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2024 Q1\nframes &lt;- image_read(list.files(\"2024_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n# 2023 Q4\nframes &lt;- image_read(list.files(\"2023_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2023 Q3\nframes &lt;- image_read(list.files(\"2023_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2023 Q2\nframes &lt;- image_read(list.files(\"2023_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2023 Q1\nframes &lt;- image_read(list.files(\"2023_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n# 2022 Q4\nframes &lt;- image_read(list.files(\"2022_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2022 Q3\nframes &lt;- image_read(list.files(\"2022_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2022 Q2\nframes &lt;- image_read(list.files(\"2022_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2022 Q1\nframes &lt;- image_read(list.files(\"2022_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n# 2021 Q4\nframes &lt;- image_read(list.files(\"2021_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2021 Q3\nframes &lt;- image_read(list.files(\"2021_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2021 Q2\nframes &lt;- image_read(list.files(\"2021_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2021 Q1\nframes &lt;- image_read(list.files(\"2021_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\nObservations"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#references",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#references",
    "title": "Take-home Exercise 1",
    "section": "References",
    "text": "References\nCrawley, M. J. (2007). The R Book. Wiley.\nThe Stata Journal. (2003). Adaptive kernel density estimation. Sage Journals. https://journals.sagepub.com/doi/pdf/10.1177/1536867X0300300204"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#spatio-temporal-kde",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#spatio-temporal-kde",
    "title": "Take-home Exercise 1",
    "section": "7. Spatio-Temporal KDE",
    "text": "7. Spatio-Temporal KDE\nWe focus on the continuous time"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#install-required-libraries",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#install-required-libraries",
    "title": "In-class Exercise 5",
    "section": "1.1 Install Required Libraries",
    "text": "1.1 Install Required Libraries\nWe will first want to install the GWModel package from CRAN\n\ninstall.packages(\"GWmodel\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#importing-libraries-into-r",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#importing-libraries-into-r",
    "title": "In-class Exercise 5",
    "section": "1.2 Importing Libraries into R",
    "text": "1.2 Importing Libraries into R\nIn this in-class exercise, sf, spdep, tmap, tidyverse, knitr and GWmodel will be used.\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr, GWmodel)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-datasets",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-datasets",
    "title": "In-class Exercise 5",
    "section": "1.3 Preparing the Datasets",
    "text": "1.3 Preparing the Datasets\nI will be using the Hunan dataset used in the Hands-on Exercise 5 spatial weights and applications.\n\n1.3.1 Importing Geospatial Data\nFirstly, we will import the Hunan county boundary layer. This is a geospatial data set in ESRI shapefile format. The code chunk below uses¬†st_read()¬†of¬†sf¬†package.\n\nhunan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex5\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n1.3.2 Importing Aspatial Data\nNext, I will import the aspatial data set. This data is a csv file containing selected Hunan‚Äôs local development indicators in 2012.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n1.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan‚Äôs SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan_sf &lt;- left_join(hunan_sf, hunan2012) %&gt;%\n  select(1:3, 7, 15, 16, 31, 32)\n\nJoining with `by = join_by(County)`"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#mapping-gdppc",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#mapping-gdppc",
    "title": "In-class Exercise 5",
    "section": "2. Mapping GDPPC",
    "text": "2. Mapping GDPPC\nNow, we will use qtm() function of tmap package to create a basemap and a choropleth map showing the distribution of GDPPC 2012.\n\nbasemap &lt;- tm_shape(hunan_sf) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan_sf, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#determine-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#determine-adaptive-bandwidth",
    "title": "In-class Exercise 5",
    "section": "4.1 Determine adaptive bandwidth",
    "text": "4.1 Determine adaptive bandwidth\n\n1) Using Cross-Validation\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = TRUE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nAdaptive bandwidth: 62 CV score: 15515442343 \nAdaptive bandwidth: 46 CV score: 14937956887 \nAdaptive bandwidth: 36 CV score: 14408561608 \nAdaptive bandwidth: 29 CV score: 14198527496 \nAdaptive bandwidth: 26 CV score: 13898800611 \nAdaptive bandwidth: 22 CV score: 13662299974 \nAdaptive bandwidth: 22 CV score: 13662299974 \n\n\n\n\n2) Using AIC\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = TRUE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1923.156 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1920.469 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 1917.324 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 1916.661 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1914.897 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-geographically-wieghted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-geographically-wieghted-summary-statistics",
    "title": "In-class Exercise 5",
    "section": "4.2 Computing geographically wieghted summary statistics",
    "text": "4.2 Computing geographically wieghted summary statistics\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-output-data",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-output-data",
    "title": "In-class Exercise 5",
    "section": "4.3 Preparing the output data",
    "text": "4.3 Preparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#visualising-geographically-weighted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#visualising-geographically-weighted-summary-statistics",
    "title": "In-class Exercise 5",
    "section": "4.4 Visualising geographically weighted summary statistics",
    "text": "4.4 Visualising geographically weighted summary statistics\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically weightted mean\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.text.size = 1,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#determine-fixed-bandwidth",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#determine-fixed-bandwidth",
    "title": "In-class Exercise 5",
    "section": "5.1 Determine fixed bandwidth",
    "text": "5.1 Determine fixed bandwidth\n\n5.1.1 Cross-Validation\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nFixed bandwidth: 357.4897 CV score: 16265191728 \nFixed bandwidth: 220.985 CV score: 14954930931 \nFixed bandwidth: 136.6204 CV score: 14134185837 \nFixed bandwidth: 84.48025 CV score: 13693362460 \nFixed bandwidth: 52.25585 CV score: Inf \nFixed bandwidth: 104.396 CV score: 13891052305 \nFixed bandwidth: 72.17162 CV score: 13577893677 \nFixed bandwidth: 64.56447 CV score: 14681160609 \nFixed bandwidth: 76.8731 CV score: 13444716890 \nFixed bandwidth: 79.77877 CV score: 13503296834 \nFixed bandwidth: 75.07729 CV score: 13452450771 \nFixed bandwidth: 77.98296 CV score: 13457916138 \nFixed bandwidth: 76.18716 CV score: 13442911302 \nFixed bandwidth: 75.76323 CV score: 13444600639 \nFixed bandwidth: 76.44916 CV score: 13442994078 \nFixed bandwidth: 76.02523 CV score: 13443285248 \nFixed bandwidth: 76.28724 CV score: 13442844774 \nFixed bandwidth: 76.34909 CV score: 13442864995 \nFixed bandwidth: 76.24901 CV score: 13442855596 \nFixed bandwidth: 76.31086 CV score: 13442847019 \nFixed bandwidth: 76.27264 CV score: 13442846793 \nFixed bandwidth: 76.29626 CV score: 13442844829 \nFixed bandwidth: 76.28166 CV score: 13442845238 \nFixed bandwidth: 76.29068 CV score: 13442844678 \nFixed bandwidth: 76.29281 CV score: 13442844691 \nFixed bandwidth: 76.28937 CV score: 13442844698 \nFixed bandwidth: 76.2915 CV score: 13442844676 \nFixed bandwidth: 76.292 CV score: 13442844679 \nFixed bandwidth: 76.29119 CV score: 13442844676 \nFixed bandwidth: 76.29099 CV score: 13442844676 \nFixed bandwidth: 76.29131 CV score: 13442844676 \nFixed bandwidth: 76.29138 CV score: 13442844676 \nFixed bandwidth: 76.29126 CV score: 13442844676 \nFixed bandwidth: 76.29123 CV score: 13442844676 \n\n\n\n\n5.1.2 AIC\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nFixed bandwidth: 357.4897 AICc value: 1927.631 \nFixed bandwidth: 220.985 AICc value: 1921.547 \nFixed bandwidth: 136.6204 AICc value: 1919.993 \nFixed bandwidth: 84.48025 AICc value: 1940.603 \nFixed bandwidth: 168.8448 AICc value: 1919.457 \nFixed bandwidth: 188.7606 AICc value: 1920.007 \nFixed bandwidth: 156.5362 AICc value: 1919.41 \nFixed bandwidth: 148.929 AICc value: 1919.527 \nFixed bandwidth: 161.2377 AICc value: 1919.392 \nFixed bandwidth: 164.1433 AICc value: 1919.403 \nFixed bandwidth: 159.4419 AICc value: 1919.393 \nFixed bandwidth: 162.3475 AICc value: 1919.394 \nFixed bandwidth: 160.5517 AICc value: 1919.391"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-fixed-bandwidth",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-fixed-bandwidth",
    "title": "In-class Exercise 5",
    "section": "5.2 Computing Fixed Bandwidth",
    "text": "5.2 Computing Fixed Bandwidth\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = FALSE,\n               longlat = T)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-output-data-1",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-output-data-1",
    "title": "In-class Exercise 5",
    "section": "5.3 Preparing the output data",
    "text": "5.3 Preparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#visualising-geographically-weighted-summary-statistics-1",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#visualising-geographically-weighted-summary-statistics-1",
    "title": "In-class Exercise 5",
    "section": "5.4 Visualising geographically weighted summary statistics",
    "text": "5.4 Visualising geographically weighted summary statistics\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically weightted mean\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.text.size = 1,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-geographically-weightted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-geographically-weightted-summary-statistics",
    "title": "In-class Exercise 5",
    "section": "4.2 Computing geographically weightted summary statistics",
    "text": "4.2 Computing geographically weightted summary statistics\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#my-reflection",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#my-reflection",
    "title": "Take-home Exercise 1",
    "section": "9. My Reflection",
    "text": "9. My Reflection\nThis Take-Home Exercise 1 had greatly stretched my learnings and provided me the opportunity to apply spatial and spatio-temporal point pattern analysis on a real-world dataset.\nA key takeaway from Myanmar‚Äôs humanitarian conflict is how complex the interplay of ethnic, political, and religious struggles has been and continues to be an on-going humanitarian crisis, involving the military, ethnic armed groups, political militias, and civilians.\nMy analysis has shown that central and southern states (notably, Sagaing, Mandalay, Magway and Yangon) have experienced intense clashes between state forces and politica/identity militias, while northern Myanmar sees high conflict levels between rebels and political militias.\n&lt;INSERT TEMPORAL ANALYSIS&gt;\nRunning computationally intensive codes such as envelope() for 2nd order spatial analysis may differ in processing speed from one student to another. My system, however, is running with 8 cores and it does take a lengthy amount of time for processing the large 40,000+ rows of conflict data. (1 simulation takes around 2 minutes to run). Hence, I would suggest other students to begin their take-home exercise as early as possible to provide buffer time for processing the data, handling code bugs and even data cleaning errors."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "In this hands-on exercise, I will compute the global and local measures of spatial autocorrelation by using spdep package. These are the goals of this exercise:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics.\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord‚Äôs Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#overview",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "In this hands-on exercise, I will compute the global and local measures of spatial autocorrelation by using spdep package. These are the goals of this exercise:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics.\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord‚Äôs Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#getting-started",
    "title": "Hands-on Exercise 6",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n2.1 The Analytical Question\nIn this case study, we will examine the spatial pattern of GDP per capita of Hunan Provice, People Republic of China by answering the questions:\n\nIs it evenly distributed spatially? (obviously no)\nIs there sign of spatial clustering?\nWhere are the clusters?\n\n\n\n2.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan‚Äôs local development indicators in 2012.\n\n\n\n2.3 Setting the Analytical Tools\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#data-preprocessing",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#data-preprocessing",
    "title": "Hands-on Exercise 6",
    "section": "3. Data Preprocessing",
    "text": "3. Data Preprocessing\n\n3.1 Import the data\n\n\n\n\n\n\n\n\n3.2 Performing relational join\nNow we need to update the attribute table of hunan‚Äôs SpatialPolygonDataFrame with the attribute fields of hunan2012 dataframe using leftjoin().\n\nhunan &lt;- left_join(hunan,hunan2012) |&gt;\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\n\n\n3.3 Visualizing regional development factor\nusing qtm(), we can prepare a basemap and a choropleth map showing the distribution of GDPPC 2012.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#global-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#global-measures-of-spatial-autocorrelation",
    "title": "Hands-on Exercise 6",
    "section": "4. Global Measures of Spatial Autocorrelation",
    "text": "4. Global Measures of Spatial Autocorrelation\n\n4.1 Computing contiguity spatial weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e.¬†county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a ‚Äúqueen‚Äù argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don‚Äôt specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\nüí° Observations: The summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n\n4.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=‚ÄúW‚Äù). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values.\nWhile this is the most intuitive way to summaries the neighbors‚Äô values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we‚Äôll stick with the style=‚ÄúW‚Äù option for simplicity‚Äôs sake but note that other more robust options are available, notably style=‚ÄúB‚Äù.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\nüí° What can we learn from the code chunk above?\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values ‚ÄúW‚Äù, ‚ÄúB‚Äù, ‚ÄúC‚Äù, ‚ÄúU‚Äù, ‚Äúminmax‚Äù and ‚ÄúS‚Äù. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al.¬†1999, p.¬†167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#global-measures-of-spatial-autocorrelation-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#global-measures-of-spatial-autocorrelation-morans-i",
    "title": "Hands-on Exercise 6",
    "section": "5. Global Measures of Spatial Autocorrelation: Moran‚Äôs I",
    "text": "5. Global Measures of Spatial Autocorrelation: Moran‚Äôs I\n\n5.1 Moran‚Äôs I test\nIn this section, you will learn how to perform Moran‚Äôs I statistics testing by using¬†moran.test()¬†of¬†spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nFrom the result above, we can claim that there is a positive spatial autocorrelation since we have extremely low p-value of 1.095e-06 and positive Moran I statistic of 0.300749970. We can expect a Moran I statistic of -0.011494253 if there is no spatial autocorrelation. We can tell that there is spatial clustering from this test.\n\n\n5.2 Computing Monte Carlo Moran‚Äôs I\nThe code chunk below performs permutation test for Moran‚Äôs I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nGiven that our observed statistic has a rank of 1000, this observation possesses the highest Moran I value among the 999 randomly simulated observations. This further justifies our previous claim\n\n\n5.3 Visualising Monte Carlo Moran‚Äôs I\nIt is always a good practice for us the examine the simulated Moran‚Äôs I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\nWe can also plot the histogram using ggplot2 package.\n\nggplot() +\n  geom_histogram(aes(bperm$res), color = 'black',fill = 'lightgray') +\n  geom_vline(aes(xintercept = mean(bperm$res)), color = 'red') +\n  xlab(\"Simulated Moran's I\") +\n  ylab(\"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "title": "Hands-on Exercise 6",
    "section": "6. Global Measures of Spatial Autocorrelation: Geary‚Äôs C",
    "text": "6. Global Measures of Spatial Autocorrelation: Geary‚Äôs C\n\n6.1 Geary‚Äôc C test\nThe code chunk below performs Geary‚Äôs C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q   \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\nüí° Observations: From the result of the test above, we can see that the high Geary C statistic , low p-value, and Geary C statistic that is lower than the expectation. Hence, we can claim that spatial autocorrelation exists.\n\n\n\n6.2 Computing Monte Carlo Geary‚Äôs C\nThe code chunk below performs permutation test for Geary‚Äôs C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nGiven that our observed statistic has a rank of 1, this observation possesses the lowest Geary‚Äôs C value among the 999 randomly simulated observations. This further justifies our previous claim\n\n\n6.3 Visualising the Monte Carlo Geary‚Äôs C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\n\nabline(v=1, col=\"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#spatial-correlogram",
    "title": "Hands-on Exercise 6",
    "section": "7. Spatial Correlogram",
    "text": "7. Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated pairs are from spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran‚Äôs I or Geary‚Äôs c) against distance. Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n7.1 Moran‚Äôs I\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran‚Äôs I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n7.2 Geary‚Äôs C\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary‚Äôs C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#local-measures-of-spatial-association-lisa",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#local-measures-of-spatial-association-lisa",
    "title": "Hands-on Exercise 6",
    "section": "8. Local Measures of Spatial Association (LISA)",
    "text": "8. Local Measures of Spatial Association (LISA)\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, I will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran‚ÄôI to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n8.1 Computing local Moran‚Äôs I\nTo compute local Moran‚Äôs I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran‚Äôs I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran‚Äôs I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n\n8.2 Mapping the local Moran‚Äôs I\nBefore mapping the local Moran‚Äôs I map, it is wise to append the local Moran‚Äôs I dataframe (i.e.¬†localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI &lt;- cbind(hunan,localMI) |&gt;\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n8.3 Mapping local Moran‚Äôs I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran‚Äôs I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\n\n8.4 Mapping local Moran‚Äôs I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran‚Äôs I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n8.5 Mapping both local Moran‚Äôs I values and p-values\nFor effective interpretation, it is better to plot both the local Moran‚Äôs I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#creating-a-lisa-cluster-map",
    "title": "Hands-on Exercise 6",
    "section": "9. Creating a LISA Cluster Map",
    "text": "9. Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n9.1 Plotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n9.2 Plotting Moran scatterplot with standardised variable\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector\n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\n9.3 Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e.¬†GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nThis is follow by centering the local Moran‚Äôs around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05\n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nIn fact, we can combined all the steps into one single code chunk as shown below:\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n9.4 Plotting LISA map\nNow, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran‚Äôs I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\n\n\n\n\nWe can also include the local Moran‚Äôs I map and p-value map as shown below for easy comparison."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on Exercise 6",
    "section": "10. Hot Spot and Cold Spot Area Analysis",
    "text": "10. Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‚Äòhot spot‚Äô has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n10.1 Getis and Ord‚Äôs G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord‚Äôs G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n10.2 Deriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n10.2.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n10.2.2 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n10.2.3 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext,¬†nb2listw()¬†is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nThe output spatial weights object is called¬†wm62_lw.\n\n\n\n10.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext,¬†nb2listw()¬†is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#computing-gi-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#computing-gi-statistics",
    "title": "Hands-on Exercise 6",
    "section": "11. Computing Gi statistics",
    "text": "11. Computing Gi statistics\n\n11.1 Gi statistics using fixed distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes ‚Äúgstari‚Äù set to TRUE or FALSE, ‚Äúcall‚Äù set to the function call, and class ‚ÄúlocalG‚Äù.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) |&gt;\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e.¬†gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n11.2 Mapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\nVariable(s) \"gstat_fixed\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\n\n11.3 Gi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) |&gt;\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n11.4 Mapping Gi values with adaptive distance weights\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\nVariable(s) \"gstat_adaptive\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#st-order-spatio-temporal-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#st-order-spatio-temporal-point-pattern-analysis",
    "title": "Take-home Exercise 1",
    "section": "7. 1st Order Spatio-Temporal Point Pattern Analysis",
    "text": "7. 1st Order Spatio-Temporal Point Pattern Analysis\n\n\nSet up DayofYear variable per quarter\nQ2_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2024 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2024 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\n\n\n7.1 Creating ppp object\nIn the code chunk below, DayofYear from the fire_sf data frame is selected and is included in the output ppp object.\n\n\nCreate ppp object per quarter\nQ2_2024_ppp &lt;- Q2_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2024_ppp &lt;- Q1_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2023_ppp &lt;- Q4_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2023_ppp &lt;- Q3_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2023_ppp &lt;- Q2_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2023_ppp &lt;- Q1_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2022_ppp &lt;- Q4_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2022_ppp &lt;- Q3_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2022_ppp &lt;- Q2_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2022_ppp &lt;- Q1_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2021_ppp &lt;- Q4_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2021_ppp &lt;- Q3_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2021_ppp &lt;- Q2_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2021_ppp &lt;- Q1_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\n\n7.2 Combining ppp with owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\n\nMask the ppp object with owin object\nQ2_2024_owin &lt;- Q2_2024_ppp[myanmar_owin]\n\nQ1_2024_owin &lt;- Q1_2024_ppp[myanmar_owin]\n\nQ4_2023_owin &lt;- Q4_2023_ppp[myanmar_owin]\n\nQ3_2023_owin &lt;- Q3_2023_ppp[myanmar_owin]\n\nQ2_2023_owin &lt;- Q2_2023_ppp[myanmar_owin]\n\nQ1_2023_owin &lt;- Q1_2023_ppp[myanmar_owin]\n\nQ4_2022_owin &lt;- Q4_2022_ppp[myanmar_owin]\n\nQ3_2022_owin &lt;- Q3_2022_ppp[myanmar_owin]\n\nQ2_2022_owin &lt;- Q2_2022_ppp[myanmar_owin]\n\nQ1_2022_owin &lt;- Q1_2022_ppp[myanmar_owin]\n\nQ4_2021_owin &lt;- Q4_2021_ppp[myanmar_owin]\n\nQ3_2021_owin &lt;- Q3_2021_ppp[myanmar_owin]\n\nQ2_2021_owin &lt;- Q2_2021_ppp[myanmar_owin]\n\nQ1_2021_owin &lt;- Q1_2021_ppp[myanmar_owin]\n\n\nNow, I will perform a spatio-temporal kernel density estimate on the owin object which gives us insights into where and when conflict event occurrences are concentrated within the specified observation window.\n\n\nPerform spatial temporal KDE per quarter\nQ2_2024_stkde &lt;- spattemp.density(Q2_2024_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ1_2024_stkde &lt;- spattemp.density(Q1_2024_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ4_2023_stkde &lt;- spattemp.density(Q4_2023_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ3_2023_stkde &lt;- spattemp.density(Q3_2023_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ2_2023_stkde &lt;- spattemp.density(Q2_2023_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ1_2023_stkde &lt;- spattemp.density(Q1_2023_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ4_2022_stkde &lt;- spattemp.density(Q4_2022_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ3_2022_stkde &lt;- spattemp.density(Q3_2022_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ2_2022_stkde &lt;- spattemp.density(Q2_2022_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ1_2022_stkde &lt;- spattemp.density(Q1_2022_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ4_2021_stkde &lt;- spattemp.density(Q4_2021_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ3_2021_stkde &lt;- spattemp.density(Q3_2021_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ2_2021_stkde &lt;- spattemp.density(Q2_2021_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ1_2021_stkde &lt;- spattemp.density(Q1_2021_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\n\n\nSTKDE of 2024 Q2\n# Load necessary libraries\nlibrary(spatstat)\nlibrary(magick)\nlibrary(viridis)\n\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2024_Q2_frames\")) {\n  dir.create(\"STKDE/2024_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2024_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2024_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2024_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2024 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2024_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2024_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2024 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2024_Q1_frames\")) {\n  dir.create(\"STKDE/2024_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2024_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2024_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2024_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2024 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2024_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2024_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q4\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2023_Q4_frames\")) {\n  dir.create(\"STKDE/2023_Q4_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q4_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q4_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2023_Q4_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q4 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2023_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2023_Q4_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q3\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2023_Q3_frames\")) {\n  dir.create(\"STKDE/2023_Q3_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q3_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q3_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2023_Q3_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q3 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2023_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2023_Q3_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q2\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2023_Q2_frames\")) {\n  dir.create(\"STKDE/2023_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2023_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2023_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2023_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2023_Q1_frames\")) {\n  dir.create(\"STKDE/2023_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2023_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2023_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2023_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q4\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2022_Q4_frames\")) {\n  dir.create(\"STKDE/2022_Q4_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q4_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q4_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2022_Q4_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q4 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2022_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2022_Q4_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q3\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2022_Q3_frames\")) {\n  dir.create(\"STKDE/2022_Q3_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q3_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q3_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2022_Q3_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q3 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2022_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2022_Q3_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q2\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2022_Q2_frames\")) {\n  dir.create(\"STKDE/2022_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2022_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2022_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2022_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2022_Q1_frames\")) {\n  dir.create(\"STKDE/2022_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2022_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2022_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2022_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q4\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2021_Q4_frames\")) {\n  dir.create(\"STKDE/2021_Q4_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q4_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q4_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2021_Q4_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2021 Q4 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2021_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2021_Q4_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q3\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2021_Q3_frames\")) {\n  dir.create(\"STKDE/2021_Q3_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q3_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q3_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2021_Q3_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2021 Q3 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2021_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2021_Q3_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q2\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2021_Q2_frames\")) {\n  dir.create(\"STKDE/2021_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2021_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2021 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2021_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2021_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"STKDE/2021_Q1_frames\")) {\n  dir.create(\"STKDE/2021_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"STKDE/2021_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"STKDE/2021_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"STKDE/2021_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\n7.3 Plotting STKDE Outputs\nLet‚Äôs plot our animated spatio-temporal KDE outputs for each quarter.\n\n7.3.1 2024 Q1-2 STKDE\n\nlibrary(spatstat)\nlibrary(magick)\nlibrary(viridis)\n\n# 2024 Q2\nframes &lt;- image_read(list.files(\"STKDE/2024_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n# 2024 Q1\nframes &lt;- image_read(list.files(\"STKDE/2024_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAt the start of 2024 Q1, we see that the kernel density estimate of Myanmar conflicts tend to move sporadically where there are no specific patterns but in 2024 Q2, we start seeing conflict events occurring more intensely in Central and Southern Myanmar.\n\n\n\n\n7.3.2 2023 Q1-Q4 STKDE\n\n# 2023 Q4\nframes &lt;- image_read(list.files(\"STKDE/2023_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n# 2023 Q3\nframes &lt;- image_read(list.files(\"STKDE/2023_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n# 2023 Q2\nframes &lt;- image_read(list.files(\"STKDE/2023_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n# 2023 Q1\nframes &lt;- image_read(list.files(\"STKDE/2023_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom 2023 Q1 to Q3, there is a noticeable cluster of conflict events happening in Central and Southern regions of Myanmar than the other parts of the country. In 2023 Q4, we can notice more dispersion in conflict events across Myanmar, spreading into Western and Eastern regions. Throughout 2023, conflict events are least observed in North Myanmar.\n\n\n\n\n7.3.3 2022 Q1-Q4 STKDE\n\n# 2022 Q4\nframes &lt;- image_read(list.files(\"STKDE/2022_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n# 2022 Q3\nframes &lt;- image_read(list.files(\"STKDE/2022_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n# 2022 Q2\nframes &lt;- image_read(list.files(\"STKDE/2022_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n# 2022 Q1\nframes &lt;- image_read(list.files(\"STKDE/2022_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nSimilar to 2023, the spread of conflict events in 2022 is largely clustered in Central and Southen parts of Myanmar. However, we do see some jitters in conflict trends in 2022 Q1 with a rare sight of conflicts in the far North of Myanmar and occasional conflicts in Southern Myanmar.\n\n\n\n\n7.3.4 2021 Q1-Q4 STKDE\n\n# 2021 Q4\nframes &lt;- image_read(list.files(\"STKDE/2021_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n# 2021 Q3\nframes &lt;- image_read(list.files(\"STKDE/2021_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n# 2021 Q2\nframes &lt;- image_read(list.files(\"STKDE/2021_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n# 2021 Q1\nframes &lt;- image_read(list.files(\"STKDE/2021_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFinally, armed conflicts in 2021 is by far the most random spread of armed conflicts throughout Myanmar. There is also a high intensity of conflicts in South Myanmar, particularly in the state of Yangon while we rarely see conflicts occurring in the extreme North of Myanmar.\n\n\n\n\n7.3.5 OpenStreetMap in Myanmar - Spatio Temporal\nUPDATE THIS USING THE APPROPRIATE QUARTER AND YEAR\n\nQ2_2024_owin &lt;- rescale(Q2_2024_owin, 1000, \"km\")\nQ2_2024_stkde &lt;- density(Q2_2024_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q2_2024_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ1_2024_owin &lt;- rescale(Q1_2024_owin, 1000, \"km\")\nQ1_2024_stkde &lt;- density(Q1_2024_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q1_2024_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ4_2023_owin &lt;- rescale(Q4_2023_owin, 1000, \"km\")\nQ4_2023_stkde &lt;- density(Q4_2023_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q4_2023_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ3_2023_owin &lt;- rescale(Q3_2023_owin, 1000, \"km\")\nQ3_2023_stkde &lt;- density(Q3_2023_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q3_2023_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ2_2023_owin &lt;- rescale(Q2_2023_owin, 1000, \"km\")\nQ2_2023_stkde &lt;- density(Q2_2023_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q2_2023_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ1_2023_owin &lt;- rescale(Q1_2023_owin, 1000, \"km\")\nQ1_2023_stkde &lt;- density(Q1_2023_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q1_2023_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ4_2022_owin &lt;- rescale(Q4_2022_owin, 1000, \"km\")\nQ4_2022_stkde &lt;- density(Q4_2022_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q4_2022_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ3_2022_owin &lt;- rescale(Q3_2022_owin, 1000, \"km\")\nQ3_2022_stkde &lt;- density(Q3_2022_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q3_2022_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ2_2022_owin &lt;- rescale(Q2_2022_owin, 1000, \"km\")\nQ2_2022_stkde &lt;- density(Q2_2022_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q2_2022_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ1_2022_owin &lt;- rescale(Q1_2022_owin, 1000, \"km\")\nQ1_2022_stkde &lt;- density(Q1_2022_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q1_2022_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ4_2021_owin &lt;- rescale(Q4_2021_owin, 1000, \"km\")\nQ4_2021_stkde &lt;- density(Q4_2021_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q4_2021_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ3_2021_owin &lt;- rescale(Q3_2021_owin, 1000, \"km\")\nQ3_2021_stkde &lt;- density(Q3_2021_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q3_2021_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ2_2021_owin &lt;- rescale(Q2_2021_owin, 1000, \"km\")\nQ2_2021_stkde &lt;- density(Q2_2021_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q2_2021_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\nQ1_2021_owin &lt;- rescale(Q1_2021_owin, 1000, \"km\")\nQ1_2021_stkde &lt;- density(Q1_2021_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nWarning in density.ppp(Q1_2021_owin, sigma = bw.CvL, edge = TRUE, kernel =\n\"quartic\"): Bandwidth selection will be based on Gaussian kernel\n\n\n\n\nSet up raster and projection\nraster_Q2_2024 &lt;- raster(Q2_2024_stkde)\nraster_Q1_2024 &lt;- raster(Q1_2024_stkde)\nraster_Q4_2023 &lt;- raster(Q4_2023_stkde)\nraster_Q3_2023 &lt;- raster(Q3_2023_stkde)\nraster_Q2_2023 &lt;- raster(Q2_2023_stkde)\nraster_Q1_2023 &lt;- raster(Q1_2023_stkde)\nraster_Q4_2022 &lt;- raster(Q4_2022_stkde)\nraster_Q3_2022 &lt;- raster(Q3_2022_stkde)\nraster_Q2_2022 &lt;- raster(Q2_2022_stkde)\nraster_Q1_2022 &lt;- raster(Q1_2022_stkde)\nraster_Q4_2021 &lt;- raster(Q4_2021_stkde)\nraster_Q3_2021 &lt;- raster(Q3_2021_stkde)\nraster_Q2_2021 &lt;- raster(Q2_2021_stkde)\nraster_Q1_2021 &lt;- raster(Q1_2021_stkde)\nprojection(raster_Q2_2024) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q1_2024) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q4_2023) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q3_2023) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q2_2023) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q1_2023) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q4_2022) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q3_2022) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q2_2022) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q1_2022) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q4_2021) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q3_2021) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q2_2021) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q1_2021) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\n\n\n\n# Plot KDE Map on OpenStreetMap\nraster_quarters &lt;- list('raster_Q2_2024','raster_Q1_2024','raster_Q4_2023','raster_Q3_2023','raster_Q1_2023','raster_Q4_2022','raster_Q3_2022','raster_Q2_2022','raster_Q1_2022','raster_Q4_2021','raster_Q3_2021','raster_Q2_2021','raster_Q1_2021')\n\n# Set tmap mode to view\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\n# Function to create tmap for each raster\ncreate_tmap &lt;- function(raster_var) {\n  kde_fixed_output &lt;- tm_basemap(server = \"OpenStreetMap.HOT\") +\n    tm_basemap(server = \"Esri.WorldImagery\") +\n    tm_shape(get(raster_var)) +  # Dynamically get the raster variable\n    tm_raster(\"layer\",\n              n = 10,\n              title = paste(raster_var),\n              alpha = 0.6,\n              palette = c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\")) +\n    tm_shape(boundary_sf) +\n    tm_polygons(alpha=0.1, id=\"DT\") +\n    tmap_options(check.and.fix = TRUE)\n  \n  return(kde_fixed_output)\n}\n\n# Create a list of tmap objects by iterating over the raster_quarters list\ntmap_list &lt;- lapply(raster_quarters, create_tmap)\n\n# Arrange and display the tmaps\ntmap_arrange(tmap_list, ncol = 2, nrow = 2, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#nd-order-spatio-temporal-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#nd-order-spatio-temporal-point-pattern-analysis",
    "title": "Take-home Exercise 1",
    "section": "8. 2nd Order Spatio-Temporal Point Pattern Analysis",
    "text": "8. 2nd Order Spatio-Temporal Point Pattern Analysis\nSimilar to the 2nd order spatial analysis in section 6, I want to explore spatio-temporal trends of Myanmar‚Äôs conflicts and how the events differ in distribution, from quarter to quarter. I‚Äôll delve into the four districts I‚Äôm most interested in, that is the districts with the highest proportions of conflicts - Yinmarbin, Shwebo, Pakokku and Mandalay.\n\n8.1 Using K-Function Estimation\n\n8.1.1 Yinmarbin District\nWe‚Äôll want to compute the K-Function by quarters via Kest() by iterating over each unique quarter in date format, then plotting an animated graph of the K-Function outputs per quarter.\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Yinmarbin District\n# Set up yinmarbin_ppp_owin_list\nconflict_yinmarbin &lt;- conflict_data_sf %&gt;% filter(DT == \"Yinmarbin\")\nunique_quarter &lt;- unique(conflict_yinmarbin$year_quarter)\nboundary_yinmarbin &lt;- filter(boundary_sf, DT == \"Yinmarbin\")\nyinmarbin_owin &lt;- as.owin(boundary_yinmarbin)\nyinmarbin_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_yinmarbin %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[yinmarbin_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  yinmarbin_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\nPlot K-function for Yinmarbin District\nlibrary(magick)\n\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_k_function\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n  K_result &lt;- Kest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_k_function/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_k_function/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_k_function/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"sec_order_k_function/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can observe how the observed line (K-iso) is constantly above the actual line (K-pois) from 2021 Q2 to 2024 Q2. This confirms that conflict points in Yinmarbin are highly clustered. In fact, it is more clustered together than expected by the null hypothesis. There is no K-function outputted for 2021 Q1 as no conflict points were observed in Yinmarbin District for that time period.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRipley‚Äôs Correction provides a way to mitigate the bias introduced by points near the edge of the study window by accounting for the reduced area for point comparisons near the edges. Hence, this results in a a more accurate estimate of the spatial distribution as seen in the slight difference between actual and expected K-function.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monta Carlo simulation test) will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of conflict events in Myanmar are randomly distributed.\nH1= The distribution of conflict events in Myanmar are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nBy using envelop(), we can get a more robust interpretation by comparing the observed K-function against a simulation envelope of K-functions generated under the null hypothesis.\n\n\nMonta Carlo Simulation for Yinmarbin District\nlibrary(magick)\n\n# Unique quarters of the year in the dataset\nunique_quarter &lt;- unique(conflict_yinmarbin$year_quarter)\n\n# Create a directory to store PNG frames\npath &lt;- file.path(\"k_function_monta_carlo\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n  \n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"k_function_monta_carlo/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"k_function_monta_carlo/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"k_function_monta_carlo/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"k_function_monta_carlo/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nI noticed that the observed line(in solid) has a somewhat substantial deviation above the upper envelop (shaded region) generated from the Monte Carlo simulation. This indicates a strong tendancy for points to be clustered in the Yinmarbin district. Since this occurs across a wide range of distances, it implies that clustering is a prominent feature of the spatial distribution.\n\n\n\n\n8.1.2 Shwebo District\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Shwebo District\n# Set up shwebo_ppp_owin_list\nconflict_shwebo &lt;- conflict_data_sf %&gt;% filter(DT == \"Shwebo\")\nunique_quarter &lt;- unique(conflict_shwebo$year_quarter)\nboundary_shwebo &lt;- filter(boundary_sf, DT == \"Shwebo\")\nshwebo_owin &lt;- as.owin(boundary_shwebo)\nshwebo_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_shwebo %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[shwebo_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  shwebo_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\nPlot K-function for Shwebo District\nlibrary(magick)\n\n# Unique quarters of the year in the dataset\nunique_quarter &lt;- unique(conflict_shwebo$year_quarter)\n\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_k_function\", \"conflict_shwebo\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(shwebo_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- shwebo_ppp_owin_list[[quarter]]\n  \n  # Calculate the K-function\n  K_result &lt;- Kest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_k_function/conflict_shwebo\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Shwebo District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_k_function/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_k_function/kfunction_shwebo.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"sec_order_k_function/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFor the Shwebo District, the observed line (K-iso) also lies constantly above the actual line (K-pois) from 2021 Q2 to 2024 Q2, indicating clustering at all distances ‚Äòr‚Äô of our spatial points. Interestingly, conflict events in 2021 Q1 displayed some tendancy towards a dispersed point pattern as the observedK-function lies slightly below the actual line at a distance of 17km.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monta Carlo simulation test) will be conducted.\n\n\nMonta Carlo Simulation for Shwebo District\n# Create a directory to store PNG frames\npath &lt;- file.path(\"k_function_monta_carlo\", \"conflict_shwebo\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(shwebo_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- shwebo_ppp_owin_list[[quarter]]\n\n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"k_function_monta_carlo/conflict_shwebo\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Shwebo District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"k_function_monta_carlo/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"k_function_monta_carlo/kfunction_shwebo.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"k_function_monta_carlo/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIn 2021 Q1, a sharp jagged plot is generated due to the smaller dataset size of conflicts in Shwebo. In 2024 Q2, we see the highest variability in spatial patterns which results in a larger envelope size\nGenerally, the observed line has a smaller deviation above the upper envelop than in the Yinmarbin data. This indicates a a somewhat intense level of clustering in the Shwebo district and remains more clustered than would be expected if points were distributed randomly..\n\n\n\n\n8.1.3 Pakokku District\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Pakokku District\n# Set up pakokku_ppp_owin_list\nconflict_pakokku &lt;- conflict_data_sf %&gt;% filter(DT == \"Pakokku\")\nunique_quarter &lt;- unique(conflict_pakokku$year_quarter)\nboundary_pakokku &lt;- filter(boundary_sf, DT == \"Pakokku\")\npakokku_owin &lt;- as.owin(boundary_pakokku)\npakokku_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_pakokku %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[pakokku_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  pakokku_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\nPlot K-function for Pakokku District\nlibrary(magick)\n\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_k_function\", \"conflict_pakokku\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(pakokku_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- pakokku_ppp_owin_list[[quarter]]\n  K_result &lt;- Kest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_k_function/conflict_pakokku\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Pakokku District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_k_function/conflict_pakokku\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_k_function/kfunction_pakokku.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"sec_order_k_function/conflict_pakokku\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe observed K-function lies above the expected K-function outputted from 2021 to 2024, indicating clustering at all distances ‚Äòr‚Äô of our spatial points. However, we can observe milder clustering from 2021 Q4 to 2024 Q2 as the magnitude of deviation is significantly smaller than the period of 2021 Q1 to 2021 Q3.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monta Carlo simulation test) will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of conflict events in Myanmar are randomly distributed.\nH1= The distribution of conflict events in Myanmar are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\n\n\nMonta Carlo Simulation for Pakokku District\n# Create a directory to store PNG frames\npath &lt;- file.path(\"k_function_monta_carlo\", \"conflict_pakokku\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(pakokku_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- pakokku_ppp_owin_list[[quarter]]\n  \n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"k_function_monta_carlo/conflict_pakokku\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Pakokku District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"k_function_monta_carlo/conflict_pakokku\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"k_function_monta_carlo/kfunction_pakokku.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"k_function_monta_carlo/conflict_pakokku\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIn 2021 Q2, we see a large envelope which reflects greater variability in the expected K-function and this could stem from the mix of both clustering and dispersion across Myanmar for that period. Generally from 2021 Q1 to 2024 Q2, clustering is evident as the observed K-function continues to lie above the envelope.\n\n\n\n\n8.1.4 Mandalay District\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Mandalay District\n# Set up mandalay_ppp_owin_list\nconflict_mandalay &lt;- conflict_data_sf %&gt;% filter(DT == \"Mandalay\")\nunique_quarter &lt;- unique(conflict_mandalay$year_quarter)\nboundary_mandalay &lt;- filter(boundary_sf, DT == \"Mandalay\")\nmandalay_owin &lt;- as.owin(boundary_mandalay)\nmandalay_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_mandalay %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[mandalay_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  mandalay_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\nPlot K-function for Mandalay District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_k_function\", \"conflict_mandalay\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(mandalay_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- mandalay_ppp_owin_list[[quarter]]\n  \n  # Calculate the K-function\n  K_result &lt;- Kest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_k_function/conflict_mandalay\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Mandalay District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_k_function/conflict_mandalay\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_k_function/kfunction_mandalay.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"sec_order_k_function/conflict_mandalay\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe degree of clustering is less pronounced in Mandalay than the other districts since the gap between the observed line and the theoretical line is smaller. Clustering is large-scale in Mandalay where statistically significant clustering patterns are seen in larger distance ranges.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monta Carlo simulation test) will be conducted.\n\n\nMonta Carlo Simulation for Mandalay District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"k_function_monta_carlo\", \"conflict_mandalay\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(mandalay_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- mandalay_ppp_owin_list[[quarter]]\n\n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"k_function_monta_carlo/conflict_mandalay\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Mandalay District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"k_function_monta_carlo/conflict_mandalay\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"k_function_monta_carlo/kfunction_mandalay.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"k_function_monta_carlo/conflict_mandalay\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe see the highest variability in spatial patterns in 2024 Q1 and Q2 which indicates that some areas in Mandalay District are more clustered with conflict events while other areas are more evenly spaced.\nWe can confirm statistically significant clustering from 2021 Q1 to 2023 Q3 as K value is larger than the upper confidence of the envelope than in 2024 Q1 and Q2. There is also strong evidence of clustering especially at larger distances which means Mandalay District faces large-scale clustering.\n\n\n\n\n\n8.2 Using L-Function Estimation\n\n8.2.1 Yinmarbin District\n1) Computing L-function Estimation\n\n\nPlot L-function for Yinmarbin District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_L_function\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n  \n  # Calculate the L-function\n  L_result &lt;- Lest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_L_function/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(L_result, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_L_function/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFirstly, the L-function plot appears more zig-zag than the K-function as the L-function can magnify small fluctuations in the K-function. Secondly, the theoretical line (dotted) is fixed at 0 where L(d)‚àír=0 which is an expected behavior under Complete Spatial Randomness.\nFrom 2022 Q1 to 2022 Q3, all of 2023 and 2024 Q2, there is large scale clustering as the observed line (K-iso) consistently lies above the theoretical line (K-pois) particularly at larger spatial distances. This suggests conflicts in Yinmarbin are more clustered than expected during this time period.\nFrom 2021, 2022 Q4 and 2024 Q1, there is a decreasing K value observed which indicates more localised clustering.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n\nMonta Carlo Simulation for Yinmarbin District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"L_function_monta_carlo\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n\n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"L_function_monta_carlo/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"L_function_monta_carlo/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe periods of 2022 Q4, 2024 Q1 and 2024 Q2 shows a wider envelope, that is that some areas are clustered while others are more evenly spaced in Yinmarbin. There‚Äôs also more widespread clustering observed in 2024 Q2 where L values are higher at larger distances.\n\n\n\n\n8.2.2 Shwebo District\n1) Computing L-function Estimation\n\n\nPlot L-function for Shwebo District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_L_function\", \"conflict_shwebo\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(shwebo_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- shwebo_ppp_owin_list[[quarter]]\n  \n  # Calculate the L-function\n  L_result &lt;- Lest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_L_function/conflict_shwebo\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(L_result, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Shwebo District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_L_function/kfunction_shwebo.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe 2021 Q1 observed K values are much more jagged than other quarters as we have a smalller dataset of conflicts in Shwebo district of that quarter. In general, we see stronger clustering in bigger areas of the district which suggests wide-scale clustering of conflicts.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n\nMonta Carlo Simulation for Shwebo District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"L_function_monta_carlo\", \"conflict_shwebo\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(shwebo_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- shwebo_ppp_owin_list[[quarter]]\n  \n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"L_function_monta_carlo/conflict_shwebo\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Shwebo District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"L_function_monta_carlo/kfunction_shwebo.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\n2021 Q2 and 2024 Q2 has the highest variability in spatial points with both clustering and dispersion patterns under the CRS test. Nonetheless, we can confirm strong significance of clustering throughout all quarters in Shwebo where the observed L values like above the theoretical values simulated.\n\n\n\n\n8.2.3 Pakokku District\n1) Computing L-function Estimation\n\n\nPlot L-function for Pakokku District\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_L_function\", \"conflict_pakokku\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(pakokku_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- pakokku_ppp_owin_list[[quarter]]\n  \n  # Calculate the L-function\n  L_result &lt;- Lest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_L_function/conflict_pakokku\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(L_result, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Pakokku District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_pakokku\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_L_function/kfunction_pakokku.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_pakokku\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe generally see strong evidence of localised clusterings in the Pakokku district with weaker significance of clustering in longer distances. The observed L values in 2021 Q1 and Q2 shows a jagged output which stems from our smaller data conflict points.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n\nMonta Carlo Simulation for Pakokku District\n# Create a directory to store PNG frames\npath &lt;- file.path(\"L_function_monta_carlo\", \"conflict_pakokku\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(pakokku_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- pakokku_ppp_owin_list[[quarter]]\n\n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"L_function_monta_carlo/conflict_pakokku\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Pakokku District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_pakokku\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"L_function_monta_carlo/kfunction_pakokku.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_pakokku\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe results of the simulation produces the greatest variability in conflict activity in Shwebo in 2021 Q2 and 2024 Q1. There are periods of high concentration (clustering) in certain areas and more dispersed activity in others. That said, our observed pattern exhibits strong clustering throughout all quarters in Shwebo.\n\n\n\n\n8.2.4 Mandalay District\n1) Computing L-function Estimation\n\n\nPlot L-function for Mandalay District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_L_function\", \"conflict_mandalay\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(mandalay_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- mandalay_ppp_owin_list[[quarter]]\n  \n  # Calculate the L-function\n  L_result &lt;- Lest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_L_function/conflict_mandalay\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(L_result, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Mandalay District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_mandalay\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_L_function/kfunction_mandalay.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_mandalay\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nGenerally from 2021 to 2024, we see an upward trend of observed L values with sharp fluctuations as distance increases.\nAt larger distance scales, we observe greater deviations of the observed line above the theoretical line compared to the deviations seen at smaller distances. This suggests strong clustering across Mandalay, though clustering is also present at local areas.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n\nMonta Carlo Simulation for Mandalay District\n# Create a directory to store PNG frames\npath &lt;- file.path(\"L_function_monta_carlo\", \"conflict_mandalay\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(mandalay_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- mandalay_ppp_owin_list[[quarter]]\n  \n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"L_function_monta_carlo/conflict_mandalay\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Mandalay District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_mandalay\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"L_function_monta_carlo/kfunction_mandalay.gif\"\nimage_write(animation, path = output_path)\n\n\n\n# Display Plot\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIn 2021 Q1 and 2024 Q1-2, we can observe a bigger envelope from the CSR test especially at larger distances. This means certain areas in Mandalay show strong clustering (hotspots), while others are more dispersed, leading to greater variability in the overall spatial structure of conflict events.\nAdditionally, throughout 2021 to 2024, there isis prominent evidence of hotspots and clustering throughout Mandalay since observed L values deviate above the theoretical L values, across a range of distances."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#st-order-spatio-temporal-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#st-order-spatio-temporal-point-pattern-analysis",
    "title": "Take-home Exercise 1 - Part 1",
    "section": "7. 1st Order Spatio-Temporal Point Pattern Analysis",
    "text": "7. 1st Order Spatio-Temporal Point Pattern Analysis\n\n\nSet up DayofYear variable per quarter\nQ2_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2024 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2024 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\n\n\n7.1 Creating ppp object\nIn the code chunk below, DayofYear from the fire_sf data frame is selected and is included in the output ppp object.\n\n# Create ppp object per quarter\nQ2_2024_ppp &lt;- Q2_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2024_ppp &lt;- Q1_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2023_ppp &lt;- Q4_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2023_ppp &lt;- Q3_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2023_ppp &lt;- Q2_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2023_ppp &lt;- Q1_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2022_ppp &lt;- Q4_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2022_ppp &lt;- Q3_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2022_ppp &lt;- Q2_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2022_ppp &lt;- Q1_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2021_ppp &lt;- Q4_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2021_ppp &lt;- Q3_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2021_ppp &lt;- Q2_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2021_ppp &lt;- Q1_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\n7.2 Combining ppp with owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\n# Mask the ppp object with owin object\nQ2_2024_owin &lt;- Q2_2024_ppp[myanmar_owin]\n\nQ1_2024_owin &lt;- Q1_2024_ppp[myanmar_owin]\n\nQ4_2023_owin &lt;- Q4_2023_ppp[myanmar_owin]\n\nQ3_2023_owin &lt;- Q3_2023_ppp[myanmar_owin]\n\nQ2_2023_owin &lt;- Q2_2023_ppp[myanmar_owin]\n\nQ1_2023_owin &lt;- Q1_2023_ppp[myanmar_owin]\n\nQ4_2022_owin &lt;- Q4_2022_ppp[myanmar_owin]\n\nQ3_2022_owin &lt;- Q3_2022_ppp[myanmar_owin]\n\nQ2_2022_owin &lt;- Q2_2022_ppp[myanmar_owin]\n\nQ1_2022_owin &lt;- Q1_2022_ppp[myanmar_owin]\n\nQ4_2021_owin &lt;- Q4_2021_ppp[myanmar_owin]\n\nQ3_2021_owin &lt;- Q3_2021_ppp[myanmar_owin]\n\nQ2_2021_owin &lt;- Q2_2021_ppp[myanmar_owin]\n\nQ1_2021_owin &lt;- Q1_2021_ppp[myanmar_owin]\n\nNow, I will perform a spatio-temporal kernel density estimate on the owin object which gives us insights into where and when conflict event occurrences are concentrated within the specified observation window.\n\n\nPerform spatial temporal KDE per quarter\nlibrary(spatstat)\nQ2_2024_stkde &lt;- spattemp.density(Q2_2024_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ1_2024_stkde &lt;- spattemp.density(Q1_2024_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ4_2023_stkde &lt;- spattemp.density(Q4_2023_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ3_2023_stkde &lt;- spattemp.density(Q3_2023_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ2_2023_stkde &lt;- spattemp.density(Q2_2023_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ1_2023_stkde &lt;- spattemp.density(Q1_2023_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ4_2022_stkde &lt;- spattemp.density(Q4_2022_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ3_2022_stkde &lt;- spattemp.density(Q3_2022_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ2_2022_stkde &lt;- spattemp.density(Q2_2022_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ1_2022_stkde &lt;- spattemp.density(Q1_2022_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ4_2021_stkde &lt;- spattemp.density(Q4_2021_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ3_2021_stkde &lt;- spattemp.density(Q3_2021_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ2_2021_stkde &lt;- spattemp.density(Q2_2021_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\nPerform spatial temporal KDE per quarter\nQ1_2021_stkde &lt;- spattemp.density(Q1_2021_owin)\n\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\n\n\n\n7.3 Plotting STKDE Outputs\nLet‚Äôs plot our animated spatio-temporal KDE outputs for each quarter.\n\n7.3.1 2024 Q1-2 STKDE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAt the start of 2024 Q1, we see that the kernel density estimate of Myanmar conflicts tend to move sporadically where there are no specific patterns but in 2024 Q2, we start seeing conflict events occurring more intensely in Central and Southern Myanmar.\n\n\n\n\n7.3.2 2023 Q1-Q4 STKDE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom 2023 Q1 to Q3, there is a noticeable cluster of conflict events happening in Central and Southern regions of Myanmar than the other parts of the country. In 2023 Q4, we can notice more dispersion in conflict events across Myanmar, spreading into Western and Eastern regions. Throughout 2023, conflict events are least observed in North Myanmar.\n\n\n\n\n7.3.3 2022 Q1-Q4 STKDE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nSimilar to 2023, the spread of conflict events in 2022 is largely clustered in Central and Southen parts of Myanmar. However, we do see some jitters in conflict trends in 2022 Q1 with a rare sight of conflicts in the far North of Myanmar and occasional conflicts in Southern Myanmar.\n\n\n\n\n7.3.4 2021 Q1-Q4 STKDE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFinally, armed conflicts in 2021 is by far the most random spread of armed conflicts throughout Myanmar. There is also a high intensity of conflicts in South Myanmar, particularly in the state of Yangon while we rarely see conflicts occurring in the extreme North of Myanmar.\n\n\n\n\n7.3.5 OpenStreetMap in Myanmar - Spatio Temporal\nWe can observe the spatio-temporal conflict patterns per quarter using OpenStreetMap as well.\n\nQ2_2024_owin &lt;- rescale(Q2_2024_owin, 1000, \"km\")\nQ2_2024_stkde &lt;- density(Q2_2024_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ1_2024_owin &lt;- rescale(Q1_2024_owin, 1000, \"km\")\nQ1_2024_stkde &lt;- density(Q1_2024_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ4_2023_owin &lt;- rescale(Q4_2023_owin, 1000, \"km\")\nQ4_2023_stkde &lt;- density(Q4_2023_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ3_2023_owin &lt;- rescale(Q3_2023_owin, 1000, \"km\")\nQ3_2023_stkde &lt;- density(Q3_2023_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ2_2023_owin &lt;- rescale(Q2_2023_owin, 1000, \"km\")\nQ2_2023_stkde &lt;- density(Q2_2023_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ1_2023_owin &lt;- rescale(Q1_2023_owin, 1000, \"km\")\nQ1_2023_stkde &lt;- density(Q1_2023_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ4_2022_owin &lt;- rescale(Q4_2022_owin, 1000, \"km\")\nQ4_2022_stkde &lt;- density(Q4_2022_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ3_2022_owin &lt;- rescale(Q3_2022_owin, 1000, \"km\")\nQ3_2022_stkde &lt;- density(Q3_2022_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ2_2022_owin &lt;- rescale(Q2_2022_owin, 1000, \"km\")\nQ2_2022_stkde &lt;- density(Q2_2022_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ1_2022_owin &lt;- rescale(Q1_2022_owin, 1000, \"km\")\nQ1_2022_stkde &lt;- density(Q1_2022_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ4_2021_owin &lt;- rescale(Q4_2021_owin, 1000, \"km\")\nQ4_2021_stkde &lt;- density(Q4_2021_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ3_2021_owin &lt;- rescale(Q3_2021_owin, 1000, \"km\")\nQ3_2021_stkde &lt;- density(Q3_2021_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ2_2021_owin &lt;- rescale(Q2_2021_owin, 1000, \"km\")\nQ2_2021_stkde &lt;- density(Q2_2021_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nQ1_2021_owin &lt;- rescale(Q1_2021_owin, 1000, \"km\")\nQ1_2021_stkde &lt;- density(Q1_2021_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\n\n\nSet up raster and projection\nraster_Q2_2024 &lt;- raster(Q2_2024_stkde)\nraster_Q1_2024 &lt;- raster(Q1_2024_stkde)\nraster_Q4_2023 &lt;- raster(Q4_2023_stkde)\nraster_Q3_2023 &lt;- raster(Q3_2023_stkde)\nraster_Q2_2023 &lt;- raster(Q2_2023_stkde)\nraster_Q1_2023 &lt;- raster(Q1_2023_stkde)\nraster_Q4_2022 &lt;- raster(Q4_2022_stkde)\nraster_Q3_2022 &lt;- raster(Q3_2022_stkde)\nraster_Q2_2022 &lt;- raster(Q2_2022_stkde)\nraster_Q1_2022 &lt;- raster(Q1_2022_stkde)\nraster_Q4_2021 &lt;- raster(Q4_2021_stkde)\nraster_Q3_2021 &lt;- raster(Q3_2021_stkde)\nraster_Q2_2021 &lt;- raster(Q2_2021_stkde)\nraster_Q1_2021 &lt;- raster(Q1_2021_stkde)\nprojection(raster_Q2_2024) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q1_2024) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q4_2023) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q3_2023) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q2_2023) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q1_2023) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q4_2022) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q3_2022) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q2_2022) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q1_2022) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q4_2021) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q3_2021) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q2_2021) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_Q1_2021) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\n\n\n\n# Plot KDE Map on OpenStreetMap\nraster_quarters &lt;- list('raster_Q2_2024','raster_Q1_2024','raster_Q4_2023','raster_Q3_2023','raster_Q2_2023','raster_Q1_2023','raster_Q4_2022','raster_Q3_2022','raster_Q2_2022','raster_Q1_2022','raster_Q4_2021','raster_Q3_2021','raster_Q2_2021','raster_Q1_2021')\n\n# Set tmap mode to view\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\n# Function to create tmap for each raster\ncreate_tmap &lt;- function(raster_var) {\n  kde_fixed_output &lt;- tm_basemap(server = \"OpenStreetMap.HOT\") +\n    tm_basemap(server = \"Esri.WorldImagery\") +\n    tm_shape(get(raster_var)) +  # Dynamically get the raster variable\n    tm_raster(\"layer\",\n              n = 10,\n              title = paste(raster_var),\n              alpha = 0.6,\n              palette = c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\")) +\n    tm_shape(boundary_sf) +\n    tm_polygons(alpha=0.1, id=\"DT\") +\n    tmap_options(check.and.fix = TRUE)\n  \n  return(kde_fixed_output)\n}\n\n# Create a list of tmap objects by iterating over the raster_quarters list\ntmap_list &lt;- lapply(raster_quarters, create_tmap)\n\n# Arrange and display the tmaps\ntmap_arrange(tmap_list, ncol = 2, nrow = 2, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe plots uncover the increasing levels of conflicts across each quarter. Interestingly, 2021 Q1 begins with higher concentration of armed conflicts in Southern and Eastern parts of Myanmar, but gradually becomes more concentrated in the Central districts. From 2022 Q4 onwards, we see that conflicts have spread outside the core central region of Myanmar towards the Western region. Conflicts return to occurring more frequently in the Central districts like Yinmarbin and Sagaing in 2024 Q1 and Q2,\n\n\nContinue to Part 2 &gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#nd-order-spatio-temporal-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#nd-order-spatio-temporal-point-pattern-analysis",
    "title": "Take-home Exercise 1",
    "section": "8. 2nd Order Spatio-Temporal Point Pattern Analysis",
    "text": "8. 2nd Order Spatio-Temporal Point Pattern Analysis\nSimilar to the 2nd order spatial analysis in section 6, I want to explore spatio-temporal trends of Myanmar‚Äôs conflicts and how the events differ in distribution, from quarter to quarter. I‚Äôll delve into the four districts I‚Äôm most interested in, that is the districts with the highest proportions of conflicts - Yinmarbin, Shwebo, Pakokku and Mandalay.\n\n8.1 Using K-Function Estimation\n\n8.1.1 Yinmarbin District\nWe‚Äôll want to compute the K-Function by quarters via Kest() by iterating over each unique quarter in date format, then plotting an animated graph of the K-Function outputs per quarter.\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Yinmarbin District\n# Set up yinmarbin_ppp_owin_list\nconflict_yinmarbin &lt;- conflict_data_sf %&gt;% filter(DT == \"Yinmarbin\")\nunique_quarter &lt;- unique(conflict_yinmarbin$year_quarter)\nboundary_yinmarbin &lt;- filter(boundary_sf, DT == \"Yinmarbin\")\nyinmarbin_owin &lt;- as.owin(boundary_yinmarbin)\nyinmarbin_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_yinmarbin %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[yinmarbin_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  yinmarbin_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\nPlot K-function for Yinmarbin District\nlibrary(magick)\n\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_k_function\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n  K_result &lt;- Kest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_k_function/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_k_function/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_k_function/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can observe how the observed line (K-iso) is constantly above the actual line (K-pois) from 2021 Q2 to 2024 Q2. This confirms that conflict points in Yinmarbin are highly clustered. In fact, it is more clustered together than expected by the null hypothesis. There is no K-function outputted for 2021 Q1 as no conflict points were observed in Yinmarbin District for that time period.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRipley‚Äôs Correction provides a way to mitigate the bias introduced by points near the edge of the study window by accounting for the reduced area for point comparisons near the edges. Hence, this results in a a more accurate estimate of the spatial distribution as seen in the slight difference between actual and expected K-function.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monta Carlo simulation test) will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of conflict events in Myanmar are randomly distributed.\nH1= The distribution of conflict events in Myanmar are not randomly distributed.\nThe null hypothesis will be rejected if the observed K-function lies above/below the theoretical K-function and envelope.\n\nBy using envelop(), we can get a more robust interpretation by comparing the observed K-function against a simulation envelope of K-functions generated under the null hypothesis.\n\n\nMonta Carlo Simulation for Yinmarbin District\nlibrary(magick)\n\n# Unique quarters of the year in the dataset\nunique_quarter &lt;- unique(conflict_yinmarbin$year_quarter)\n\n# Create a directory to store PNG frames\npath &lt;- file.path(\"k_function_monta_carlo\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n  \n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"k_function_monta_carlo/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"k_function_monta_carlo/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"k_function_monta_carlo/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nI noticed that the observed line has a somewhat substantial deviation above the upper envelop (shaded region) generated from the Monte Carlo simulation. This indicates a strong tendancy for points to be clustered in the Yinmarbin district. Since this occurs across a wide range of distances, it implies that clustering is a prominent feature of the spatial distribution.\n\n\n\n\n8.1.2 Shwebo District\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Shwebo District\n# Set up shwebo_ppp_owin_list\nconflict_shwebo &lt;- conflict_data_sf %&gt;% filter(DT == \"Shwebo\")\nunique_quarter &lt;- unique(conflict_shwebo$year_quarter)\nboundary_shwebo &lt;- filter(boundary_sf, DT == \"Shwebo\")\nshwebo_owin &lt;- as.owin(boundary_shwebo)\nshwebo_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_shwebo %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[shwebo_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  shwebo_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFor the Shwebo District, the observed line (K-iso) also lies constantly above the actual line (K-pois) from 2021 Q2 to 2024 Q2, indicating clustering at all distances ‚Äòr‚Äô of our spatial points. Interestingly, conflict events in 2021 Q1 displayed some tendancy towards a dispersed point pattern as the observed K-function lies slightly below the actual line at a distance of 17km.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monta Carlo simulation test) will be conducted.\n\n\n\n\n\n\n\nObservations\n\n\n\nIn 2021 Q1, a sharp jagged plot is generated due to the smaller dataset size of conflicts in Shwebo. In 2024 Q2, we see the highest variability in spatial patterns which results in a larger envelope size\nGenerally, the observed line has a smaller deviation above the upper envelop than in the Yinmarbin data. This indicates a a somewhat intense level of clustering in the Shwebo district and remains more clustered than would be expected if points were distributed randomly..\n\n\n\n\n8.1.3 Pakokku District\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Pakokku District\n# Set up pakokku_ppp_owin_list\nconflict_pakokku &lt;- conflict_data_sf %&gt;% filter(DT == \"Pakokku\")\nunique_quarter &lt;- unique(conflict_pakokku$year_quarter)\nboundary_pakokku &lt;- filter(boundary_sf, DT == \"Pakokku\")\npakokku_owin &lt;- as.owin(boundary_pakokku)\npakokku_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_pakokku %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[pakokku_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  pakokku_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe observed K-function lies above the expected K-function outputted from 2021 to 2024, indicating clustering at all distances ‚Äòr‚Äô of our spatial points. However, we can observe milder clustering from 2021 Q4 to 2024 Q2 as the magnitude of deviation is significantly smaller than the period of 2021 Q1 to 2021 Q3.\n\n\n2) Performing Complete Spatial Randomness Test\n\n\n\n\n\n\n\nObservations\n\n\n\nIn 2021 Q2, we see a large envelope which reflects greater variability in the expected K-function and this could stem from the mix of both clustering and dispersion across Myanmar for that period. Generally from 2021 Q1 to 2024 Q2, clustering is evident as the observed K-function continues to lie above the envelope.\n\n\n\n\n8.1.4 Mandalay District\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Mandalay District\n# Set up mandalay_ppp_owin_list\nconflict_mandalay &lt;- conflict_data_sf %&gt;% filter(DT == \"Mandalay\")\nunique_quarter &lt;- unique(conflict_mandalay$year_quarter)\nboundary_mandalay &lt;- filter(boundary_sf, DT == \"Mandalay\")\nmandalay_owin &lt;- as.owin(boundary_mandalay)\nmandalay_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_mandalay %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[mandalay_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  mandalay_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe degree of clustering is less pronounced in Mandalay than the other districts since the gap between the observed line and the theoretical line is smaller. Clustering is large-scale in Mandalay where statistically significant clustering patterns are seen in larger distance ranges.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monta Carlo simulation test) will be conducted.\n\n\n\n\n\n\n\nObservations\n\n\n\nWe see the highest variability in spatial patterns in 2024 Q1 and Q2 which indicates that some areas in Mandalay District are more clustered with conflict events while other areas are more evenly spaced.\nWe can confirm statistically significant clustering from 2021 Q1 to 2023 Q3 as K value is larger than the upper confidence of the envelope than in 2024 Q1 and Q2. There is also strong evidence of clustering especially at larger distances which means Mandalay District faces large-scale clustering.\n\n\n\n\n\n8.2 Using L-Function Estimation\n\n8.2.1 Yinmarbin District\n1) Computing L-function Estimation\n\n\nPlot L-function for Yinmarbin District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_L_function\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n  \n  # Calculate the L-function\n  L_result &lt;- Lest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_L_function/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(L_result, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_L_function/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFirstly, the L-function plot appears more zig-zag than the K-function as the L-function can magnify small fluctuations in the K-function. Secondly, the theoretical line (dotted) is fixed at 0 where L(d)‚àír=0 which is an expected behavior under Complete Spatial Randomness.\nFrom 2022 Q1- Q3, all of 2023 and 2024 Q2, there is large scale clustering as the observed line (K-iso) consistently lies above the theoretical line (K-pois) particularly at larger spatial distances. This suggests conflicts in Yinmarbin are clustered across the district during this time period.\nFrom 2021, 2022 Q4 and 2024 Q1, there is a decreasing K value observed which indicates more localised clustering in specific towns on Yinmarbin.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n\nMonta Carlo Simulation for Yinmarbin District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"L_function_monta_carlo\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n\n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"L_function_monta_carlo/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"L_function_monta_carlo/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe periods of 2022 Q4, 2024 Q1 and 2024 Q2 shows a wider envelope which indicates that some areas are clustered while others are more evenly spaced in Yinmarbin. There‚Äôs also more widespread clustering observed in all of 2023 and in 2024 Q2 where deviation of L values above the envelope are signifiantly larger at bigger distances scales.\n\n\n\n\n8.2.2 Shwebo District\n1) Computing L-function Estimation\n\n# Display Plot\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe 2021 Q1 observed L values are much more jagged than other quarters as we have a smalller dataset of conflicts in Shwebo district of that quarter. In general, we see stronger clustering in bigger areas of the district which suggests wide-scale clustering of conflicts.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n# Display Plot\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\n2021 Q2 and 2024 Q2 has the highest variability in spatial points with both clustering and dispersion patterns under the CRS test in Shwebo district. Nonetheless, we can confirm strong significance of widespread clustering of conflicts throughout all quarters in Shwebo where the observed L values like above the theoretical values simulated, particularly at larger distance scales.\n\n\n\n\n8.2.3 Pakokku District\n1) Computing L-function Estimation\n\n\n\n\n\n\n\nObservations\n\n\n\nWe generally see strong evidence of localised clustering in the Pakokku district with weaker significance of clustering in bigger distance scales. This means that specific towns in Pakokku have more intense hotspots than others.It is worth noting that the jagged ouputs from the observed L values in 2021 Q1 and Q2 stems from our smaller data conflict points for these periods.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n\n\n\n\n\n\nObservations\n\n\n\nThe results of the simulation produces the greatest variability in conflict activity in Pakokku in 2021 Q2 and 2024 Q1. The CSR tests confirms significant clustering throughout all quarters in Pakokku. We can confirm that localised clustering are more promoinent in 2021 Q2, 2022 Q3 and 2024 Q1 where the L-values deviates more above the envelope at smaller distances. The remaning quarters show strong evidence of widespread clustering across a bigger distribution of Pakokku district.\n\n\n\n\n8.2.4 Mandalay District\n1) Computing L-function Estimation\n\n\n\n\n\n\n\nObservations\n\n\n\nGenerally from 2021 to 2024, we see an upward trend of observed L values with sharp fluctuations as distance increases.\nAt both small and large distance scales, we can observe deviations of the observed line above the theoretical line which suggests statistically strong evidence of clustering across Mandalay, where clustering is present at local towns and across Mandalay.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n\n\n\n\n\n\nObservations\n\n\n\nIn 2021 Q1 and 2024 Q1-2, we can observe a bigger envelope from the CSR test especially at larger distances. This means certain areas in Mandalay show strong clustering (hotspots), while others are more dispersed, leading to greater variability in the overall spatial structure of conflict events.\nAdditionally, throughout 2021 to 2024, there is prominent evidence of hotspots and clustering throughout Mandalay since observed L values deviate above the theoretical L values, across a range of distances."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#conclusion-and-reflection",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#conclusion-and-reflection",
    "title": "Take-home Exercise 1",
    "section": "9. Conclusion and Reflection",
    "text": "9. Conclusion and Reflection\nThis take-home exercise has significantly expanded my understanding and underscored the severe humanitarian conflicts in Myanmar, revealed through this spatial and spatio-temporal point pattern analysis.\nA key takeaway from Myanmar‚Äôs humanitarian conflict is how complex the interplay of ethnic, political, and religious struggles has been and continues to be an on-going humanitarian crisis, involving the military, ethnic armed groups, political militias, and civilians.\nMy analysis has revealed how central and southern states (notably, Sagaing, Mandalay, Magway and Yangon) have experienced intense clashes between state forces and politica/identity militias, while northern Myanmar sees high conflict levels between rebels and political militias. However, one should also consider the area of a state/district when computing KDE since a smaller area could increase the density of conflict quite significantly.\nThe political militia are found to me to be less involved with unarmed civilians than state forces, rebels and other policial militia. Conversely, civilians are seen to be embroiled mostly in conflicts resulting from strategic development and violence against civilians events, primarily in Central and Western Myanmar. Fortunately, conflicts involving explosions or remote violence is not as intense against civilians but this raises a great concern on the humanitarian crisis faced by civilians in Myanmar, especially since armed conflicts tend to occur repeatedly in the same parts of each state which indicates an unceasing cycle of conflicts in Myanmar.\nAdditionally, results from the Monte Carlo CSR test indicated a strong tendency for points to be clustered across a wide range of distances, implying that clustering is a prominent feature of the spatial distribution in districts like Sagaing, Mandalay, Magway and Yangon. Conversely, there are specific conflict hot spots observed in certain towns of the district which means localised clustering are also present.\nIt can also be derived that in 2021 Q1 and Q2, conflict points are less intense and slightly more random in its distribution. The intensity of conflicts continue to soar from 2022 Q1 to 2024 Q2 with both localised and widespread conflict events during these period of time.\nLastly, running computationally intensive codes such as envelope() for 2nd order spatial analysis may differ in processing speed from one student to another. My system, however, is running with 8 cores and it does take a lengthy amount of time for processing the large 40,000+ rows of conflict data. (1 simulation takes around 2 minutes to run). Hence, I would suggest other students to begin their take-home exercise as early as possible to provide buffer time for processing the data, handling code bugs and even data cleaning errors."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#conclusion",
    "title": "Take-home Exercise 1",
    "section": "9. Conclusion",
    "text": "9. Conclusion\nMy analysis has revealed how central districts (i.e., Yinmarbin, Shwebo, Pakokku and Mandalay) and western states (i.e.¬†Yangon) have experienced intense clashes between state forces and politica/identity militias, while northern Myanmar sees high conflict levels between rebels and political militias. However, one should also consider the size of a state/district when computing KDE since a smaller area could increase the density of conflict quite significantly.\nThe political militia are found to be less involved with unarmed civilians than state forces, rebels and other policial militia. Conversely, civilians are seen to be embroiled mostly in conflicts resulting from strategic development and violence against civilians events, primarily in Central and Western Myanmar. Fortunately, conflicts involving explosions or remote violence is not as intense against civilians but this raises a great concern on the humanitarian crisis faced by civilians in Myanmar, especially since armed conflicts tend to occur repeatedly in the same parts of each state which indicates an unceasing cycle of conflicts in Myanmar.\nAdditionally, results from the Monte Carlo CSR test indicated a strong tendency for points to be clustered across a wide range of distances, implying that clustering is a prominent feature of the spatial distribution in districts like Sagaing, Mandalay, Magway and Yangon. Conversely, there are specific conflict hot spots observed in certain towns of the district which means localised clustering are also present.\nIt can also be derived that in 2021 Q1 and Q2, conflict points are less intense and slightly more random in its distribution. The intensity of conflicts continue to soar from 2022 Q1 to 2024 Q2 with both localised and widespread conflict events during these period of time."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#my-reflections",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#my-reflections",
    "title": "Take-home Exercise 1",
    "section": "10. My Reflections",
    "text": "10. My Reflections\nThis take-home exercise has significantly expanded my understanding and underscored the severe humanitarian conflicts in Myanmar, revealed through this spatial and spatio-temporal point pattern analysis.\nA key takeaway from Myanmar‚Äôs humanitarian conflict is how complex the interplay of ethnic, political, and religious struggles has been and continues to be an on-going humanitarian crisis, involving the military, ethnic armed groups, political militias, and civilians.\nLastly, running computationally intensive codes such as envelope() for 2nd order spatial analysis may differ in processing speed from one student to another. My system, however, is running with 8 cores and it does take a lengthy amount of time for processing the large 40,000+ rows of conflict data.\nHence, I would suggest other students to begin their take-home exercise as early as possible to provide buffer time for processing the data, handling code bugs and even data cleaning errors."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html",
    "title": "In-class Exercise 6",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#install-required-libraries",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#install-required-libraries",
    "title": "In-class Exercise 6",
    "section": "1.1 Install Required Libraries",
    "text": "1.1 Install Required Libraries\nWe will first want to install the GWModel package from CRAN\n\ninstall.packages(\"GWmodel\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#importing-libraries-into-r",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#importing-libraries-into-r",
    "title": "In-class Exercise 6",
    "section": "1.2 Importing Libraries into R",
    "text": "1.2 Importing Libraries into R\nIn this in-class exercise, sf, sfdep, tmap, and tidyverse will be used.\n\nsf provides a standardized way to work with spatial data in R. It allows for the manipulation and analysis of geospatial data in simple feature format\nsfdep is designed for spatial dependency and autocorrelation analysis. It specifically integrates with sf to calculate spatial autocorrelation statistics such as Moran‚Äôs I, Geary‚Äôs C, and other local or global spatial measures.\ntmap is a powerful package for visualizing spatial data through thematic maps. It supports both static and interactive mapping, making it ideal for displaying spatial patterns, clusters, and the results of autocorrelation analysis.\ntidyverse is a collection of packages (e.g., dplyr, ggplot2, purrr, tibble) that are designed for data manipulation, visualization, and functional programming in R. It is not specific to spatial analysis but is essential for general data wrangling.\n\n\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#preparing-the-datasets",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#preparing-the-datasets",
    "title": "In-class Exercise 6",
    "section": "1.3 Preparing the Datasets",
    "text": "1.3 Preparing the Datasets\nI will be using the Hunan dataset used in the Hands-on Exercise 5 spatial weights and applications.\n\n1.3.1 Importing Geospatial Data\nFirstly, we will import the Hunan county boundary layer. This is a geospatial data set in ESRI shapefile format. The code chunk below uses¬†st_read()¬†of¬†sf¬†package.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex6\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n1.3.2 Importing Aspatial Data\nNext, I will import the aspatial data set. This data is a csv file containing selected Hunan‚Äôs local development indicators in 2012.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n1.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan‚Äôs SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\n\n\n1.4 Plotting A Choropleth Map\nNext, let‚Äôs plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by county, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#global-measures-of-spatial-association",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#global-measures-of-spatial-association",
    "title": "In-class Exercise 6",
    "section": "2. Global Measures of Spatial Association",
    "text": "2. Global Measures of Spatial Association\n\nStep 1: Deriving Queen‚Äôs contiguity weights: sfdep methods\nNotice that st_weights() provides tree arguments,\n\nnb: a neighbour list object as created by st_neighbors().\nstyle: Default ‚ÄúW‚Äù for row standardized weights. This value can also be ‚ÄúB‚Äù, ‚ÄúC‚Äù, ‚ÄúU‚Äù, ‚Äúminmax‚Äù, and ‚ÄúS‚Äù. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al.¬†1999, p.¬†167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors. 8\n\n\n# wm_q &lt;- hunan_GDPPC %&gt;%\n#   mutate(nb = st_contiguity(geometry),\n#          wt = st_weights(nb, style = \"W\"),\n#          .before = 1)\n\n\n\nComputing Global Moran‚Äôs I\nIn the code chunk below, global _moran() function is used to compute the Moran‚Äôs / value. Different from spdep package, the output is a tibble data.frame.\n\nk is the average neighbours found\n\n\n# moranI &lt;- global_moran(wm_1$GDPPC,\n#                        wm_q$nb,\n#                        wm_q$wt)\n# glimpse(moranI)\n\n\n\nPerforming Global Moran‚Äôs I Permutation Test\nIn practice, Monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by globel_ moran_perm().\nStep 1:\nIt‚Äôs always good practice to to use set.seed() before performing simulation. This is to ensure that the computation is reproducible.\n\n# set.ssed(1234)\n\nStep 2\nNext, global_moral_perm() is used to perform Monte Carlo simulation.\n\n# global_moran(wm_1$GDPPC,\n#              wm_q$nb,\n#              wm_q$wt,\n#              nsim = 99)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html",
    "title": "Take-home Exercise 1 - Part 2",
    "section": "",
    "text": "&lt; Back to Part 1"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html#st-order-spatio-temporal-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html#st-order-spatio-temporal-point-pattern-analysis",
    "title": "Take-home Exercise 1 - Part 2",
    "section": "7. 1st Order Spatio-Temporal Point Pattern Analysis",
    "text": "7. 1st Order Spatio-Temporal Point Pattern Analysis\n\n\nSet up DayofYear variable per quarter\nQ2_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2024 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2024 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\n\n\n7.1 Creating ppp object\nIn the code chunk below, DayofYear from the fire_sf data frame is selected and is included in the output ppp object.\n\n# Create ppp object per quarter\nQ2_2024_ppp &lt;- Q2_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2024_ppp &lt;- Q1_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2023_ppp &lt;- Q4_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2023_ppp &lt;- Q3_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2023_ppp &lt;- Q2_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2023_ppp &lt;- Q1_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2022_ppp &lt;- Q4_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2022_ppp &lt;- Q3_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2022_ppp &lt;- Q2_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2022_ppp &lt;- Q1_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2021_ppp &lt;- Q4_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2021_ppp &lt;- Q3_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2021_ppp &lt;- Q2_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2021_ppp &lt;- Q1_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\n7.2 Combining ppp with owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\n# Mask the ppp object with owin object\nQ2_2024_owin &lt;- Q2_2024_ppp[myanmar_owin]\n\nQ1_2024_owin &lt;- Q1_2024_ppp[myanmar_owin]\n\nQ4_2023_owin &lt;- Q4_2023_ppp[myanmar_owin]\n\nQ3_2023_owin &lt;- Q3_2023_ppp[myanmar_owin]\n\nQ2_2023_owin &lt;- Q2_2023_ppp[myanmar_owin]\n\nQ1_2023_owin &lt;- Q1_2023_ppp[myanmar_owin]\n\nQ4_2022_owin &lt;- Q4_2022_ppp[myanmar_owin]\n\nQ3_2022_owin &lt;- Q3_2022_ppp[myanmar_owin]\n\nQ2_2022_owin &lt;- Q2_2022_ppp[myanmar_owin]\n\nQ1_2022_owin &lt;- Q1_2022_ppp[myanmar_owin]\n\nQ4_2021_owin &lt;- Q4_2021_ppp[myanmar_owin]\n\nQ3_2021_owin &lt;- Q3_2021_ppp[myanmar_owin]\n\nQ2_2021_owin &lt;- Q2_2021_ppp[myanmar_owin]\n\nQ1_2021_owin &lt;- Q1_2021_ppp[myanmar_owin]\n\nNow, I will perform a spatio-temporal kernel density estimate on the owin object which gives us insights into where and when conflict event occurrences are concentrated within the specified observation window.\n\n\nPerform spatial temporal KDE per quarter\nlibrary(spatstat)\nQ2_2024_stkde &lt;- spattemp.density(Q2_2024_owin)\n\nQ1_2024_stkde &lt;- spattemp.density(Q1_2024_owin)\n\nQ4_2023_stkde &lt;- spattemp.density(Q4_2023_owin)\n\nQ3_2023_stkde &lt;- spattemp.density(Q3_2023_owin)\n\nQ2_2023_stkde &lt;- spattemp.density(Q2_2023_owin)\n\nQ1_2023_stkde &lt;- spattemp.density(Q1_2023_owin)\n\nQ4_2022_stkde &lt;- spattemp.density(Q4_2022_owin)\n\nQ3_2022_stkde &lt;- spattemp.density(Q3_2022_owin)\n\nQ2_2022_stkde &lt;- spattemp.density(Q2_2022_owin)\n\nQ1_2022_stkde &lt;- spattemp.density(Q1_2022_owin)\n\nQ4_2021_stkde &lt;- spattemp.density(Q4_2021_owin)\n\nQ3_2021_stkde &lt;- spattemp.density(Q3_2021_owin)\n\nQ2_2021_stkde &lt;- spattemp.density(Q2_2021_owin)\n\nQ1_2021_stkde &lt;- spattemp.density(Q1_2021_owin)\n\n\n\n\n7.3 Plotting STKDE Outputs\nLet‚Äôs plot our animated spatio-temporal KDE outputs for each quarter.\n\n7.3.1 2024 Q1-2 STKDE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAt the start of 2024 Q1, we see that the kernel density estimate of Myanmar conflicts tend to move sporadically where there are no specific patterns but in 2024 Q2, we start seeing conflict events occurring more intensely in Central and Southern Myanmar.\n\n\n\n\n7.3.2 2023 Q1-Q4 STKDE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom 2023 Q1 to Q3, there is a noticeable cluster of conflict events happening in Central and Southern regions of Myanmar than the other parts of the country. In 2023 Q4, we can notice more dispersion in conflict events across Myanmar, spreading into Western and Eastern regions. Throughout 2023, conflict events are least observed in North Myanmar.\n\n\n\n\n7.3.3 2022 Q1-Q4 STKDE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nSimilar to 2023, the spread of conflict events in 2022 is largely clustered in Central and Southen parts of Myanmar. However, we do see some jitters in conflict trends in 2022 Q1 with a rare sight of conflicts in the far North of Myanmar and occasional conflicts in Southern Myanmar.\n\n\n\n\n7.3.4 2021 Q1-Q4 STKDE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFinally, armed conflicts in 2021 is by far the most random spread of armed conflicts throughout Myanmar. There is also a high intensity of conflicts in South Myanmar, particularly in the state of Yangon while we rarely see conflicts occurring in the extreme North of Myanmar.\n\n\n\n\n7.3.5 OpenStreetMap in Myanmar - Spatio Temporal\nWe can observe the spatio-temporal conflict patterns per quarter using OpenStreetMap as well.\n\n# Get yearly conflict data\nyear_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter %in% c(\"2024 Q1\", \"2024 Q2\")) %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nyear_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter %in% c(\"2023 Q1\", \"2023 Q2\", \"2023 Q3\", \"2023 Q4\")) %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nyear_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter %in% c(\"2022 Q1\", \"2022 Q2\", \"2022 Q3\", \"2022 Q4\")) %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nyear_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter %in% c(\"2021 Q1\", \"2021 Q2\", \"2021 Q3\", \"2021 Q4\")) %&gt;%\n  mutate(DayofYear = yday(event_date))\n\n# Create ppp object per quarter\nyear_2024_ppp &lt;- year_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nyear_2023_ppp &lt;- year_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nyear_2022_ppp &lt;- year_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nyear_2021_ppp &lt;- year_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n# Mask the ppp object with owin object\nyear_2024_owin &lt;- year_2024_ppp[myanmar_owin]\nyear_2023_owin &lt;- year_2023_ppp[myanmar_owin]\nyear_2022_owin &lt;- year_2022_ppp[myanmar_owin]\nyear_2021_owin &lt;- year_2021_ppp[myanmar_owin]\n\nyear_2024_owin &lt;- rescale(year_2024_owin, 1000, \"km\")\nyear_2024_stkde &lt;- density(year_2024_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nyear_2023_owin &lt;- rescale(year_2023_owin, 1000, \"km\")\nyear_2023_stkde &lt;- density(year_2023_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nyear_2022_owin &lt;- rescale(year_2022_owin, 1000, \"km\")\nyear_2022_stkde &lt;- density(year_2022_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\nyear_2021_owin &lt;- rescale(year_2021_owin, 1000, \"km\")\nyear_2021_stkde &lt;- density(year_2021_owin, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\n\n\n\nSet up raster and projection\nraster_2024 &lt;- raster(year_2024_stkde)\nraster_2023 &lt;- raster(year_2023_stkde)\nraster_2022 &lt;- raster(year_2022_stkde)\nraster_2021 &lt;- raster(year_2021_stkde)\n\nprojection(raster_2024) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_2023) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_2022) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\nprojection(raster_2021) &lt;- CRS(\"+init=EPSG:32647 +units=km\")\n\n\n\n# Plot KDE Map on OpenStreetMap\nraster_quarters &lt;- list('raster_2024','raster_2023','raster_2022','raster_2021')\n\n# Set tmap mode to view\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\n# Function to create tmap for each raster\ncreate_tmap &lt;- function(raster_var) {\n  kde_fixed_output &lt;- tm_basemap(server = \"OpenStreetMap.HOT\") +\n    tm_basemap(server = \"Esri.WorldImagery\") +\n    tm_shape(get(raster_var)) +  # Dynamically get the raster variable\n    tm_raster(\"layer\",\n              n = 10,\n              title = paste(raster_var),\n              alpha = 0.6,\n              palette = c(\"#fafac3\",\"#fd953b\",\"#f02a75\",\"#b62385\",\"#021c9e\")) +\n    tm_shape(boundary_sf) +\n    tm_polygons(alpha=0.1, id=\"DT\") +\n    tmap_options(check.and.fix = TRUE)\n  \n  return(kde_fixed_output)\n}\n\n# Create a list of tmap objects by iterating over the raster_quarters list\ntmap_list &lt;- lapply(raster_quarters, create_tmap)\n\n# Arrange and display the tmaps\ntmap_arrange(tmap_list, ncol = 2, nrow = 2, sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe plots uncover the increasing levels of conflicts across each quarter. Interestingly, 2021 Q1 begins with higher concentration of armed conflicts in Southern and Eastern parts of Myanmar, but gradually becomes more concentrated in the Central districts. From 2022 Q4 onwards, we see that conflicts have spread outside the core central region of Myanmar towards the Western region. Conflicts return to occurring more frequently in the Central districts like Yinmarbin and Sagaing in 2024 Q1 and Q2,"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html#nd-order-spatio-temporal-point-pattern-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html#nd-order-spatio-temporal-point-pattern-analysis",
    "title": "Take-home Exercise 1 - Part 2",
    "section": "8. 2nd Order Spatio-Temporal Point Pattern Analysis",
    "text": "8. 2nd Order Spatio-Temporal Point Pattern Analysis\nSimilar to the 2nd order spatial analysis in section 6, I want to explore spatio-temporal trends of Myanmar‚Äôs conflicts and how the events differ in distribution, from quarter to quarter. I‚Äôll delve into the four districts I‚Äôm most interested in, that is the districts with the highest proportions of conflicts - Yinmarbin, Shwebo, Pakokku and Mandalay.\n\n8.1 Using K-Function Estimation\n\n8.1.1 Yinmarbin District\nWe‚Äôll want to compute the K-Function by quarters via Kest() by iterating over each unique quarter in date format, then plotting an animated graph of the K-Function outputs per quarter.\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Yinmarbin District\n# Set up yinmarbin_ppp_owin_list\nconflict_yinmarbin &lt;- conflict_data_sf %&gt;% filter(DT == \"Yinmarbin\")\nunique_quarter &lt;- unique(conflict_yinmarbin$year_quarter)\nboundary_yinmarbin &lt;- filter(boundary_sf, DT == \"Yinmarbin\")\nyinmarbin_owin &lt;- as.owin(boundary_yinmarbin)\nyinmarbin_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_yinmarbin %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[yinmarbin_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  yinmarbin_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\nPlot K-function for Yinmarbin District\nlibrary(magick)\n\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_k_function\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n  K_result &lt;- Kest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_k_function/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_k_function/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_k_function/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can observe how the observed line (K-iso) is constantly above the actual line (K-pois) from 2021 Q2 to 2024 Q2. This confirms that conflict points in Yinmarbin are highly clustered. In fact, it is more clustered together than expected by the null hypothesis. There is no K-function outputted for 2021 Q1 as no conflict points were observed in Yinmarbin District for that time period.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRipley‚Äôs Correction provides a way to mitigate the bias introduced by points near the edge of the study window by accounting for the reduced area for point comparisons near the edges. Hence, this results in a a more accurate estimate of the spatial distribution as seen in the slight difference between actual and expected K-function.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monta Carlo simulation test) will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of conflict events in Myanmar are randomly distributed.\nH1= The distribution of conflict events in Myanmar are not randomly distributed.\nThe null hypothesis will be rejected if the observed K-function lies above/below the theoretical K-function and envelope.\n\nBy using envelop(), we can get a more robust interpretation by comparing the observed K-function against a simulation envelope of K-functions generated under the null hypothesis.\n\n\nMonta Carlo Simulation for Yinmarbin District\nlibrary(magick)\n\n# Unique quarters of the year in the dataset\nunique_quarter &lt;- unique(conflict_yinmarbin$year_quarter)\n\n# Create a directory to store PNG frames\npath &lt;- file.path(\"k_function_monta_carlo\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the K-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n  \n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"k_function_monta_carlo/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"k_function_monta_carlo/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"k_function_monta_carlo/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nI noticed that the observed line has a somewhat substantial deviation above the upper envelop (shaded region) generated from the Monte Carlo simulation. This indicates a strong tendancy for points to be clustered in the Yinmarbin district. Since this occurs across a wide range of distances, it implies that clustering is a prominent feature of the spatial distribution.\n\n\n\n\n8.1.2 Shwebo District\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Shwebo District\n# Set up shwebo_ppp_owin_list\nconflict_shwebo &lt;- conflict_data_sf %&gt;% filter(DT == \"Shwebo\")\nunique_quarter &lt;- unique(conflict_shwebo$year_quarter)\nboundary_shwebo &lt;- filter(boundary_sf, DT == \"Shwebo\")\nshwebo_owin &lt;- as.owin(boundary_shwebo)\nshwebo_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_shwebo %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[shwebo_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  shwebo_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFor the Shwebo District, the observed line (K-iso) also lies constantly above the actual line (K-pois) from 2021 Q2 to 2024 Q2, indicating clustering at all distances ‚Äòr‚Äô of our spatial points. Interestingly, conflict events in 2021 Q1 displayed some tendancy towards a dispersed point pattern as the observed K-function lies slightly below the actual line at a distance of 17km.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monta Carlo simulation test) will be conducted.\n\n\n\n\n\n\n\nObservations\n\n\n\nIn 2021 Q1, a sharp jagged plot is generated due to the smaller dataset size of conflicts in Shwebo. In 2024 Q2, we see the highest variability in spatial patterns which results in a larger envelope size\nGenerally, the observed line has a smaller deviation above the upper envelop than in the Yinmarbin data. This indicates a a somewhat intense level of clustering in the Shwebo district and remains more clustered than would be expected if points were distributed randomly..\n\n\n\n\n8.1.3 Pakokku District\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Pakokku District\n# Set up pakokku_ppp_owin_list\nconflict_pakokku &lt;- conflict_data_sf %&gt;% filter(DT == \"Pakokku\")\nunique_quarter &lt;- unique(conflict_pakokku$year_quarter)\nboundary_pakokku &lt;- filter(boundary_sf, DT == \"Pakokku\")\npakokku_owin &lt;- as.owin(boundary_pakokku)\npakokku_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_pakokku %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[pakokku_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  pakokku_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe observed K-function lies above the expected K-function outputted from 2021 to 2024, indicating clustering at all distances ‚Äòr‚Äô of our spatial points. However, we can observe milder clustering from 2021 Q4 to 2024 Q2 as the magnitude of deviation is significantly smaller than the period of 2021 Q1 to 2021 Q3.\n\n\n2) Performing Complete Spatial Randomness Test\n\n\n\n\n\n\n\nObservations\n\n\n\nIn 2021 Q2, we see a large envelope which reflects greater variability in the expected K-function and this could stem from the mix of both clustering and dispersion across Myanmar for that period. Generally from 2021 Q1 to 2024 Q2, clustering is evident as the observed K-function continues to lie above the envelope.\n\n\n\n\n8.1.4 Mandalay District\n1) Computing K-function Estimation\n\n\nPrepare Dataset for Mandalay District\n# Set up mandalay_ppp_owin_list\nconflict_mandalay &lt;- conflict_data_sf %&gt;% filter(DT == \"Mandalay\")\nunique_quarter &lt;- unique(conflict_mandalay$year_quarter)\nboundary_mandalay &lt;- filter(boundary_sf, DT == \"Mandalay\")\nmandalay_owin &lt;- as.owin(boundary_mandalay)\nmandalay_ppp_owin_list &lt;- list()\n\nfor (quarter in unique_quarter) {\n  quarter_data &lt;- conflict_mandalay %&gt;% \n    filter(year_quarter == quarter)\n\n  ppp_obj &lt;- as.ppp(quarter_data$geometry)\n  ppp_owin_obj &lt;- ppp_obj[mandalay_owin]\n  ppp_owin_obj_km &lt;- rescale(ppp_owin_obj, 1000, \"km\")\n  mandalay_ppp_owin_list[[quarter]] &lt;- ppp_owin_obj_km\n}\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe degree of clustering is less pronounced in Mandalay than the other districts since the gap between the observed line and the theoretical line is smaller. Clustering is large-scale in Mandalay where statistically significant clustering patterns are seen in larger distance ranges.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e.¬†Monta Carlo simulation test) will be conducted.\n\n\n\n\n\n\n\nObservations\n\n\n\nWe see the highest variability in spatial patterns in 2024 Q1 and Q2 which indicates that some areas in Mandalay District are more clustered with conflict events while other areas are more evenly spaced.\nWe can confirm statistically significant clustering from 2021 Q1 to 2023 Q3 as K value is larger than the upper confidence of the envelope than in 2024 Q1 and Q2. There is also strong evidence of clustering especially at larger distances which means Mandalay District faces large-scale clustering.\n\n\n\n\n\n8.2 Using L-Function Estimation\n\n8.2.1 Yinmarbin District\n1) Computing L-function Estimation\n\n\nPlot L-function for Yinmarbin District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"sec_order_L_function\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n  \n  # Calculate the L-function\n  L_result &lt;- Lest(ppp_owin_obj_km, correction = \"Ripley\")\n\n  # Create PNG filename\n  png_filename &lt;- file.path(\"sec_order_L_function/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(L_result, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(km)\",\n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"sec_order_L_function/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFirstly, the L-function plot appears more zig-zag than the K-function as the L-function can magnify small fluctuations in the K-function. Secondly, the theoretical line (dotted) is fixed at 0 where L(d)‚àír=0 which is an expected behavior under Complete Spatial Randomness.\nFrom 2022 Q1- Q3, all of 2023 and 2024 Q2, there is large scale clustering as the observed line (K-iso) consistently lies above the theoretical line (K-pois) particularly at larger spatial distances. This suggests conflicts in Yinmarbin are clustered across the district during this time period.\nFrom 2021, 2022 Q4 and 2024 Q1, there is a decreasing K value observed which indicates more localised clustering in specific towns on Yinmarbin.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n\nMonta Carlo Simulation for Yinmarbin District\nlibrary(magick)\n# Create a directory to store PNG frames\npath &lt;- file.path(\"L_function_monta_carlo\", \"conflict_yinmarbin\")\nif (!dir.exists(path)) {\n  dir.create(path, recursive = TRUE)\n}\n\n# Loop through each unique quarter and plot the L-function\nfor (quarter in names(yinmarbin_ppp_owin_list)) {\n  ppp_owin_obj_km &lt;- yinmarbin_ppp_owin_list[[quarter]]\n\n  # Calculate the envelope\n  K_result &lt;- envelope(ppp_owin_obj_km, Kest, nsim = 39, \n                       rank = 1, glocal=TRUE)\n  # Create PNG filename\n  png_filename &lt;- file.path(\"L_function_monta_carlo/conflict_yinmarbin\", \n                            sprintf(\"frame_%s.png\", quarter))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(K_result, \n       main = paste(\"Yinmarbin District -\",quarter))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_yinmarbin\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1)\noutput_path &lt;- \"L_function_monta_carlo/kfunction_yinmarbin.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe periods of 2022 Q4, 2024 Q1 and 2024 Q2 shows a wider envelope which indicates that some areas are clustered while others are more evenly spaced in Yinmarbin. There‚Äôs also more widespread clustering observed in all of 2023 and in 2024 Q2 where deviation of L values above the envelope are signifiantly larger at bigger distances scales.\n\n\n\n\n8.2.2 Shwebo District\n1) Computing L-function Estimation\n\n# Display Plot\nframes &lt;- image_read(list.files(\"sec_order_L_function/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe 2021 Q1 observed L values are much more jagged than other quarters as we have a smalller dataset of conflicts in Shwebo district of that quarter. In general, we see stronger clustering in bigger areas of the district which suggests wide-scale clustering of conflicts.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n# Display Plot\nframes &lt;- image_read(list.files(\"L_function_monta_carlo/conflict_shwebo\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 1) \nanimation\n\n\n\n\n\n\n\n\nObservations\n\n\n\n2021 Q2 and 2024 Q2 has the highest variability in spatial points with both clustering and dispersion patterns under the CRS test in Shwebo district. Nonetheless, we can confirm strong significance of widespread clustering of conflicts throughout all quarters in Shwebo where the observed L values like above the theoretical values simulated, particularly at larger distance scales.\n\n\n\n\n8.2.3 Pakokku District\n1) Computing L-function Estimation\n\n\n\n\n\n\n\nObservations\n\n\n\nWe generally see strong evidence of localised clustering in the Pakokku district with weaker significance of clustering in bigger distance scales. This means that specific towns in Pakokku have more intense hotspots than others.It is worth noting that the jagged ouputs from the observed L values in 2021 Q1 and Q2 stems from our smaller data conflict points for these periods.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n\n\n\n\n\n\nObservations\n\n\n\nThe results of the simulation produces the greatest variability in conflict activity in Pakokku in 2021 Q2 and 2024 Q1. The CSR tests confirms significant clustering throughout all quarters in Pakokku. We can confirm that localised clustering are more promoinent in 2021 Q2, 2022 Q3 and 2024 Q1 where the L-values deviates more above the envelope at smaller distances. The remaning quarters show strong evidence of widespread clustering across a bigger distribution of Pakokku district.\n\n\n\n\n8.2.4 Mandalay District\n1) Computing L-function Estimation\n\n\n\n\n\n\n\nObservations\n\n\n\nGenerally from 2021 to 2024, we see an upward trend of observed L values with sharp fluctuations as distance increases.\nAt both small and large distance scales, we can observe deviations of the observed line above the theoretical line which suggests statistically strong evidence of clustering across Mandalay, where clustering is present at local towns and across Mandalay.\n\n\n2) Performing Complete Spatial Randomness Test\nI will also perform monta carlo simulation test using envelope() of the¬†spatstat¬†package.\n\n\n\n\n\n\n\nObservations\n\n\n\nIn 2021 Q1 and 2024 Q1-2, we can observe a bigger envelope from the CSR test especially at larger distances. This means certain areas in Mandalay show strong clustering (hotspots), while others are more dispersed, leading to greater variability in the overall spatial structure of conflict events.\nAdditionally, throughout 2021 to 2024, there is prominent evidence of hotspots and clustering throughout Mandalay since observed L values deviate above the theoretical L values, across a range of distances."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html#conclusion",
    "title": "Take-home Exercise 1 - Part 2",
    "section": "9. Conclusion",
    "text": "9. Conclusion\nMy analysis has revealed how central districts (i.e., Yinmarbin, Shwebo, Pakokku and Mandalay) and western states (i.e.¬†Yangon) have experienced intense clashes between state forces and politica/identity militias, while northern Myanmar sees high conflict levels between rebels and political militias. However, one should also consider the size of a state/district when computing KDE since a smaller area could increase the density of conflict quite significantly.\nThe political militia are found to be less involved with unarmed civilians than state forces, rebels and other policial militia. Conversely, civilians are seen to be embroiled mostly in conflicts resulting from strategic development and violence against civilians events, primarily in Central and Western Myanmar. Fortunately, conflicts involving explosions or remote violence is not as intense against civilians but this raises a great concern on the humanitarian crisis faced by civilians in Myanmar, especially since armed conflicts tend to occur repeatedly in the same parts of each state which indicates an unceasing cycle of conflicts in Myanmar.\nAdditionally, results from the Monte Carlo CSR test indicated a strong tendency for points to be clustered across a wide range of distances, implying that clustering is a prominent feature of the spatial distribution in districts like Sagaing, Mandalay, Magway and Yangon. Conversely, there are specific conflict hot spots observed in certain towns of the district which means localised clustering are also present.\nIt can also be derived that in 2021 Q1 and Q2, conflict points are less intense and slightly more random in its distribution. The intensity of conflicts continue to soar from 2022 Q1 to 2024 Q2 with both localised and widespread conflict events during these period of time."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html#my-reflections",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html#my-reflections",
    "title": "Take-home Exercise 1 - Part 2",
    "section": "10. My Reflections",
    "text": "10. My Reflections\nThis take-home exercise has significantly expanded my understanding and underscored the severe humanitarian conflicts in Myanmar, revealed through this spatial and spatio-temporal point pattern analysis.\nA key takeaway from Myanmar‚Äôs humanitarian conflict is how complex the interplay of ethnic, political, and religious struggles has been and continues to be an on-going humanitarian crisis, involving the military, ethnic armed groups, political militias, and civilians.\nLastly, running computationally intensive codes such as envelope() for 2nd order spatial analysis may differ in processing speed from one student to another. My system, however, is running with 8 cores and it does take a lengthy amount of time for processing the large 40,000+ rows of conflict data.\nHence, I would suggest other students to begin their take-home exercise as early as possible to provide buffer time for processing the data, handling code bugs and even data cleaning errors."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html#references",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1_Part2.html#references",
    "title": "Take-home Exercise 1 - Part 2",
    "section": "11. References",
    "text": "11. References\n\nCrawley, M. J. (2007). The R Book. Wiley.\nCrisis Group. (2024, August 27). Breaking Away: The Battle for Myanmar‚Äôs Rakhine State. https://www.crisisgroup.org/asia/south-east-asia/myanmar/339-breaking-away-battle-myanmars-rakhine-state¬†\nFarge, E., & Mantovani, C. (2024, September 17). Myanmar military stepping up civilian killings and arrests, says UN report. https://www.reuters.com/world/asia-pacific/myanmar-military-intensifies-civilian-killings-arrests-says-un-report-2024-09-17/#:~:text=The%20report%20by%20the%20United,the%20military%20since%20the%20coup.¬†\nFishbein, E., & Lusan, N. N. (2022, December 14). ‚ÄòAfraid of the gun‚Äô: Military coup fuels Myanmar resource grab. Al Jazeera. https://www.aljazeera.com/news/2022/12/14/afraid-of-the-gun-military-coup-fuels-myanmar-resource-grab¬†\nRajagopalan, B., Lall, U., & Tarboton, D. (1997). Evaluation of kernel density estimation methods for daily precipitation resampling. Springer-Verlag.\nShen, B., Xiang Xu, Plaza, A., & Huang, Q. (2020, November 15). Unfolding Spatial-Temporal Patterns of Taxi Trip based on an Improved Network Kernel Density Estimation. MDPI. Retrieved September 22, 2024, from https://www.mdpi.com/2220-9964/9/11/683¬†\nThe Stata Journal. (2003). Adaptive kernel density estimation. Sage Journals. https://journals.sagepub.com/doi/pdf/10.1177/1536867X0300300204"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "pacman::p_load(sf, st, tidyverse, lubridate, sfdep, tmap, ggplot2, knitr, Kendall)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#setting-the-scene-geospatial-analysis-of-drug-abuse-in-thailand",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#setting-the-scene-geospatial-analysis-of-drug-abuse-in-thailand",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "In 2022, 567,609 drug users in ASEAN were treated, in which Thailand was found to have the highest number of drug users requiring treatment among ASEAN countries, followed by Malaysia, Indonesia, Laos, the Philippines, and Singapore. (Kahanto M., et al, 2022) Drug abuse is a significant social issue in Thailand, with profound health, financial, and societal implications. Positioned near the Golden Triangle‚Äîone of the largest drug production areas in Asia‚ÄîThailand faces ongoing challenges due to its geographical proximity and extensive transportation routes, which facilitate drug trafficking. Within Thailand, drug abuse is particularly prevalent among the youth, with approximately 2.7 million young people involved. Of those aged 15‚Äì19, around 300,000 are in need of drug treatment, and vocational students are disproportionately affected compared to their peers in secondary school.\nThis underscores the importance of drug treatment in addressing the complex problem of substance abuse and reduces the societal costs associated with drug abuse, such as healthcare expenses, lost productivity, and crime. Hence, to better allocate resources and develop targeted interventions, it is crucial to understand where drug abuse is most concentrated and how it spreads geographically. This is where geospatial analysis becomes essential.¬†\nIn this exercise, I will utilise geospatial analysis methods to explore the province-level dynamics of drug abuse in Thailand. This will involve preparing a study area layer as sf polygon features at the province level, including Bangkok, and creating a drug abuse indicators layer within this study area. Using these extracted data layers, I will conduct global spatial autocorrelation analysis using sfdep methods, followed by local spatial autocorrelation analysis. Finally, I will describe the spatial patterns revealed by determining whether key indicators are spatially dependent, and identifying trends of clusters, outliers and hotspots over time."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#methods-used",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#methods-used",
    "title": "Take-home Exercise 2",
    "section": "2. Methods Used",
    "text": "2. Methods Used\n\n2.1 Spatial Autocorrelation\n\n\n\n\n\nThis quote from Tobler (1970) highlights the essence of spatial autocorrelation, emphasising the importance of studying how values of the same variable are interconnected across space. By examining spatial dependence, we can better understand local and global patterns and variations. This law suggests that phenomena that are geographically close to each other are more likely to be similar or have some kind of spatial relationship compared to phenomena that are farther apart.¬†\nIt is also important to note that spatial structure and spatial autocorrelation are inherently interconnected (Tiefelsdorf, 1998):\n\nSpatial structure encompasses all the connections through which the autocorrelated phenomenon spreads.\nWithout a significant autocorrelated process, spatial structure cannot be empirically observed.\n\nThus, the observed spatial distribution is regarded as a reflection of the underlying spatial process. When spatial autocorrelation is present, the value of a variable at a given observation is connected to the values of that same variable at neighbouring observations:\n\n\n\n\n\nConfiguration of areas showing different types of autocorrelation (Nguyen K. et al., 2022)\n\nPositive Spatial Autocorrelation occurs when similar values of the variable cluster geographically.\nNegative Spatial Autocorrelation arises when dissimilar values are geographically close, indicating that nearby locations tend to differ more than those that are farther apart. This situation often reflects spatial competition.\nIn the absence of spatial autocorrelation, the distribution of observations can be considered random.\n\n\n\n2.2 Cluster and Outlier Analysis\nCluster and Outlier Analysis can be effectively applied using Local Moran‚Äôs I, Local Geary‚Äôs C, Moran scatterplots, and LISA Cluster Maps to identify and understand spatial patterns in data. Here‚Äôs how each of these methods can be utilised\n\n2.2.1 Local Moran‚Äôs I and Local Geary‚Äôs C\n\nLocal Moran‚Äôs I: This statistic assesses local spatial autocorrelation by measuring the degree of similarity of a location‚Äôs value to those of its neighbours. It identifies clusters of high or low values (hotspots and cold spots) and outliers (areas where a value is significantly different from its neighbours). By calculating Local Moran‚Äôs I for each location, we can highlight areas with significant spatial dependence, helping to identify regions where interventions may be needed.\nLocal Geary‚Äôs C: Similar to Local Moran‚Äôs I, Local Geary‚Äôs C focuses on differences rather than similarities. It quantifies the spatial variation between nearby locations, emphasising dissimilarity. This method can help detect spatial competition, where nearby areas have contrasting values. By using Local Geary‚Äôs C, we can uncover regions that may experience conflicting trends or behaviours, providing insights into localised dynamics.\n\n\n\n2.22. Moran Scatterplot\n\nThe Moran scatterplot visualises the relationship between the value of a variable at a location and the average value of its neighbours. We can plot scatterplot to represent a location, with the x-axis showing the local mean of neighbouring values and the y-axis showing the local value.¬†\nPlots like this can help us in identifying clusters (high-high or low-low) and outliers (high-low or low-high). The scatterplot can reveal spatial patterns that are not immediately obvious.\n\n\n\n2.2.3. LISA Cluster Map\n\nA LISA Cluster Map visually represents the results of Local Indicators of Spatial Association, indicating the spatial clusters of similar values (hotspots) and outliers.¬†\nI will use these maps to quickly identify regions of interest such that areas identified as hotspots will be marked in red to signify high values surrounded by high values, while cold spots will be marked in blue for low values surrounded by low values. Outliers will be highlighted in contrasting colours.\n\n\n\n\n2.3 Emerging Hot Spot Analysis\nThere are different methods for analysing spatial patterns and detecting hotspots including spatial autocorrelation and cluster analysis. Emerging Hot Spot Analysis (EHSA) is a specific spatio-temporal method used to examine hotspots over a designated observation period. It integrates two well-known techniques: the traditional Getis-Ord Gi* statistic for hotspot detection and the Mann-Kendall test for assessing monotonic trends over time. The main goal of EHSA is to analyse how hot and cold spots change over time, focusing on whether these areas are increasing in intensity, decreasing, or remaining constant."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#importing-packages-into-r",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#importing-packages-into-r",
    "title": "Take-home Exercise 2",
    "section": "3. Importing Packages into R",
    "text": "3. Importing Packages into R\nLet‚Äôs load all the required packages for conducting our analysis.\n\nsf : provides a standardised way to encode spatial vector data in R environment, facilitating spatial data operations and analysis.\nst : create simple features from numeric vectors, matrices, or lists, enabling the representation and manipulation of spatial structures in R.\ntidyverse : a collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structure.\nsfdep : for computing spatial weights, global and local spatial autocorrelation statistics. Offers a more streamlined approach with sf objects.\ntmap : for creating static and interactive visualisations and maps.\nggplot2 : for creating advanced visualisations, graphics and maps using the Grammar of Graphics.\nknitr : for dynamic report generation in R using Literate Programming techniques.\nKendall : for computing the Kendall rank correlation and Mann-Kendall trend test\n\n\npacman::p_load(sf, st, tidyverse, lubridate, sfdep, tmap, ggplot2, knitr, Kendall)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#importing-datasets-into-r",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#importing-datasets-into-r",
    "title": "Take-home Exercise 2",
    "section": "4. Importing Datasets into R",
    "text": "4. Importing Datasets into R\n\n\n\n\n\nWe will be leveraging two datasets in this exercise. The first dataset to be used is Thailand‚Äôs provincial boundary is tha_admbnda_adm1_rtsd_20220121 which exists in ESRI .shp format and is based on the Thailand geographic coordinate system. This dataset is extracted from Thailand - Subnational Administrative Boundaries via the HDX portal.\nThe second dataset thai_drug_offenses_2017_2022 consists of aspatial data in a CSV format that contains reported cases of drug offences in Thailand from 2017 to 2022. The dataset is extracted from Thailand Drug Offenses [2017-2022] in Kaggle.\n\n4.1 Importing Geospatial Data\nIn this section, st_read() of sf package will be used to import tha_admbnda_adm1_rtsd_20220121 dataset into the R environment. The st_transform() function below converts the CRS of the sf object to EPSG:32647 which maps to Thailand‚Äôs UTM zone, particularly for Western/Central parts.\n\nthai_boundary &lt;- st_read(dsn = \"data/geospatial\",layer = \"tha_admbnda_adm1_rtsd_20220121\") %&gt;% st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Take-home_Ex\\Take-home_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n# Inspect data\nglimpse(thai_boundary)\n\nRows: 77\nColumns: 17\n$ Shape_Leng &lt;dbl&gt; 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,‚Ä¶\n$ Shape_Area &lt;dbl&gt; 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,‚Ä¶\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P‚Ä¶\n$ ADM1_TH    &lt;chr&gt; \"‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£\", \"‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£\", \"‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ\", \"‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ\", \"‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‚Ä¶\n$ ADM1_PCODE &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH‚Ä¶\n$ ADM1_REF   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ ADM1ALT1EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ ADM1ALT2EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ ADM1ALT1TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ ADM1ALT2TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ ADM0_EN    &lt;chr&gt; \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\",‚Ä¶\n$ ADM0_TH    &lt;chr&gt; \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\", \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\", \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\", \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\", \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‚Ä¶\n$ ADM0_PCODE &lt;chr&gt; \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\",‚Ä¶\n$ date       &lt;date&gt; 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18‚Ä¶\n$ validOn    &lt;date&gt; 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22‚Ä¶\n$ validTo    &lt;date&gt; -001-11-30, -001-11-30, -001-11-30, -001-11-30, -001-11-30‚Ä¶\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((674339.8 15..., MULTIPOLYGON (‚Ä¶\n\n\nLet‚Äôs verify the coordinate reference systems of the thai_boundary object to ensure the assignment of the correct CRS value.\n\nst_crs(thai_boundary)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96¬∞E and 102¬∞E, northern hemisphere between equator and 84¬∞N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nBefore we delve into further data analysis, it is crucial that we first understand the levels of administration that makes up Thailand today. In particular, Thailand has 4 levels of administration, i.e.¬†level 0 (country), 1 (province), 2 (district), and 3 (sub-district) boundaries. Thailand comprises 76 provinces (known as ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î in Thai or changwat in English), along with one special administrative area, Bangkok, the capital.¬†\n\nThese provinces function as the main local government units and possess legal personhood.¬†\nEach province is subdivided into amphoe (districts), which are further broken down into tambon (sub-districts), representing the next tier of local governance.¬†\nFor this analysis, I will only focus on the province administration level.\n\nWe can visualise the structure of our geospatial object as such.\n\ntmap_mode(\"plot\")\n\n# Plot the provinces of Thailand with labels\ntm_shape(thai_boundary) +\n  tm_borders(col = \"black\", lwd = 0.3, alpha = 0.6) + \n  tm_polygons()+\n  tm_layout(\n    main.title = \"Provinces of Thailand\",\n    main.title.size = 1,\n    main.title.position = \"center\",\n    legend.show = FALSE,\n    frame = FALSE\n  ) +\n  tm_text(\"ADM1_EN\", size = 0.2)\n\n\n\n\n4.2 Importing Aspatial Data\nIn this section, read_csv() of sf package will be used to import the csv file into the R environment. The output is a R dataframe class containing 5 unique columns of drug offences in Thailand from 2017 - 2022.\n\ndrug_cases &lt;- read_csv(\"data/aspatial/thai_drug_offenses_2017_2022.csv\")\n\nRows: 7392 Columns: 5\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): types_of_drug_offenses, province_th, province_en\ndbl (2): fiscal_year, no_cases\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(drug_cases)\n\n# A tibble: 6 √ó 5\n  fiscal_year types_of_drug_offenses no_cases province_th   province_en         \n        &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;               \n1        2017 drug_use_cases            11871 ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£  Bangkok             \n2        2017 drug_use_cases              200 ‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó         Chai Nat            \n3        2017 drug_use_cases              553 ‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ         Nonthaburi          \n4        2017 drug_use_cases              450 ‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ        Pathum Thani        \n5        2017 drug_use_cases              378 ‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤ Phra Nakhon Si Ayut‚Ä¶\n6        2017 drug_use_cases              727 ‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ          Loburi              \n\n\n\n\n\n\n\n\n\n\nColumn Name\nData Type\nDescription\n\n\nfiscal_year\n&lt;dbl&gt;\nThe fiscal year during which the drug offenses were recorded.\n\n\ntypes_of_drug_offenses\n&lt;chr&gt;\nThe specific type or category of drug offence being reported.\n\n\nno_cases\n&lt;dbl&gt;\nThe total number of cases recorded for the specific combination of fiscal year\n\n\nprovince_th\n&lt;chr&gt;\nThe name of the province in Thailand, written in Thai.\n\n\nprovince_en\n&lt;chr&gt;\nThe name of the province in Thailand, written in English.\n\n\n\n\n\n4.3 Performing a Join on Geometry Column\nNext, we will want to aggregate the total number of drug use cases according to each 77 provinces in Thailand. Here, I use left_join() to associate each drug use case to its respective province (i.e.¬†the geometry of the drug case).\n\nlibrary(dplyr)\n\n# Step 1: Count rows before the join\ninitial_row_count &lt;- nrow(drug_cases)\n\n# Step 2: Perform the join and convert to sf\ndrug_cases &lt;- drug_cases %&gt;%\n  left_join(thai_boundary %&gt;% select(geometry, ADM1_EN, ADM1_PCODE), \n            by = c(\"province_en\" = \"ADM1_EN\")) %&gt;%\n  st_as_sf()\n\n# Step 3: Count rows after the join\nfinal_row_count &lt;- nrow(drug_cases)\n\n# Step 4: Check if rows were dropped\nif (initial_row_count != final_row_count) {\n  cat(\"Rows were dropped during the join.\\n\")\n  cat(\"Rows before join:\", initial_row_count, \"\\n\")\n  cat(\"Rows after join:\", final_row_count, \"\\n\")\n} else {\n  cat(\"No rows were dropped during the join.\\n\")\n}\n\nNo rows were dropped during the join.\n\nglimpse(drug_cases)\n\nRows: 7,392\nColumns: 7\n$ fiscal_year            &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017,‚Ä¶\n$ types_of_drug_offenses &lt;chr&gt; \"drug_use_cases\", \"drug_use_cases\", \"drug_use_c‚Ä¶\n$ no_cases               &lt;dbl&gt; 11871, 200, 553, 450, 378, 727, 820, 69, 127, 2‚Ä¶\n$ province_th            &lt;chr&gt; \"‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£\", \"‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó\", \"‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ\", \"‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ\", \"‡∏û‡∏£‚Ä¶\n$ province_en            &lt;chr&gt; \"Bangkok\", \"Chai Nat\", \"Nonthaburi\", \"Pathum Th‚Ä¶\n$ ADM1_PCODE             &lt;chr&gt; \"TH10\", \"TH18\", \"TH12\", \"TH13\", \"TH14\", NA, \"TH‚Ä¶\n$ geometry               &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((674339.8 15..., MU‚Ä¶"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-wrangling",
    "title": "Take-home Exercise 2",
    "section": "5. Data Wrangling",
    "text": "5. Data Wrangling\n\n5.1 Reduce Data Size\nTo reduce the memory load, we can drop the province names in Thai from our aspatial dataset as it is not relevant for this study.\n\ndrug_cases &lt;- subset(drug_cases, select = c(-province_th))\nhead(drug_cases)\n\nSimple feature collection with 6 features and 5 fields (with 1 geometry empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 577383.3 ymin: 1492136 xmax: 710569.3 ymax: 1704842\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 6 √ó 6\n  fiscal_year types_of_drug_offenses no_cases province_en             ADM1_PCODE\n        &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;                   &lt;chr&gt;     \n1        2017 drug_use_cases            11871 Bangkok                 TH10      \n2        2017 drug_use_cases              200 Chai Nat                TH18      \n3        2017 drug_use_cases              553 Nonthaburi              TH12      \n4        2017 drug_use_cases              450 Pathum Thani            TH13      \n5        2017 drug_use_cases              378 Phra Nakhon Si Ayuttha‚Ä¶ TH14      \n6        2017 drug_use_cases              727 Loburi                  &lt;NA&gt;      \n# ‚Ñπ 1 more variable: geometry &lt;MULTIPOLYGON [m]&gt;\n\n\nWe‚Äôll also only retain columns that are most useful from thai_boundary.\n\nthai_boundary &lt;- subset(thai_boundary, select = c(Shape_Leng, Shape_Area, ADM1_EN, ADM1_PCODE, geometry))\nhead(thai_boundary)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 628303 ymin: 1490796 xmax: 712440.5 ymax: 1636901\nProjected CRS: WGS 84 / UTM zone 47N\n  Shape_Leng Shape_Area                  ADM1_EN ADM1_PCODE\n1   2.417227 0.13133873                  Bangkok       TH10\n2   1.695100 0.07926199             Samut Prakan       TH11\n3   1.251111 0.05323766               Nonthaburi       TH12\n4   1.884945 0.12698345             Pathum Thani       TH13\n5   3.041716 0.21393797 Phra Nakhon Si Ayutthaya       TH14\n6   1.739908 0.07920961                Ang Thong       TH15\n                        geometry\n1 MULTIPOLYGON (((674339.8 15...\n2 MULTIPOLYGON (((687139.8 15...\n3 MULTIPOLYGON (((644817.9 15...\n4 MULTIPOLYGON (((704086 1575...\n5 MULTIPOLYGON (((662941.6 16...\n6 MULTIPOLYGON (((643472.8 16...\n\n\n\n\n5.2 Fixing Missing Values\nThere are no troublesome rows in the drug_cases dataframe.\n\nany(is.na(thai_boundary))\n\n[1] FALSE\n\n#thai_boundary[!complete.cases(st_drop_geometry(thai_boundary)), ]\n\n\nany(is.na(drug_cases))\n\n[1] TRUE\n\n\n\n5.2.1 Identify Missing Values\nWe can observe 192 reported offences that do not include the ADM1_PCODE as seen from how some rows contain empty values ‚ÄòNA‚Äô. We are also getting the 192 empty geometry values as returned from the null test below.\n\ndrug_cases %&gt;%\n  filter(is.na(ADM1_PCODE)) %&gt;%\n  select(province_en, ADM1_PCODE)\n\nSimple feature collection with 192 features and 2 fields (with 192 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 192 √ó 3\n   province_en ADM1_PCODE           geometry\n   &lt;chr&gt;       &lt;chr&gt;      &lt;MULTIPOLYGON [m]&gt;\n 1 Loburi      &lt;NA&gt;                    EMPTY\n 2 buogkan     &lt;NA&gt;                    EMPTY\n 3 Loburi      &lt;NA&gt;                    EMPTY\n 4 buogkan     &lt;NA&gt;                    EMPTY\n 5 Loburi      &lt;NA&gt;                    EMPTY\n 6 buogkan     &lt;NA&gt;                    EMPTY\n 7 Loburi      &lt;NA&gt;                    EMPTY\n 8 buogkan     &lt;NA&gt;                    EMPTY\n 9 Loburi      &lt;NA&gt;                    EMPTY\n10 buogkan     &lt;NA&gt;                    EMPTY\n# ‚Ñπ 182 more rows\n\n\nThe root cause of this problem lies in how these two province names were incorrectly spelled in the province_en column of drug_casses , causing an incomplete left_join() to be executed.\n\nunique(drug_cases[!complete.cases(st_drop_geometry(drug_cases)), ][c('province_en')])\n\nSimple feature collection with 2 features and 1 field (with 2 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 2 √ó 2\n  province_en           geometry\n  &lt;chr&gt;       &lt;MULTIPOLYGON [m]&gt;\n1 Loburi                   EMPTY\n2 buogkan                  EMPTY\n\n\n\nthai_boundary %&gt;%\n  filter(ADM1_EN == \"Lop Buri\" | ADM1_EN == \"Bueng Kan\") %&gt;%\n  select(ADM1_PCODE, ADM1_EN) %&gt;%\n  as_tibble()\n\n# A tibble: 2 √ó 3\n  ADM1_PCODE ADM1_EN                                                    geometry\n  &lt;chr&gt;      &lt;chr&gt;                                            &lt;MULTIPOLYGON [m]&gt;\n1 TH16       Lop Buri  (((751293.3 1742960, 751337.4 1742928, 751437.2 1742942,‚Ä¶\n2 TH38       Bueng Kan (((965496 2045531, 965625.5 2045528, 965836.6 2045537, 9‚Ä¶\n\n\n\n\n5.2.2 Fix Missing Values Discovered\nLet‚Äôs fix these NA values by transforming the two province names to their correct names.\n\nStep 1: Replace Loburi ‚Äì&gt; Lop Buri, buogkan ‚Äì&gt; Bueng Kan\nStep 2: Replace ‚ÄòNA‚Äô ADM1_PCDOE with the correct province code - TH16 and TH38 for Lop Buri and Bueng Kan respectively\nStep 3: Replace the existing geometry column in drug_cases with the right geometry\n\n\n# Extract the geometry for Lop Buri\nlop_buri_geometry &lt;- drug_cases %&gt;%\n  filter(province_en == \"Lop Buri\") %&gt;%\n  reframe(first_geometry = st_union(geometry)) %&gt;%  \n  pull(first_geometry)\n\n# Extract the geometry for Bueng Kan\nbueng_kan_geometry &lt;- drug_cases %&gt;%\n  filter(province_en == \"Bueng Kan\") %&gt;%\n  reframe(first_geometry = st_union(geometry)) %&gt;%\n  pull(first_geometry)\n\n# Fix incorrect province names\ndrug_cases &lt;- drug_cases %&gt;%\n  mutate(\n    province_en = case_when(\n      province_en == \"Loburi\" ~ \"Lop Buri\",\n      province_en == \"buogkan\" ~ \"Bueng Kan\",\n      TRUE ~ province_en  \n    )\n  )\n\n# Fix empty province code\ndrug_cases &lt;- drug_cases %&gt;%\n  mutate(ADM1_PCODE = ifelse(province_en == \"Lop Buri\" & \n                             is.na(ADM1_PCODE),\"TH16\", \n                      ifelse(province_en == \"Bueng Kan\" & \n                             is.na(ADM1_PCODE),\"TH38\", ADM1_PCODE)))\n\n# Fix empty geometry\nthai_boundary_no_geom &lt;- thai_boundary %&gt;%\n  select(ADM1_EN, geometry)\ndrug_cases &lt;- drug_cases %&gt;%\n  st_drop_geometry() %&gt;%\n  left_join(thai_boundary_no_geom, by = c(\"province_en\" = \"ADM1_EN\")) \ndrug_cases &lt;- st_as_sf(drug_cases)\n\nWe have successfully removed all NA values found in drug_cases.\n\nany(is.na(drug_cases))\n\n[1] FALSE\n\n\n\n# Check the updated drug_cases\ndrug_cases[!complete.cases(st_drop_geometry(drug_cases)), ]\n\nSimple feature collection with 0 features and 5 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 0 √ó 6\n# ‚Ñπ 6 variables: fiscal_year &lt;dbl&gt;, types_of_drug_offenses &lt;chr&gt;,\n#   no_cases &lt;dbl&gt;, province_en &lt;chr&gt;, ADM1_PCODE &lt;chr&gt;,\n#   geometry &lt;GEOMETRY [m]&gt;\n\n\n\n\n\n5.3 Create New total_cases Column\nNext, I create a new dataframe drug_cases_province to count the total number of cases per province.\n\ndrug_cases_province &lt;- drug_cases %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases), \n    ADM1_PCODE = first(ADM1_PCODE),              \n    geometry = first(geometry), \n    .groups = \"drop\"                           \n  ) %&gt;%\n  st_as_sf()\n\nprint(drug_cases_province)\n\nSimple feature collection with 77 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 325178.8 ymin: 620860.6 xmax: 1213656 ymax: 2263241\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 77 √ó 4\n   province_en   total_cases ADM1_PCODE                                 geometry\n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;                            &lt;MULTIPOLYGON [m]&gt;\n 1 Amnat Charoen       35435 TH37       (((1137720 1809629, 1137724 1809622, 11‚Ä¶\n 2 Ang Thong           16168 TH15       (((643472.8 1636469, 643496 1636423, 64‚Ä¶\n 3 Bangkok            286480 TH10       (((674339.8 1543300, 674382.3 1543278, ‚Ä¶\n 4 Bueng Kan           35287 TH38       (((965496 2045531, 965625.5 2045528, 96‚Ä¶\n 5 Buri Ram            57352 TH31       (((921217 1750212, 921217 1750211, 9212‚Ä¶\n 6 Chachoengsao        53514 TH24       (((722656.1 1546054, 722796 1546041, 72‚Ä¶\n 7 Chai Nat            15310 TH18       (((620165.4 1704256, 620291.4 1704247, ‚Ä¶\n 8 Chaiyaphum          64497 TH36       (((772997.4 1851276, 773104.5 1851216, ‚Ä¶\n 9 Chanthaburi         31473 TH22       (((853764.8 1360716, 853783.1 1360713, ‚Ä¶\n10 Chiang Mai         121812 TH50       (((554883.3 2226795, 555000.6 2226791, ‚Ä¶\n# ‚Ñπ 67 more rows\n\n\nI will also create a new drug_cases_province_year to aggregate the total number of cases for each province and based on each year, regardless of drug offense type.\n\ndrug_cases_province_year &lt;- drug_cases %&gt;%\n  group_by(province_en, fiscal_year) %&gt;%\n  summarise(\n    fiscal_year = first(fiscal_year),\n    total_cases = sum(no_cases), \n    ADM1_PCODE = first(ADM1_PCODE),              \n    geometry = first(geometry), \n    .groups = \"drop\"                           \n  ) %&gt;%\n  st_as_sf()\n\nprint(drug_cases_province_year)\n\nSimple feature collection with 462 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 325178.8 ymin: 620860.6 xmax: 1213656 ymax: 2263241\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 462 √ó 5\n   province_en   fiscal_year total_cases ADM1_PCODE                     geometry\n   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                &lt;MULTIPOLYGON [m]&gt;\n 1 Amnat Charoen        2017        5076 TH37       (((1137720 1809629, 1137724‚Ä¶\n 2 Amnat Charoen        2018        5651 TH37       (((1137720 1809629, 1137724‚Ä¶\n 3 Amnat Charoen        2019        7339 TH37       (((1137720 1809629, 1137724‚Ä¶\n 4 Amnat Charoen        2020        3949 TH37       (((1137720 1809629, 1137724‚Ä¶\n 5 Amnat Charoen        2021        8961 TH37       (((1137720 1809629, 1137724‚Ä¶\n 6 Amnat Charoen        2022        4459 TH37       (((1137720 1809629, 1137724‚Ä¶\n 7 Ang Thong            2017        1614 TH15       (((643472.8 1636469, 643496‚Ä¶\n 8 Ang Thong            2018        2717 TH15       (((643472.8 1636469, 643496‚Ä¶\n 9 Ang Thong            2019        2781 TH15       (((643472.8 1636469, 643496‚Ä¶\n10 Ang Thong            2020        2636 TH15       (((643472.8 1636469, 643496‚Ä¶\n# ‚Ñπ 452 more rows"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#deriving-queens-contiguity-weights-sfdep-methods",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#deriving-queens-contiguity-weights-sfdep-methods",
    "title": "In-class Exercise 6",
    "section": "2.1 Deriving Queen‚Äôs contiguity weights: sfdep methods",
    "text": "2.1 Deriving Queen‚Äôs contiguity weights: sfdep methods\nNotice that st_weights() provides tree arguments,\n\nnb: a neighbour list object as created by st_neighbors().\nstyle: Default ‚ÄúW‚Äù for row standardized weights. This value can also be ‚ÄúB‚Äù, ‚ÄúC‚Äù, ‚ÄúU‚Äù, ‚Äúminmax‚Äù, and ‚ÄúS‚Äù. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al.¬†1999, p.¬†167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors. 8\n\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\n# Inspect the dataframe\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#computing-global-morans-i",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#computing-global-morans-i",
    "title": "In-class Exercise 6",
    "section": "2.2 Computing Global Moran‚Äôs I",
    "text": "2.2 Computing Global Moran‚Äôs I\nIn the code chunk below,¬†global_moran()¬†function is used to compute the Moran‚Äôs I value. Different from spdep package, the output is a tibble data.frame.\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#performing-global-moransi-test",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#performing-global-moransi-test",
    "title": "In-class Exercise 6",
    "section": "2.3 Performing Global Moran‚ÄôsI test",
    "text": "2.3 Performing Global Moran‚ÄôsI test\nIn general, Moran‚Äôs I test will be performed instead of just computing the Moran‚Äôs I statistics. With sfdep package, Moran‚Äôs I test can be performed by using global_moran_test() as shown in the code chunk below.\n\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe default for alternative argument is ‚Äútwo.sided‚Äù. Other supported arguments are ‚Äúgreater‚Äù or ‚Äúless‚Äù. randomization, and\nBy default the randomization argument is TRUE. If FALSE, under the assumption of normality."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#performing-global-morans-i-permutation-test",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#performing-global-morans-i-permutation-test",
    "title": "In-class Exercise 6",
    "section": "2.4 Performing Global Moran‚Äôs I Permutation Test",
    "text": "2.4 Performing Global Moran‚Äôs I Permutation Test\nIn practice, Monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by globel_ moran_perm().\nStep 1:\nIt‚Äôs always good practice to to use set.seed() before performing simulation. This is to ensure that the computation is reproducible.\n\nset.seed(1234)\n\nStep 2\nNext, global_moral_perm() is used to perform Monte Carlo simulation.\n\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nStep 3: Analyse!\n\n\n\n\n\n\nNote\n\n\n\nThe statistical report on previous tab shows that the p-value is smaller than alpha value of 0.05. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of GPD per capita are resemble random distribution (i.e.¬†independent from spatial).\nSince the Moran‚Äôs I statistics is greater than 0, we can infer that the spatial distribution shows sign of clustering. i.e. it indicates positive spatial autocorrelation, which means that similar values (either high or low) tend to cluster together in space."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#computing-local-morans-i",
    "title": "In-class Exercise 6",
    "section": "3.1 Computing local Moran‚Äôs I",
    "text": "3.1 Computing local Moran‚Äôs I\nIn this section, I will learn how to compute Local Moran‚Äôs I of GDPPC at county level by using local_moran() of sfdep package.\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-local-morans-i",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-local-morans-i",
    "title": "In-class Exercise 6",
    "section": "3.2 Visualising local Moran‚Äôs I",
    "text": "3.2 Visualising local Moran‚Äôs I\nIn this code chunk below, tmap functions are used prepare a choropleth map by using value in the ii field.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of GDPPC\",\n    main.title.size = 2)\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-p-value-of-local-morans-i",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-p-value-of-local-morans-i",
    "title": "In-class Exercise 6",
    "section": "3.3 Visualising p-value of local Moran‚Äôs I",
    "text": "3.3 Visualising p-value of local Moran‚Äôs I\nIn the code chunk below, tmap functions are used prepare a choropleth map by using value in the p_ii_sim field.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor p-values, the appropriate classification should be 0.001, 0.01, 0.05 and not significant instead of using default classification scheme.\nSuggested Classification for p-values:\n\np &lt; 0.001: Highly significant clustering\np &lt; 0.01: Very significant clustering\np &lt; 0.05: Significant clustering\nNot significant (p &gt;= 0.05): No significant clustering"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-local-morans-i-and-p-value",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-local-morans-i-and-p-value",
    "title": "In-class Exercise 6",
    "section": "3.4 Visualising local Moran‚Äôs I and p-value",
    "text": "3.4 Visualising local Moran‚Äôs I and p-value\nFor effective comparison, it will be better for us to plot both maps next to each other.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#plotting-a-lisa-map",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#plotting-a-lisa-map",
    "title": "In-class Exercise 6",
    "section": "3.5 Plotting a LISA Map",
    "text": "3.5 Plotting a LISA Map\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are¬†mean,¬†median¬†and¬†pysal. In general, classification in¬†mean¬†will be used as shown in the code chunk below.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#computing-local-gi-statistics",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#computing-local-gi-statistics",
    "title": "In-class Exercise 6",
    "section": "4.1 Computing local Gi* statistics",
    "text": "4.1 Computing local Gi* statistics\nAs usual, we will need to derive a spatial weight matrix before we can compute local Gi* statistics. Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.\n\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wts = st_inverse_distance(nb, \n                              geometry, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\n‚Ñπ In argument: `wts = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nGi* and local Gi* are distance-based spatial statistics. Hence, distance methods instead of contiguity methods should be used to derive the spatial weight matrix.\nSince we are going to compute Gi* statistics, include_self()is used.\n\n\n\nNow, we will compute the local Gi* by using the code chunk below.\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 √ó 19\n    gi_star cluster     e_gi  var_gi std_dev p_value p_sim p_folded_sim skewness\n      &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.261   Low     0.00126  1.07e-7  0.283  7.78e-1  0.66         0.33    0.783\n 2 -0.276   Low     0.000969 4.76e-8 -0.123  9.02e-1  0.98         0.49    0.713\n 3  0.00573 High    0.00156  2.53e-7 -0.0571 9.54e-1  0.78         0.39    0.972\n 4  0.528   High    0.00155  2.97e-7  0.321  7.48e-1  0.56         0.28    0.942\n 5  0.466   High    0.00137  2.76e-7  0.386  7.00e-1  0.52         0.26    1.32 \n 6 -0.445   High    0.000992 7.08e-8 -0.588  5.57e-1  0.68         0.34    0.692\n 7  2.99    High    0.000700 4.05e-8  3.13   1.74e-3  0.04         0.02    0.975\n 8  2.04    High    0.00152  1.58e-7  1.77   7.59e-2  0.16         0.08    1.26 \n 9  4.42    High    0.00130  1.18e-7  4.22   2.39e-5  0.02         0.01    1.20 \n10  1.21    Low     0.00175  1.25e-7  1.49   1.36e-1  0.18         0.09    0.408\n# ‚Ñπ 78 more rows\n# ‚Ñπ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, NAME_2 &lt;chr&gt;,\n#   ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;, ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [¬∞]&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-gi",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-gi",
    "title": "In-class Exercise 6",
    "section": "4.2 Visualising Gi*",
    "text": "4.2 Visualising Gi*\nIn the code chunk below, tmap functions are used to plot the local Gi* (i.e.¬†gi_star) at the province level.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-p-value-of-hcsa",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-p-value-of-hcsa",
    "title": "In-class Exercise 6",
    "section": "4.3 Visualising p-value of HCSA",
    "text": "4.3 Visualising p-value of HCSA\nIn the code chunk below, tmap functions are used to plot the p-values of local Gi* (i.e.¬†p_sim) at the province level\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visuaising-local-hcsa",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visuaising-local-hcsa",
    "title": "In-class Exercise 6",
    "section": "4.4 Visuaising local HCSA",
    "text": "4.4 Visuaising local HCSA\nFor effective comparison, you can plot both maps next to each other as shown below.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-hot-spot-and-cold-spot-areas",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#visualising-hot-spot-and-cold-spot-areas",
    "title": "In-class Exercise 6",
    "section": "4.5 Visualising hot spot and cold spot areas",
    "text": "4.5 Visualising hot spot and cold spot areas\nNow, we are ready to plot the significant (i.e.¬†p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFigure above reveals that there is one hot spot area and two cold spot areas. Interestingly, the hot spot areas coincide with the High-high cluster identifies by using local Moran‚Äôs I method in the earlier sub-section."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#setting-the-scene-drug-abuse-in-thailand",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#setting-the-scene-drug-abuse-in-thailand",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "In 2022, 567,609 drug users in ASEAN were treated, in which Thailand was found to have the highest number of drug users requiring treatment among ASEAN countries, followed by Malaysia, Indonesia, Laos, the Philippines, and Singapore. (Kahanto M., et al, 2022) Drug abuse is a significant social issue in Thailand, with profound health, financial, and societal implications. Positioned near the Golden Triangle‚Äîone of the largest drug production areas in Asia‚ÄîThailand faces ongoing challenges due to its geographical proximity and extensive transportation routes, which facilitate drug trafficking. Within Thailand, drug abuse is particularly prevalent among the youth, with approximately 2.7 million young people involved. Of those aged 15‚Äì19, around 300,000 are in need of drug treatment, and vocational students are disproportionately affected compared to their peers in secondary school.\nThis underscores the importance of drug treatment in addressing the complex problem of substance abuse and reduces the societal costs associated with drug abuse, such as healthcare expenses, lost productivity, and crime. Hence, to better allocate resources and develop targeted interventions, it is crucial to understand where drug abuse is most concentrated and how it spreads geographically. This is where geospatial analysis becomes essential.¬†\nIn this exercise, I will utilise geospatial analysis methods to explore the province-level dynamics of drug abuse in Thailand. This will involve preparing a study area layer as sf polygon features at the province level, including Bangkok, and creating a drug abuse indicators layer within this study area. Using these extracted data layers, I will conduct global spatial autocorrelation analysis using sfdep methods, followed by local spatial autocorrelation analysis. Finally, I will describe the spatial patterns revealed by determining whether key indicators are spatially dependent, and identifying trends of clusters, outliers and hotspots over time."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#exploratory-geospatial-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#exploratory-geospatial-data-analysis",
    "title": "Take-home Exercise 2",
    "section": "6. Exploratory Geospatial Data Analysis",
    "text": "6. Exploratory Geospatial Data Analysis\n\n6.1 Overall Histogram of Drug Cases\nI employed the geom_histogram function of the ggplot package to plot histogram distributions of the spread of the number of drug cases found in Thailand from 2017 to 2022.\nOverall, the number of cases appears to be relatively right-skewed with some outliers indicating a significantly high number of drug cases.\n\nggplot(drug_cases, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Histogram of Total Drug Cases in Thailand (2017-2022)\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n6.2 Histogram of Drug Cases by Year\nBased on each year category, we see that there is a similar pattern of drug cases skewed towards the lower end with an uneven distribution.\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(dplyr)\n\n# Create histogram for each year\ndrug_cases_2017 &lt;- filter(drug_cases, fiscal_year == 2017)\nhist_2017 &lt;- ggplot(drug_cases_2017, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2017\") +\n  theme_minimal(base_size = 9) \n\ndrug_cases_2018 &lt;- filter(drug_cases, fiscal_year == 2018)\nhist_2018 &lt;- ggplot(drug_cases_2018, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2018\") +\n  theme_minimal(base_size = 9)\n\ndrug_cases_2019 &lt;- filter(drug_cases, fiscal_year == 2019)\nhist_2019 &lt;- ggplot(drug_cases_2019, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2019\") +\n  theme_minimal(base_size = 9)\n\ndrug_cases_2020 &lt;- filter(drug_cases, fiscal_year == 2020)\nhist_2020 &lt;- ggplot(drug_cases_2020, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2020\") +\n  theme_minimal(base_size = 9)\n\ndrug_cases_2021 &lt;- filter(drug_cases, fiscal_year == 2021)\nhist_2021 &lt;- ggplot(drug_cases_2021, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2021\") +\n  theme_minimal(base_size = 9)\n\ndrug_cases_2022 &lt;- filter(drug_cases, fiscal_year == 2022)\nhist_2022 &lt;- ggplot(drug_cases_2022, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2022\") +\n  theme_minimal(base_size = 9)\n\n# Arrange all histograms in a grid layout\ngrid.arrange(hist_2017, hist_2018, hist_2019, hist_2020, hist_2021, hist_2022, nrow = 2)\n\n\n\n\n6.3 Overall Plot of Drug Cases\nAs such, an quantile interval will be more suitable in plotting the total number of cases for each province as shown in the tmap output below. Additionally, we can use the default break of 5 to capture a suitable level of granularity across all provinces. For comparison purposes, I will also plot the equal scale as shown.\n\n# First plot using quantile interval\nplot_quantile &lt;- tm_shape(drug_cases_province) +\n  tm_polygons(\"total_cases\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Number of Drug Cases\") +\n  tm_layout(main.title = \"Distribution of Drug Cases in Thailand \\n by Province (Quantile Interval)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", size = 2, position = c(\"right\", \"top\")) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n# Second plot using equal interval\nplot_equal &lt;- tm_shape(drug_cases_province) +\n  tm_polygons(\"total_cases\", \n          style = \"equal\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Number of Drug Cases\") +\n  tm_layout(main.title = \"Distribution of Drug Cases in Thailand \\n by Province (Equal Interval)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", size = 2, position = c(\"right\", \"top\")) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n# Combine both plots side by side\ntmap_arrange(plot_quantile, plot_equal, nrow = 1)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nOverall, we can observe that the highest number of drug cases are found in the north-western, central, eastern and southern provinces of Thailand as seen in the darker regions. Namely..\n\nNorth-west: Chiang Mai, Chiang Rai\nCentral: Chaiyaphum, Khon Kaen and Nakhon-Ratchasima\nEast: Sisaket, Ubon-Ratchathani\nSouth: Surat Thani, Nakhon Si Thammarat, Phuket\n\nIt is worth noting that smaller provinces are likely to be lighter in shade due to their smaller geographic area.\n\n\n\n\n6.4 Plot of Drug Cases by Year\nWhen we further categorise the drug cases by year, we can see that there is an even spread of cases\n\nplot_quantile_year &lt;- tm_shape(drug_cases_province_year) +\n  tm_polygons(\"total_cases\", \n          style = \"quantile\", \n          palette = \"Blues\") +\n  tm_facets(by=\"fiscal_year\", free.coords = FALSE) +\n  tm_layout(main.title = \"Distribution of Drug Cases in Thailand \\n by Province for Each Year (Quantile Interval)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) \n\nplot_quantile_year\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWhen the drug cases are visualised by year, we can observe consistent patterns where high drug abuse cases are consistently located at the Southern parts of Thailand. Central Thailand in 2017 showed an average number of drug cases but this increased in the 2018 and continues to increase in the following years till 2022. In 2022, central Thailand faces a high concentration of drug cases which appears to have spread to its neighbouring provinces, particularly with the greatest spread in 2021.\n\n\nSimilar observations can be found when plotting the drug cases using the knitr package as shown below.\n\n# Create temporal maps\ntemporal_maps &lt;- tm_shape(drug_cases_province_year) +  \n  tm_polygons(\"total_cases\",  \n              palette = \"Blues\",  \n              style = \"quantile\", \n              title = \"Drug Cases\") + \n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_layout(legend.title.size = 1.8,\n            legend.text.size = 1.5) +\n  tm_facets(along = \"fiscal_year\", free.coords = FALSE)\n\n# Generate animation\ntmap_animation(temporal_maps, filename = \"thailand_drugs_temporal.gif\", \n               delay = 150, width = 1000, height = 1200)\n\n\n# Include the generated GIF\nknitr::include_graphics(\"thailand_drugs_temporal.gif\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#global-measures-of-spatial-autocorrelation",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#global-measures-of-spatial-autocorrelation",
    "title": "Take-home Exercise 2",
    "section": "7. Global Measures of Spatial Autocorrelation",
    "text": "7. Global Measures of Spatial Autocorrelation\nThe Second Law of Geography articulated by Waldo Tobler in 1970, states that spatial relationships (or correlations) can vary depending on the context and the specific characteristics of different regions. This concept of spatial non-stationarity can be effectively measured and analysed using spatial autocorrelation statistics.\n\n7.1 Methods Used\nThis section delves into analysing the spatial autocorrelation of our drug cases dataset and assessing how the presence of drug abuse in a province may influence and form clusters around it.\nSpatial relationships are characterised by their multidirectional and multilateral nature, setting them apart from temporal relationships, which are linear and follow a past-present-future sequence. The codifying process of spatial relationships, illustrated in the figure below, enables the transformation of complex geographic space into a structured dataset suitable for computer analysis.\n\n\n\n\n\nThere are multiple approaches to defining spatial neighbours. Two most common methods are:\n\nAdjacency Measures: establish links between spatial units that are directly adjacent to one another. In this context, spatial units are considered neighbours if they share a boundary or a point.\nDistance Measures: selects neighbours based on proximity, where the nearest points to a given spatial unit are identified as neighbours. This approach considers varying distances, enabling a more nuanced understanding of spatial relationships beyond mere adjacency.\n\nRook Criterion: Neighbours are areas that share a common edge via horizontal and vertical connections between spatial units.\nBishop Criterion: Neighbours are areas that share a common corner but not necessarily an edge. This approach is similar to the diagonal movements.\nQueen Criterion: Neighbours are areas that share either an edge or corner, which includes both vertical, horizontal, and diagonal connections.\n\n\n\n\n\n\n\n\n\n7.2 Computing Contiguity Neighbours\nI decided to use the Queen criterion in deriving our neighbour list object since it covers the most neighbours of the three. To do so, we will utilise the st_contiguity() function from the sfdep package to create contiguity weight matrices for the study area. This function generates a list of neighbors based on provinces that share contiguous boundaries. It is worth noting that the function only supports the rook and queen criteria which is suitable for our analysis.\n\nthailand_nb_q &lt;- st_contiguity(drug_cases_province$geometry, queen=TRUE)\nsummary(thailand_nb_q)\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 352 \nPercentage nonzero weights: 5.93692 \nAverage number of links: 4.571429 \n1 region with no links:\n48\n2 disjoint connected subgraphs\nLink number distribution:\n\n 0  1  2  3  4  5  6  7  8  9 \n 1  1  5 17 15 17 10  5  4  2 \n1 least connected region:\n71 with 1 link\n2 most connected regions:\n17 69 with 9 links\n\n\nEach number returned above is an ID assigned to each province from the st_contiguity() function. Let‚Äôs find out the specific provinces highlighted by the summary above!\n\nprovince_ids &lt;- data.frame(\n  region_id = seq_along(drug_cases_province$province_en),\n  province_en = drug_cases_province$province_en,\n  total_cases = drug_cases_province$total_cases\n)\n\nprovince_ids[province_ids$region_id %in% c(17, 48, 69, 71), ]\n\n   region_id province_en total_cases\n17        17   Khon Kaen       93905\n48        48      Phuket       75321\n69        69         Tak       23530\n71        71        Trat       13496\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nI can observe a total of 77 provinces (regions) in the dataset with 352 neighbouring links between the provinces\nThere is roughly 5.94% non-zero neighbour relationships. This indicates only a small proportion of total possible connections have neighbours.\nOn average, we can also observe that each province has 4.57 neighbouring regions.\nRegion 48 (Phuket province) has no neighbouring regions\nRegion 71 (Trat province) is one of the least connected provinces with only 1 neighbouring region\nRegions 17 (Khon Kaen province) and 69 (Tak province) are the two provinces with the most neighbours, each sharing boundaries with 9 other provinces\n\n\n\n\n\n7.3 Computing Row-Standardised Weight Matrix\nNext, I attempt to calculate spatial weights but the isolated region (48) is causing issues as spatial weights calculations require all regions to have neighbors.\nTo resolve the issue caused by the isolated region, I will use the allow_zero = TRUE option when calculating spatial weights, which will assign zero as a lagged value to allow the analysis to proceed despite isolated regions.\n\nthailand_wt &lt;- st_weights(thailand_nb_q, style = \"W\", allow_zero = TRUE)\n\nWe will mutate the newly created neighbour list object¬†thailand_nb_1 and weight matrix¬†thailand_wt¬†into our existing¬†drug_cases_province. This results in a newly created object called wm_1.\n\nwm_q &lt;- drug_cases_province %&gt;%\n  mutate(nb = thailand_nb_q,\n         wt = thailand_wt,\n         .before = 1) \n\n# Inspect\nwm_q\n\nSimple feature collection with 77 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 325178.8 ymin: 620860.6 xmax: 1213656 ymax: 2263241\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 77 √ó 6\n   nb        wt     province_en total_cases ADM1_PCODE                  geometry\n * &lt;nb&gt;      &lt;list&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;             &lt;MULTIPOLYGON [m]&gt;\n 1 &lt;int [3]&gt; &lt;dbl&gt;  Amnat Char‚Ä¶       35435 TH37       (((1137720 1809629, 1137‚Ä¶\n 2 &lt;int [4]&gt; &lt;dbl&gt;  Ang Thong         16168 TH15       (((643472.8 1636469, 643‚Ä¶\n 3 &lt;int [6]&gt; &lt;dbl&gt;  Bangkok          286480 TH10       (((674339.8 1543300, 674‚Ä¶\n 4 &lt;int [3]&gt; &lt;dbl&gt;  Bueng Kan         35287 TH38       (((965496 2045531, 96562‚Ä¶\n 5 &lt;int [5]&gt; &lt;dbl&gt;  Buri Ram          57352 TH31       (((921217 1750212, 92121‚Ä¶\n 6 &lt;int [8]&gt; &lt;dbl&gt;  Chachoengs‚Ä¶       53514 TH24       (((722656.1 1546054, 722‚Ä¶\n 7 &lt;int [4]&gt; &lt;dbl&gt;  Chai Nat          15310 TH18       (((620165.4 1704256, 620‚Ä¶\n 8 &lt;int [4]&gt; &lt;dbl&gt;  Chaiyaphum        64497 TH36       (((772997.4 1851276, 773‚Ä¶\n 9 &lt;int [5]&gt; &lt;dbl&gt;  Chanthaburi       31473 TH22       (((853764.8 1360716, 853‚Ä¶\n10 &lt;int [5]&gt; &lt;dbl&gt;  Chiang Mai       121812 TH50       (((554883.3 2226795, 555‚Ä¶\n# ‚Ñπ 67 more rows\n\n\n\n\n7.4 Visualising Contiguity Weights\nLet‚Äôs also visualise the Queen‚Äôs neighbour map. Here, we retrieve the centroid coordinates of each province by combining the longitude and latitude into a single object via st_centroid(). The outputs of the coordinate looks correct as shown.\n\n# Extract centroid geometries\ncentroids &lt;- st_centroid(drug_cases_province)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nhead(centroids)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 645239 ymin: 1505514 xmax: 1115483 ymax: 2013040\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 6 √ó 4\n  province_en   total_cases ADM1_PCODE           geometry\n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             &lt;POINT [m]&gt;\n1 Amnat Charoen       35435 TH37        (1115483 1765518)\n2 Ang Thong           16168 TH15         (645239 1617118)\n3 Bangkok            286480 TH10       (675514.6 1523087)\n4 Bueng Kan           35287 TH38       (998785.3 2013040)\n5 Buri Ram            57352 TH31       (925999.3 1642136)\n6 Chachoengsao        53514 TH24       (762475.6 1505514)\n\n\nNow, let‚Äôs plot the contiguity weights using the Queen‚Äôs method for all provinces.\n\n# Plot all regions (showing the boundaries)\nplot(drug_cases_province$geometry, border = \"lightgrey\", main = \"Queen's Contiguity Weights (Including All Regions)\")\n\n# Loop through each region and plot neighbors if they exist\nfor (i in seq_along(thailand_nb_q)) {\n  # Check if the region has neighbors\n  if (length(thailand_nb_q[[i]]) &gt; 0) {\n    for (neighbor in thailand_nb_q[[i]]) {\n      current_coords &lt;- st_coordinates(centroids[i, ])\n      neighbor_coords &lt;- st_coordinates(centroids[neighbor, ])\n      \n      # Create a LINESTRING directly from the two centroid coordinates\n      combined_geom &lt;- matrix(c(current_coords[1], current_coords[2],\n                                 neighbor_coords[1], neighbor_coords[2]),\n                              ncol = 2, byrow = TRUE)\n      \n      # Draw lines between the current region and its neighbour\n      lines(combined_geom, col = \"blue\")\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe cam observe that most connections between the centroids of each province is found at the lower central regions of Thailand which shows signs of higher interactivity between provinces here. Surprisingly, there are fewer connecting relationships between provinces in the central parts of Thailand.\n\n\n\n\n7.5 Global Moran‚Äôs I\nMoran‚Äôs I is a inferential static measure of the correlation between a variable and the values of its neighbouring regions to determine statistical significance. It reflects the extent to which individual features deviate from the overall values in the study area, assessing the similarity between each region and its neighbours, and averages these evaluations.\nTo examine spatial autocorrelation, we must test the following hypotheses:\n\nNull Hypothesis (H0): This states that there is either no spatial autocorrelation (H0) or that negative spatial autocorrelation exists (H0).\nAlternative Hypothesis (H1): This suggests that positive spatial autocorrelation is present.\n\nThe values of Moran‚Äôs I typically range from -1 to 1.\n\n-1 is perfect clustering of dissimilar values (perfect dispersion).\n0 is no autocorrelation (perfect randomness.)\n+1 indicates perfect clustering of similar values (the opposite of dispersion).\n\n\n7.5.1 Computing Global Moran‚Äôs I\nI will employ the global_moran()¬†function to compute the Moran‚Äôs I value which outputs a tibble dataframe. The zero.policy has been set to TRUE to allow the function to appropriately handle areas with no neighbours.\n\nmoranI &lt;- global_moran(wm_q$total_cases,\n                       wm_q$nb,\n                       wm_q$wt, \n                       zero.policy = TRUE)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.119\n $ K: num 15.8\n\n\n\n\n\n\n\n\nObservations\n\n\n\nBased on the outputs‚Ä¶\n\nThe Moran‚Äôs I value of 0.119 suggests a slight positive spatial autocorrelation, indicating clustering of similar values (e.g., areas with similar drug case counts).\nThe K value of 15.8 indicates that each region, on average, has about 15.8 neighbours, which is a moderate level of connectivity among the regions being analysed.\n\n\n\n\n\n7.5.2 Global Moran‚Äôs I Test\nThe Global Moran‚Äôs I test, which can be performed using the global_moran_test() function from the sfdep package, is a tool for assessing spatial autocorrelation. The main objective of this test is to ascertain the presence of systemic spatial variations of drug abuse cases. In other words, how the number of drug cases in each province vary according to its surrounding provinces compared to that under spatial randomness.\nFor this analysis, we will specify alternative = \"greater\" in line with our alternative hypothesis.\n\nglobal_moran_test(wm_q$total_cases,\n                       wm_q$nb,\n                       wm_q$wt,\n                       zero.policy = TRUE,\n                       alternative = \"greater\")\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.8848, p-value = 0.02973\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.117752268      -0.013333333       0.004837196 \n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe calculated Moran I statistic returns a positive value of 0.118, suggesting positive spatial autocorrelation and a tendency for similar values to be more clustered together than would be expected by chance. Hence, the spatial distribution of drug cases is not random.\nThe standard deviation (or z-score)of the observed Moran I statistic is 1.8848 s.d. away from the expected value (which is typically 0 under the null hypothesis). The positive s.d. indicates that the observed clustering is stronger than what would be expected under the null hypothesis.\nSince the p-value of 0.02973 &lt; alpha value of 0.05, assuming a 95% confience interval, there is statistically significant evidence to reject the null hypothesis (which posits that there is no spatial autocorrelation) in favour of the alternative hypothesis, confirming that positive spatial autocorrelation is present in the data.\n\n\n\n\n7.5.3 Performing Global Moran‚Äôs I Permutation Test\nFrom a frequentist approach, sampling the p-value once is not sufficient for determining the long-run behaviour of estimators and tests. Hence, let us strengthen our findings by repeated sampling, that is to perform Monte Carlo simulation and then, observe the results.\n\nset.seed(1234)\ngmoran_MC &lt;- global_moran_perm(wm_q$total_cases,\n                  wm_q$nb,\n                  wm_q$wt,\n                  zero.policy = TRUE,\n                  nsim = 999)\ngmoran_MC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.11775, observed rank = 957, p-value = 0.086\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is always good practice to use¬†set.seed()¬†before performing simulation. This is to ensure that the computation is reproducible.\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAfter 1000 simulations, our observed result confirms that the p-value is indeed smaller than the alpha value of 0.05 and the Moran I statistic is &gt; 0. Hence, we can reject the H0 in favour of H1, meaning that the results of global_moran_test() test is stable and statistically significant, and spatial distribution of drug cases resemble clustering distribution patterns.\n\n\nNow, we can analyse the spatial distribution of drug cases in a histogram and its summary statistics.\n\nggplot() + \n  aes(gmoran_MC$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"skyblue\") + \n  geom_vline(aes(xintercept = mean(gmoran_MC$res)), color = 'red') +\n  labs(title = \"Histogram of Simulated Moran's I For Thailand's Drug Cases\",\n       x = \"Simulated Moran's I\",\n       y = \"Occurences\") +\n  theme_minimal()+ \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )\n\n\n\n# Summary Statistics\nsummary(gmoran_MC$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.19043 -0.06200 -0.01834 -0.01352  0.03085  0.32088 \n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can confirm that the observed Moran I statistic (0.11775) is higher than the median (‚àí0.01834) and mean (‚àí0.01352) of the simulated values. This suggests that the observed spatial pattern is more clustered than what would be expected if the values were randomly distributed.\n\n\n\n\n\n7.6 Global Geary‚Äôs C Test\nIn practice, when analysing spatial data, it is often recommended to use both Geary‚Äôs C and Moran‚Äôs I to gain a more comprehensive understanding of spatial patterns. However, Geary‚Äôs C can often fail to detect localised clustering compared to other spatial autocorrelation tests like Moran‚Äôs I.\n\nGeary‚Äôs C is more sensitive to local dissimilarities between individual pairs of neighbouring areas rather than overall clustering, which makes it less effective at identifying larger-scale spatial structures or clusters.\nMoran‚Äôs I is better at identifying global patterns of clustering, where areas with similar values are grouped together in broader spatial clusters.\n\nNonetheless, let‚Äôs explore what Geary‚Äôs C results look like for us.\n\n7.6.1 Computing Global Geary‚Äôs C\nGeary‚Äôs C test is developed by Geary (1954), which examines the intensity of a specific characteristic in spatial objects using a weight matrix. It can be performed using global_c_test() of the sfdep package in R, similar to how we conducted the Global Moran‚Äôs I test for evaluating spatial autocorrelation.\n\nglobal_c(\n  wm_q$total_cases, \n  wm_q$nb, \n  wm_q$wt,\n  allow_zero = TRUE\n)\n\n$C\n[1] 0.9878824\n\n$K\n[1] 15.80246\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe Geary‚Äôs C statistic returned is 0.9879 which, unlike the results returned by Moran‚Äôs I, suggests a weak positive spatial autocorrelation since it is very close to 1, indicating little to no spatial autocorrelation. As mentioned, Geary‚Äôs C tends to focus on local clusters which might miss larger clusters.\n\n\n\n\n7.5.2 Global Geary C‚Äôs Test\nLet us perform the Geary C‚Äôs Test which can be implemented using global_c_test() from the sfdep package.\n\nglobal_c_test(wm_q$total_cases,\n                  wm_q$nb,\n                  wm_q$wt,\n                  allow_zero = TRUE,\n                  alternative = \"greater\")\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = 0.22762, p-value = 0.41\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n       0.97488398        1.00000000        0.01217483 \n\n\n\n\n\n\n\n\nObservations\n\n\n\nAfter performing the Geary C test, we can observe that‚Ä¶\n\nGeary‚Äôs C statistic is close to 1, suggesting little or no spatial autocorrelation.\nThe p-value of 0.41 indicates that the observed pattern is not significantly different from random.\nTherefore, the data does not show strong spatial clustering or dispersal, and the spatial distribution of values is likely random.\n\n\n\n\n\n7.5.3 Performing Global Geary‚Äôs C Permutation Test\n\nggeary_MC &lt;- global_c_perm(wm_q$total_cases,\n                  wm_q$nb,\n                  wm_q$wt,\n                  allow_zero = TRUE,\n                  alternative = \"greater\",\n                  nsim = 999)\nggeary_MC\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.97488, observed rank = 459, p-value = 0.459\nalternative hypothesis: greater\n\n\nWe can see that the distribution of drug cases resulting from the Geary‚Äôs C permutation test has a much higher mean (0.9851) and median (0.9860) than what was outputted by the Global Moran‚Äôs I test. Likewise, the observed Geary‚Äôs C statistics of 0.97488 suggests that the data does not show strong clustering/dispersed, but is random.\n\nggplot() + \n  aes(ggeary_MC$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"skyblue\") + \n  geom_vline(aes(xintercept = mean(ggeary_MC$res)), color = 'red') +\n  labs(title = \"Histogram of Simulated Geary's C For Thailand's Drug Cases\",\n       x = \"Simulated Geary's C\",\n       y = \"Occurences\") +\n  theme_minimal()+ \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )\n\n\n\n# Summary Statistics\nsummary(ggeary_MC$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4489  0.9151  0.9859  0.9882  1.0662  1.3445 \n\n\n\n\n\n\n\n\nObservations\n\n\n\nAs shown, Geary‚Äôs C indeed indicate randomness since it doesn‚Äôt detect the global pattern of spatial clustering, even when such a pattern is present. This local focus of Geary‚Äôs C could make it miss larger clusters and conclude that the spatial distribution is more random than it actually is."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#local-indicators-of-spatial-association-lisa",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#local-indicators-of-spatial-association-lisa",
    "title": "Take-home Exercise 2",
    "section": "8. Local Indicators of Spatial Association (LISA)",
    "text": "8. Local Indicators of Spatial Association (LISA)\nPreviously, we used Global Moran‚Äôs I and Geary‚Äôs C in which we discovered whether spatial clustering exists across the whole Thailand region.\nHowever, I would also like to perform local spatial autocorrelation to identify specific areas of clustering at a local level using LISA methods (Anselin, 1995). If these methods detect significant local clusters, it can help confirm that there are indeed clustered patterns that Geary‚Äôs C might have missed.\n\n8.1 Local Moran‚Äôs Ii\n\n8.1.1 Computing Local Moran‚Äôs Ii\nLocal Moran‚Äôs Ii is an extension of Global Moran‚Äôs I, designed to identify local clusters and spatial outliers within a dataset. Local Moran‚Äôs Ii provides a measure of autocorrelation at individual locations, identifying where significant clustering or outliers exist.\nLet‚Äôs utilise the¬†local_moran()¬†function of sfdep to handle the computations. Once again, we‚Äôll set zero.policy to TRUE to allow the analysis to continue despite the one province with 0 neighbours.\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(total_cases, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Inspect\nlisa\n\nSimple feature collection with 77 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 325178.8 ymin: 620860.6 xmax: 1213656 ymax: 2263241\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 77 √ó 18\n        ii      eii   var_ii   z_ii   p_ii p_ii_sim p_folded_sim skewness\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.125  -0.0343  0.0590   -0.375 0.708      0.48         0.24   -1.38 \n 2  0.318  -0.0464  0.236     0.750 0.453      0.54         0.27   -0.943\n 3  0.136  -0.607   2.84      0.441 0.659      0.68         0.34    0.261\n 4  0.119   0.00744 0.0477    0.510 0.610      0.66         0.33   -1.85 \n 5  0.0356 -0.0157  0.00322   0.904 0.366      0.36         0.18    1.36 \n 6  0.0430 -0.00211 0.000311  2.56  0.0105     0.06         0.03    0.994\n 7  0.604  -0.0901  0.306     1.25  0.210      0.06         0.03   -1.86 \n 8  0.144  -0.00995 0.0329    0.847 0.397      0.26         0.13    1.80 \n 9 -0.106  -0.0109  0.0559   -0.403 0.687      0.52         0.26   -1.17 \n10 -0.534   0.0101  0.680    -0.660 0.509      0.58         0.29    1.02 \n# ‚Ñπ 67 more rows\n# ‚Ñπ 10 more variables: kurtosis &lt;dbl&gt;, mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;,\n#   nb &lt;nb&gt;, wt &lt;list&gt;, province_en &lt;chr&gt;, total_cases &lt;dbl&gt;, ADM1_PCODE &lt;chr&gt;,\n#   geometry &lt;MULTIPOLYGON [m]&gt;\n\n\n\n\n\n\n\n\nHow to read the table output\n\n\n\nThe output from the local_moran() function is an sf data frame that includes the following columns: ii, eii, var_ii, z_ii, p_ii, p_ii_sim, andp_folded_sim.\n\nii: This represents the local Moran statistic.\neii: This denotes the expected value of the local Moran statistic; for localmoran_perm, it corresponds to the means from the permutation samples.\nvar_ii: This indicates the variance of the local Moran statistic; for localmoran_perm, it reflects the standard deviations from the permutation samples.\nz_ii: This is the standard deviation of the local Moran statistic; for localmoran_perm, it is calculated based on the means and standard deviations from the permutation samples.\np_ii: This is the p-value for the local Moran statistic, derived using the pnorm() function; for localmoran_perm, it utilizes standard deviations based on the means and standard deviations from the permutation samples.\np_ii_sim: For localmoran_perm(), this represents the rank of the observed statistic in relation to a uniform distribution for [0, 1] p-values, using the specified alternative hypothesis.\np_folded_sim: This reflects the simulation of ranked p-values within the folded range of [0, 0.5], based on a specific implementation found in the GitHub repository.\n\n\n\n\n\n8.1.2 Visualising Local Moran‚Äôs Ii\nTo ease our analysis, an approach we can take is to plot the local Moran‚Äôs I values across to visualise the observed values across each province. We‚Äôll use a choropleth map from the tmap package to analyse the spatial patterns.\n\ntm_shape(lisa) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",\"green1\",\"orange\",\"red\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Province-Level Spatial Autocorrelation \\nof Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.5,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.4,\n            legend.text.size = 0.4,\n            legend.hist.size = 0.4,\n            frame = TRUE,\n            asp = 1.5) +\n  tm_compass(type = \"8star\", text.size = 0.5, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.4, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe local spatial autocorrelation using Moran‚Äôs I outputs a total of 4 different regions, each with a different range of Moran‚Äôs I value.\n\nProvinces in blue indicate a local Moran‚Äôs I value ranging from -1 to 0 with low-high spatial association or no similarity with its neighbours.\n\nThese province are outliers since they exhibit lower intensity of drug cases compared to its surrounding provinces where drug cases are higher in intensity\n\nProvinces in green, orange and red has a local Moran‚Äôs I value ranging from 0 to 3 with high-high spatial association.\n\nThese provinces tend to exhibit a high incidence of drug cases and are surrounded by other provinces with similarly high values.\nSamut Prakan province (in red) shows the strongest clustering effect i.e.¬†it is strongly associated with its high drug-cases neighbours.\nFollowed by Nakhon Si Thammarat (in orange) which shows the 2nd strongest clustering effect.\n\n\nOverall, the spatial autocorrelation of drug cases is a prevalent and widespread issue across provinces in Thailand. The two provinces, Samut Prakan and Nakhon Si Thammarat, suggests to be high-risk regions for the spread of drug cases.\nHowever, for a complete understanding of these spatial autocorrelation patterns, we are required to evaluate the statistical significance associated with each Local Moran‚Äôs I value.\n\n\n\n\n8.1.3 Visualising Local Moran‚Äôs Ii P-value\nAs mentioned in the section above, we shall not hastily conclude the clustering results observed. Instead, let us also evaluate whether the observed clustering (high-high or low-low) is statistically significant or could have occurred by chance. Hence, we can derive the p-values from Local Moran‚Äôs I by using the p_ii_sim field to determine statistical signficance across provinces.\n\n# Remove lisa record with 0 neighbours\nlisa_clean &lt;- lisa %&gt;% filter(!is.na(p_ii_sim))\n\ntm_shape(lisa_clean) +\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"green3\",\"lightyellow\",\"orange\",\"orange4\",\"red\"),\n          title = \"p-value\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistical Signifance of Spatial Autocorrelation\\n of Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.5,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.4,\n            legend.text.size = 0.4,\n            legend.hist.size = 0.4,\n            frame = TRUE,\n            asp = 1.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.4, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the map above, not every province exhibits statistically significant Local Moran I‚Äôs value (i.e.¬†p-value &lt; 0.05)\n\n\n\n\n8.1.4 Visualising Statistically Significant Local Moran‚Äôs Ii\nWith that said, I would like to switch our focus to provinces that display statistically significant local Moran‚Äôs I values. To execute this, I will attempt to remove all local Moran‚Äôs I values with p-values greater than 0.05. Subsequently, I will use the tmap function to plot the choropleth of statistically significant local spatial autocorrelation on the map of Thailand.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"green3\",\"lightyellow\",\"yellow\",\n                      \"orange2\",\"orangered2\",\"red\"),\n          title = \"Local Moran's I (p-value &lt; 0.05)\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistical Signifance of Spatial Autocorrelation\\n of Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.5,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.4,\n            legend.text.size = 0.4,\n            legend.hist.size = 0.4,\n            frame = TRUE,\n            asp = 1.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.4, alpha = 0.1)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nPreviously, we mentioned that spatial autocorrelation of drug cases is widespread across provinces in Thailand, particularly in the provinces, Samut Prakan and Nakhon Si Thammarat. However, we can see that majority of provinces in Thailand is in fact not statistically significant. As such, we cannot conclude that most of these provinces are high-risk regions for the spread of drug cases.\nHigh-high spatial association\n\nCentral provinces (in yellow), Kamphaeng-Phet and Nakhon Sawan, suggests to have some high-high spatial association and hence, has some levels of clustering, as shown from its low local Moran‚Äôs I values ranging from 0 to 0.5.\nSamut Prakan province (in red) has the strongest association with its neighbours where both itself and its neighbours exhibit high clustering of drug cases.\n\nOther interesting findings\nWe can observe that the provinces in gray are not statistically signifcant enough, especially provinces which displayed negative autocorrelation (i.e.¬†&lt; 0 Moran‚Äôs I) previously. This means that a low-high spatial association is, in reality, not observable in the spread of drug cases in Thailand.\nWe also see that the two provinces found in light yellow regions are located adjacent to each other and both have the lowest local Moran I‚Äôs values (0 - 0.5). This might suggest potential boundary effects where drug cases in certain regions do spill over into other regions. However, a temporal study of these regions will be required.\n\n\n\n\n8.1.5 Visualising Statistically Significant Local Moran‚Äôs Ii (By Drug Case)\n\nunique(lisa$types_of_drug_offenses)\n\nWarning: Unknown or uninitialised column: `types_of_drug_offenses`.\n\n\nNULL\n\n#drug_case_1 &lt;- lisa %&gt;% filter(types_of_drug_offenses == \"Annan District\")\n\n\n\n8.1.6 Visualising Statistically Significant Local Moran‚Äôs Ii (By Fiscal Year)\n\n\n8.1.7 Visualising Statistically Significant Local Moran‚Äôs Ii (By Provinces)\nKamphaeng-Phet and Nakhon Sawan (Moran‚Äôs I: 0 to 0.5)\nSamut Prakan (Moran‚Äôs I Values: 2.5 to 3)\nBangkok\nPhuket"
  },
  {
    "objectID": "In-class_Ex/In-class_ex7/In-class_Ex7.html",
    "href": "In-class_Ex/In-class_ex7/In-class_Ex7.html",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_ex7/In-class_Ex7.html#install-package",
    "href": "In-class_Ex/In-class_ex7/In-class_Ex7.html#install-package",
    "title": "In-class Exercise 7",
    "section": "1. Install Package",
    "text": "1. Install Package\nFirstly, we install the shiny package in our R terminal. This will allow us to work with interactive packages\n\ninstall.packages(\"shiny\")"
  },
  {
    "objectID": "In-class_Ex/In-class_ex7/In-class_Ex7.html#building-a-fluid-page",
    "href": "In-class_Ex/In-class_ex7/In-class_Ex7.html#building-a-fluid-page",
    "title": "In-class Exercise 7",
    "section": "2. Building a Fluid Page",
    "text": "2. Building a Fluid Page\nNext, we will start creating our first Shiny app using the Hunan 2012 dataset used in previous in-class exercises.\nRStudio acts as a server here in which our shinyApp() runs the server logic function defined in server. Additionally, the shinyApp() function takes in the UI defined by the ui variable and outputs a dashboard containing a choropleth along with dropdown and slider features for the user to use.\n\nlibrary(shiny)\npacman::p_load(shiny, sf, tmap, bslib, tidyverse)\n\n# Load spatial data\nhunan &lt;- st_read(dsn = \"data/geospatial\", layer = \"Hunan\")\ndata &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_data &lt;- left_join(hunan, data, by = c(\"County\" = \"County\"))\n\n# Define UI with default width sidebar\nui &lt;- fluidPage(\n  titlePanel(\"Choropleth Mapping\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"variable\", \n                  label = \"Mapping variable\",\n                  choices = list(\"Gross Domestic Product, GDP\" = \"GDP\",\n                                 \"Gross Domestic Product Per Capita\" = \"GDPPC\",\n                                 \"Gross Industry Output\" = \"GIO\",\n                                 \"Output Value of Agriculture\" = \"OVA\",\n                                 \"Output Value of Service\" = \"OVS\"),\n                  selected = \"GDPPC\"),\n      sliderInput(inputId = \"classes\",\n                  label = \"Number of classes\",\n                  min = 5,\n                  max = 10,\n                  value = 6)\n    ),\n    \n    mainPanel(\n      plotOutput(\"mapPlot\")  # Map output placeholder\n    )\n  )\n)\n\n# Define server logic function\nserver &lt;- function(input, output) {\n  \n  output$mapPlot &lt;- renderPlot({\n    # Generate a choropleth map based on the selected input variable\n    tmap_options(check.and.fix = TRUE) +\n      tm_shape(hunan_data) +\n      tm_fill(col = input$variable, \n              n = input$classes, \n              style = \"quantile\") +\n      tm_borders(lwd = 0.1, alpha = 1)\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex7/In-class_Ex7.html#building-a-navigation-bar-page",
    "href": "In-class_Ex/In-class_ex7/In-class_Ex7.html#building-a-navigation-bar-page",
    "title": "In-class Exercise 7",
    "section": "3. Building a Navigation Bar Page",
    "text": "3. Building a Navigation Bar Page\nThis time, we will build a Shiny dashboard which allows us to select our preferred Shiny theme for beautifying the UI. We expose additional UI such as the a slider panel for classification methods, for colour and the level of transparency which allows the user to interact with.\nLet‚Äôs load the required shinydashboard and shinythemes packages first.\n\npacman::p_load(shiny, sf, tmap, bslib, tidyverse,\n               sfdep, shinydashboard, shinythemes)\n\n\ntm_view(set.zoom.limits = c(6.5, 8) allows you to set the limit of which your user can zoom in. This allows us to reduce the computational load and resources of the laptop\nchoices = c(\"Queen\" = TRUE, \"Rook\" = FALSE) we will let the user choose either Queen or Rook as a form of design consideratio but at the back-end, we‚Äôll just consider the TRUE/FALSE boolean\nst_contiguity(geometry, queen = !!input$Contiguity1) here we use double exclamation marks since the computer won;t be able to read the ‚ÄúQueen‚Äù and ‚ÄúRook‚Äù but TRUE/FALSE values.\nWe use plotOutput() for regular maps but since we are using tmap, we shall use tmapOutput()\n\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"Hunan\")\ndata &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_profile &lt;- left_join(hunan, data,\n                        by = c(\"County\" = \"County\"))\n\n#========================#\n###### Shiny UI ######\n#========================#  \n\nui &lt;- navbarPage(\n  title = \"GLSA Application\",\n  fluid = TRUE,\n  theme=shinytheme(\"flatly\"),\n  id = \"navbarID\",\n  tabPanel(\"GeoVisualisation\",\n           sidebarLayout(\n             sidebarPanel(\n               selectInput(inputId = \"variable\",\n                           label = \"Mapping variable\",\n                           choices = list(\"Gross Domestic Product, GDP\" = \"GDP\",\n                                          \"Gross Domestic Product Per Capita\" = \"GDPPC\",\n                                          \"Gross Industry Output\" = \"GIO\",\n                                          \"Output Value of Agriculture\" = \"OVA\",\n                                          \"Output Value of Service\" = \"OVS\"),\n                           selected = \"GDPPC\"),\n               selectInput(inputId = \"classification\",\n                           label = \"Classification method:\",\n                           choices = list(\"sd\" = \"sd\", \n                                          \"equal\" = \"equal\", \n                                          \"pretty\" = \"pretty\", \n                                          \"quantile\" = \"quantile\", \n                                          \"kmeans\" = \"kmeans\", \n                                          \"hclust\" = \"hclust\", \n                                          \"bclust\" = \"bclust\", \n                                          \"fisher\" = \"fisher\", \n                                          \"jenks\" = \"jenks\"),\n                           selected = \"pretty\"),\n               sliderInput(inputId = \"classes\",\n                           label = \"Number of classes\",\n                           min = 5,\n                           max = 10,\n                           value = c(6)),\n               selectInput(inputId = \"colour\",\n                           label = \"Colour scheme:\",\n                           choices = list(\"blues\" = \"Blues\", \n                                          \"reds\" = \"Reds\", \n                                          \"greens\" = \"Greens\",\n                                          \"Yellow-Orange-Red\" = \"YlOrRd\",\n                                          \"Yellow-Orange-Brown\" = \"YlOrBr\",\n                                          \"Yellow-Green\" = \"YlGn\",\n                                          \"Orange-Red\" = \"OrRd\"),\n                           selected = \"YlOrRd\"),\n               sliderInput(inputId = \"opacity\",\n                           label = \"Level of transparency\",\n                           min = 0,\n                           max = 1,\n                           value = c(0.5))\n               ),\n             mainPanel(\n               tmapOutput(\"mapPlot\",\n                          width = \"100%\", \n                          height = 580)\n               )\n             )\n           ),\n  navbarMenu(\"Global Measures\",\n             tabPanel(\"Moran's I\"),\n             tabPanel(\"Geary's c\"),\n             tabPanel(\"Getis-Ord Global G\")\n             ),\n  navbarMenu(\"Local Measures\",\n             tabPanel(\"Local Moran\",\n                      sidebarLayout(\n                        sidebarPanel(\n                          selectInput(inputId = \"variable\",\n                                      label = \"Mapping variable\",\n                                      choices = list(\"Gross Domestic Product, GDP\" = \"GDP\",\n                                                     \"Gross Domestic Product Per Capita\" = \"GDPPC\",\n                                                     \"Gross Industry Output\" = \"GIO\",\n                                                     \"Output Value of Agriculture\" = \"OVA\",\n                                                     \"Output Value of Service\" = \"OVS\"),\n                                      selected = \"GDPPC\"),\n                          radioButtons(inputId = \"Contiguity1\",\n                                       label = \"Contiguity Method\",\n                                       choices = c(\"Queen\" = TRUE, \n                                                   \"Rook\" = FALSE),\n                                       selected = \"TRUE\",\n                                       inline = TRUE),\n                          selectInput(\"MoranWeights\", \"Spatial Weights Style\",\n                                      choices = c(\"W: Row standardised\" = \"W\",\n                                                  \"B: Binary\" = \"B\",\n                                                  \"C: Globally standardised\" = \"C\",\n                                                  \"U: C / no of neighbours\" = \"U\",\n                                                  \"minmax\" = \"minmax\",\n                                                  \"S: Variance\" = \"S\"),\n                                      selected = \"W\"),\n                          sliderInput(inputId = \"MoranSims\", \n                                      label = \"Number of Simulations:\", \n                                      min = 99, max = 499,\n                                      value = 99, step = 100),\n                          actionButton(\"MoranUpdate\", \"Update Plot\"),\n                          hr(),\n                          radioButtons(inputId = \"MoranConf\",\n                                       label = \"Select Confidence level\",\n                                       choices = c(\"0.95\" = 0.05, \n                                                   \"0.99\" = 0.01),\n                                       selected = 0.05,\n                                       inline = TRUE),\n                          selectInput(\"LisaClass\", \"Select Lisa Classification\",\n                                      choices = c(\"mean\" = \"mean\",\n                                                  \"median\" = \"median\",\n                                                  \"pysal\" = \"pysal\"),\n                                      selected = \"mean\"),\n                          selectInput(\"localmoranstats\", \"Select Local Moran's Stat:\",\n                                      choices = c(\"local moran(ii)\" = \"local moran(ii)\",\n                                                  \"expectation(eii)\" = \"expectation(eii)\",\n                                                  \"variance(var_ii)\" = \"variance(var_ii)\",\n                                                  \"std deviation(z_ii)\" = \"std deviation(z_ii)\",\n                                                  \"P-value\" = \"p_value\"),\n                                      selected = \"local moran(ii)\")\n                        ),\n                        mainPanel(\n                          fluidRow(\n                            column(6, tmapOutput(\"LocalMoranMap\")),\n                            column(6, tmapOutput(\"LISA\"))\n                          )\n                        )\n                      )\n                      ),\n             tabPanel(\"Local Gi\")\n             ),\n  navbarMenu(\"Emerging Hot Spot Analysis\")\n)\n\n#========================#\n###### Shiny Server ######\n#========================# \n\nserver &lt;- function(input, output){\n    output$mapPlot &lt;- renderTmap({\n      tmap_options(check.and.fix = TRUE) +\n        tm_shape(hunan_profile)+\n        tm_fill(input$variable,\n                n = input$classes,\n                style = input$classification,\n                palette = input$colour,\n                alpha = input$opacity) +\n        tm_borders(lwd = 0.1,  alpha = 1) +\n        tm_view(set.zoom.limits = c(6.5, 8)\n                )\n    })\n    \n    #==========================================================\n    # Local Measures of Spatial AutoCorrelation\n    #==========================================================   \n    \n    localMIResults &lt;- eventReactive(input$MoranUpdate,{\n      \n      if(nrow(hunan_profile) == 0) return(NULL)  # Exit if no data\n      \n      # Computing Contiguity Spatial Weights\n      wm_q &lt;- hunan_profile %&gt;%\n        mutate(nb = st_contiguity(geometry, \n                                  queen = !!input$Contiguity1),\n               wt = st_weights(nb,\n                               style = input$MoranWeights))\n\n      # Computing Local Moran's I\n\n      lisa &lt;- wm_q %&gt;%\n        mutate(local_moran = local_moran(\n          hunan_profile$GDPPC, nb, wt, \n          nsim = as.numeric(input$MoranSims)),\n          .before = 5) %&gt;%\n        unnest(local_moran)\n\n      lisa &lt;- lisa %&gt;%\n        rename(\"local moran(ii)\" = \"ii\", \"expectation(eii)\" = \"eii\",\n               \"variance(var_ii)\" = \"var_ii\", \"std deviation(z_ii)\" = \"z_ii\",\n               \"p_value\" = \"p_ii\")\n      \n      return(lisa)       \n    })\n    \n    #==========================================================\n    # Render output maps\n    #==========================================================\n    \n    #Render local Moran I statistics\n    output$LocalMoranMap &lt;- renderTmap({\n      df &lt;- localMIResults()\n      \n      if(is.null(df) || nrow(df) == 0) return()  # Exit if no data\n      \n      # Map creation using tmap\n      localMI_map &lt;- tm_shape(df) +\n        tm_fill(col = input$localmoranstats, \n                style = \"pretty\", \n                palette = \"RdBu\", \n                title = input$localmoranstats) +\n        tm_borders() +\n        tm_view(set.zoom.limits = c(6, 7))\n      \n      localMI_map \n    })\n\n    #Render LISA map \n    output$LISA &lt;- renderTmap({\n      df &lt;- localMIResults()\n      if(is.null(df)) return()\n      \n      \n      lisa_sig &lt;- df  %&gt;%\n        filter(p_value &lt; as.numeric(input$MoranConf))  \n      \n      lisamap &lt;- tm_shape(df) +\n        tm_polygons() +\n        tm_borders() +\n        \n        tm_shape(lisa_sig) +\n        tm_fill(col = input$LisaClass,  \n                palette = \"-RdBu\",  \n                title = (paste(\"Significance:\", input$LisaClass))) +\n        tm_borders(alpha = 0.4) +\n        tm_view(set.zoom.limits = c(6, 7))\n\n      lisamap \n    })\n}\n\nshinyApp (ui=ui, server=server)"
  },
  {
    "objectID": "In-class_Ex/In-class_ex7/In-class_Ex7.html#deploying-shiny-app-to-the-cloud",
    "href": "In-class_Ex/In-class_ex7/In-class_Ex7.html#deploying-shiny-app-to-the-cloud",
    "title": "In-class Exercise 7",
    "section": "4. Deploying Shiny App to the Cloud",
    "text": "4. Deploying Shiny App to the Cloud\nWe‚Äôll set up our ShinyApp.io account and save the token in our local device.\n\nNOTE: rename your app3.R file to app.R. Else, RStudio will not be able to find the file\n\nLet‚Äôs download the required rsconnect package\n\ninstall.packages(\"rsconnect\")\n\nAfter running the app.R file, click the ‚Äòpublish‚Äô buttona, select ShinyApp.io and enter in the token. You can monitor website metrics on ShinyApp.io\n\nHere is my link to the deployed app: https://samanthafoo.shinyapps.io/In-class_ex7/"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#emerging-hot-spot-analysis-ehsa",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#emerging-hot-spot-analysis-ehsa",
    "title": "Take-home Exercise 2",
    "section": "9. Emerging Hot Spot Analysis (EHSA)",
    "text": "9. Emerging Hot Spot Analysis (EHSA)\nAs mentioned in the Methods Used section, Emerging Hot Spot Analysis is a useful technique to identify trends and changes in spatial patterns over time, particularly for identifying evolving intensifying or diminishing clusters (hot spots or cold spots) over time.\nThe main idea of ESHA is that it integrates the Getis-Ord Gi* statistic to identify hotspots with the Mann-Kendall test to assess trends over time. In our study, we will begin by calculating the Getis-Ord Gi* statistic, followed by the Mann-Kendall trend test. Finally, we will perform the EHSA to complete the analysis.\n\n9.1 Local Getis-Ord Gi* for Hot Spot and Cold Spot Area Analysis (HSCA)\n\n9.1.1 A Brief Introduction\nThe concept of hot spot analysis originated in the field of spatial statistics during the late 20th century, with contributions from statisticians such as Arthur Getis and J. Keith Ord, who developed the Getis-Ord G and G* statistics in the early 1990s.\nThe Gi* statistic is calculated as the ratio of the weighted average of values from neighboring locations to the total sum of all values, excluding the value at the specific location in question. In contrast, the G statistic includes the value from the focal (or self) location within the neighborhood. Both G and Gi* statistics employ a distance-based approach for spatial weights, meaning that the spatial weights identify statistically significant hot and cold spots by evaluating nearby locations within a defined distance.\n\n\n9.1.2 Compute Spatial Weight Matrix\nWith that said, I will employ the the local Gi* statistics but before we compute it, we need to calculate the spatial weight matrix. To do so, I will employ st_contiguity() to create a neighbour list and use¬†include_self()¬†to include the focal observation in the neighbour list. Next, we use the neighbour list to create a weight list using¬†st_inverse_distance()¬†function.\n1) For Drug Cases by Province\n\nwm_idw_province &lt;- drug_cases_province %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n2) For Drug Cases by Province and Year\n\nwm_idw_year &lt;- drug_cases_province_year %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n3) For Drug Cases by Province and Case Type\n\n\nCode\ncompute_spatial_weight &lt;- function(filtered_data) {\n  spatial_weight &lt;- filtered_data %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n  \n  return(spatial_weight)\n}\n\n# Filter by offense type and then group by province_en\ndrug_use_cases_gi &lt;- drug_cases %&gt;%\n  filter(types_of_drug_offenses %in% c(\"drug_use_cases\", \"suspects_in_drug_use_cases\")) %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases),\n    ADM1_PCODE = first(ADM1_PCODE),\n    geometry = first(geometry),\n    .groups = \"drop\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  compute_spatial_weight()\n\npossession_cases_gi &lt;- drug_cases %&gt;%\n  filter(types_of_drug_offenses %in% c(\"possession_cases\", \"suspects_in_possession_cases\", \n                                       \"possession_with_intent_to_distribute_cases\", \n                                       \"suspects_in_possession_with_intent_to_distribute_cases\")) %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases),\n    ADM1_PCODE = first(ADM1_PCODE),\n    geometry = first(geometry),\n    .groups = \"drop\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  compute_spatial_weight()\n\ntrafficking_cases_gi &lt;- drug_cases %&gt;%\n  filter(types_of_drug_offenses %in% c(\"trafficking_cases\", \"suspects_in_trafficking_cases\")) %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases),\n    ADM1_PCODE = first(ADM1_PCODE),\n    geometry = first(geometry),\n    .groups = \"drop\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  compute_spatial_weight()\n\nimport_export_cases_gi &lt;- drug_cases %&gt;%\n  filter(types_of_drug_offenses %in% c(\"import_cases\", \"suspects_in_import_cases\", \n                                       \"export_cases\", \"suspects_in_export_cases\")) %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases),\n    ADM1_PCODE = first(ADM1_PCODE),\n    geometry = first(geometry),\n    .groups = \"drop\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  compute_spatial_weight()\n\nconspiracy_cases_gi &lt;- drug_cases %&gt;%\n  filter(types_of_drug_offenses %in% c(\"conspiracy_cases\", \"suspects_in_conspiracy_cases\")) %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases),\n    ADM1_PCODE = first(ADM1_PCODE),\n    geometry = first(geometry),\n    .groups = \"drop\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  compute_spatial_weight()\n\n\n\n\n9.1.3 Compute Local Gi* Statistics\nNext, we will compute the local Gi* statistic using the local_gstart_perm() function from the sfdep package. This function takes a neighbor list (nb) and a weight list (wt) as inputs and generates Gi* statistics through a Monte Carlo permutation with a specified number of simulations (nsim). The results will be saved into a new object named HCSA__province and HCSA_year.\n1) For Drug Cases by Province\n\nHCSA_province &lt;- wm_idw_province %&gt;% \n  mutate(local_Gi_star = local_gstar_perm(\n    total_cases, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi_star)\n\n2) For Drug Cases by Province and Year\n\nHCSA_year &lt;- wm_idw_year %&gt;% \n  mutate(local_Gi_star = local_gstar_perm(\n    total_cases, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi_star)\n\n3) For Drug Cases by Province and Case Type\n\ncompute_gi_star &lt;- function(filtered_data) {\n  gi_star_results &lt;- filtered_data %&gt;% \n                    mutate(local_Gi_star = local_gstar_perm(\n                      total_cases, nb, wt, nsim = 99),\n                           .before = 1) %&gt;%\n                      unnest(local_Gi_star) %&gt;%\n                      filter(p_sim &lt; 0.05) %&gt;% \n                      mutate(label = paste(ADM1_PCODE, province_en))\n    \n  return(gi_star_results)\n}\n\n# Filter by offense type and then group by province_en\ndrug_use_cases_gi_sig &lt;- drug_use_cases_gi %&gt;%\n  compute_gi_star()\n\npossession_cases_gi_sig &lt;- possession_cases_gi %&gt;%\n  compute_gi_star()\n\ntrafficking_cases_gi_sig &lt;- trafficking_cases_gi %&gt;%\n  compute_gi_star() \n\nimport_export_cases_gi_sig &lt;- import_export_cases_gi %&gt;%\n  compute_gi_star()\n\nconspiracy_cases_gi_sig &lt;- conspiracy_cases_gi %&gt;%\n  compute_gi_star()\n\n\n\n9.1.4 Create a New label Column\nSimilar to what we did in Moran I‚Äôs computations, we‚Äôll create a new label column to assist us with our visualisations later.\n\nHCSA_province &lt;- HCSA_province %&gt;%\n  mutate(label = paste(ADM1_PCODE, province_en))\n\n\nHCSA_year &lt;- HCSA_year %&gt;%\n  mutate(label = paste(ADM1_PCODE, province_en))\n\n\n\n\n9.2 Visualising Local Getis-Ord Gi* Results\n\n1) Overall Local Gi* Results\n\n\nPlot overall hotspots and coldspots\ntmap_mode(\"plot\")  \ngi_star_plot &lt;- tm_shape(HCSA_province)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, red\n          palette = c(\"#87CEFA\",\"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n          colorNA = \"#FFFFFF\",\n          title = \"Gi*\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Hotspots & Coldspots of Drug Cases\\n in Study Area (Using Gi*)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ncluster_plot &lt;- tm_shape(HCSA_province)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Hotspots & Coldspots of Drug Cases\\n in Study Area (Using Cluster Category)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(gi_star_plot, cluster_plot, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThere is a majority of negative local Gi* values (-2 to 0) found interspersed across the central and western provinces of Thailand (blue) which coincides with low cluster categories, suggesting that these are clusters of provinces that have well-controlled the spread of drug offenses.\nOn the other hand, out of all hotspots found on the right chart (yellow), majority of the positive Gi* values were found to be in the lower range (0 to 2). The implication here is that while these provinces are classified as hotspots, they aren‚Äôt showing statistically significant or intensely concentrated drug activity, especially compared to areas with Gi* values higher than 2. These provinces may still be of concern for law enforcement, but the clustering of drug cases isn‚Äôt as severe as in other regions.\nIn contrast, of all the high cluster categories (blue), there are four provinces in south-central Thailand (orange & red) marked with a higher Gi* value (i.e., &gt; 2) which indicates much stronger clustering of drug offenses, suggesting more serious or widespread drug-related issues in those locations.\n\n\n\n\n2) Statistically Significant Local Gi* Results\n\n# Retrieve statistically significant data\nHCSA_province_sig &lt;- HCSA_province %&gt;%\n  filter(p_sim &lt; 0.05)\n\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\ntm_shape(HCSA_province_sig)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, red\n          palette = c(\"#87CEFA\",\"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area (Using Gi*)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(HCSA_province_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area (Using Cluster Type)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nOf all statistically significant local Gi* observed (left chart), there are four provinces in the West of Thailand with negative Gi* values (blue) which coincides with coldspots areas (blue) on the right chart where low observations of drug offenses are spatially clustered.\nIn contrast, there are two provinces displaying positive Gi* values in south-central (red) and southern (orange) Thailand. The former (south-central) shows a statistically stronger clustering of high drug offenses compared to the latter (southern Thailand), where the clustering of high offenses is less significant.\nHowever, the province in the south of Thailand on the right chart tells a different story. Instead of high values, it shows a statistically significant clustering of low drug offenses. This indicates a coldspot, where drug offenses are less frequent than expected, signaling an area where drug-related activities are significantly lower compared to other regions. This could happen as the cluster type takes a broader spatial context into account.\n\n\n\n\n3) Top 3 Hotspots Observed\nIn the context of Getis-Ord Gi* statistics, hotspots and cold spots are generally identified based on the values of the Gi* statistic calculated for spatial data as such:\n\nHotspot: Gi* &gt; 2 (high concentration of high values)\nCold Spot: Gi* &lt; -2 (high concentration of low values)\n\nHowever, with reference to the histogram below, we can see that the spread of Gi* values range from -2 to 0 and 0 to 10, with only a few values exceeding 2. Hence, I will adjust the definitions accordingly.\n\nHotspot: Gi* &gt; 0 (high concentration of high values)\nCold Spot: Gi* &lt; 0 (high concentration of low values)\n\n\nggplot(HCSA_province, aes(x = gi_star)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue2\") + \n  labs(x = \"Gi*\", y = \"Count\", title = \"Histogram of Local Gi* Statistics\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )\n\n\nHence, let‚Äôs retrieve the three provinces with highest local Gi* values and lowest local Gi* values respectively. These provinces will highlight areas that need further investigation via the¬†Mann-Kendall Trend Test¬†later.\n\nset.seed(123)\n\n\nOverall HotspotsStatistically Significant Hotspots\n\n\nBy identifying all provinces with Gi* &gt; 0, we discovered the top 3 provinces with the greatest clustering of drug cases are found in Amnat Charoen, Bangkok and Buri Ram (red). In other words, these provinces showed to be the greatest hotspot for drug offenses to occur both locally and in its surrounding provinces. Such high concentration of drug crimes suggest a need for interventions of stricter law enforcement to curb the further spread of drug cases to its neighbours.\n\nthree_hotspots_overall &lt;- (head((HCSA_province[HCSA_province$gi_star &gt; 0,]), 3)$label)\nthree_hotspots_overall\n\n[1] \"TH37 Amnat Charoen\" \"TH10 Bangkok\"       \"TH31 Buri Ram\"     \n\n\n\n\nCode\nHCSA_three_hotspots_overall &lt;- HCSA_province %&gt;% filter(label %in% three_hotspots_overall)\n\ntmap_mode(\"plot\")\ntm_shape(thai_boundary) + \n  tm_borders(col = \"black\", lwd = 1) + \n  tm_fill(col = \"#F5F5F5\")+\ntm_shape(HCSA_three_hotspots_overall)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#ff6b6b\")) + # red\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"label\",size = 0.5, col = \"white\", just = c(\"RIGHT\",\"bottom\"),\n          bg.color = \"black\", bg.alpha = 0.7, fontface = \"bold\",\n          remove.overlap = TRUE) +\n  tm_layout(main.title = \"Top 3 Overall Hotspots\\nof Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\n\n\n\n\nWith that said, we can observe that statistically significant hotspots (i.e.¬†p-value &lt; 0.05) resulted in a different set of provinces, namely Nonthaburi, Phatthalung and Samut Prakan. As indicated in the red regions below, Nonthaburi and Samut Prakan provinces are relatively close to each other and are likely part of a larger cluster of drug hotspots with the potential risk of causing a spill-over effect to its neighbours.\n\nthree_hotspots_sig &lt;- (head((HCSA_province_sig[HCSA_province_sig$gi_star &gt; 0,]), 3)$label)\nthree_hotspots_sig\n\n[1] \"TH12 Nonthaburi\"   \"TH11 Samut Prakan\"\n\n\n\n\nCode\nHCSA_three_hotspots_sig &lt;- HCSA_province_sig %&gt;% filter(label %in% three_hotspots_sig)\n\ntmap_mode(\"plot\")\ntm_shape(thai_boundary) + \n  tm_borders(col = \"black\", lwd = 1) + \n  tm_fill(col = \"#F5F5F5\")+\ntm_shape(HCSA_three_hotspots_sig)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#ff6b6b\")) + # red\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"label\",size = 0.5, col = \"black\", just = c(\"left\",\"bottom\"),\n          fontface = \"bold\") +\n  tm_layout(main.title = \"Top 3 Most Statistically Significant Hotspots\\nof Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\n\n\n\n\n\n\n\n4) Top 3 Coldspots Observed\n\nOverall ColdspotsStatistically Significant Coldspots\n\n\nBy filtering all observations where Gi* &lt; 0, we identified that the top three provinces with the strongest signs of clustering of low number of drug cases are found in Ang Thong, Bueng Kan and Chai Nat provinces (blue). The provinces Ang Thong and Chai Nat are relatively close to each other which indicates a region where strong clusters of low number of drug cases are located together.\n\nthree_coldpots_overall &lt;- (head((HCSA_province[HCSA_province$gi_star &lt; 0,]), 3)$label)\nthree_coldpots_overall\n\n[1] \"TH15 Ang Thong\" \"TH38 Bueng Kan\" \"TH18 Chai Nat\" \n\n\n\n\nCode\nHCSA_three_coldpots_overall &lt;- HCSA_province %&gt;% filter(label %in% three_coldpots_overall)\n\ntmap_mode(\"plot\")\ntm_shape(thai_boundary) + \n  tm_borders(col = \"black\", lwd = 1) + \n  tm_fill(col = \"#F5F5F5\")+\ntm_shape(HCSA_three_coldpots_overall)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#87CEFA\")) + # blue\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"label\",size = 0.5, col = \"black\", just = c(\"right\",\"bottom\"),\n          fontface = \"bold\") +\n  tm_layout(main.title = \"Top 3 Overall Coldspots\\nof Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\n\n\n\n\nWe are more interested in the statistically significant coldspots (i.e., p-value &lt; 0.05) as we can more confidently say that these provinces are clusters of low number of drug offenses. In particular, the top three provinces with lowest Gi* values are Chai Nat, Kamphaeng Phet and Nakhon Sawan.\n\nthree_coldpots_sig &lt;- (head((HCSA_province_sig[HCSA_province_sig$gi_star &lt; 0,]), 3)$label)\nthree_coldpots_sig\n\n[1] \"TH18 Chai Nat\"       \"TH62 Kamphaeng Phet\" \"TH60 Nakhon Sawan\"  \n\n\n\n\nCode\nHCSA_three_coldspots_sig &lt;- HCSA_province_sig %&gt;% filter(label %in% three_coldpots_sig)\n\ntmap_mode(\"plot\")\ntm_shape(thai_boundary) + \n  tm_borders(col = \"black\", lwd = 1) + \n  tm_fill(col = \"#F5F5F5\")+\ntm_shape(HCSA_three_coldspots_sig)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#87CEFA\")) + # blue\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"label\",size = 0.5, col = \"black\", just = c(\"center\",\"right\"),\n          fontface = \"bold\",remove.overlap = TRUE) +\n  tm_layout(main.title = \"Top 3 Most Statistically Significant Coldspots\\nof Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\n\n\n\n\n\n\n\n\n9.3 Visualising Local Getis-Ord Gi* by Fiscal Year\n\n1) Overall Local Gi* Results\n\n\nPlot animated yearly hot and cold spots\ntmap_mode(\"plot\")\nHCSA_maps_list &lt;- list()\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(HCSA_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- HCSA_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    HCSA_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  palette = c(\"#87CEFA\",\"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n                  colorNA = \"#FFFFFF\",\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Overall Hotspots & Coldspots of Drug Cases in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    HCSA_maps_list[[year]] &lt;- HCSA_map\n  }\n}\ntemporal_maps &lt;- HCSA_maps_list[!sapply(HCSA_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAcross 2017 to 2022, we see a consistent mapping of local Gi* values for each province, meaning there is no difference in the spatial distribution of drug offense hotspots (yellow, orange & red) and coldspots (blue) in Thailand during these periods.\nSince provinces identified as hotspots for drug offenses have not shifted over time, this could indicate persistent problems in certain provinces (orange & red), where drug-related activities remain high year after year.\nHence, the lack of variation in hotspots and coldspots may suggest that any interventions or law enforcement efforts aimed at reducing drug offenses have not had a substantial impact. Additionally, there could be underlying socio-economic or environmental factors that continue to perpetuate these drug offenses year on year.\n\n\n\n\n2) Statistically Significant Local Gi* Results\n\n\nPlot animated statistically significant yearly hot and cold spots\ntmap_mode(\"plot\")\nHCSA_year_sig &lt;- HCSA_year %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_maps_list &lt;- list()\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(HCSA_year_sig$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- HCSA_year_sig %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    HCSA_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  palette = c(\"#87CEFA\",\"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of Drug Cases in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    HCSA_maps_list[[year]] &lt;- HCSA_map\n  }\n}\ntemporal_maps &lt;- HCSA_maps_list[!sapply(HCSA_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/HCSA_Sig_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe statisitcally significant Gi* results (i.e.¬†p-value &lt; 0.05) show us similar spatial trends where there is low to no shifts in drug hotspots and coldspots in Thailand.\nOverall, the lack of significant change in Gi* values across the years underscores the ongoing challenges in managing and mitigating drug-related issues in Thailand, requiring sustained efforts to tackle these persistent hotspots.\n\n\n\n\n\n9.3 Visualising Local Getis-Ord Gi* by Drug Case Type\nWith that said, it will be more interesting to delve into the five drug case types previously discussed for a more nuanced analysis of drug hotspots and coldspots, measured by local Gi* values, across Thailand‚Äôs provinces. I will also delve into the spatio-temporal changes in patterns observed from 2017 to 2022.\n\n# Prepare local Gi* computations by case type and year\nsummarize_cases &lt;- function(filtered_data, offense_types) {\n  filtered_data %&gt;%\n    filter(types_of_drug_offenses %in% offense_types) %&gt;%\n    group_by(province_en, fiscal_year) %&gt;%\n    summarise(\n      total_cases = sum(no_cases),\n      ADM1_PCODE = first(ADM1_PCODE),\n      geometry = first(geometry),\n      .groups = \"drop\"\n    ) %&gt;%\n    st_as_sf() %&gt;%\n    compute_spatial_weight()\n}\n\n# General function to compute the Gi* statistic\ncompute_gi_star &lt;- function(filtered_data) {\n  filtered_data %&gt;%\n    mutate(local_Gi_star = local_gstar_perm(total_cases, nb, wt, nsim = 99), .before = 1) %&gt;%\n    unnest(local_Gi_star) %&gt;%\n    filter(p_sim &lt; 0.05) %&gt;%\n    mutate(label = paste(ADM1_PCODE, province_en))\n}\n\n# Offense types categories\noffense_types_list &lt;- list(\n  drug_use = c(\"drug_use_cases\", \"suspects_in_drug_use_cases\"),\n  possession = c(\"possession_cases\", \"suspects_in_possession_cases\", \n                 \"possession_with_intent_to_distribute_cases\", \n                 \"suspects_in_possession_with_intent_to_distribute_cases\"),\n  trafficking = c(\"trafficking_cases\", \"suspects_in_trafficking_cases\"),\n  import_export = c(\"import_cases\", \"suspects_in_import_cases\", \n                    \"export_cases\", \"suspects_in_export_cases\"),\n  conspiracy = c(\"conspiracy_cases\", \"suspects_in_conspiracy_cases\")\n)\n\n# Apply the summarize_cases and compute_gi_star functions for each offense type\ngi_star_results_list &lt;- lapply(offense_types_list, function(offense_types) {\n  summarize_cases(drug_cases, offense_types) %&gt;% compute_gi_star()\n})\n\n# Access results for each offense type\ndrug_use_cases_gi_sig_year &lt;- gi_star_results_list$drug_use\npossession_cases_gi_sig_year &lt;- gi_star_results_list$possession\ntrafficking_cases_gi_sig_year &lt;- gi_star_results_list$trafficking\nimport_export_cases_gi_sig_year &lt;- gi_star_results_list$import_export\nconspiracy_cases_gi_sig_year &lt;- gi_star_results_list$conspiracy\n\n\n1) Case Type: Drug Use\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\ntm_shape(drug_use_cases_gi_sig)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, red\n          palette = c(\"#87CEFA\",\"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Gi* (Drug Use)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(drug_use_cases_gi_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Cluster (Drug Use)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\nPlot animated yearly drug use cases\ntmap_mode(\"plot\")\ndrug_use_maps_list &lt;- list()\n\n# Define the case type for the title\ncase_type &lt;- \"Drug Use Cases\"\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(drug_use_cases_gi_sig_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- drug_use_cases_gi_sig_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    drug_use_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\") +\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  palette = c(\"#87CEFA\", \"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of\", case_type, \"in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    drug_use_maps_list[[year]] &lt;- drug_use_map\n  }\n}\n\n# Filter out null maps and create the animation\ntemporal_maps &lt;- drug_use_maps_list[!sapply(drug_use_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/Drug_Use_HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom 2017 to 2022, there are consistently negative significant Gi* values observed in the western provinces and a province found in the extreme south (blue). These provinces have proved to be effective in their control of drug use cases as they persist as regions of coldspots. Similarly, there is a strong presence of statistically significant hotspots that has formed near the boundary of Thailand (orange & red) which is worrying as this shows no improvement in curbing the spread of drug use cases. In other words, these provinces continue to be concentrated with high drug use activities.\n\n\n\n\n2) Case Type: Possession\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n  tm_borders(col = \"black\", lwd = 1) + \n  tm_fill(col = \"#F5F5F5\") +\n\n  tm_shape(possession_cases_gi_sig) +\n  tm_fill(\"gi_star\", \n          palette = c(\"#87CEFA\", \"#4682B4\"),  # Light blue and darker blue\n          breaks = c(-Inf, -2, -1.5, 0),      \n          title = \"Gi*\",\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n\n  tm_borders(col = \"black\", alpha = 0.6) +\n  \n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Gi* (Possession)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.5, alpha = 0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(possession_cases_gi_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Cluster (Possession)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\nPlot animated yearly possession cases\ntmap_mode(\"plot\")\npossession_maps_list &lt;- list()\n\n# Define the case type for the title\ncase_type &lt;- \"Possession Cases\"\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(possession_cases_gi_sig_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- possession_cases_gi_sig_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    possession_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\") +\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  palette = c(\"#87CEFA\", \"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of\", case_type, \"in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    possession_maps_list[[year]] &lt;- possession_map\n  }\n}\n\n# Filter out null maps and create the animation\ntemporal_maps &lt;- possession_maps_list[!sapply(possession_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/Possession_HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWhen it comes to drug possession cases, it follows the general trend where coldspots (i.e.¬†low negative Gi* values) of drug possession offenses persist in the Western provinces of Thailand (blue) while hotspots for drug possession activities are more concentrated in the Southern provinces (orange) with the highest offenses found in a south-central province (red).\nLikewise, the stability of Gi* values across each year indicates that the spatial distribution of drug offenses remains relatively unchanged over time. This consistency may imply that the underlying factors contributing to the clustering of offenses are persistent. Similarly, areas that consistently exhibit negative Gi* values are coldspots, indicating lower spread and incidences of drug offenses.\n\n\n\n\n3) Case Type: Trafficking\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\ntm_shape(trafficking_cases_gi_sig)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, orange-red, red\n          palette = c(\"#87CEFA\",\"#fcd34d\",\"#f7a87d\",\"#f88379\", \"#ff6b6b\"),\n          breaks = c(-2, 0, 1, 2, 3, 4),   \n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Gi* (Trafficking)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(trafficking_cases_gi_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Cluster (Trafficking)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\nPlot animated yearly trafficking cases\ntmap_mode(\"plot\")\ntrafficking_maps_list &lt;- list()\n\n# Define the case type for the title\ncase_type &lt;- \"Trafficking Cases\"\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(trafficking_cases_gi_sig_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- trafficking_cases_gi_sig_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    trafficking_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\") +\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  # dark blue, blue, light yellow,\n                  # yellow, orange, orange-red, red\n                  palette = c(\"skyblue2\",\"#87CEFA\",\"lightyellow\",\"#fcd34d\",\n                              \"#f7a87d\", \"#f88379\", \"#ff6b6b\"),\n                  breaks = c(-5,-2.5,0,2.5,5,7.5,10),  \n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of\", case_type, \"in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    trafficking_maps_list[[year]] &lt;- trafficking_map\n  }\n}\n\n# Filter out null maps and create the animation\ntemporal_maps &lt;- trafficking_maps_list[!sapply(trafficking_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/Trafficking_HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThroughout 2017 to 2022, the western provinces (dark blue) has stayed relatively consistent as a stiatistically significant coldspot, which are great signs of low trafficking of drugs. However, there are two provinces in North-Western and Eastern Thailand (red) that displayed consistently high positive Gi* values across multiple years. These locations may require targeted interventions especially along maritime routes or other transport routes to address the ongoing issue.\n\n\n\n\n4) Case Type: Import and Export\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\ntm_shape(import_export_cases_gi_sig)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, orange-red, red\n          palette = c(\"#87CEFA\",\"#fcd34d\",\"#f7a87d\",\"#f88379\", \"#ff6b6b\"),\n          breaks = c(-2, 0, 2, 4, 6, 8),\n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Gi* (Import & Export)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(import_export_cases_gi_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Cluster (Import & Export)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\nPlot animated yearly import & export cases\ntmap_mode(\"plot\")\nimport_export_maps_list &lt;- list()\n\n# Define the case type for the title\ncase_type &lt;- \"Import & Export Cases\"\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(import_export_cases_gi_sig_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- import_export_cases_gi_sig_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    import_export_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\") +\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  # blue, yellow, orange, red\n                  palette = c(\"#87CEFA\",\"#fcd34d\",\"#f7a87d\", \"#ff6b6b\"),\n                  breaks = c(-4, 0, 4, 8, 12),\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of\", case_type, \"in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    import_export_maps_list[[year]] &lt;- import_export_map\n  }\n}\n\n# Filter out null maps and create the animation\ntemporal_maps &lt;- import_export_maps_list[!sapply(import_export_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/Import_Export_HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nInterestingly, the plots reveal that majority of provinces, particularly in central and western Thailand, indicated consistently low import and export drug activities (blue). However, from 2018 to 2021, there is a spill over of import and exprt drug cases into provinces in the southern part of Thailand, meaning there are increased levels of drug imports and exports into these provinces and its surrounding provinces. Strong road, rail and air transport links can allow for high volumes of legitimate cross-border trade which could easily conceal illegal drug operations within normal trading activities.\nAnother interesting observation is shown in how some coldspots are located adjacent to mild hotspot regions, especially in 2017 and 2022. This contrast could be explained by stricter law enforcement in one province over another, and could also be due to other underlying socio‚Äìeconomic factors.\n\n\n\n\n5) Case Type: Conspiracy\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\ntm_shape(conspiracy_cases_gi_sig)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, red\n          palette = c(\"#87CEFA\",\"#fcd34d\",\"#f7a87d\",\"#f88379\", \"#ff6b6b\"),\n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Gi* (Conspiracy)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(conspiracy_cases_gi_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Cluster (Conspiracy)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\nPlot animated yearly conspiracy cases\ntmap_mode(\"plot\")\nconspiracy_maps_list &lt;- list()\n\n# Define the case type for the title\ncase_type &lt;- \"Conspiracy Cases\"\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(conspiracy_cases_gi_sig_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- conspiracy_cases_gi_sig_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    conspiracy_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\") +\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  # blue, yellow, orange, red\n                  palette = c(\"#87CEFA\",\"#fcd34d\",\"#f7a87d\", \"#ff6b6b\"),\n                  breaks = c(-4, 0, 4, 8, 12),\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of\", case_type, \"in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    conspiracy_maps_list[[year]] &lt;- conspiracy_map\n  }\n}\n\n# Filter out null maps and create the animation\ntemporal_maps &lt;- conspiracy_maps_list[!sapply(conspiracy_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/Conspiracy_HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAcross all years, drug-related conspiracy cases seem to be the least widespread as compared to other drug case types.\nConspiracy laws in Thailand require proof of intent and agreement between two or more parties to commit an illegal act. However, proving such intent in court requires more than just suspicion, which can make conspiracy cases more challenging than straightforward drug offenses, like possession or trafficking.\nHence, it is not surprising that conspiracy cases are more localised to certain provinces, e.g.¬†south-central region (red), and with little to no improvements/worsening of drug cases across each year. These provinces coincides with the high cluster category (yellow) observed above. However, the complexity of uncovering and prosecuting drug conspiracy cases in Thailand may mean drug conspiracy cases can be more under-represented than we expect.\n\n\n\n\n\n9.4 Local Getis-Ord Gi* for Each Fiscal Year\nIn this section, I will focus on preparing data that delves into the yearly drug cases found at the province level which will provide us insights into the spatial and temporal aspects of drug offenses in Thailand‚Äôs provinces. I will employ the drug_cases_spt spacetime object we previously created.\n\ndrugs_nb &lt;- drug_cases_spt %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n    set_nbs(\"nb\") %&gt;%\n    set_wts(\"wt\")\n\n! Polygon provided. Using point on surface.\n\n\n\nhead(drugs_nb)\n\nspacetime ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nContext:`data`\n\n\n77 locations `ADM1_PCODE`\n\n\n6 time periods `fiscal_year`\n\n\n‚îÄ‚îÄ data context ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n# A tibble: 6 √ó 6\n  ADM1_PCODE province_en              fiscal_year total_cases nb        wt    \n  &lt;chr&gt;      &lt;chr&gt;                          &lt;dbl&gt;       &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;\n1 TH10       Bangkok                         2017       60067 &lt;int [7]&gt; &lt;dbl&gt; \n2 TH11       Samut Prakan                    2017       12452 &lt;int [3]&gt; &lt;dbl&gt; \n3 TH12       Nonthaburi                      2017        7348 &lt;int [5]&gt; &lt;dbl&gt; \n4 TH13       Pathum Thani                    2017        7616 &lt;int [7]&gt; &lt;dbl&gt; \n5 TH14       Phra Nakhon Si Ayutthaya        2017        6221 &lt;int [8]&gt; &lt;dbl&gt; \n6 TH15       Ang Thong                       2017        1614 &lt;int [5]&gt; &lt;dbl&gt; \n\n\nNext, I will harness group_by() to group our data by the fiscal_year variable since we are interested in understanding the spatio-temporal dynamics of drug cases across each year from 2017 to 2022.\n\ngi_stars_year &lt;- drugs_nb %&gt;%\n  group_by(fiscal_year) %&gt;%\n  mutate(gi_star = local_gstar_perm(total_cases, nb, wt)) %&gt;%\n  unnest(gi_star)\n\nhead(gi_stars_year)\n\n# A tibble: 6 √ó 16\n# Groups:   fiscal_year [1]\n  ADM1_PCODE province_en     fiscal_year total_cases nb    wt    gi_star cluster\n  &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;lis&gt; &lt;lis&gt;   &lt;dbl&gt; &lt;fct&gt;  \n1 TH10       Bangkok                2017       60067 &lt;int&gt; &lt;dbl&gt;   5.47  High   \n2 TH11       Samut Prakan           2017       12452 &lt;int&gt; &lt;dbl&gt;   4.43  High   \n3 TH12       Nonthaburi             2017        7348 &lt;int&gt; &lt;dbl&gt;   3.00  High   \n4 TH13       Pathum Thani           2017        7616 &lt;int&gt; &lt;dbl&gt;   2.45  High   \n5 TH14       Phra Nakhon Si‚Ä¶        2017        6221 &lt;int&gt; &lt;dbl&gt;  -0.504 Low    \n6 TH15       Ang Thong              2017        1614 &lt;int&gt; &lt;dbl&gt;  -1.10  Low    \n# ‚Ñπ 8 more variables: e_gi &lt;dbl&gt;, var_gi &lt;dbl&gt;, std_dev &lt;dbl&gt;, p_value &lt;dbl&gt;,\n#   p_sim &lt;dbl&gt;, p_folded_sim &lt;dbl&gt;, skewness &lt;dbl&gt;, kurtosis &lt;dbl&gt;\n\n\n\n\n9.5 Mann-Kendall Test for Trends\nThe next phase of our analysis involves applying the Mann-Kendall Trend Test. This test helps determine whether a dataset shows a statistically significant upward or downward trend over time. Being a non-parametric test, it doesn‚Äôt require the data to follow a specific distribution and instead compares the relative magnitudes of data points rather than the actual values (Gilbert, 1987).\nThe Mann-Kendall test produces two main results: Kendall‚Äôs Tau (œÑ) and the Kendall Score (S). Kendall‚Äôs Tau (œÑ) is a correlation measure that assesses the strength of association between two variables, ranging between 0 and 1. It is calculated using the formula œÑ = (C - D) / (C + D), where C is the count of concordant pairs and D is the count of discordant pairs. The hypotheses for Kendall‚Äôs Tau are:\n\nNull Hypothesis: œÑ = 0 (No correlation exists.)\nAlternative Hypothesis: œÑ ‚â† 0 (A correlation exists.)\n\nAs described by Kendall M.G. (1975), the Kendall Score (S) starts at zero, with the assumption of no trend. For each data point that is higher than a previous one, S is incremented, and for each point that is lower, S is decremented. A high positive S indicates an increasing trend, while a strongly negative S suggests a decreasing trend. The p-value is used to determine the statistical significance of the trend.\nFor this study, the MannKendall() function from the Kendall package is used to conduct this test. We will apply the Mann-Kendall test to the three key hotspots and three key coldspots identified in our HCSA analysis.\n\n9.5.1 Trend Test of Top 3 Statistically Significant Hotspots\nPreviously, we identified that Amnat Charoen TH37), Bangkok (TH10) and Buri Ram (TH31) are the top provinces with most significant hotspots.\n\nth37_hotspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Amnat Charoen\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\nth10_hotspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Bangkok\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\nth31_hotspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Buri Ram\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\nNext, we can visualise the trend of Gi* values for the three hotspot provinces identified above, namely using the plotly() function as shown.\n\ntop_hotspots &lt;- ggplot() +\n  geom_line(data = th37_hotspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Amnat Charoen Province\")) +\n  geom_line(data = th10_hotspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Bangkok Special Admin Area\")) +\n  geom_line(data = th31_hotspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Buri Ram Province\")) +\n  labs(x = \"Fiscal Year\", y = \"Gi* Value\",\n       title = \"Gi* of Three Most Significant Hotspots From 2017 to 2022\",\n       color = \"Province\")\n\nplotly::ggplotly(top_hotspots)\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt is interesting to see that two provinces (Bangkok and Buri Ram) share a similar upward moving trend from 2017 to 2022 - though the latter experiences some fluctuations. The Gi* values peaked once in 2019, followed by a dip in 2020 (presumably due to COVID-19) and peaked once again in 2021.\nOn the other hand, the Amnat Charoen province experiences a downward moving trend with the biggest drop in Gi* values from 2020 to 2021. This results is particularly surprising since Amnat Charoen started as the most concentrated hotspot for drug activities in 2017. By the end of 2022, we see that Bangkok has even surpassed it in Gi* values, meaning Amnat Charoen had improved drastically in its drug control efforts!\nIt‚Äôs worth noting that these provinces were identified as significant hotspots due to the cumulative count of drug cases. However, just identifying hotspots can overlook the temporal dynamics and shifts in drug clustering over time. If not for the Mann-Kendall trend test, we would have not have realised these provinces have been stable or somewhat better in their control of drug cases, except for Bangkok.\n\n\nWe will now calculate the Kendall‚Äôs tau and Kendall score¬†S¬†for each province using¬†MannKendall()¬†function from¬†Kendall¬†package.\n\nAmnat Charoen ProvinceBangkok Special Admin AreaBuri Ram Province\n\n\nAnmat Charoen shows a weak upward trend in the data (tau = 0.2, S &gt; 0). This is surprising as the graph earlier indicated a downward moving trend in the number of drug cases.\nRecall that we previously decided the threshold for statistically significant results is Gi* &gt; 0 instead of Gi* &gt; 2, since only two hotspots fall under the p-value &lt; 0.05 (Bangkok and Buri Ram). Hence, it is unsurprising that the sl returned is &gt; 0.05, i.e.¬†the test does not find sufficient evidence to claim a clear upward or downward trend in the data.\n\nth37_hotspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n    tau    sl     S     D  varS\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.200 0.707     3  15.0  28.3\n\n\n\n\nThe results shows a very strong downward trend in Bangkok (tau = -0.999, S &lt; 0), and the p-value confirms that this trend is statistically significant. This suggests a significant decrease in the measured number of drug cases over time.\n\nth10_hotspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n    tau      sl     S     D  varS\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -1.00 0.00853   -15  15.0  28.3\n\n\n\n\nThis province showed a strong upward trend (tau = 0.733, S &gt; 0), but the p-value is slightly above 0.05, meaning this trend is not statistically significant at the 5% level. However, it is close to being significant, suggesting some evidence of an increasing trend.\n\nth31_hotspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n    tau     sl     S     D  varS\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.733 0.0603    11  15.0  28.3\n\n\n\n\n\n\n\n9.5.2 Trend Test of Top 3 Statistically Significant Coldspots\nPreviously, we also identified that Chai Nat (TH18), Kamphaeng Phet (TH62) and Nakhon Sawan (TH60) are the top provinces with most significant coldspots.\n\nth18_coldspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Chai Nat\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\nth62_coldspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Kamphaeng Phet\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\nth60_coldspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Nakhon Sawan\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\n\ntop_hotspots &lt;- ggplot() +\n  geom_line(data = th18_coldspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Chai Nat Province\")) +\n  geom_line(data = th62_coldspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Kamphaeng Phet Province\")) +\n  geom_line(data = th60_coldspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Nakhon Sawan Province\")) +\n  labs(x = \"Fiscal Year\", y = \"Gi* Value\",\n       title = \"Gi* of Three Most Significant Coldspots From 2017 to 2022\",\n       color = \"Province\")\n\nplotly::ggplotly(top_hotspots)\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAcross 2017 to 2022, we see a very similar downward trend in Gi* values for the three provinces we identified to be the most significant coldspots for drug cases. In fact, from 2017 to 2018, Kamphaeng Phet and Nakhon Sawan showed weakening of clusters of low drug cases as observed from the rise in Gi* values. This is followed by a dip in Gi* values over time where these provinces had evolved into stronger coldspots with a much lower concentration of drug cases compared to other provinces.\nThe Chai Nat province shares a similar negative trend in Gi* values, suggesting stronger clustering of low drug cases. However, we see a slight increase in Gi* values from 2021.\n\n\nSimilarly, we can calculate the Kendall‚Äôs tau and Kendall score¬†S¬†for each province\n\nChai Nat ProvinceKamphaeng Phet ProvinceNakhon Sawan Province\n\n\nThere is a very strong downward trend in the clustering of low drug cases as observed from tau = -0.866 and S &gt; 0. These results are also statistically significant as seen from sl &lt; 0.05, meaning that there was sufficient evidence in the test to prove this negative trend.\n\nth18_coldspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n     tau     sl     S     D  varS\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.867 0.0242   -13  15.0  28.3\n\n\n\n\nThe results shows a somewhat weak downward trend in Kamphaeng Phet province as seen from tau = -0.466, meaning it is closer to 0 than -1. The S &lt; 0 results just confirms the negative trends observed. However, since sl returned is greater than 0.05, the trends observed here could be due to random chance.\n\nth62_coldspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n     tau    sl     S     D  varS\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.467 0.260    -7  15.0  28.3\n\n\n\n\nThis time, we see a relatively strong negative associations where the tau value returned is -0.733 and S &lt; 0. Additionally, a sl of 0.06 is returned, that is slightly higher than the threshold for statistical significance. This means that the observations from Nakhon Sawan province could also be due to random chance.\n\nth60_coldspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n     tau     sl     S     D  varS\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.733 0.0603   -11  15.0  28.3\n\n\n\n\n\n\n\n\n9.6 Performing Emerging Hot Spot Analysis\nLastly, we will perform EHSA analysis by using¬†emerging_hotspot_analysis()¬†of the sfdep package. We will utilise the spacetime object¬†drug_cases_spt, and the name of the variable of interest¬†total_cases¬†for the.var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Finally,¬†nsim¬†map numbers of simulation to be performed.\nThe method classifies locations into different categories, such as:\n\n\n\n\n\n\n\nPattern Type\nDefinition\n\n\n\n\nNo Pattern Detected\nDoes not fall into any of the hot or cold spot patterns defined below.\n\n\nSporadic Hot Spot\nA statistically significant hot spot for the final time-step interval with a history of also being an on-again and off-again hot spot. Less than 90 percent of the time-step intervals have been statistically significant hot spots and none of the time-step intervals have been statistically cold spots.\n\n\nSporadic Cold Spot\nA statistically significant cold spot for the final time-step interval with a history of also being an on-again and off-again cold spot. Less than 90 percent of the time-step intervals have been statistically significant cold spots and none of the time-step intervals have been statistically significant hot spots.\n\n\nPersistent Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals with no discernible trend in the intensity of clustering of counts over time.\n\n\nNew Cold Spot\nA location that is a statistically significant cold spot for the final time step and has never been a statistically significant cold spot before.\n\n\nConsecutive Cold Spot\nA location with a single uninterrupted run of at least two statistically significant cold spot bins in the final time-step intervals. The location has never been a statistically significant cold spot prior to the final cold spot run and less than 90 percent of all bins are statistically significant cold spots.\n\n\nConsecutive Hot Spot\nA location with a single uninterrupted run of at least two statistically significant hot spot bins in the final time-step intervals. The location has never been a statistically significant hot spot prior to the final hot spot run and less than 90 percent of all bins are statistically significant hot spots.\n\n\n\nThis classification helps in identifying regions that require immediate attention, those where trends are improving or deteriorating, and areas where consistent patterns persist.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = drug_cases_spt,\n  .var = \"total_cases\",\n  k = 1,\n  nsim = 99\n)\n\nthailand_ehsa &lt;- drug_cases_province_year %&gt;%\n  left_join(ehsa,\n            by = join_by(ADM1_PCODE == location)) %&gt;%\n  mutate(label = paste(ADM1_PCODE,province_en))\n\nWe are now ready to plot the bar charts of each category type.\n\nlibrary(gridExtra)\nthailand_ehsa_sig &lt;- thailand_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\n\nplot1 &lt;- ggplot(data = ehsa,\n                aes(y = classification, fill = classification)) +\n  geom_bar(show.legend = FALSE) +\n  labs(title = \"Overall EHSA Classification\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))  # Reducing size and bolding the title\n\nplot2 &lt;- ggplot(data = thailand_ehsa_sig,\n                aes(y = classification, fill = classification)) +\n  geom_bar(show.legend = FALSE) +\n  labs(title = \"Statistically Significant EHSA Classification\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))  # Reducing size and bolding the title\n\ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWhen analysing the overall classification of EHSA concerning drug cases, it appears that the majority of observed patterns do not distinctly categorise areas as hotspots (regions with a high concentration of drug cases) or cold spots (regions with a low concentration). This may suggest that many regions do not experience pronounced issues related to drug activities, or it could indicate potential limitations in the data or the methods used for analysis.\nAdditionally, we can see that sporadic coldspots and consecutive coldspots constitute the highest EHSA class for both overall and statistically significant respectively. These two results are rather contrasting since one shows fluctuations in low drug case offenses while the other suggests consistently low drug cases observed.\nOn the other hand, among the hotspot classifications, sporadic hotspots and consecutive hotspots account for the highest counts in both the overall and statistically significant datasets, respectively. This indicates that certain regions experience occasional peaks in drug cases, while others maintain a more stable pattern of elevated drug activity. These results underscore the complexity of drug offenses in the studied areas, revealing both variability and consistency in patterns of drug-related incidents.\n\n\nWe can also plot these ESHA classes onto our map of Thailand for visualisation of province-level dynamics of drug cases. This plot revealed that there is a consecutive hotspot found in a Southern province (red) which could be due to underlying socio-economic factors causing such high intensity of drug cluster. Interestingly, there is also a conseutive coldspot (purple) found adjacent to a new coldspot (green) province. The proximity of these coldspots implies that effective interventions in one area could have a positive impact on adjacent regions, potentially leading to a broader reduction in drug-related issues.\n\nlibrary(tmap)\npalette &lt;- c(\n  consecutive_coldspot = \"#C9DAF8\",     # Light Purple\n  consecutive_hotspot = \"#D74B32\",    # Orange-Red\n  new_coldspot = \"#D9EAD3\",           # Light Green\n  no_patterns_detected = \"#FFFFFF\",    # White\n  persistent_coldspot = \"#4F81BD\",    # Medium Blue\n  sporadic_coldspot = \"#A6C8E0\"      # Light Blue\n)\n\n# Plot with your specifications\ntm_shape(thailand_ehsa) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6) +\ntm_shape(thailand_ehsa_sig) +\n  tm_fill(\"classification\", \n          palette = palette,\n          title = \"ESHA Classification\",\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Emerging Statistically Significant Hotspots &\\n Coldspots of Drug Cases in Thailand\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,  # Increased size for better visibility\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.5,  # Increased for better visibility\n            legend.text.size = 0.4,\n            legend.hist.size = 0.4,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.5, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.4, alpha = 0.1)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#conclusion-and-reflections",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#conclusion-and-reflections",
    "title": "Take-home Exercise 2",
    "section": "10. Conclusion and Reflections",
    "text": "10. Conclusion and Reflections\nAs I come to the end of this study, I have successfully uncovered deeper insights into Thailand‚Äôs drug abuse via spatial and spatio-temporal autocorrelation for the period of 2017 to 2022, namely using global Moran‚Äôs I, global Geary‚Äôs C, local Moran‚Äôs I, LISA classifications, local Getis-Ord Gi* and emerging hot spot analysis.\nThis study reveals a compelling need for greater intervention for drug control and law enforcement since a majority of high drug case clusters continue remaining high across the years. We also observed a tendency for low-low and low-high clusters to evolve into high-high drug clusters, meaning that the presence of drug rings / networks in neighbour provinces increases the potential of such drug cases spilling over into provinces that were once less affected by drug offenses. This could be due to strong transport networks and the movement of goods and people through these transport routes.\nInterestingly, there were some provinces that are geographically close but exhibited starkly different patterns in drug case distribution, e.g.¬†hotspots and coldspots found adjacent to each other. Such disparities can be due to various factors, e.g.¬†differences in local governance, law enforcement and socio-economic conditions. Though these are beyond the take-home exercise, I believe that including factors (e.g.¬†GDPPC) could act as potentially interesting future analyses.\nThrough this study, it is evident that the analysis of drug cases reveals a complex landscape where sporadic cold spots and consecutive cold spots highlight fluctuations and consistency in low drug offenses, while sporadic and consecutive hotspots indicate varying intensities of drug-related activity across different regions.\nHowever, there is an interconnectedness of drug dynamics across different provinces, where the presence of coldspot regions can influence low drug cases in its surroundings too. Hence, this highlights the importance of coordinated strategies in addressing drug challenges in a national context and the need for greater targeted interventions with a deeper understanding of underlying factors influencing these patterns.\nWith that said, I would like to extend my gratitude to Prof.¬†Kam Tin Seong for his valuable guidance and mentorship! :) This exercise requires some intensive computational load, depending on the level of granularity of this analysis. Therefore, it is most recommended to begin the take-home exercise early to make room for processing time and debugging. This marks the end of my study and I thank you for taking the time to read till here! üéâüéâüéâ"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#references",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#references",
    "title": "Take-home Exercise 2",
    "section": "11. References",
    "text": "11. References\n\nCRAN. (2024, September 5). Classes and Methods for Spatio-Temporal Data.\nCrawley, M. J. (2007). The R Book. Wiley.\nHandbook of Spatial Analysis. (2018). Insee. https://www.insee.fr/en/information/3635545¬†\nKanato, M., Sarasiri, R., & Leyatikul, P. (2022). ASEAN Drug Monitoring Report.\nNguyen, K.-B., Choi, J., & Yang, J.-S. (2022). Checkerboard Dropout: A Structured Dropout With Checkerboard Pattern for Convolutional Neural Networks.\nParry, J., & Locke, D. H. (n.d.). The Basics of sfdep. Retrieved September 26, 2024, from https://sfdep.josiahparry.com/articles/basics-of-sfdep¬†\nParry, J., & Locke, D. H. (n.d.). Spacetime and spacetime cubes ‚Ä¢ sfdep. sfdep. Retrieved October 6, 2024, from https://sfdep.josiahparry.com/articles/spacetime-s3.html¬†\nPeay, A. (n.d.). Spatial Regression Analysis Advanced Data Analytics. chrismgentry.github.io. https://chrismgentry.github.io/Spatial-Regression/¬†\nvsp.pnnl.gov. (n.d.). Mann-Kendall Test For Monotonic Trend. Design Trend Mann-Kendall. Retrieved October 6, 2024, from https://vsp.pnnl.gov/help/vsample/design_trend_mann_kendall.htm"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html",
    "title": "Take-home Exercise 2 - Part 1",
    "section": "",
    "text": "In 2022, 567,609 drug users in ASEAN were treated, in which Thailand was found to have the highest number of drug users requiring treatment among ASEAN countries, followed by Malaysia, Indonesia, Laos, the Philippines, and Singapore. (Kahanto M., et al, 2022) Drug abuse is a significant social issue in Thailand, with profound health, financial, and societal implications. Positioned near the Golden Triangle‚Äîone of the largest drug production areas in Asia‚ÄîThailand faces ongoing challenges due to its geographical proximity and extensive transportation routes, which facilitate drug trafficking. Within Thailand, drug abuse is particularly prevalent among the youth, with approximately 2.7 million young people involved. Of those aged 15‚Äì19, around 300,000 are in need of drug treatment, and vocational students are disproportionately affected compared to their peers in secondary school.\nThis underscores the importance of drug treatment in addressing the complex problem of substance abuse and reduces the societal costs associated with drug abuse, such as healthcare expenses, lost productivity, and crime. Hence, to better allocate resources and develop targeted interventions, it is crucial to understand where drug abuse is most concentrated and how it spreads geographically. This is where geospatial analysis becomes essential.¬†\nIn this exercise, I will utilise geospatial analysis methods to explore the province-level dynamics of drug abuse in Thailand. This will involve preparing a study area layer as sf polygon features at the province level, including Bangkok, and creating a drug abuse indicators layer within this study area. Using these extracted data layers, I will conduct global spatial autocorrelation analysis using sfdep methods, followed by local spatial autocorrelation analysis. Finally, I will describe the spatial patterns revealed by determining whether key indicators are spatially dependent, and identifying trends of clusters, outliers and hotspots over time.\n\nThis take-home exercise consists of two parts:\n\nTake-home Ex 2 - Part 1\nTake-home Ex 2 - Part 2"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#setting-the-scene-drug-abuse-in-thailand",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#setting-the-scene-drug-abuse-in-thailand",
    "title": "Take-home Exercise 2 - Part 1",
    "section": "",
    "text": "In 2022, 567,609 drug users in ASEAN were treated, in which Thailand was found to have the highest number of drug users requiring treatment among ASEAN countries, followed by Malaysia, Indonesia, Laos, the Philippines, and Singapore. (Kahanto M., et al, 2022) Drug abuse is a significant social issue in Thailand, with profound health, financial, and societal implications. Positioned near the Golden Triangle‚Äîone of the largest drug production areas in Asia‚ÄîThailand faces ongoing challenges due to its geographical proximity and extensive transportation routes, which facilitate drug trafficking. Within Thailand, drug abuse is particularly prevalent among the youth, with approximately 2.7 million young people involved. Of those aged 15‚Äì19, around 300,000 are in need of drug treatment, and vocational students are disproportionately affected compared to their peers in secondary school.\nThis underscores the importance of drug treatment in addressing the complex problem of substance abuse and reduces the societal costs associated with drug abuse, such as healthcare expenses, lost productivity, and crime. Hence, to better allocate resources and develop targeted interventions, it is crucial to understand where drug abuse is most concentrated and how it spreads geographically. This is where geospatial analysis becomes essential.¬†\nIn this exercise, I will utilise geospatial analysis methods to explore the province-level dynamics of drug abuse in Thailand. This will involve preparing a study area layer as sf polygon features at the province level, including Bangkok, and creating a drug abuse indicators layer within this study area. Using these extracted data layers, I will conduct global spatial autocorrelation analysis using sfdep methods, followed by local spatial autocorrelation analysis. Finally, I will describe the spatial patterns revealed by determining whether key indicators are spatially dependent, and identifying trends of clusters, outliers and hotspots over time.\n\nThis take-home exercise consists of two parts:\n\nTake-home Ex 2 - Part 1\nTake-home Ex 2 - Part 2"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#methods-used",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#methods-used",
    "title": "Take-home Exercise 2 - Part 1",
    "section": "2. Methods Used",
    "text": "2. Methods Used\n\n2.1 Spatial Autocorrelation\n\n\n\n\n\nThis quote from Tobler (1970) highlights the essence of spatial autocorrelation, emphasising the importance of studying how values of the same variable are interconnected across space. By examining spatial dependence, we can better understand local and global patterns and variations. This law suggests that phenomena that are geographically close to each other are more likely to be similar or have some kind of spatial relationship compared to phenomena that are farther apart.¬†\nIt is also important to note that spatial structure and spatial autocorrelation are inherently interconnected (Tiefelsdorf, 1998):\n\nSpatial structure encompasses all the connections through which the autocorrelated phenomenon spreads.\nWithout a significant autocorrelated process, spatial structure cannot be empirically observed.\n\nThus, the observed spatial distribution is regarded as a reflection of the underlying spatial process. When spatial autocorrelation is present, the value of a variable at a given observation is connected to the values of that same variable at neighbouring observations:\nConfiguration of areas showing different types of autocorrelation (Nguyen K. et al., 2022)\n\n\n\n\n\n\nPositive Spatial Autocorrelation occurs when similar values of the variable cluster geographically.\nNegative Spatial Autocorrelation arises when dissimilar values are geographically close, indicating that nearby locations tend to differ more than those that are farther apart. This situation often reflects spatial competition.\nIn the absence of spatial autocorrelation, the distribution of observations can be considered random.\n\n\n\n2.2 Cluster and Outlier Analysis\nCluster and Outlier Analysis can be effectively applied using Local Moran‚Äôs I, Local Geary‚Äôs C, Moran scatterplots, and LISA Cluster Maps to identify and understand spatial patterns in data. Here‚Äôs how each of these methods can be utilised\n\n2.2.1 Local Moran‚Äôs I and Local Geary‚Äôs C\n\nLocal Moran‚Äôs I: This statistic assesses local spatial autocorrelation by measuring the degree of similarity of a location‚Äôs value to those of its neighbours. It identifies clusters of high or low values (hotspots and cold spots) and outliers (areas where a value is significantly different from its neighbours). By calculating Local Moran‚Äôs I for each location, we can highlight areas with significant spatial dependence, helping to identify regions where interventions may be needed.\nLocal Geary‚Äôs C: Similar to Local Moran‚Äôs I, Local Geary‚Äôs C focuses on differences rather than similarities. It quantifies the spatial variation between nearby locations, emphasising dissimilarity. This method can help detect spatial competition, where nearby areas have contrasting values. By using Local Geary‚Äôs C, we can uncover regions that may experience conflicting trends or behaviours, providing insights into localised dynamics.\n\n\n\n2.2.2 Moran Scatterplot\n\nThe Moran scatterplot visualises the relationship between the value of a variable at a location and the average value of its neighbours. We can plot scatterplot to represent a location, with the x-axis showing the local mean of neighbouring values and the y-axis showing the local value.¬†\nPlots like this can help us in identifying clusters (high-high or low-low) and outliers (high-low or low-high). The scatterplot can reveal spatial patterns that are not immediately obvious.\n\n\n\n2.2.3 LISA Cluster Map\n\nA LISA Cluster Map visually represents the results of Local Indicators of Spatial Association, indicating the spatial clusters of similar values (hotspots) and outliers.¬†\nI will use these maps to quickly identify regions of interest such that areas identified as hotspots will be marked in red to signify high values surrounded by high values, while cold spots will be marked in blue for low values surrounded by low values. Outliers will be highlighted in contrasting colours.\n\n\n\n\n2.3 Emerging Hot Spot Analysis\nThere are different methods for analysing spatial patterns and detecting hotspots including spatial autocorrelation and cluster analysis. Emerging Hot Spot Analysis (EHSA) is a specific spatio-temporal method used to examine hotspots over a designated observation period. It integrates two well-known techniques: the traditional Getis-Ord Gi* statistic for hotspot detection and the Mann-Kendall test for assessing monotonic trends over time. The main goal of EHSA is to analyse how hot and cold spots change over time, focusing on whether these areas are increasing in intensity, decreasing, or remaining constant."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#importing-packages-into-r",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#importing-packages-into-r",
    "title": "Take-home Exercise 2 - Part 1",
    "section": "3. Importing Packages into R",
    "text": "3. Importing Packages into R\nLet‚Äôs load all the required packages for conducting our analysis.\n\nsf : provides a standardised way to encode spatial vector data in R environment, facilitating spatial data operations and analysis.\nst : create simple features from numeric vectors, matrices, or lists, enabling the representation and manipulation of spatial structures in R.\ntidyverse : a collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structure.\nsfdep : for computing spatial weights, global and local spatial autocorrelation statistics. Offers a more streamlined approach with sf objects.\ntmap : for creating static and interactive visualisations and maps.\nggplot2 : for creating advanced visualisations, graphics and maps using the Grammar of Graphics.\nknitr : for dynamic report generation in R using Literate Programming techniques.\nKendall : for computing the Kendall rank correlation and Mann-Kendall trend test\n\n\npacman::p_load(sf, st, tidyverse, lubridate, sfdep, tmap, ggplot2, knitr, Kendall)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#importing-datasets-into-r",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#importing-datasets-into-r",
    "title": "Take-home Exercise 2 - Part 1",
    "section": "4. Importing Datasets into R",
    "text": "4. Importing Datasets into R\n\n\n\n\n\nWe will be leveraging two datasets in this exercise. The first dataset to be used is Thailand‚Äôs provincial boundary is tha_admbnda_adm1_rtsd_20220121 which exists in ESRI .shp format and is based on the Thailand geographic coordinate system. This dataset is extracted from Thailand - Subnational Administrative Boundaries via the HDX portal.\nThe second dataset thai_drug_offenses_2017_2022 consists of aspatial data in a CSV format that contains reported cases of drug offences in Thailand from 2017 to 2022. The dataset is extracted from Thailand Drug Offenses [2017-2022] in Kaggle.\n\n4.1 Importing Geospatial Data\nIn this section, st_read() of sf package will be used to import tha_admbnda_adm1_rtsd_20220121 dataset into the R environment. The st_transform() function below converts the CRS of the sf object to EPSG:32647 which maps to Thailand‚Äôs UTM zone, particularly for Western/Central parts.\n\nthai_boundary &lt;- st_read(dsn = \"data/geospatial\",layer = \"tha_admbnda_adm1_rtsd_20220121\") %&gt;% st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Take-home_Ex\\Take-home_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n# Inspect data\nglimpse(thai_boundary)\n\nRows: 77\nColumns: 17\n$ Shape_Leng &lt;dbl&gt; 2.417227, 1.695100, 1.251111, 1.884945, 3.041716, 1.739908,‚Ä¶\n$ Shape_Area &lt;dbl&gt; 0.13133873, 0.07926199, 0.05323766, 0.12698345, 0.21393797,‚Ä¶\n$ ADM1_EN    &lt;chr&gt; \"Bangkok\", \"Samut Prakan\", \"Nonthaburi\", \"Pathum Thani\", \"P‚Ä¶\n$ ADM1_TH    &lt;chr&gt; \"‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£\", \"‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£\", \"‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ\", \"‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ\", \"‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‚Ä¶\n$ ADM1_PCODE &lt;chr&gt; \"TH10\", \"TH11\", \"TH12\", \"TH13\", \"TH14\", \"TH15\", \"TH16\", \"TH‚Ä¶\n$ ADM1_REF   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ ADM1ALT1EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ ADM1ALT2EN &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ ADM1ALT1TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ ADM1ALT2TH &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ ADM0_EN    &lt;chr&gt; \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\", \"Thailand\",‚Ä¶\n$ ADM0_TH    &lt;chr&gt; \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\", \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\", \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\", \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢\", \"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‚Ä¶\n$ ADM0_PCODE &lt;chr&gt; \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\", \"TH\",‚Ä¶\n$ date       &lt;date&gt; 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18, 2019-02-18‚Ä¶\n$ validOn    &lt;date&gt; 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22, 2022-01-22‚Ä¶\n$ validTo    &lt;date&gt; -001-11-30, -001-11-30, -001-11-30, -001-11-30, -001-11-30‚Ä¶\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((674339.8 15..., MULTIPOLYGON (‚Ä¶\n\n\nLet‚Äôs verify the coordinate reference systems of the thai_boundary object to ensure the assignment of the correct CRS value.\n\nst_crs(thai_boundary)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96¬∞E and 102¬∞E, northern hemisphere between equator and 84¬∞N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nBefore we delve into further data analysis, it is crucial that we first understand the levels of administration that makes up Thailand today. In particular, Thailand has 4 levels of administration, i.e.¬†level 0 (country), 1 (province), 2 (district), and 3 (sub-district) boundaries. Thailand comprises 76 provinces (known as ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î in Thai or changwat in English), along with one special administrative area, Bangkok, the capital.¬†\n\nThese provinces function as the main local government units and possess legal personhood.¬†\nEach province is subdivided into amphoe (districts), which are further broken down into tambon (sub-districts), representing the next tier of local governance.¬†\nFor this analysis, I will only focus on the province administration level.\n\nWe can visualise the structure of our geospatial object as such.\n\ntmap_mode(\"plot\")  \n# Plot the provinces of Thailand with labels \ntm_shape(thai_boundary) +   \n  tm_borders(col = \"black\", lwd = 0.3, alpha = 0.6) +    \n  tm_polygons()+\n  tm_layout(main.title = \"Provinces of Thailand\",main.title.size = 1,    \n            main.title.position = \"center\", legend.show = FALSE, \n            frame = FALSE   ) +   \n  tm_text(\"ADM1_EN\", size = 0.2)\n\n\n\n\n4.2 Importing Aspatial Data\nIn this section, read_csv() of sf package will be used to import the csv file into the R environment. The output is a R dataframe class containing 5 unique columns of drug offences in Thailand from 2017 - 2022.\n\ndrug_cases &lt;- read_csv(\"data/aspatial/thai_drug_offenses_2017_2022.csv\") \n\nRows: 7392 Columns: 5\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): types_of_drug_offenses, province_th, province_en\ndbl (2): fiscal_year, no_cases\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(drug_cases)\n\n# A tibble: 6 √ó 5\n  fiscal_year types_of_drug_offenses no_cases province_th   province_en         \n        &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;               \n1        2017 drug_use_cases            11871 ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£  Bangkok             \n2        2017 drug_use_cases              200 ‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó         Chai Nat            \n3        2017 drug_use_cases              553 ‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ         Nonthaburi          \n4        2017 drug_use_cases              450 ‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ        Pathum Thani        \n5        2017 drug_use_cases              378 ‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤ Phra Nakhon Si Ayut‚Ä¶\n6        2017 drug_use_cases              727 ‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ          Loburi              \n\n\n\n\n\n\n\n\n\n\nColumn Name\nData Type\nDescription\n\n\nfiscal_year\n&lt;dbl&gt;\nThe fiscal year during which the drug offenses were recorded.\n\n\ntypes_of_drug_offenses\n&lt;chr&gt;\nThe specific type or category of drug offence being reported.\n\n\nno_cases\n&lt;dbl&gt;\nThe total number of cases recorded for the specific combination of fiscal year\n\n\nprovince_th\n&lt;chr&gt;\nThe name of the province in Thailand, written in Thai.\n\n\nprovince_en\n&lt;chr&gt;\nThe name of the province in Thailand, written in English.\n\n\n\n\n\n4.3 Performing a Join on Geometry Column\nNext, we will want to aggregate the total number of drug use cases according to each 77 provinces in Thailand. Here, I use left_join() to associate each drug use case to its respective province (i.e.¬†the geometry of the drug case).\n\nlibrary(dplyr)\n\n# Step 1: Count rows before the join\ninitial_row_count &lt;- nrow(drug_cases)\n\n# Step 2: Perform the join and convert to sf\ndrug_cases &lt;- drug_cases %&gt;%\n  left_join(thai_boundary %&gt;% select(geometry, ADM1_EN, ADM1_PCODE), \n            by = c(\"province_en\" = \"ADM1_EN\")) %&gt;%\n  st_as_sf()\n\n# Step 3: Count rows after the join\nfinal_row_count &lt;- nrow(drug_cases)\n\n# Step 4: Check if rows were dropped\nif (initial_row_count != final_row_count) {\n  cat(\"Rows were dropped during the join.\\n\")\n  cat(\"Rows before join:\", initial_row_count, \"\\n\")\n  cat(\"Rows after join:\", final_row_count, \"\\n\")\n} else {\n  cat(\"No rows were dropped during the join.\\n\")\n}\n\nNo rows were dropped during the join.\n\nglimpse(drug_cases)\n\nRows: 7,392\nColumns: 7\n$ fiscal_year            &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017,‚Ä¶\n$ types_of_drug_offenses &lt;chr&gt; \"drug_use_cases\", \"drug_use_cases\", \"drug_use_c‚Ä¶\n$ no_cases               &lt;dbl&gt; 11871, 200, 553, 450, 378, 727, 820, 69, 127, 2‚Ä¶\n$ province_th            &lt;chr&gt; \"‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£\", \"‡∏ä‡∏±‡∏¢‡∏ô‡∏≤‡∏ó\", \"‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ\", \"‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ\", \"‡∏û‡∏£‚Ä¶\n$ province_en            &lt;chr&gt; \"Bangkok\", \"Chai Nat\", \"Nonthaburi\", \"Pathum Th‚Ä¶\n$ ADM1_PCODE             &lt;chr&gt; \"TH10\", \"TH18\", \"TH12\", \"TH13\", \"TH14\", NA, \"TH‚Ä¶\n$ geometry               &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((674339.8 15..., MU‚Ä¶"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#data-wrangling",
    "title": "Take-home Exercise 2 - Part 1",
    "section": "5. Data Wrangling",
    "text": "5. Data Wrangling\n\n5.1 Reduce Data Size\nTo reduce the memory load, we can drop the province names in Thai from our aspatial dataset as it is not relevant for this study.\n\ndrug_cases &lt;- subset(drug_cases, select = c(-province_th)) \nhead(drug_cases)\n\nSimple feature collection with 6 features and 5 fields (with 1 geometry empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 577383.3 ymin: 1492136 xmax: 710569.3 ymax: 1704842\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 6 √ó 6\n  fiscal_year types_of_drug_offenses no_cases province_en             ADM1_PCODE\n        &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;                   &lt;chr&gt;     \n1        2017 drug_use_cases            11871 Bangkok                 TH10      \n2        2017 drug_use_cases              200 Chai Nat                TH18      \n3        2017 drug_use_cases              553 Nonthaburi              TH12      \n4        2017 drug_use_cases              450 Pathum Thani            TH13      \n5        2017 drug_use_cases              378 Phra Nakhon Si Ayuttha‚Ä¶ TH14      \n6        2017 drug_use_cases              727 Loburi                  &lt;NA&gt;      \n# ‚Ñπ 1 more variable: geometry &lt;MULTIPOLYGON [m]&gt;\n\n\nWe‚Äôll also only retain columns that are most useful from thai_boundary.\n\nthai_boundary &lt;- subset(thai_boundary, select = c(Shape_Leng, Shape_Area, ADM1_EN, ADM1_PCODE, geometry)) \n\nhead(thai_boundary)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 628303 ymin: 1490796 xmax: 712440.5 ymax: 1636901\nProjected CRS: WGS 84 / UTM zone 47N\n  Shape_Leng Shape_Area                  ADM1_EN ADM1_PCODE\n1   2.417227 0.13133873                  Bangkok       TH10\n2   1.695100 0.07926199             Samut Prakan       TH11\n3   1.251111 0.05323766               Nonthaburi       TH12\n4   1.884945 0.12698345             Pathum Thani       TH13\n5   3.041716 0.21393797 Phra Nakhon Si Ayutthaya       TH14\n6   1.739908 0.07920961                Ang Thong       TH15\n                        geometry\n1 MULTIPOLYGON (((674339.8 15...\n2 MULTIPOLYGON (((687139.8 15...\n3 MULTIPOLYGON (((644817.9 15...\n4 MULTIPOLYGON (((704086 1575...\n5 MULTIPOLYGON (((662941.6 16...\n6 MULTIPOLYGON (((643472.8 16...\n\n\n\n\n5.2 Fixing Missing Values\nThere are no troublesome rows in the drug_cases dataframe.\n\nany(is.na(thai_boundary))\n\n[1] FALSE\n\n\n\nany(is.na(drug_cases))\n\n[1] TRUE\n\n\n\n5.2.1 Identify Missing Values\nWe can observe 192 reported offences that do not include the ADM1_PCODE as seen from how some rows contain empty values ‚ÄòNA‚Äô. We are also getting the 192 empty geometry values as returned from the null test below.\n\ndrug_cases %&gt;%\n  filter(is.na(ADM1_PCODE)) %&gt;%\n  select(province_en, ADM1_PCODE)\n\nSimple feature collection with 192 features and 2 fields (with 192 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 192 √ó 3\n   province_en ADM1_PCODE           geometry\n   &lt;chr&gt;       &lt;chr&gt;      &lt;MULTIPOLYGON [m]&gt;\n 1 Loburi      &lt;NA&gt;                    EMPTY\n 2 buogkan     &lt;NA&gt;                    EMPTY\n 3 Loburi      &lt;NA&gt;                    EMPTY\n 4 buogkan     &lt;NA&gt;                    EMPTY\n 5 Loburi      &lt;NA&gt;                    EMPTY\n 6 buogkan     &lt;NA&gt;                    EMPTY\n 7 Loburi      &lt;NA&gt;                    EMPTY\n 8 buogkan     &lt;NA&gt;                    EMPTY\n 9 Loburi      &lt;NA&gt;                    EMPTY\n10 buogkan     &lt;NA&gt;                    EMPTY\n# ‚Ñπ 182 more rows\n\n\nThe root cause of this problem lies in how these two province names were incorrectly spelled in the province_en column of drug_casses , causing an incomplete left_join() to be executed.\n\nunique(drug_cases[!complete.cases(st_drop_geometry(drug_cases)), ][c('province_en')])\n\nSimple feature collection with 2 features and 1 field (with 2 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 2 √ó 2\n  province_en           geometry\n  &lt;chr&gt;       &lt;MULTIPOLYGON [m]&gt;\n1 Loburi                   EMPTY\n2 buogkan                  EMPTY\n\n\n\nthai_boundary %&gt;%\n  filter(ADM1_EN == \"Lop Buri\" | ADM1_EN == \"Bueng Kan\") %&gt;%\n  select(ADM1_PCODE, ADM1_EN) %&gt;%\n  as_tibble()\n\n# A tibble: 2 √ó 3\n  ADM1_PCODE ADM1_EN                                                    geometry\n  &lt;chr&gt;      &lt;chr&gt;                                            &lt;MULTIPOLYGON [m]&gt;\n1 TH16       Lop Buri  (((751293.3 1742960, 751337.4 1742928, 751437.2 1742942,‚Ä¶\n2 TH38       Bueng Kan (((965496 2045531, 965625.5 2045528, 965836.6 2045537, 9‚Ä¶\n\n\n\n\n5.2.2 Fix Missing Values Discovered\nLet‚Äôs fix these NA values by transforming the two province names to their correct names.\n\nStep 1: Replace Loburi ‚Äì&gt; Lop Buri, buogkan ‚Äì&gt; Bueng Kan\nStep 2: Replace ‚ÄòNA‚Äô ADM1_PCDOE with the correct province code - TH16 and TH38 for Lop Buri and Bueng Kan respectively\nStep 3: Replace the existing geometry column in drug_cases with the right geometry\n\n\n# Extract the geometry for Lop Buri\nlop_buri_geometry &lt;- drug_cases %&gt;%\n  filter(province_en == \"Lop Buri\") %&gt;%\n  reframe(first_geometry = st_union(geometry)) %&gt;%  \n  pull(first_geometry)\n\n# Extract the geometry for Bueng Kan\nbueng_kan_geometry &lt;- drug_cases %&gt;%\n  filter(province_en == \"Bueng Kan\") %&gt;%\n  reframe(first_geometry = st_union(geometry)) %&gt;%\n  pull(first_geometry)\n\n# Fix incorrect province names\ndrug_cases &lt;- drug_cases %&gt;%\n  mutate(\n    province_en = case_when(\n      province_en == \"Loburi\" ~ \"Lop Buri\",\n      province_en == \"buogkan\" ~ \"Bueng Kan\",\n      TRUE ~ province_en  \n    )\n  )\n\n# Fix empty province code\ndrug_cases &lt;- drug_cases %&gt;%\n  mutate(ADM1_PCODE = ifelse(province_en == \"Lop Buri\" & \n                             is.na(ADM1_PCODE),\"TH16\", \n                      ifelse(province_en == \"Bueng Kan\" & \n                             is.na(ADM1_PCODE),\"TH38\", ADM1_PCODE)))\n\n# Fix empty geometry\nthai_boundary_no_geom &lt;- thai_boundary %&gt;%\n  select(ADM1_EN, geometry)\ndrug_cases &lt;- drug_cases %&gt;%\n  st_drop_geometry() %&gt;%\n  left_join(thai_boundary_no_geom, by = c(\"province_en\" = \"ADM1_EN\")) \ndrug_cases &lt;- st_as_sf(drug_cases)\n\nWe have successfully removed all NA values found in drug_cases.\n\nany(is.na(drug_cases))\n\n[1] FALSE\n\n\n\n\n\n5.3 Create New total_cases Column\nNext, I create a new dataframe drug_cases_province to count the total number of cases per province.\n\ndrug_cases_province &lt;- drug_cases %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases), \n    ADM1_PCODE = first(ADM1_PCODE),              \n    geometry = first(geometry),\n    .groups = \"drop\"                           \n  ) %&gt;%\n  st_as_sf()\n\nprint(drug_cases_province)\n\nSimple feature collection with 77 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 325178.8 ymin: 620860.6 xmax: 1213656 ymax: 2263241\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 77 √ó 4\n   province_en   total_cases ADM1_PCODE                                 geometry\n   &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;                            &lt;MULTIPOLYGON [m]&gt;\n 1 Amnat Charoen       35435 TH37       (((1137720 1809629, 1137724 1809622, 11‚Ä¶\n 2 Ang Thong           16168 TH15       (((643472.8 1636469, 643496 1636423, 64‚Ä¶\n 3 Bangkok            286480 TH10       (((674339.8 1543300, 674382.3 1543278, ‚Ä¶\n 4 Bueng Kan           35287 TH38       (((965496 2045531, 965625.5 2045528, 96‚Ä¶\n 5 Buri Ram            57352 TH31       (((921217 1750212, 921217 1750211, 9212‚Ä¶\n 6 Chachoengsao        53514 TH24       (((722656.1 1546054, 722796 1546041, 72‚Ä¶\n 7 Chai Nat            15310 TH18       (((620165.4 1704256, 620291.4 1704247, ‚Ä¶\n 8 Chaiyaphum          64497 TH36       (((772997.4 1851276, 773104.5 1851216, ‚Ä¶\n 9 Chanthaburi         31473 TH22       (((853764.8 1360716, 853783.1 1360713, ‚Ä¶\n10 Chiang Mai         121812 TH50       (((554883.3 2226795, 555000.6 2226791, ‚Ä¶\n# ‚Ñπ 67 more rows\n\n\nI will also create a new drug_cases_province_year to aggregate the total number of cases for each province and based on each year, regardless of drug offense type.\n\ndrug_cases_province_year &lt;- drug_cases %&gt;%\n  group_by(province_en, fiscal_year) %&gt;%\n  summarise(\n    fiscal_year = first(fiscal_year),\n    total_cases = sum(no_cases), \n    ADM1_PCODE = first(ADM1_PCODE),              \n    geometry = first(geometry),\n    .groups = \"drop\"                           \n  ) %&gt;%\n  st_as_sf()\n\nprint(drug_cases_province_year)\n\nSimple feature collection with 462 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 325178.8 ymin: 620860.6 xmax: 1213656 ymax: 2263241\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 462 √ó 5\n   province_en   fiscal_year total_cases ADM1_PCODE                     geometry\n   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                &lt;MULTIPOLYGON [m]&gt;\n 1 Amnat Charoen        2017        5076 TH37       (((1137720 1809629, 1137724‚Ä¶\n 2 Amnat Charoen        2018        5651 TH37       (((1137720 1809629, 1137724‚Ä¶\n 3 Amnat Charoen        2019        7339 TH37       (((1137720 1809629, 1137724‚Ä¶\n 4 Amnat Charoen        2020        3949 TH37       (((1137720 1809629, 1137724‚Ä¶\n 5 Amnat Charoen        2021        8961 TH37       (((1137720 1809629, 1137724‚Ä¶\n 6 Amnat Charoen        2022        4459 TH37       (((1137720 1809629, 1137724‚Ä¶\n 7 Ang Thong            2017        1614 TH15       (((643472.8 1636469, 643496‚Ä¶\n 8 Ang Thong            2018        2717 TH15       (((643472.8 1636469, 643496‚Ä¶\n 9 Ang Thong            2019        2781 TH15       (((643472.8 1636469, 643496‚Ä¶\n10 Ang Thong            2020        2636 TH15       (((643472.8 1636469, 643496‚Ä¶\n# ‚Ñπ 452 more rows\n\n\n\n\n5.4 Create Spacetime Cubes\nWe will also utilise the new s3 class introduced by Edzer Pebesma (2012) from the sfdep package to better represent spatio-temporal data (source code can be found here).\nA spacetime object is considered a spacetime cube when each location in the dataset has a corresponding value for every point in time. In other words, the dataset contains a regular time series for each location. Hence, the concept of a spacetime cube can be valuable for understanding how the spatial patterns of these drug cases evolve over time.\n\nIn ESRI terminology, the basic unit of a spacetime cube is called a bin which represents a unique combination of a specific geographic location and a particular time index.\nThe collection of all locations for a given time point forms a time slice.\nSimilarly, for a single location, the set of bins across all time points is referred to as a bin time-series.\n\n\n\n\n\n\nBefore we construct our spacetime cube object, we need to carry out data wrangling to prepare the requred data. It is worth noting that geometries are currently stored as Multipolygon rather than Polygon data. Hence, for the creation of the space-time object, I will need to split the Multipolygon geometries into individual Polygon components, then retain the largest polygon for each province.\n\nSplit Multipolygon into Polygon: we will use st_cast to convert Multipolygon geometries into Polygon geometries.\nRetain the Largest Polygon: For each province, we can calculate the area of each polygon and retain only the polygon with the largest area into geo.\nConvert the sf Attribute Data to a Dataframe: The spacetime() function will require useful attributes from drug_cases_province_year (e.g.¬†total_cases) which will also require the geometry column to be dropped.\nCreate the Space-Time Cube: After preparing the data, I‚Äôll be able to use this for space-time analysis.\n\n\nlibrary(sf)\nlibrary(dplyr)\n\n# Step 1: Split multipolygon into individual polygons and compute area\nprovinces_split &lt;- thai_boundary %&gt;%\n  st_cast(\"POLYGON\") %&gt;%\n  mutate(area = st_area(geometry))\n\n# Step 2: Retain the largest polygon for each province\ngeo &lt;- provinces_split %&gt;%\n  group_by(ADM1_PCODE) %&gt;%\n  filter(area == max(area)) %&gt;%\n  ungroup() %&gt;%\n  select(ADM1_PCODE, geometry)  \n\n# Step 3: Prepare the data without geometry for analysis\ndf_drug_cases &lt;- st_drop_geometry(drug_cases_province_year) %&gt;%\n  select(ADM1_PCODE, province_en, fiscal_year, total_cases)\n\n# Step 4: Ensure there are no duplicate rows in the data\ndf_drug_cases &lt;- distinct(df_drug_cases)\n\n\n# Inspect the result\nhead(df_drug_cases)\n\n# A tibble: 6 √ó 4\n  ADM1_PCODE province_en   fiscal_year total_cases\n  &lt;chr&gt;      &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n1 TH37       Amnat Charoen        2017        5076\n2 TH37       Amnat Charoen        2018        5651\n3 TH37       Amnat Charoen        2019        7339\n4 TH37       Amnat Charoen        2020        3949\n5 TH37       Amnat Charoen        2021        8961\n6 TH37       Amnat Charoen        2022        4459\n\n\n\n# Ensure output is an sf object\nclass(geo)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n# Ensure output is a dataframe object\nclass(df_drug_cases)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWith the data prepared, we can now create the spacetime object called drug_cases_spt using the following data:\n\ndata: df_drug_cases\ngeometry: st_data\nlocation identifier: ADM1_PCODE\ntime: fiscal_year\n\n\n# Create the spacetime object\ndrug_cases_spt &lt;- spacetime(\n  .data = df_drug_cases,      \n  .geometry = geo, \n  .loc_col = \"ADM1_PCODE\",     \n  .time_col = \"fiscal_year\",    \n  active = \"data\"               \n)\n\n\n# Check if it's a valid spacetime cube\nis_spacetime_cube(drug_cases_spt)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nNote\n\n\n\nA key takeaway and something to note, we need to dissolve the MULTIPOLYGON data to a GEOMETRY spatial object, else it will lead to an error where duplicated geometry is found. Additionally, after multiple trials and errors, I figured out that this error will also surface if the .data and .geometry parameters don‚Äôt receive a dataframe and sf object respectively."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#exploratory-geospatial-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#exploratory-geospatial-data-analysis",
    "title": "Take-home Exercise 2 - Part 1",
    "section": "6. Exploratory Geospatial Data Analysis",
    "text": "6. Exploratory Geospatial Data Analysis\n\n6.1 Overall Histogram of Drug Cases\nI employed the geom_histogram function of the ggplot package to plot histogram distributions of the spread of the number of drug cases found in Thailand from 2017 to 2022.\nOverall, the number of cases appears to be relatively right-skewed with some outliers indicating a significantly high number of drug cases.\n\nggplot(drug_cases, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue2\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Histogram of Total Drug Cases in Thailand (2017-2022)\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )\n\n\n\n\n6.2 Histogram of Drug Cases by Year\nBased on each year category, we see that there is a similar pattern of drug cases skewed towards the lower end with an uneven distribution.\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(dplyr)\n\n# Create histogram for each year\ndrug_cases_2017 &lt;- filter(drug_cases, fiscal_year == 2017)\nhist_2017 &lt;- ggplot(drug_cases_2017, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2017\") +\n  theme_minimal(base_size = 9) \n\ndrug_cases_2018 &lt;- filter(drug_cases, fiscal_year == 2018)\nhist_2018 &lt;- ggplot(drug_cases_2018, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2018\") +\n  theme_minimal(base_size = 9)\n\ndrug_cases_2019 &lt;- filter(drug_cases, fiscal_year == 2019)\nhist_2019 &lt;- ggplot(drug_cases_2019, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2019\") +\n  theme_minimal(base_size = 9)\n\ndrug_cases_2020 &lt;- filter(drug_cases, fiscal_year == 2020)\nhist_2020 &lt;- ggplot(drug_cases_2020, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2020\") +\n  theme_minimal(base_size = 9)\n\ndrug_cases_2021 &lt;- filter(drug_cases, fiscal_year == 2021)\nhist_2021 &lt;- ggplot(drug_cases_2021, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2021\") +\n  theme_minimal(base_size = 9)\n\ndrug_cases_2022 &lt;- filter(drug_cases, fiscal_year == 2022)\nhist_2022 &lt;- ggplot(drug_cases_2022, aes(x = no_cases)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue3\") + \n  labs(x = \"Number of Cases\", y = \"Count\", title = \"Drug Cases in 2022\") +\n  theme_minimal(base_size = 9)\n\n# Arrange all histograms in a grid layout\ngrid.arrange(hist_2017, hist_2018, hist_2019, hist_2020, hist_2021, hist_2022, nrow = 2)\n\n\n\n\n\n6.3 Total Cases by Drug Case Type\nNext, we can take a peak into the spread of number of drug offense based on the drug case type. It appears that the highest number of drug case types involve drug use, drug possession and trafficking cases, while drug export cases is discovered to be the least in number of offenses.\n\ndrug_cases_category_sum &lt;- drug_cases_type %&gt;%\n  mutate(category = case_when(\n    types_of_drug_offenses %in% c(\"drug_use_cases\", \"suspects_in_drug_use_cases\") ~ \"Drug Use Cases\",\n    types_of_drug_offenses %in% c(\"possession_cases\", \"suspects_in_possession_cases\", \"possession_with_intent_to_distribute_cases\", \"suspects_in_possession_with_intent_to_distribute_cases\") ~ \"Possession Cases\",\n    types_of_drug_offenses %in% c(\"trafficking_cases\", \"suspects_in_trafficking_cases\") ~ \"Trafficking Cases\",\n    types_of_drug_offenses %in% c(\"production_cases\", \"suspects_in_production_cases\") ~ \"Production Cases\",\n    types_of_drug_offenses %in% c(\"import_cases\", \"suspects_in_import_cases\", \"export_cases\", \"suspects_in_export_cases\") ~ \"Import/Export Cases\",\n    types_of_drug_offenses %in% c(\"conspiracy_cases\", \"suspects_in_conspiracy_cases\") ~ \"Conspiracy Cases\",\n    TRUE ~ \"Other\"\n  )) %&gt;%\n  group_by(category) %&gt;%\n  summarise(total_cases_sum = sum(total_cases))\n\n\nggplot(drug_cases_type, aes(x = reorder(types_of_drug_offenses, total_cases), y = total_cases, fill = category)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = total_cases), hjust = -0.2, size = 4) +  # Add total cases labels\n  labs(title = \"Total Number of Drug Cases by Drug Offense Type\",\n       x = \"Drug Case Type\",\n       y = \"Total Cases\") +\n  scale_fill_brewer(palette = \"Blues\") + \n  theme_minimal(base_size = 16) +  \n  theme(axis.text.y = element_text(size = 16),  \n        axis.title = element_text(size = 16),   \n        plot.title = element_text(hjust = 0.5, size = 20),\n        legend.position = \"bottom\") +  \n  coord_flip()  # Flip the bars horizontally\n\n\n\n\n6.4 Overall Plot of Drug Cases\nAs such, an quantile interval will be more suitable in plotting the total number of cases for each province as shown in the tmap output below. Additionally, we can use the default break of 5 to capture a suitable level of granularity across all provinces. For comparison purposes, I will also plot the equal scale as shown.\n\n# First plot using quantile interval\nplot_quantile &lt;- tm_shape(drug_cases_province) +\n  tm_polygons(\"total_cases\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Number of Drug Cases\") +\n  tm_layout(main.title = \"Distribution of Drug Cases in Thailand \\n by Province (Quantile Interval)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", size = 2, position = c(\"right\", \"top\")) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n# Second plot using equal interval\nplot_equal &lt;- tm_shape(drug_cases_province) +\n  tm_polygons(\"total_cases\", \n          style = \"equal\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Number of Drug Cases\") +\n  tm_layout(main.title = \"Distribution of Drug Cases in Thailand \\n by Province (Equal Interval)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", size = 2, position = c(\"right\", \"top\")) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n# Combine both plots side by side\ntmap_arrange(plot_quantile, plot_equal, nrow = 1)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nOverall, we can observe that the highest number of drug cases are found in the north-western, central, eastern and southern provinces of Thailand as seen in the darker regions. Namely..\n\nNorth-west: Chiang Mai, Chiang Rai\nCentral: Chaiyaphum, Khon Kaen and Nakhon-Ratchasima\nEast: Sisaket, Ubon-Ratchathani\nSouth: Surat Thani, Nakhon Si Thammarat, Phuket\n\nIt is worth noting that smaller provinces are likely to be lighter in shade due to their smaller geographic area.\n\n\n\n\n6.5 Plot of Drug Cases by Year\nWhen we further categorise the drug cases by year, we can see that there is an even spread of cases\n\nplot_quantile_year &lt;- tm_shape(drug_cases_province_year) +\n  tm_polygons(\"total_cases\", \n          style = \"quantile\", \n          palette = \"Blues\") +\n  tm_facets(by=\"fiscal_year\", free.coords = FALSE) +\n  tm_layout(main.title = \"Distribution of Drug Cases in Thailand \\n by Province for Each Year (Quantile Interval)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) \n\nplot_quantile_year\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWhen the drug cases are visualised by year, we can observe consistent patterns where high drug abuse cases are consistently located at the Southern parts of Thailand. Central Thailand in 2017 showed an average number of drug cases but this increased in the 2018 and continues to increase in the following years till 2022. In 2022, central Thailand faces a high concentration of drug cases which appears to have spread to its neighbouring provinces, particularly with the greatest spread in 2021.\n\n\nSimilar observations can be found when plotting the drug cases using the knitr package as shown below.\n\n# Create temporal maps\ntemporal_maps &lt;- tm_shape(drug_cases_province_year) +  \n  tm_polygons(\"total_cases\",  \n              palette = \"Blues\",  \n              style = \"quantile\", \n              title = \"Drug Cases\") + \n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_layout(legend.title.size = 1.8,\n            legend.text.size = 1.1) +\n  tm_facets(along = \"fiscal_year\", free.coords = FALSE)\n\n# Generate animation\ntmap_animation(temporal_maps, filename = \"thailand_drugs_temporal.gif\", \n               delay = 150, width = 1000, height = 1200)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#global-measures-of-spatial-autocorrelation",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#global-measures-of-spatial-autocorrelation",
    "title": "Take-home Exercise 2 - Part 1",
    "section": "7. Global Measures of Spatial Autocorrelation",
    "text": "7. Global Measures of Spatial Autocorrelation\nThe Second Law of Geography articulated by Waldo Tobler in 1970, states that spatial relationships (or correlations) can vary depending on the context and the specific characteristics of different regions. This concept of spatial non-stationarity can be effectively measured and analysed using spatial autocorrelation statistics.\n\n7.1 Methods Used\nThis section delves into analysing the spatial autocorrelation of our drug cases dataset and assessing how the presence of drug abuse in a province may influence and form clusters around it.\nSpatial relationships are characterised by their multidirectional and multilateral nature, setting them apart from temporal relationships, which are linear and follow a past-present-future sequence. The codifying process of spatial relationships, illustrated in the figure below, enables the transformation of complex geographic space into a structured dataset suitable for computer analysis.\n\n\n\n\n\nThere are multiple approaches to defining spatial neighbours. Two most common methods are:\n\nAdjacency Measures: establish links between spatial units that are directly adjacent to one another. In this context, spatial units are considered neighbours if they share a boundary or a point.\nDistance Measures: selects neighbours based on proximity, where the nearest points to a given spatial unit are identified as neighbours. This approach considers varying distances, enabling a more nuanced understanding of spatial relationships beyond mere adjacency.\n\nRook Criterion: Neighbours are areas that share a common edge via horizontal and vertical connections between spatial units.\nBishop Criterion: Neighbours are areas that share a common corner but not necessarily an edge. This approach is similar to the diagonal movements.\nQueen Criterion: Neighbours are areas that share either an edge or corner, which includes both vertical, horizontal, and diagonal connections.\n\n\n\n\n\n\n\n\n\n7.2 Computing Contiguity Neighbours\nI decided to use the Queen criterion in deriving our neighbour list object since it covers the most neighbours of the three. To do so, we will utilise the st_contiguity() function from the sfdep package to create contiguity weight matrices for the study area. This function generates a list of neighbors based on provinces that share contiguous boundaries. It is worth noting that the function only supports the rook and queen criteria which is suitable for our analysis.\n\nthailand_nb_q &lt;- st_contiguity(drug_cases_province$geometry, queen=TRUE)\nsummary(thailand_nb_q)\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 352 \nPercentage nonzero weights: 5.93692 \nAverage number of links: 4.571429 \n1 region with no links:\n48\n2 disjoint connected subgraphs\nLink number distribution:\n\n 0  1  2  3  4  5  6  7  8  9 \n 1  1  5 17 15 17 10  5  4  2 \n1 least connected region:\n71 with 1 link\n2 most connected regions:\n17 69 with 9 links\n\n\nEach number returned above is an ID assigned to each province from the st_contiguity() function. Let‚Äôs find out the specific provinces highlighted by the summary above!\n\nprovince_ids &lt;- data.frame(\n  region_id = seq_along(drug_cases_province$province_en),\n  province_en = drug_cases_province$province_en,\n  total_cases = drug_cases_province$total_cases\n)\n\nprovince_ids[province_ids$region_id %in% c(17, 48, 69, 71), ]\n\n   region_id province_en total_cases\n17        17   Khon Kaen       93905\n48        48      Phuket       75321\n69        69         Tak       23530\n71        71        Trat       13496\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nI can observe a total of 77 provinces (regions) in the dataset with 352 neighbouring links between the provinces\nThere is roughly 5.94% non-zero neighbour relationships. This indicates only a small proportion of total possible connections have neighbours.\nOn average, we can also observe that each province has 4.57 neighbouring regions.\nRegion 48 (Phuket province) has no neighbouring regions\nRegion 71 (Trat province) is one of the least connected provinces with only 1 neighbouring region\nRegions 17 (Khon Kaen province) and 69 (Tak province) are the two provinces with the most neighbours, each sharing boundaries with 9 other provinces\n\n\n\n\n\n7.3 Computing Row-Standardised Weight Matrix\nNext, I attempt to calculate spatial weights but the isolated region (48) is causing issues as spatial weights calculations require all regions to have neighbors.\nTo resolve the issue caused by the isolated region, I will use the allow_zero = TRUE option when calculating spatial weights, which will assign zero as a lagged value to allow the analysis to proceed despite isolated regions.\n\nthailand_wt &lt;- st_weights(thailand_nb_q, style = \"W\", allow_zero = TRUE)\n\nWe will mutate the newly created neighbour list object¬†thailand_nb_1 and weight matrix¬†thailand_wt¬†into our existing¬†drug_cases_province. This results in a newly created object called wm_1.\n\nwm_q &lt;- drug_cases_province %&gt;%\n  mutate(nb = thailand_nb_q,\n         wt = thailand_wt,\n         .before = 1) \n\n# Inspect\nwm_q\n\nSimple feature collection with 77 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 325178.8 ymin: 620860.6 xmax: 1213656 ymax: 2263241\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 77 √ó 6\n   nb        wt     province_en total_cases ADM1_PCODE                  geometry\n * &lt;nb&gt;      &lt;list&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;             &lt;MULTIPOLYGON [m]&gt;\n 1 &lt;int [3]&gt; &lt;dbl&gt;  Amnat Char‚Ä¶       35435 TH37       (((1137720 1809629, 1137‚Ä¶\n 2 &lt;int [4]&gt; &lt;dbl&gt;  Ang Thong         16168 TH15       (((643472.8 1636469, 643‚Ä¶\n 3 &lt;int [6]&gt; &lt;dbl&gt;  Bangkok          286480 TH10       (((674339.8 1543300, 674‚Ä¶\n 4 &lt;int [3]&gt; &lt;dbl&gt;  Bueng Kan         35287 TH38       (((965496 2045531, 96562‚Ä¶\n 5 &lt;int [5]&gt; &lt;dbl&gt;  Buri Ram          57352 TH31       (((921217 1750212, 92121‚Ä¶\n 6 &lt;int [8]&gt; &lt;dbl&gt;  Chachoengs‚Ä¶       53514 TH24       (((722656.1 1546054, 722‚Ä¶\n 7 &lt;int [4]&gt; &lt;dbl&gt;  Chai Nat          15310 TH18       (((620165.4 1704256, 620‚Ä¶\n 8 &lt;int [4]&gt; &lt;dbl&gt;  Chaiyaphum        64497 TH36       (((772997.4 1851276, 773‚Ä¶\n 9 &lt;int [5]&gt; &lt;dbl&gt;  Chanthaburi       31473 TH22       (((853764.8 1360716, 853‚Ä¶\n10 &lt;int [5]&gt; &lt;dbl&gt;  Chiang Mai       121812 TH50       (((554883.3 2226795, 555‚Ä¶\n# ‚Ñπ 67 more rows\n\n\n\n\n7.4 Visualising Contiguity Weights\nLet‚Äôs also visualise the Queen‚Äôs neighbour map. Here, we retrieve the centroid coordinates of each province by combining the longitude and latitude into a single object via st_centroid(). The outputs of the coordinate looks correct as shown.\n\n# Extract centroid geometries\ncentroids &lt;- st_centroid(drug_cases_province)\nhead(centroids)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 645239 ymin: 1505514 xmax: 1115483 ymax: 2013040\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 6 √ó 4\n  province_en   total_cases ADM1_PCODE           geometry\n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;             &lt;POINT [m]&gt;\n1 Amnat Charoen       35435 TH37        (1115483 1765518)\n2 Ang Thong           16168 TH15         (645239 1617118)\n3 Bangkok            286480 TH10       (675514.6 1523087)\n4 Bueng Kan           35287 TH38       (998785.3 2013040)\n5 Buri Ram            57352 TH31       (925999.3 1642136)\n6 Chachoengsao        53514 TH24       (762475.6 1505514)\n\n\nNow, let‚Äôs plot the contiguity weights using the Queen‚Äôs method for all provinces.\n\n# Plot all regions (showing the boundaries)\nplot(drug_cases_province$geometry, border = \"lightgrey\", main = \"Queen's Contiguity Weights (Including All Regions)\")\n\n# Loop through each region and plot neighbors if they exist\nfor (i in seq_along(thailand_nb_q)) {\n  # Check if the region has neighbors\n  if (length(thailand_nb_q[[i]]) &gt; 0) {\n    for (neighbor in thailand_nb_q[[i]]) {\n      current_coords &lt;- st_coordinates(centroids[i, ])\n      neighbor_coords &lt;- st_coordinates(centroids[neighbor, ])\n      \n      # Create a LINESTRING directly from the two centroid coordinates\n      combined_geom &lt;- matrix(c(current_coords[1], current_coords[2],\n                                 neighbor_coords[1], neighbor_coords[2]),\n                              ncol = 2, byrow = TRUE)\n      \n      # Draw lines between the current region and its neighbour\n      lines(combined_geom, col = \"blue\")\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe cam observe that most connections between the centroids of each province is found at the lower central regions of Thailand which shows signs of higher interactivity between provinces here. Surprisingly, there are fewer connecting relationships between provinces in the central parts of Thailand.\n\n\n\n\n7.5 Global Moran‚Äôs I\nMoran‚Äôs I is a inferential static measure of the correlation between a variable and the values of its neighbouring regions to determine statistical significance. It reflects the extent to which individual features deviate from the overall values in the study area, assessing the similarity between each region and its neighbours, and averages these evaluations.\nTo examine spatial autocorrelation, we must test the following hypotheses:\n\nNull Hypothesis (H0): This states that there is either no spatial autocorrelation (H0) or that negative spatial autocorrelation exists (H0).\nAlternative Hypothesis (H1): This suggests that positive spatial autocorrelation is present.\n\nThe values of Moran‚Äôs I typically range from -1 to 1.\n\n-1 is perfect clustering of dissimilar values (perfect dispersion).\n0 is no autocorrelation (perfect randomness.)\n+1 indicates perfect clustering of similar values (the opposite of dispersion).\n\n\n7.5.1 Computing Global Moran‚Äôs I\nI will employ the global_moran()¬†function to compute the Moran‚Äôs I value which outputs a tibble dataframe. The zero.policy has been set to TRUE to allow the function to appropriately handle areas with no neighbours.\n\nmoranI &lt;- global_moran(wm_q$total_cases,\n                       wm_q$nb,\n                       wm_q$wt, \n                       zero.policy = TRUE)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.119\n $ K: num 15.8\n\n\n\n\n\n\n\n\nObservations\n\n\n\nBased on the outputs‚Ä¶\n\nThe Moran‚Äôs I value of 0.119 suggests a slight positive spatial autocorrelation, indicating clustering of similar values (e.g., areas with similar drug case counts).\nThe K value of 15.8 indicates that each region, on average, has about 15.8 neighbours, which is a moderate level of connectivity among the regions being analysed.\n\n\n\n\n\n7.5.2 Global Moran‚Äôs I Test\nThe Global Moran‚Äôs I test, which can be performed using the global_moran_test() function from the sfdep package, is a tool for assessing spatial autocorrelation. The main objective of this test is to ascertain the presence of systemic spatial variations of drug abuse cases. In other words, how the number of drug cases in each province vary according to its surrounding provinces compared to that under spatial randomness.\nFor this analysis, we will specify alternative = \"greater\" in line with our alternative hypothesis.\n\nglobal_moran_test(wm_q$total_cases,\n                       wm_q$nb,\n                       wm_q$wt,\n                       zero.policy = TRUE,\n                       alternative = \"greater\")\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.8848, p-value = 0.02973\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.117752268      -0.013333333       0.004837196 \n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe calculated Moran I statistic returns a positive value of 0.118, suggesting positive spatial autocorrelation and a tendency for similar values to be more clustered together than would be expected by chance. Hence, the spatial distribution of drug cases is not random.\nThe standard deviation (or z-score)of the observed Moran I statistic is 1.8848 s.d. away from the expected value (which is typically 0 under the null hypothesis). The positive s.d. indicates that the observed clustering is stronger than what would be expected under the null hypothesis.\nSince the p-value of 0.02973 &lt; alpha value of 0.05, assuming a 95% confience interval, there is statistically significant evidence to reject the null hypothesis (which posits that there is no spatial autocorrelation) in favour of the alternative hypothesis, confirming that positive spatial autocorrelation is present in the data.\n\n\n\n\n7.5.3 Performing Global Moran‚Äôs I Permutation Test\nFrom a frequentist approach, sampling the p-value once is not sufficient for determining the long-run behaviour of estimators and tests. Hence, let us strengthen our findings by repeated sampling, that is to perform Monte Carlo simulation and then, observe the results.\n\nset.seed(1234)\ngmoran_MC &lt;- global_moran_perm(wm_q$total_cases,\n                  wm_q$nb,\n                  wm_q$wt,\n                  zero.policy = TRUE,\n                  nsim = 999)\ngmoran_MC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.11775, observed rank = 957, p-value = 0.086\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is always good practice to use¬†set.seed()¬†before performing simulation. This is to ensure that the computation is reproducible.\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAfter 1000 simulations, our observed result confirms that the p-value is indeed smaller than the alpha value of 0.05 and the Moran I statistic is &gt; 0. Hence, we can reject the H0 in favour of H1, meaning that the results of global_moran_test() test is stable and statistically significant, and spatial distribution of drug cases resemble clustering distribution patterns.\n\n\nNow, we can analyse the spatial distribution of drug cases in a histogram and its summary statistics.\n\nggplot() + \n  aes(gmoran_MC$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"skyblue\") + \n  geom_vline(aes(xintercept = mean(gmoran_MC$res)), color = 'red') +\n  labs(title = \"Histogram of Simulated Moran's I For Thailand's Drug Cases\",\n       x = \"Simulated Moran's I\",\n       y = \"Occurences\") +\n  theme_minimal()+ \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )\n\n\n\n# Summary Statistics\nsummary(gmoran_MC$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.19043 -0.06200 -0.01834 -0.01352  0.03085  0.32088 \n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can confirm that the observed Moran I statistic (0.11775) is higher than the median (‚àí0.01834) and mean (‚àí0.01352) of the simulated values. This suggests that the observed spatial pattern is more clustered than what would be expected if the values were randomly distributed.\n\n\n\n\n\n7.6 Global Geary‚Äôs C Test\nIn practice, when analysing spatial data, it is often recommended to use both Geary‚Äôs C and Moran‚Äôs I to gain a more comprehensive understanding of spatial patterns. However, Geary‚Äôs C can often fail to detect localised clustering compared to other spatial autocorrelation tests like Moran‚Äôs I.\n\nGeary‚Äôs C is more sensitive to local dissimilarities between individual pairs of neighbouring areas rather than overall clustering, which makes it less effective at identifying larger-scale spatial structures or clusters.\nMoran‚Äôs I is better at identifying global patterns of clustering, where areas with similar values are grouped together in broader spatial clusters.\n\nNonetheless, let‚Äôs explore what Geary‚Äôs C results look like for us.\n\n7.6.1 Computing Global Geary‚Äôs C\nGeary‚Äôs C test is developed by Geary (1954), which examines the intensity of a specific characteristic in spatial objects using a weight matrix. It can be performed using global_c_test() of the sfdep package in R, similar to how we conducted the Global Moran‚Äôs I test for evaluating spatial autocorrelation.\n\nglobal_c(\n  wm_q$total_cases, \n  wm_q$nb, \n  wm_q$wt,\n  allow_zero = TRUE\n)\n\n$C\n[1] 0.9878824\n\n$K\n[1] 15.80246\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe Geary‚Äôs C statistic returned is 0.9879 which, unlike the results returned by Moran‚Äôs I, suggests a weak positive spatial autocorrelation since it is very close to 1, indicating little to no spatial autocorrelation. As mentioned, Geary‚Äôs C tends to focus on local clusters which might miss larger clusters.\n\n\n\n\n7.5.2 Global Geary C‚Äôs Test\nLet us perform the Geary C‚Äôs Test which can be implemented using global_c_test() from the sfdep package.\n\nglobal_c_test(wm_q$total_cases,\n                  wm_q$nb,\n                  wm_q$wt,\n                  allow_zero = TRUE,\n                  alternative = \"greater\")\n\n\n    Geary C test under randomisation\n\ndata:  x \nweights: listw  \nn reduced by no-neighbour observations \n\nGeary C statistic standard deviate = 0.22762, p-value = 0.41\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n       0.97488398        1.00000000        0.01217483 \n\n\n\n\n\n\n\n\nObservations\n\n\n\nAfter performing the Geary C test, we can observe that‚Ä¶\n\nGeary‚Äôs C statistic is close to 1, suggesting little or no global spatial autocorrelation.\nThe p-value of 0.41 indicates that the observed pattern is not significantly different from random.\nTherefore, the data does not show strong spatial clustering or dispersal, and the spatial distribution of values is likely random.\n\n\n\n\n\n7.5.3 Performing Global Geary‚Äôs C Permutation Test\n\nggeary_MC &lt;- global_c_perm(wm_q$total_cases,\n                  wm_q$nb,\n                  wm_q$wt,\n                  allow_zero = TRUE,\n                  alternative = \"greater\",\n                  nsim = 999)\nggeary_MC\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.97488, observed rank = 459, p-value = 0.459\nalternative hypothesis: greater\n\n\nWe can see that the distribution of drug cases resulting from the Geary‚Äôs C permutation test has a much higher mean (0.9851) and median (0.9860) than what was outputted by the Global Moran‚Äôs I test. Likewise, the observed Geary‚Äôs C statistics of 0.97488 suggests that the data does not show strong clustering/dispersed, but is random.\n\n# Summary Statistics\nsummary(ggeary_MC$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4489  0.9151  0.9859  0.9882  1.0662  1.3445 \n\n\n\nggplot() + \n  aes(ggeary_MC$res[1:999]) + \n  geom_histogram(colour=\"black\", fill=\"skyblue\") + \n  geom_vline(aes(xintercept = mean(ggeary_MC$res)), color = 'red') +\n  labs(title = \"Histogram of Simulated Geary's C For Thailand's Drug Cases\",\n       x = \"Simulated Geary's C\",\n       y = \"Occurences\") +\n  theme_minimal()+ \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAs shown, Geary‚Äôs C indeed indicated randomness since it doesn‚Äôt detect the global pattern of spatial clustering very well, even when such a pattern is present. Geary‚Äôs C sensitivity to local differences between pairs of neighbours could make it miss larger, global clustering patterns and conclude that the spatial distribution is more random than it actually is. Hence, the Global Moran‚Äôs I test is more relevant in identifying overall clustering on a global-scale, i.e at a national scale, rather than by provinces."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#local-indicators-of-spatial-association-lisa",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02.html#local-indicators-of-spatial-association-lisa",
    "title": "Take-home Exercise 2 - Part 1",
    "section": "8. Local Indicators of Spatial Association (LISA)",
    "text": "8. Local Indicators of Spatial Association (LISA)\nPreviously, we used Global Moran‚Äôs I and Geary‚Äôs C in which we discovered whether spatial clustering exists across the whole Thailand region.\nHowever, I would also like to perform local spatial autocorrelation to identify specific areas of clustering at a local level using LISA methods (Anselin, 1995). If these methods detect significant local clusters, it can help confirm that there are indeed clustered patterns that Geary‚Äôs C might have missed.\n\n8.1 Local Moran‚Äôs Ii\n\n8.1.1 Computing Local Moran‚Äôs Ii\nLocal Moran‚Äôs Ii is an extension of Global Moran‚Äôs I, designed to identify local clusters and spatial outliers within a dataset. Local Moran‚Äôs Ii provides a measure of autocorrelation at individual locations, identifying where significant clustering or outliers exist.\nLet‚Äôs utilise the¬†local_moran()¬†function of sfdep to handle the computations. Once again, we‚Äôll set zero.policy to TRUE to allow the analysis to continue despite the one province with 0 neighbours.\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(total_cases, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Inspect\nlisa\n\nSimple feature collection with 77 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 325178.8 ymin: 620860.6 xmax: 1213656 ymax: 2263241\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 77 √ó 18\n        ii      eii   var_ii   z_ii   p_ii p_ii_sim p_folded_sim skewness\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.125  -0.0343  0.0590   -0.375 0.708      0.48         0.24   -1.38 \n 2  0.318  -0.0464  0.236     0.750 0.453      0.54         0.27   -0.943\n 3  0.136  -0.607   2.84      0.441 0.659      0.68         0.34    0.261\n 4  0.119   0.00744 0.0477    0.510 0.610      0.66         0.33   -1.85 \n 5  0.0356 -0.0157  0.00322   0.904 0.366      0.36         0.18    1.36 \n 6  0.0430 -0.00211 0.000311  2.56  0.0105     0.06         0.03    0.994\n 7  0.604  -0.0901  0.306     1.25  0.210      0.06         0.03   -1.86 \n 8  0.144  -0.00995 0.0329    0.847 0.397      0.26         0.13    1.80 \n 9 -0.106  -0.0109  0.0559   -0.403 0.687      0.52         0.26   -1.17 \n10 -0.534   0.0101  0.680    -0.660 0.509      0.58         0.29    1.02 \n# ‚Ñπ 67 more rows\n# ‚Ñπ 10 more variables: kurtosis &lt;dbl&gt;, mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;,\n#   nb &lt;nb&gt;, wt &lt;list&gt;, province_en &lt;chr&gt;, total_cases &lt;dbl&gt;, ADM1_PCODE &lt;chr&gt;,\n#   geometry &lt;MULTIPOLYGON [m]&gt;\n\n\n\n\n\n\n\n\nHow to read the table output\n\n\n\nThe output from the local_moran() function is an sf data frame that includes the following columns: ii, eii, var_ii, z_ii, p_ii, p_ii_sim, andp_folded_sim.\n\nii: This represents the local Moran statistic.\neii: This denotes the expected value of the local Moran statistic; for localmoran_perm, it corresponds to the means from the permutation samples.\nvar_ii: This indicates the variance of the local Moran statistic; for localmoran_perm, it reflects the standard deviations from the permutation samples.\nz_ii: This is the standard deviation of the local Moran statistic; for localmoran_perm, it is calculated based on the means and standard deviations from the permutation samples.\np_ii: This is the p-value for the local Moran statistic, derived using the pnorm() function; for localmoran_perm, it utilizes standard deviations based on the means and standard deviations from the permutation samples.\np_ii_sim: For localmoran_perm(), this represents the rank of the observed statistic in relation to a uniform distribution for [0, 1] p-values, using the specified alternative hypothesis.\np_folded_sim: This reflects the simulation of ranked p-values within the folded range of [0, 0.5], based on a specific implementation found in the GitHub repository.\n\n\n\n\n\n8.1.2 Visualising Local Moran‚Äôs Ii\nTo ease our analysis, an approach we can take is to plot the local Moran‚Äôs I values across to visualise the observed values across each province. We‚Äôll use a choropleth map from the tmap package to analyse the spatial patterns.\n\ntm_shape(lisa) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",\"green1\",\"orange\",\"red\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Province-Level Spatial Autocorrelation \\nof Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.position = c(\"right\",\"bottom\"),\n            asp = 1.1,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.2)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe local spatial autocorrelation using Moran‚Äôs I outputs a total of 4 different regions, each with a different range of Moran‚Äôs I value.\n\nProvinces in blue indicate a local Moran‚Äôs I value ranging from -1 to 0 with low-high spatial association or no similarity with its neighbours.\n\nThese province are outliers since they exhibit lower intensity of drug cases compared to its surrounding provinces where drug cases are higher in intensity\n\nProvinces in green, orange and red has a local Moran‚Äôs I value ranging from 0 to 3 with high-high spatial association.\n\nThese provinces tend to exhibit a high incidence of drug cases and are surrounded by other provinces with similarly high values.\nSamut Prakan province (in red) shows the strongest clustering effect i.e.¬†it is strongly associated with its high drug-cases neighbours.\nFollowed by Nakhon Si Thammarat (in orange) which shows the 2nd strongest clustering effect.\n\n\nOverall, the spatial autocorrelation of drug cases is a prevalent and widespread issue across provinces in Thailand. The two provinces, Samut Prakan and Nakhon Si Thammarat, suggests to be high-risk regions for the spread of drug cases.\nHowever, for a complete understanding of these spatial autocorrelation patterns, we are required to evaluate the statistical significance associated with each Local Moran‚Äôs I value.\n\n\n\n\n8.1.3 Visualising Local Moran‚Äôs Ii P-value\nAs mentioned in the section above, we shall not hastily conclude the clustering results observed. Instead, let us also evaluate whether the observed clustering (high-high or low-low) is statistically significant or could have occurred by chance. Hence, we can derive the p-values from Local Moran‚Äôs I by using the p_ii_sim field to determine statistical signficance across provinces.\n\n# Remove lisa record with 0 neighbours\nlisa_clean &lt;- lisa %&gt;% filter(!is.na(p_ii_sim))\n\ntm_shape(lisa_clean) +\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"green3\",\"lightyellow\",\"orange\",\"orange4\",\"red\"),\n          title = \"p-value\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistical Signifance of Spatial Autocorrelation\\n of Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.position = c(\"right\",\"bottom\"),\n            asp = 1.1,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.2)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the map above, not every province exhibits statistically significant Local Moran I‚Äôs value (i.e.¬†p-value &lt; 0.05)\n\n\n\n\n8.1.4 Visualising Statistically Significant Local Moran‚Äôs Ii\nWith that said, I would like to switch our focus to provinces that display statistically significant local Moran‚Äôs I values. To execute this, I will attempt to remove all local Moran‚Äôs I values with p-values greater than 0.05. Subsequently, I will use the tmap function to plot the choropleth of statistically significant local spatial autocorrelation on the map of Thailand.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"skyblue4\",\"skyblue\",\"lightblue1\",\"yellow\",\n                      \"orange\",\"orange4\",\"red\"),\n          title = \"Local Moran's I (p-value &lt; 0.05)\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistical Signifance of Spatial Autocorrelation\\n of Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.position = c(\"right\",\"bottom\"),\n            asp = 1.1,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.2)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nPreviously, we mentioned that spatial autocorrelation of drug cases is widespread across provinces in Thailand, particularly in the provinces, Samut Prakan and Nakhon Si Thammarat. However, we can see that majority of provinces in Thailand is in fact not statistically significant. As such, we cannot conclude that most of these provinces are high-risk regions for the spread of drug cases.\nPotential Spill-over of Drug Cases in Central Thailand\n\nCentral provinces (in orange), Kamphaeng-Phet and Nakhon Sawan, suggests to have some levels of high drug clusters, as shown from its low local Moran‚Äôs I values ranging from 0.2 to 0.4.\nMoreover, a neighbouring province (in red) has an even higher local Moran‚Äôs I observed (0.6 to 0.8),indicating that it has the strongest association with its neighbours where both itself and its neighbours exhibit strong clustering of high number of drug cases.\nThis poses a significant risk of drug cases spreading into other neighbouring provinces without adequate law enforcements and control, since this region has already become concentrated with high drug activites cumulatively across the years.\nOn the other hand, a province located in further down in southern Thailand (dark blue) displayed high clustering of low drug cases, as observed from its strongly negative local Moran‚Äôs I of -0.6 to -0.4.\n\nOther interesting findings\nWe can also observe that the provinces in gray are not statistically signifcant enough, especially provinces which displayed negative autocorrelation (i.e.¬†&lt; 0 Moran‚Äôs I) previously. This means that a low-high spatial association is, in reality, not observable in the spread of drug cases in Thailand.\n\n\n\n\n8.1.5 Visualising Statistically Significant Local Moran‚Äôs Ii (By Drug Case)\nI will categorise the dataset by the following drug case type to ease my analysis of how varying types of drug offense might display spatial dependence/independence from one province to another.\n\nDrug Use Cases: Instances of individuals using drugs.\n\ndrug_use_cases\nsuspects_in_drug_use_cases\n\nPossession Cases: Instances where individuals are found with drugs and/or evidence suggesting intent to sell or distribute.\n\npossession_cases\nsuspects_in_possession_cases\npossession_with_intent_to_distribute_cases\nsuspects_in_possession_with_intent_to_distribute_cases\n\nTrafficking Cases: Instances related to the illegal trade of drugs.\n\ntrafficking_cases\nsuspects_in_trafficking_cases\n\nProduction Cases: Instances involving the manufacture or cultivation of drugs.\n\nproduction_cases\nsuspects_in_production_cases\n\nImport/Export Cases: Instances related to the illegal importation & exportation of drugs.\n\nimport_cases\nsuspects_in_import_cases\nexport_cases\nsuspects_in_export_cases\n\nConspiracy Cases: Instances where individuals conspire to commit drug-related offenses.\n\nconspiracy_cases\nsuspects_in_conspiracy_cases\n\n\n\n# Compute Moran I's by Drug Case\nlibrary(dplyr)\nlibrary(sf)\nlibrary(sfdep)\n\n# Function for Moran's I calculation after filtering\ncompute_morans_I &lt;- function(filtered_data) {\n  thailand_nb_q &lt;- st_contiguity(filtered_data$geometry, queen = TRUE)\n  thailand_wt &lt;- st_weights(thailand_nb_q, style = \"W\", allow_zero = TRUE)\n  \n  filtered_data &lt;- filtered_data %&gt;%\n    mutate(nb = thailand_nb_q, wt = thailand_wt) %&gt;%\n    mutate(local_moran = local_moran(total_cases, nb, wt, \n                                     zero.policy = TRUE,\n                                     nsim = 99)) %&gt;%\n    unnest(local_moran)\n  \n  return(filtered_data)\n}\n\n# General function to process different types of drug offenses\nprocess_cases &lt;- function(offense_types) {\n  drug_cases %&gt;%\n    filter(types_of_drug_offenses %in% offense_types) %&gt;%\n    group_by(province_en) %&gt;%\n    summarise(\n      total_cases = sum(no_cases),\n      ADM1_PCODE = first(ADM1_PCODE),\n      geometry = first(geometry),\n      .groups = \"drop\"\n    ) %&gt;%\n    st_as_sf() %&gt;%\n    compute_morans_I()\n}\n\n# Define offense types for each category\ndrug_use_offenses &lt;- c(\"drug_use_cases\", \"suspects_in_drug_use_cases\")\npossession_offenses &lt;- c(\"possession_cases\", \"suspects_in_possession_cases\", \n                         \"possession_with_intent_to_distribute_cases\", \n                         \"suspects_in_possession_with_intent_to_distribute_cases\")\ntrafficking_offenses &lt;- c(\"trafficking_cases\", \"suspects_in_trafficking_cases\")\nimport_export_offenses &lt;- c(\"import_cases\", \"suspects_in_import_cases\", \n                            \"export_cases\", \"suspects_in_export_cases\")\nconspiracy_offenses &lt;- c(\"conspiracy_cases\", \"suspects_in_conspiracy_cases\")\n\n# Process each type of offense\ndrug_use_cases &lt;- process_cases(drug_use_offenses)\npossession_cases &lt;- process_cases(possession_offenses)\ntrafficking_cases &lt;- process_cases(trafficking_offenses)\nimport_export_cases &lt;- process_cases(import_export_offenses)\nconspiracy_cases &lt;- process_cases(conspiracy_offenses)\n\n# For significant cases\nsignificant_cases &lt;- function(data) {\n  data %&gt;%\n    filter(p_ii_sim &lt; 0.05) %&gt;% mutate(label = paste(ADM1_PCODE, province_en))\n}\n\n# Filter significant cases for each offense type\ndrug_use_cases_sig &lt;- significant_cases(drug_use_cases)\npossession_cases_sig &lt;- significant_cases(possession_cases)\ntrafficking_cases_sig &lt;- significant_cases(trafficking_cases)\nimport_export_cases_sig &lt;- significant_cases(import_export_cases)\nconspiracy_cases_sig &lt;- significant_cases(conspiracy_cases)\n\n\n1) Case Type: Drug Use\n\n\nPlot Moran‚Äôs I and P-values\n# Moran I's (red to green)\ndrug_use_moran_i &lt;- tm_shape(drug_use_cases) +\n  tm_polygons(\"ii\", \n              palette = c(\"#FF0000\", \"#FF4D4D\", \"#FF6666\",\n                          \"#CCFFCC\", \"#66CC66\", \"#4CAF50\"),\n              title = \"Local Moran's I\",\n              midpoint = NA) +\n  tm_layout(main.title = \"Overall Spatial Autocorrelation\\nof Drug Cases (Drug Use)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.2)\n\n# p-values (red, orange, yellow, green)\ndrug_use_p_values &lt;- tm_shape(thai_boundary)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(drug_use_cases_sig) +\n  tm_polygons(\"p_ii_sim\", \n              palette = c(\"#d21b1c\",\"#ec9a64\",\"#E5D96E\",\"#c9e3d2\"),\n              title = \"p-value\",\n              midpoint = NA) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation\\nof Drug Cases (Drug Use)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.2)\n\ntmap_arrange(drug_use_moran_i, drug_use_p_values, asp=1, ncol=2)\n\n\n\n\n\nCodes to plot map for statistically significant Moran‚Äôs I values\ndrug_use_cases_sig &lt;- drug_use_cases_sig %&gt;%\n  mutate(ii_rounded = paste(province_en, \":\", round(ii, 3)))\n\ntm_shape(drug_use_cases) +\n  tm_polygons(id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6) +\n\n  tm_shape(drug_use_cases_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#E4F1EB\", \"#F5F3A6\", \"#E5D96E\",\n                      \"#ec9a64\", \"#d21b1c\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"ii_rounded\", \n          size = 0.4, \n          col = \"white\",\n          bg.color = \"black\",\n          bg.alpha = 0.6,  \n          auto.placement = TRUE, \n          just = \"right\",  \n          id = \"province_en\") + \n\n  # Adding layout elements for map styling\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\nof Drug Use Cases in Study Area\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.position = c(\"right\",\"bottom\"),\n            asp = 1.1,\n            frame = TRUE) +\n\n  # Adding a compass for orientation\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n\n  # Adding a scale bar for map accuracy\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n\n  # Adding grid with transparency\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nOverall Observations\nWhen it comes to cases where individuals were found using drugs illegally, we can see that majority of these provinces did not display statistically significant (p&lt;0.05) Local Moran I‚Äôs values.\nObservations of statistically significant values\nOn the higher range of Local Moran‚Äôs I values, we can see that Samut Prakan province in red (top right plot) displayed a strong p-value of 0.020 to 0.025 which corresponds with a high positive Local Moran‚Äôs I value of 0.7 to 0.8 (bottom plot). This indicates that the province is indeed distinctly part of a group of similar high-value neighbours, hinting at a pronounced hotspot.\nIn the same bottom plot, we see that adjacent to the red region mentioned above, is a province in light green. To me, this is interesting as it suggests that this province has weaker clustering effects, meaning that its location has slightly higher number of drug cases than its neighbours but not as high as the province in red. There is a possibility that the spread of drug cases has not spilled to its other neighbours.\n\n\n\n\n2) Case Type: Possession\n\n\nPlot Moran‚Äôs I and P-values\n# Moran I's (blue, green, light yellow, yellow, orange, red)\npossession_moran_i &lt;- tm_shape(possession_cases) +\n  tm_polygons(\"ii\", \n              palette = c(\"#E6F4FA\",\"#c9e3d2\", \"#E1ECBB\",\n                          \"#E5D96E\", \"#ec9a64\", \"#d21b1c\"),\n              title = \"Local Moran's I\",\n              midpoint = NA) +\n  tm_layout(main.title = \"Overall Spatial Autocorrelation\\nof Drug Cases (Possession)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.2)\n\n# p-values (red, orange, yellow, green)\npossession_p_values &lt;- tm_shape(thai_boundary)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(possession_cases_sig) +\n  tm_polygons(\"p_ii_sim\", \n              palette = c(\"#d21b1c\",\"#ec9a64\",\"#E5D96E\",\"#c9e3d2\"),\n              title = \"p-value\",\n              midpoint = NA) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation\\nof Drug Cases (Possession)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.2)\n\ntmap_arrange(possession_moran_i, possession_p_values, asp=1, ncol=2)\n\n\n\n\n\nCodes to plot map for statistically significant Moran‚Äôs I values\npossession_cases_sig &lt;- possession_cases_sig %&gt;%\n  mutate(ii_rounded = paste(province_en, \":\", round(ii, 3)))\n\ntm_shape(possession_cases) +\n  tm_polygons(id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6) +\n\n  tm_shape(possession_cases_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#E4F1EB\", \"#F5F3A6\", \"#E5D96E\",\n                      \"#ec9a64\", \"#d21b1c\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"ii_rounded\", \n          size = 0.4, \n          col = \"white\",\n          bg.color = \"black\",\n          bg.alpha = 0.6,  \n          just = \"right\",  \n          id = \"province_en\") + \n\n  # Adding layout elements for map styling\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\nof Drug Possession Cases in Study Area\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.position = c(\"right\",\"bottom\"),\n            asp = 1.1,\n            frame = TRUE) +\n\n  # Adding a compass for orientation\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n\n  # Adding a scale bar for map accuracy\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n\n  # Adding grid with transparency\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nObservations of statistically significant values\nFor drug cases found to be in possession and/or with the intent of distributing drugs, there are high levels of clustering of such cases in provinces located at the West of Thailand as observed from their higher Local Moran I‚Äôs values (in orange & red regions), ranging from 0.2 to 0.6.\nInterestingly, the Nakhon Pathom province displayed a low Local Moran‚Äôs I value of -0.247, meaning it has significantly lower number of drug cases than its neighbours (light green). Ironically, it is situated beside the Nonthaburi province (red) which has a high Local Moran‚Äôs I value of 0.464, indicating that Nakhon Pathom province is not part of the significantly high clustering of drug cases observed by its neighbour. This could signal signs of boundary effects where there is no spill-over effect found in Nakkon Pathom.\n\n\n\n\n3) Case Type: Trafficking\n\n\nPlot Moran‚Äôs I and P-values\n# Moran I's (blue, green, yellow, orange, red)\ntrafficking_moran_i &lt;- tm_shape(trafficking_cases) +\n  tm_polygons(\"ii\", \n              palette = c(\"#E6F4FA\",\"#c9e3d2\",\n                          \"#E5D96E\", \"#ec9a64\", \"#d21b1c\"),\n              title = \"Local Moran's I\",\n              midpoint = NA) +\n  tm_layout(main.title = \"Overall Spatial Autocorrelation\\nof Drug Cases (Trafficking)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.2)\n\n# p-values (red, orange, yellow, green)\ntrafficking_p_values &lt;- tm_shape(thai_boundary)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(trafficking_cases_sig) +\n  tm_polygons(\"p_ii_sim\", \n              palette = c(\"#d21b1c\",\"#ec9a64\",\"#E5D96E\",\"#c9e3d2\"),\n              title = \"p-value\",\n              midpoint = NA) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation\\nof Drug Cases (Trafficking)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.2)\n\ntmap_arrange(trafficking_moran_i, trafficking_p_values, asp=1, ncol=2)\n\n\n\n\n\nCodes to plot map for statistically significant\ntrafficking_cases_sig &lt;- trafficking_cases_sig %&gt;%\n  mutate(ii_rounded = paste(province_en, \":\", round(ii, 3)))\n\ntm_shape(trafficking_cases) +\n  tm_polygons(id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6) +\n\n  tm_shape(trafficking_cases_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#E6F4FA\", \"#c9e3d2\", \"#ec9a64\", \"#d21b1c\"),\n          breaks = c(-1, 0, 1, 2, 3),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6) +\n\n  # Adding text labels for the 'ii' values over the provinces with a black background\n  tm_text(\"ii_rounded\", \n          size = 0.4, \n          col = \"white\",\n          bg.color = \"black\",\n          bg.alpha = 0.6,  \n          just = \"right\",  \n          id = \"province_en\") + \n\n  # Adding layout elements for map styling\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\nof Trafficking Cases in Study Area\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.position = c(\"right\",\"bottom\"),\n            asp = 1.1,\n            frame = TRUE) +\n\n  # Adding a compass for orientation\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n\n  # Adding a scale bar for map accuracy\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n\n  # Adding grid with transparency\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nObservations of statistically significant values\nLikewise, we see some statistically significant observations in the extreme south of Thailand (light green) in the Songkhla province which appears to have slightly higher number of drug trafficking cases than its neighbours.\nSongkhla is a key transportation hub with its capital Hat Yai having strong transportation links by road, rail, and air, making it a strategic point for drug traffickers to move illicit drugs. However, we aren‚Äôt sure where these drugs are illegally traded to/from since Moran‚Äôs I is relatively low, meaning there is low spatial dependence with its neighbours.\nSi Sa Ket province (in red) suggests to have significant clustering of drug cases with high Local Moran I‚Äôs values of 2.26. The province has high number of drugs being trafficked and is likewise surrounded by neighbouring provinces with similarly high values, suggesting spatial dependence.\n\n\n\n\n4) Case Type: Import and Export\n\n\nPlot Moran‚Äôs I and P-values\n# Moran I's (blue, green, yellow, orange, red)\nimport_export_moran_i &lt;- tm_shape(import_export_cases) +\n  tm_polygons(\"ii\", \n              palette = c(\"#E6F4FA\",\"#c9e3d2\",\n                          \"#E5D96E\", \"#ec9a64\", \"#d21b1c\"),\n              title = \"Local Moran's I\",\n              midpoint = NA) +\n  tm_layout(main.title = \"Overall Spatial Autocorrelation\\nof Drug Cases (Import & Export)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.2)\n\n# p-values (red, orange, yellow, green)\nimport_export_p_values &lt;- tm_shape(thai_boundary)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(import_export_cases_sig) +\n  tm_polygons(\"p_ii_sim\", \n              palette = c(\"#d21b1c\",\"#ec9a64\",\"#E5D96E\",\"#c9e3d2\"),\n              title = \"p-value\",\n              midpoint = NA) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation\\nof Drug Cases (Import & Export)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.2)\n\ntmap_arrange(import_export_moran_i, import_export_p_values, asp=1, ncol=2)\n\n\n\n\n\nCodes to plot map for statistically significant Moran‚Äôs I values\nimport_export_cases_sig &lt;- import_export_cases_sig %&gt;%\n  mutate(ii_rounded = paste(province_en, \":\", round(ii, 3)))\n\ntm_shape(import_export_cases) +\n  tm_polygons(id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6) +\n\n  tm_shape(import_export_cases_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#E4F1EB\",\"#E5D96E\",\"#ec9a64\",\"#d21b1c\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"ii_rounded\", \n          size = 0.4, \n          col = \"white\",\n          bg.color = \"black\",\n          bg.alpha = 0.6,  \n          just = \"right\",  \n          id = \"province_en\") + \n\n  # Adding layout elements for map styling\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\nof Import & Export Cases in Study Area\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.position = c(\"right\",\"bottom\"),\n            asp = 1.1,\n            frame = TRUE) +\n\n  # Adding a compass for orientation\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n\n  # Adding a scale bar for map accuracy\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n\n  # Adding grid with transparency\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nObservations of statistically significant values\nWe can observe another interesting occurrence of two provinces, Satun (red) and Phatthalung (blue) being geographically close, but showing very different importing and exporting of drug patterns where the former has a high positive Moran I‚Äôs value (6 to 8) while the latter has a negative observation (-2 to 0). This contrast highlights a spatial disparity where one province (Phatthalung) has significantly fewer import/export of drugs, while its neighbour (Satun) is part of a import/export drug hotspot.\n\n\n\n\n5) Case Type: Conspiracy\n\n\nPlot Moran‚Äôs I and P-values\ntmap_mode(\"plot\")\n# Moran I's (blue, green, light yellow, yellow, orange, orangered, red)\nconspiracy_moran_i &lt;- tm_shape(conspiracy_cases) +\n  tm_polygons(\"ii\", \n              palette = c(\"#E6F4FA\",\"green2\",\"#c9e3d2\", \"#E1ECBB\",\n                          \"#E5D96E\", \"#ec9a64\",\"#d21b1c\"),\n              title = \"Local Moran's I\",\n              midpoint = NA) +\n  tm_layout(main.title = \"Overall Spatial Autocorrelation\\nof Drug Cases (Conspiracy)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.2)\n\n# p-values (red, orange, yellow, green)\nconspiracy_p_values &lt;- tm_shape(thai_boundary)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(conspiracy_cases_sig) +\n  tm_polygons(\"p_ii_sim\", \n              palette = c(\"#d21b1c\",\"#ec9a64\",\"#E5D96E\",\"#c9e3d2\"),\n              title = \"p-value\",\n              midpoint = NA) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation\\nof Drug Cases (Conspiracy)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.2)\n\ntmap_arrange(conspiracy_moran_i, conspiracy_p_values, asp=1, ncol=2)\n\n\n\n\n\nCodes to plot map for statistically significant Moran‚Äôs I values\nconspiracy_cases_sig &lt;- conspiracy_cases_sig %&gt;%\n  mutate(ii_rounded = paste(province_en, \":\", round(ii, 3)))\n\ntm_shape(conspiracy_cases) +\n  tm_polygons(id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6) +\n\n  tm_shape(conspiracy_cases_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#c9e3d2\", \"#E1ECBB\", \"#E5D96E\", \n                        \"#ec9a64\", \"orangered\", \"#d21b1c\"),\n          title = \"Local Moran's I (p &lt; 0.05)\",\n          midpoint = NA,\n          id = \"label\") +\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"ii_rounded\", \n          size = 0.4, \n          col = \"white\",\n          bg.color = \"black\",\n          bg.alpha = 0.6,  \n          auto.placement = TRUE, \n          just = \"right\",  \n          id = \"province_en\") + \n\n  # Adding layout elements for map styling\n  tm_layout(main.title = \"Statistical Significance of Spatial Autocorrelation\\nof Conspiracy Cases in Study Area\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.position = c(\"right\",\"bottom\"),\n            asp = 1.1,\n            frame = TRUE) +\n\n  # Adding a compass for orientation\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n\n  # Adding a scale bar for map accuracy\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n\n  # Adding grid with transparency\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nObservations of statistically significant values\nCases where individuals conspire to commit drug-related offenses produces a wider spread of statistically significant observations. In North-Western Thailand, provinces like Chang Mai, Lampang and Phayao (green) showed slightly higher number of conspiracy cases than its neighbours, with little spatial dependence on its neighbouring provinces.\nOn the other hand, provinces like Bangkok and Samut Prakan (orange & red) were found adjacent to each other and displayed high spatial dependence with its neighbours where both provinces are part of clusters with similarly high number of such conspiracy drug cases. What‚Äôs interesting is these provinces are situated beside provinces which displayed low spatiald dependence, e.g.¬†Nonthaburi, Pathum Thani and Chacheongsao (light green).\nThis reflects spatial heterogeneity, where we have areas of intense clustering of high drug cases (orange & red) adjacent to areas of intense clustering of low values. (light green)\n\n\nContinue to Part 2 &gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02_Part2.html",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02_Part2.html",
    "title": "Take-home Exercise 2 - Part 2",
    "section": "",
    "text": "&lt; Back to Part 1\npacman::p_load(sf, st, tidyverse, lubridate, sfdep, tmap, ggplot2, knitr, Kendall)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02_Part2.html#emerging-hot-spot-analysis-ehsa",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02_Part2.html#emerging-hot-spot-analysis-ehsa",
    "title": "Take-home Exercise 2 - Part 2",
    "section": "9. Emerging Hot Spot Analysis (EHSA)",
    "text": "9. Emerging Hot Spot Analysis (EHSA)\nAs mentioned in the Methods Used section, Emerging Hot Spot Analysis is a useful technique to identify trends and changes in spatial patterns over time, particularly for identifying evolving intensifying or diminishing clusters (hot spots or cold spots) over time.\nThe main idea of ESHA is that it integrates the Getis-Ord Gi* statistic to identify hotspots with the Mann-Kendall test to assess trends over time. In our study, we will begin by calculating the Getis-Ord Gi* statistic, followed by the Mann-Kendall trend test. Finally, we will perform the EHSA to complete the analysis.\n\n9.1 Local Getis-Ord Gi* for Hot Spot and Cold Spot Area Analysis (HSCA)\n\n9.1.1 A Brief Introduction\nThe concept of hot spot analysis originated in the field of spatial statistics during the late 20th century, with contributions from statisticians such as Arthur Getis and J. Keith Ord, who developed the Getis-Ord G and G* statistics in the early 1990s.\nThe Gi* statistic is calculated as the ratio of the weighted average of values from neighboring locations to the total sum of all values, excluding the value at the specific location in question. In contrast, the G statistic includes the value from the focal (or self) location within the neighborhood. Both G and Gi* statistics employ a distance-based approach for spatial weights, meaning that the spatial weights identify statistically significant hot and cold spots by evaluating nearby locations within a defined distance.\n\n\n9.1.2 Compute Spatial Weight Matrix\nWith that said, I will employ the the local Gi* statistics but before we compute it, we need to calculate the spatial weight matrix. To do so, I will employ st_contiguity() to create a neighbour list and use¬†include_self()¬†to include the focal observation in the neighbour list. Next, we use the neighbour list to create a weight list using¬†st_inverse_distance()¬†function.\n1) For Drug Cases by Province\n\nwm_idw_province &lt;- drug_cases_province %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n2) For Drug Cases by Province and Year\n\nwm_idw_year &lt;- drug_cases_province_year %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n3) For Drug Cases by Province and Case Type\n\n\nCode\ncompute_spatial_weight &lt;- function(filtered_data) {\n  spatial_weight &lt;- filtered_data %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n  \n  return(spatial_weight)\n}\n\n# Filter by offense type and then group by province_en\ndrug_use_cases_gi &lt;- drug_cases %&gt;%\n  filter(types_of_drug_offenses %in% c(\"drug_use_cases\", \"suspects_in_drug_use_cases\")) %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases),\n    ADM1_PCODE = first(ADM1_PCODE),\n    geometry = first(geometry),\n    .groups = \"drop\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  compute_spatial_weight()\n\npossession_cases_gi &lt;- drug_cases %&gt;%\n  filter(types_of_drug_offenses %in% c(\"possession_cases\", \"suspects_in_possession_cases\", \n                                       \"possession_with_intent_to_distribute_cases\", \n                                       \"suspects_in_possession_with_intent_to_distribute_cases\")) %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases),\n    ADM1_PCODE = first(ADM1_PCODE),\n    geometry = first(geometry),\n    .groups = \"drop\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  compute_spatial_weight()\n\ntrafficking_cases_gi &lt;- drug_cases %&gt;%\n  filter(types_of_drug_offenses %in% c(\"trafficking_cases\", \"suspects_in_trafficking_cases\")) %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases),\n    ADM1_PCODE = first(ADM1_PCODE),\n    geometry = first(geometry),\n    .groups = \"drop\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  compute_spatial_weight()\n\nimport_export_cases_gi &lt;- drug_cases %&gt;%\n  filter(types_of_drug_offenses %in% c(\"import_cases\", \"suspects_in_import_cases\", \n                                       \"export_cases\", \"suspects_in_export_cases\")) %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases),\n    ADM1_PCODE = first(ADM1_PCODE),\n    geometry = first(geometry),\n    .groups = \"drop\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  compute_spatial_weight()\n\nconspiracy_cases_gi &lt;- drug_cases %&gt;%\n  filter(types_of_drug_offenses %in% c(\"conspiracy_cases\", \"suspects_in_conspiracy_cases\")) %&gt;%\n  group_by(province_en) %&gt;%\n  summarise(\n    total_cases = sum(no_cases),\n    ADM1_PCODE = first(ADM1_PCODE),\n    geometry = first(geometry),\n    .groups = \"drop\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  compute_spatial_weight()\n\n\n\n\n9.1.3 Compute Local Gi* Statistics\nNext, we will compute the local Gi* statistic using the local_gstart_perm() function from the sfdep package. This function takes a neighbor list (nb) and a weight list (wt) as inputs and generates Gi* statistics through a Monte Carlo permutation with a specified number of simulations (nsim). The results will be saved into a new object named HCSA__province and HCSA_year.\n1) For Drug Cases by Province\n\nHCSA_province &lt;- wm_idw_province %&gt;% \n  mutate(local_Gi_star = local_gstar_perm(\n    total_cases, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi_star)\n\n2) For Drug Cases by Province and Year\n\nHCSA_year &lt;- wm_idw_year %&gt;% \n  mutate(local_Gi_star = local_gstar_perm(\n    total_cases, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi_star)\n\n3) For Drug Cases by Province and Case Type\n\ncompute_gi_star &lt;- function(filtered_data) {\n  gi_star_results &lt;- filtered_data %&gt;% \n                    mutate(local_Gi_star = local_gstar_perm(\n                      total_cases, nb, wt, nsim = 99),\n                           .before = 1) %&gt;%\n                      unnest(local_Gi_star) %&gt;%\n                      filter(p_sim &lt; 0.05) %&gt;% \n                      mutate(label = paste(ADM1_PCODE, province_en))\n    \n  return(gi_star_results)\n}\n\n# Filter by offense type and then group by province_en\ndrug_use_cases_gi_sig &lt;- drug_use_cases_gi %&gt;%\n  compute_gi_star()\n\npossession_cases_gi_sig &lt;- possession_cases_gi %&gt;%\n  compute_gi_star()\n\ntrafficking_cases_gi_sig &lt;- trafficking_cases_gi %&gt;%\n  compute_gi_star() \n\nimport_export_cases_gi_sig &lt;- import_export_cases_gi %&gt;%\n  compute_gi_star()\n\nconspiracy_cases_gi_sig &lt;- conspiracy_cases_gi %&gt;%\n  compute_gi_star()\n\n\n\n9.1.4 Create a New label Column\nSimilar to what we did in Moran I‚Äôs computations, we‚Äôll create a new label column to assist us with our visualisations later.\n\nHCSA_province &lt;- HCSA_province %&gt;%\n  mutate(label = paste(ADM1_PCODE, province_en))\n\n\nHCSA_year &lt;- HCSA_year %&gt;%\n  mutate(label = paste(ADM1_PCODE, province_en))\n\n\n\n\n9.2 Visualising Local Getis-Ord Gi* Results\n\n1) Overall Local Gi* Results\n\n\nPlot overall hotspots and coldspots\ntmap_mode(\"plot\")  \ngi_star_plot &lt;- tm_shape(HCSA_province)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, red\n          palette = c(\"#87CEFA\",\"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n          colorNA = \"#FFFFFF\",\n          title = \"Gi*\",\n          midpoint = 0) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Hotspots & Coldspots of Drug Cases\\n in Study Area (Using Gi*)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ncluster_plot &lt;- tm_shape(HCSA_province)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Hotspots & Coldspots of Drug Cases\\n in Study Area (Using Cluster Category)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(gi_star_plot, cluster_plot, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThere is a majority of negative local Gi* values (-2 to 0) found interspersed across the central and western provinces of Thailand (blue) which coincides with low cluster categories, suggesting that these are clusters of provinces that have well-controlled the spread of drug offenses.\nOn the other hand, out of all hotspots found on the right chart (yellow), majority of the positive Gi* values were found to be in the lower range (0 to 2). The implication here is that while these provinces are classified as hotspots, they aren‚Äôt showing statistically significant or intensely concentrated drug activity, especially compared to areas with Gi* values higher than 2. These provinces may still be of concern for law enforcement, but the clustering of drug cases isn‚Äôt as severe as in other regions.\nIn contrast, of all the high cluster categories (blue), there are four provinces in south-central Thailand (orange & red) marked with a higher Gi* value (i.e., &gt; 2) which indicates much stronger clustering of drug offenses, suggesting more serious or widespread drug-related issues in those locations.\n\n\n\n\n2) Statistically Significant Local Gi* Results\n\n# Retrieve statistically significant data\nHCSA_province_sig &lt;- HCSA_province %&gt;%\n  filter(p_sim &lt; 0.05)\n\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\ntm_shape(HCSA_province_sig)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, red\n          palette = c(\"#87CEFA\",\"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area (Using Gi*)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(HCSA_province_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area (Using Cluster Type)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nOf all statistically significant local Gi* observed (left chart), there are four provinces in the West of Thailand with negative Gi* values (blue) which coincides with coldspots areas (blue) on the right chart where low observations of drug offenses are spatially clustered.\nIn contrast, there are two provinces displaying positive Gi* values in south-central (red) and southern (orange) Thailand. The former (south-central) shows a statistically stronger clustering of high drug offenses compared to the latter (southern Thailand), where the clustering of high offenses is less significant.\nHowever, the province in the south of Thailand on the right chart tells a different story. Instead of high values, it shows a statistically significant clustering of low drug offenses. This indicates a coldspot, where drug offenses are less frequent than expected, signaling an area where drug-related activities are significantly lower compared to other regions. This could happen as the cluster type takes a broader spatial context into account.\n\n\n\n\n3) Top 3 Hotspots Observed\nIn the context of Getis-Ord Gi* statistics, hotspots and cold spots are generally identified based on the values of the Gi* statistic calculated for spatial data as such:\n\nHotspot: Gi* &gt; 2 (high concentration of high values)\nCold Spot: Gi* &lt; -2 (high concentration of low values)\n\nHowever, with reference to the histogram below, we can see that the spread of Gi* values range from -2 to 0 and 0 to 10, with only a few values exceeding 2. Hence, I will adjust the definitions accordingly.\n\nHotspot: Gi* &gt; 0 (high concentration of high values)\nCold Spot: Gi* &lt; 0 (high concentration of low values)\n\n\nggplot(HCSA_province, aes(x = gi_star)) + \n  geom_histogram(bins = 20, color = \"white\", fill = \"skyblue2\") + \n  labs(x = \"Gi*\", y = \"Count\", title = \"Histogram of Local Gi* Statistics\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")\n  )\n\n\nHence, let‚Äôs retrieve the three provinces with highest local Gi* values and lowest local Gi* values respectively. These provinces will highlight areas that need further investigation via the¬†Mann-Kendall Trend Test¬†later.\n\nset.seed(123)\n\n\nOverall HotspotsStatistically Significant Hotspots\n\n\nBy identifying all provinces with Gi* &gt; 0, we discovered the top 3 provinces with the greatest clustering of drug cases are found in Amnat Charoen, Bangkok and Buri Ram (red). In other words, these provinces showed to be the greatest hotspot for drug offenses to occur both locally and in its surrounding provinces. Such high concentration of drug crimes suggest a need for interventions of stricter law enforcement to curb the further spread of drug cases to its neighbours.\n\nthree_hotspots_overall &lt;- (head((HCSA_province[HCSA_province$gi_star &gt; 0,]), 3)$label)\nthree_hotspots_overall\n\n[1] \"TH37 Amnat Charoen\" \"TH10 Bangkok\"       \"TH31 Buri Ram\"     \n\n\n\n\nCode\nHCSA_three_hotspots_overall &lt;- HCSA_province %&gt;% filter(label %in% three_hotspots_overall)\n\ntmap_mode(\"plot\")\ntm_shape(thai_boundary) + \n  tm_borders(col = \"black\", lwd = 1) + \n  tm_fill(col = \"#F5F5F5\")+\ntm_shape(HCSA_three_hotspots_overall)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#ff6b6b\")) + # red\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"label\",size = 0.5, col = \"white\", just = c(\"RIGHT\",\"bottom\"),\n          bg.color = \"black\", bg.alpha = 0.7, fontface = \"bold\",\n          remove.overlap = TRUE) +\n  tm_layout(main.title = \"Top 3 Overall Hotspots\\nof Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\n\n\n\n\nWith that said, we can observe that statistically significant hotspots (i.e.¬†p-value &lt; 0.05) resulted in a different set of provinces, namely Nonthaburi, Phatthalung and Samut Prakan. As indicated in the red regions below, Nonthaburi and Samut Prakan provinces are relatively close to each other and are likely part of a larger cluster of drug hotspots with the potential risk of causing a spill-over effect to its neighbours.\n\nthree_hotspots_sig &lt;- (head((HCSA_province_sig[HCSA_province_sig$gi_star &gt; 0,]), 3)$label)\nthree_hotspots_sig\n\n[1] \"TH12 Nonthaburi\"   \"TH11 Samut Prakan\"\n\n\n\n\nCode\nHCSA_three_hotspots_sig &lt;- HCSA_province_sig %&gt;% filter(label %in% three_hotspots_sig)\n\ntmap_mode(\"plot\")\ntm_shape(thai_boundary) + \n  tm_borders(col = \"black\", lwd = 1) + \n  tm_fill(col = \"#F5F5F5\")+\ntm_shape(HCSA_three_hotspots_sig)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#ff6b6b\")) + # red\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"label\",size = 0.5, col = \"black\", just = c(\"left\",\"bottom\"),\n          fontface = \"bold\") +\n  tm_layout(main.title = \"Top 3 Most Statistically Significant Hotspots\\nof Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\n\n\n\n\n\n\n\n4) Top 3 Coldspots Observed\n\nOverall ColdspotsStatistically Significant Coldspots\n\n\nBy filtering all observations where Gi* &lt; 0, we identified that the top three provinces with the strongest signs of clustering of low number of drug cases are found in Ang Thong, Bueng Kan and Chai Nat provinces (blue). The provinces Ang Thong and Chai Nat are relatively close to each other which indicates a region where strong clusters of low number of drug cases are located together.\n\nthree_coldpots_overall &lt;- (head((HCSA_province[HCSA_province$gi_star &lt; 0,]), 3)$label)\nthree_coldpots_overall\n\n[1] \"TH15 Ang Thong\" \"TH38 Bueng Kan\" \"TH18 Chai Nat\" \n\n\n\n\nCode\nHCSA_three_coldpots_overall &lt;- HCSA_province %&gt;% filter(label %in% three_coldpots_overall)\n\ntmap_mode(\"plot\")\ntm_shape(thai_boundary) + \n  tm_borders(col = \"black\", lwd = 1) + \n  tm_fill(col = \"#F5F5F5\")+\ntm_shape(HCSA_three_coldpots_overall)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#87CEFA\")) + # blue\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"label\",size = 0.5, col = \"black\", just = c(\"right\",\"bottom\"),\n          fontface = \"bold\") +\n  tm_layout(main.title = \"Top 3 Overall Coldspots\\nof Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\n\n\n\n\nWe are more interested in the statistically significant coldspots (i.e., p-value &lt; 0.05) as we can more confidently say that these provinces are clusters of low number of drug offenses. In particular, the top three provinces with lowest Gi* values are Chai Nat, Kamphaeng Phet and Nakhon Sawan.\n\nthree_coldpots_sig &lt;- (head((HCSA_province_sig[HCSA_province_sig$gi_star &lt; 0,]), 3)$label)\nthree_coldpots_sig\n\n[1] \"TH18 Chai Nat\"       \"TH62 Kamphaeng Phet\" \"TH60 Nakhon Sawan\"  \n\n\n\n\nCode\nHCSA_three_coldspots_sig &lt;- HCSA_province_sig %&gt;% filter(label %in% three_coldpots_sig)\n\ntmap_mode(\"plot\")\ntm_shape(thai_boundary) + \n  tm_borders(col = \"black\", lwd = 1) + \n  tm_fill(col = \"#F5F5F5\")+\ntm_shape(HCSA_three_coldspots_sig)+\n  tm_fill(\"gi_star\", \n          palette = c(\"#87CEFA\")) + # blue\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_text(\"label\",size = 0.5, col = \"black\", just = c(\"center\",\"right\"),\n          fontface = \"bold\",remove.overlap = TRUE) +\n  tm_layout(main.title = \"Top 3 Most Statistically Significant Coldspots\\nof Drug Cases in Study Area (Thailand)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.show = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\n\n\n\n\n\n\n\n\n9.3 Visualising Local Getis-Ord Gi* by Fiscal Year\n\n1) Overall Local Gi* Results\n\n\nPlot animated yearly hot and cold spots\ntmap_mode(\"plot\")\nHCSA_maps_list &lt;- list()\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(HCSA_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- HCSA_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    HCSA_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  palette = c(\"#87CEFA\",\"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n                  colorNA = \"#FFFFFF\",\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Overall Hotspots & Coldspots of Drug Cases in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    HCSA_maps_list[[year]] &lt;- HCSA_map\n  }\n}\ntemporal_maps &lt;- HCSA_maps_list[!sapply(HCSA_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAcross 2017 to 2022, we see a consistent mapping of local Gi* values for each province, meaning there is no difference in the spatial distribution of drug offense hotspots (yellow, orange & red) and coldspots (blue) in Thailand during these periods.\nSince provinces identified as hotspots for drug offenses have not shifted over time, this could indicate persistent problems in certain provinces (orange & red), where drug-related activities remain high year after year.\nHence, the lack of variation in hotspots and coldspots may suggest that any interventions or law enforcement efforts aimed at reducing drug offenses have not had a substantial impact. Additionally, there could be underlying socio-economic or environmental factors that continue to perpetuate these drug offenses year on year.\n\n\n\n\n2) Statistically Significant Local Gi* Results\n\n\nPlot animated statistically significant yearly hot and cold spots\ntmap_mode(\"plot\")\nHCSA_year_sig &lt;- HCSA_year %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_maps_list &lt;- list()\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(HCSA_year_sig$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- HCSA_year_sig %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    HCSA_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  palette = c(\"#87CEFA\",\"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of Drug Cases in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    HCSA_maps_list[[year]] &lt;- HCSA_map\n  }\n}\ntemporal_maps &lt;- HCSA_maps_list[!sapply(HCSA_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/HCSA_Sig_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe statisitcally significant Gi* results (i.e.¬†p-value &lt; 0.05) show us similar spatial trends where there is low to no shifts in drug hotspots and coldspots in Thailand.\nOverall, the lack of significant change in Gi* values across the years underscores the ongoing challenges in managing and mitigating drug-related issues in Thailand, requiring sustained efforts to tackle these persistent hotspots.\n\n\n\n\n\n9.3 Visualising Local Getis-Ord Gi* by Drug Case Type\nWith that said, it will be more interesting to delve into the five drug case types previously discussed for a more nuanced analysis of drug hotspots and coldspots, measured by local Gi* values, across Thailand‚Äôs provinces. I will also delve into the spatio-temporal changes in patterns observed from 2017 to 2022.\n\n# Prepare local Gi* computations by case type and year\nsummarize_cases &lt;- function(filtered_data, offense_types) {\n  filtered_data %&gt;%\n    filter(types_of_drug_offenses %in% offense_types) %&gt;%\n    group_by(province_en, fiscal_year) %&gt;%\n    summarise(\n      total_cases = sum(no_cases),\n      ADM1_PCODE = first(ADM1_PCODE),\n      geometry = first(geometry),\n      .groups = \"drop\"\n    ) %&gt;%\n    st_as_sf() %&gt;%\n    compute_spatial_weight()\n}\n\n# General function to compute the Gi* statistic\ncompute_gi_star &lt;- function(filtered_data) {\n  filtered_data %&gt;%\n    mutate(local_Gi_star = local_gstar_perm(total_cases, nb, wt, nsim = 99), .before = 1) %&gt;%\n    unnest(local_Gi_star) %&gt;%\n    filter(p_sim &lt; 0.05) %&gt;%\n    mutate(label = paste(ADM1_PCODE, province_en))\n}\n\n# Offense types categories\noffense_types_list &lt;- list(\n  drug_use = c(\"drug_use_cases\", \"suspects_in_drug_use_cases\"),\n  possession = c(\"possession_cases\", \"suspects_in_possession_cases\", \n                 \"possession_with_intent_to_distribute_cases\", \n                 \"suspects_in_possession_with_intent_to_distribute_cases\"),\n  trafficking = c(\"trafficking_cases\", \"suspects_in_trafficking_cases\"),\n  import_export = c(\"import_cases\", \"suspects_in_import_cases\", \n                    \"export_cases\", \"suspects_in_export_cases\"),\n  conspiracy = c(\"conspiracy_cases\", \"suspects_in_conspiracy_cases\")\n)\n\n# Apply the summarize_cases and compute_gi_star functions for each offense type\ngi_star_results_list &lt;- lapply(offense_types_list, function(offense_types) {\n  summarize_cases(drug_cases, offense_types) %&gt;% compute_gi_star()\n})\n\n# Access results for each offense type\ndrug_use_cases_gi_sig_year &lt;- gi_star_results_list$drug_use\npossession_cases_gi_sig_year &lt;- gi_star_results_list$possession\ntrafficking_cases_gi_sig_year &lt;- gi_star_results_list$trafficking\nimport_export_cases_gi_sig_year &lt;- gi_star_results_list$import_export\nconspiracy_cases_gi_sig_year &lt;- gi_star_results_list$conspiracy\n\n\n1) Case Type: Drug Use\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\ntm_shape(drug_use_cases_gi_sig)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, red\n          palette = c(\"#87CEFA\",\"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Gi* (Drug Use)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(drug_use_cases_gi_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Cluster (Drug Use)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\nPlot animated yearly drug use cases\ntmap_mode(\"plot\")\ndrug_use_maps_list &lt;- list()\n\n# Define the case type for the title\ncase_type &lt;- \"Drug Use Cases\"\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(drug_use_cases_gi_sig_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- drug_use_cases_gi_sig_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    drug_use_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\") +\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  palette = c(\"#87CEFA\", \"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of\", case_type, \"in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    drug_use_maps_list[[year]] &lt;- drug_use_map\n  }\n}\n\n# Filter out null maps and create the animation\ntemporal_maps &lt;- drug_use_maps_list[!sapply(drug_use_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/Drug_Use_HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom 2017 to 2022, there are consistently negative significant Gi* values observed in the western provinces and a province found in the extreme south (blue). These provinces have proved to be effective in their control of drug use cases as they persist as regions of coldspots. Similarly, there is a strong presence of statistically significant hotspots that has formed near the boundary of Thailand (orange & red) which is worrying as this shows no improvement in curbing the spread of drug use cases. In other words, these provinces continue to be concentrated with high drug use activities.\n\n\n\n\n2) Case Type: Possession\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n  tm_borders(col = \"black\", lwd = 1) + \n  tm_fill(col = \"#F5F5F5\") +\n\n  tm_shape(possession_cases_gi_sig) +\n  tm_fill(\"gi_star\", \n          palette = c(\"#87CEFA\", \"#4682B4\"),  # Light blue and darker blue\n          breaks = c(-Inf, -2, -1.5, 0),      \n          title = \"Gi*\",\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n\n  tm_borders(col = \"black\", alpha = 0.6) +\n  \n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Gi* (Possession)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.5, alpha = 0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(possession_cases_gi_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Cluster (Possession)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\nPlot animated yearly possession cases\ntmap_mode(\"plot\")\npossession_maps_list &lt;- list()\n\n# Define the case type for the title\ncase_type &lt;- \"Possession Cases\"\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(possession_cases_gi_sig_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- possession_cases_gi_sig_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    possession_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\") +\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  palette = c(\"#87CEFA\", \"#fcd34d\", \"#f7a87d\", \"#ff6b6b\"),\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of\", case_type, \"in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    possession_maps_list[[year]] &lt;- possession_map\n  }\n}\n\n# Filter out null maps and create the animation\ntemporal_maps &lt;- possession_maps_list[!sapply(possession_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/Possession_HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWhen it comes to drug possession cases, it follows the general trend where coldspots (i.e.¬†low negative Gi* values) of drug possession offenses persist in the Western provinces of Thailand (blue) while hotspots for drug possession activities are more concentrated in the Southern provinces (orange) with the highest offenses found in a south-central province (red).\nLikewise, the stability of Gi* values across each year indicates that the spatial distribution of drug offenses remains relatively unchanged over time. This consistency may imply that the underlying factors contributing to the clustering of offenses are persistent. Similarly, areas that consistently exhibit negative Gi* values are coldspots, indicating lower spread and incidences of drug offenses.\n\n\n\n\n3) Case Type: Trafficking\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\ntm_shape(trafficking_cases_gi_sig)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, orange-red, red\n          palette = c(\"#87CEFA\",\"#fcd34d\",\"#f7a87d\",\"#f88379\", \"#ff6b6b\"),\n          breaks = c(-2, 0, 1, 2, 3, 4),   \n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Gi* (Trafficking)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(trafficking_cases_gi_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Cluster (Trafficking)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\nPlot animated yearly trafficking cases\ntmap_mode(\"plot\")\ntrafficking_maps_list &lt;- list()\n\n# Define the case type for the title\ncase_type &lt;- \"Trafficking Cases\"\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(trafficking_cases_gi_sig_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- trafficking_cases_gi_sig_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    trafficking_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\") +\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  # dark blue, blue, light yellow,\n                  # yellow, orange, orange-red, red\n                  palette = c(\"skyblue2\",\"#87CEFA\",\"lightyellow\",\"#fcd34d\",\n                              \"#f7a87d\", \"#f88379\", \"#ff6b6b\"),\n                  breaks = c(-5,-2.5,0,2.5,5,7.5,10),  \n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of\", case_type, \"in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    trafficking_maps_list[[year]] &lt;- trafficking_map\n  }\n}\n\n# Filter out null maps and create the animation\ntemporal_maps &lt;- trafficking_maps_list[!sapply(trafficking_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/Trafficking_HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThroughout 2017 to 2022, the western provinces (dark blue) has stayed relatively consistent as a stiatistically significant coldspot, which are great signs of low trafficking of drugs. However, there are two provinces in North-Western and Eastern Thailand (red) that displayed consistently high positive Gi* values across multiple years. These locations may require targeted interventions especially along maritime routes or other transport routes to address the ongoing issue.\n\n\n\n\n4) Case Type: Import and Export\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\ntm_shape(import_export_cases_gi_sig)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, orange-red, red\n          palette = c(\"#87CEFA\",\"#fcd34d\",\"#f7a87d\",\"#f88379\", \"#ff6b6b\"),\n          breaks = c(-2, 0, 2, 4, 6, 8),\n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Gi* (Import & Export)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(import_export_cases_gi_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Cluster (Import & Export)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\nPlot animated yearly import & export cases\ntmap_mode(\"plot\")\nimport_export_maps_list &lt;- list()\n\n# Define the case type for the title\ncase_type &lt;- \"Import & Export Cases\"\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(import_export_cases_gi_sig_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- import_export_cases_gi_sig_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    import_export_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\") +\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  # blue, yellow, orange, red\n                  palette = c(\"#87CEFA\",\"#fcd34d\",\"#f7a87d\", \"#ff6b6b\"),\n                  breaks = c(-4, 0, 4, 8, 12),\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of\", case_type, \"in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    import_export_maps_list[[year]] &lt;- import_export_map\n  }\n}\n\n# Filter out null maps and create the animation\ntemporal_maps &lt;- import_export_maps_list[!sapply(import_export_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/Import_Export_HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nInterestingly, the plots reveal that majority of provinces, particularly in central and western Thailand, indicated consistently low import and export drug activities (blue). However, from 2018 to 2021, there is a spill over of import and exprt drug cases into provinces in the southern part of Thailand, meaning there are increased levels of drug imports and exports into these provinces and its surrounding provinces. Strong road, rail and air transport links can allow for high volumes of legitimate cross-border trade which could easily conceal illegal drug operations within normal trading activities.\nAnother interesting observation is shown in how some coldspots are located adjacent to mild hotspot regions, especially in 2017 and 2022. This contrast could be explained by stricter law enforcement in one province over another, and could also be due to other underlying socio‚Äìeconomic factors.\n\n\n\n\n5) Case Type: Conspiracy\n\n\nPlot statistically significant hotspots and coldspots\ntmap_mode(\"plot\")\nsig_gi_star_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\")+\ntm_shape(conspiracy_cases_gi_sig)+\n  tm_fill(\"gi_star\", \n          # blue, yellow, orange, red\n          palette = c(\"#87CEFA\",\"#fcd34d\",\"#f7a87d\",\"#f88379\", \"#ff6b6b\"),\n          title = \"Gi*\",\n          midpoint = 0,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Gi* (Conspiracy)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\nsig_cluster_plot &lt;- tm_shape(thai_boundary) + \n      tm_borders(alpha = 0.6) + \n      tm_fill()+\ntm_shape(conspiracy_cases_gi_sig)+\n  tm_fill(\"cluster\", \n          title = \"Cluster Category\") +\n  tm_borders(alpha = 0.6)+\n  tm_layout(main.title = \"Statistically Significant Hotspots & Coldspots\\nof Drug Cases in Study Area by Cluster (Conspiracy)\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.outside = FALSE,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", text.size = 0.7, size = 2, position=c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position=c(\"LEFT\", \"BOTTOM\"), text.size=0.5) +\n  tm_grid(labels.size = 0.5, alpha =0.2)\n\ntmap_arrange(sig_gi_star_plot, sig_cluster_plot, asp=1, ncol=2)\n\n\n\n\n\nPlot animated yearly conspiracy cases\ntmap_mode(\"plot\")\nconspiracy_maps_list &lt;- list()\n\n# Define the case type for the title\ncase_type &lt;- \"Conspiracy Cases\"\n\n# Loop over each unique fiscal year to create maps\nfor (year in unique(conspiracy_cases_gi_sig_year$fiscal_year)) {\n  # Filter the data for the current year\n  year_data &lt;- conspiracy_cases_gi_sig_year %&gt;% filter(fiscal_year == year)\n  \n  # Only create the map if there are significant results\n  if (nrow(year_data) &gt; 0) {\n    conspiracy_map &lt;- tm_shape(thai_boundary) + \n      tm_borders(col = \"black\", lwd = 1) + \n      tm_fill(col = \"#F5F5F5\") +\n      tm_shape(year_data) + \n      tm_polygons(\"gi_star\", \n                  # blue, yellow, orange, red\n                  palette = c(\"#87CEFA\",\"#fcd34d\",\"#f7a87d\", \"#ff6b6b\"),\n                  breaks = c(-4, 0, 4, 8, 12),\n                  title = \"Statistically Significant Gi*\",\n                  midpoint = NA,\n                  legend.hist = TRUE, \n                  legend.is.portrait = TRUE,\n                  legend.hist.z = 0.1) +\n      tm_layout(main.title = paste(\"Statistically Significant Hotspots & Coldspots of\", case_type, \"in Thailand -\", year),\n                main.title.position = \"center\",\n                main.title.size = 1,\n                main.title.fontface = \"bold\",\n                legend.title.size = 1,\n                legend.text.size = 1,\n                legend.hist.size = 1,\n                legend.outside = TRUE,\n                legend.outside.position = \"right\",\n                frame = TRUE) +\n      tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n      tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 1) +\n      tm_grid(alpha = 0.2)\n\n    # Store the significant map in the list\n    conspiracy_maps_list[[year]] &lt;- conspiracy_map\n  }\n}\n\n# Filter out null maps and create the animation\ntemporal_maps &lt;- conspiracy_maps_list[!sapply(conspiracy_maps_list, is.null)]\ntmap_animation(temporal_maps, filename = \"images/Conspiracy_HCSA_Year.gif\", delay = 150, width = 1090, height = 674)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAcross all years, drug-related conspiracy cases seem to be the least widespread as compared to other drug case types.\nConspiracy laws in Thailand require proof of intent and agreement between two or more parties to commit an illegal act. However, proving such intent in court requires more than just suspicion, which can make conspiracy cases more challenging than straightforward drug offenses, like possession or trafficking.\nHence, it is not surprising that conspiracy cases are more localised to certain provinces, e.g.¬†south-central region (red), and with little to no improvements/worsening of drug cases across each year. These provinces coincides with the high cluster category (yellow) observed above. However, the complexity of uncovering and prosecuting drug conspiracy cases in Thailand may mean drug conspiracy cases can be more under-represented than we expect.\n\n\n\n\n\n9.4 Local Getis-Ord Gi* for Each Fiscal Year\nIn this section, I will focus on preparing data that delves into the yearly drug cases found at the province level which will provide us insights into the spatial and temporal aspects of drug offenses in Thailand‚Äôs provinces. I will employ the drug_cases_spt spacetime object we previously created.\n\ndrugs_nb &lt;- drug_cases_spt %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n    set_nbs(\"nb\") %&gt;%\n    set_wts(\"wt\")\n\n\nhead(drugs_nb)\n\nspacetime ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nContext:`data`\n\n\n77 locations `ADM1_PCODE`\n\n\n6 time periods `fiscal_year`\n\n\n‚îÄ‚îÄ data context ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n# A tibble: 6 √ó 6\n  ADM1_PCODE province_en              fiscal_year total_cases nb        wt    \n  &lt;chr&gt;      &lt;chr&gt;                          &lt;dbl&gt;       &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;\n1 TH10       Bangkok                         2017       60067 &lt;int [7]&gt; &lt;dbl&gt; \n2 TH11       Samut Prakan                    2017       12452 &lt;int [3]&gt; &lt;dbl&gt; \n3 TH12       Nonthaburi                      2017        7348 &lt;int [5]&gt; &lt;dbl&gt; \n4 TH13       Pathum Thani                    2017        7616 &lt;int [7]&gt; &lt;dbl&gt; \n5 TH14       Phra Nakhon Si Ayutthaya        2017        6221 &lt;int [8]&gt; &lt;dbl&gt; \n6 TH15       Ang Thong                       2017        1614 &lt;int [5]&gt; &lt;dbl&gt; \n\n\nNext, I will harness group_by() to group our data by the fiscal_year variable since we are interested in understanding the spatio-temporal dynamics of drug cases across each year from 2017 to 2022.\n\ngi_stars_year &lt;- drugs_nb %&gt;%\n  group_by(fiscal_year) %&gt;%\n  mutate(gi_star = local_gstar_perm(total_cases, nb, wt)) %&gt;%\n  unnest(gi_star)\n\n\nhead(gi_stars_year)\n\n# A tibble: 6 √ó 16\n# Groups:   fiscal_year [1]\n  ADM1_PCODE province_en     fiscal_year total_cases nb    wt    gi_star cluster\n  &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt; &lt;lis&gt; &lt;lis&gt;   &lt;dbl&gt; &lt;fct&gt;  \n1 TH10       Bangkok                2017       60067 &lt;int&gt; &lt;dbl&gt;   5.47  High   \n2 TH11       Samut Prakan           2017       12452 &lt;int&gt; &lt;dbl&gt;   4.43  High   \n3 TH12       Nonthaburi             2017        7348 &lt;int&gt; &lt;dbl&gt;   3.00  High   \n4 TH13       Pathum Thani           2017        7616 &lt;int&gt; &lt;dbl&gt;   2.45  High   \n5 TH14       Phra Nakhon Si‚Ä¶        2017        6221 &lt;int&gt; &lt;dbl&gt;  -0.504 Low    \n6 TH15       Ang Thong              2017        1614 &lt;int&gt; &lt;dbl&gt;  -1.10  Low    \n# ‚Ñπ 8 more variables: e_gi &lt;dbl&gt;, var_gi &lt;dbl&gt;, std_dev &lt;dbl&gt;, p_value &lt;dbl&gt;,\n#   p_sim &lt;dbl&gt;, p_folded_sim &lt;dbl&gt;, skewness &lt;dbl&gt;, kurtosis &lt;dbl&gt;\n\n\n\n\n9.5 Mann-Kendall Test for Trends\nThe next phase of our analysis involves applying the Mann-Kendall Trend Test. This test helps determine whether a dataset shows a statistically significant upward or downward trend over time. Being a non-parametric test, it doesn‚Äôt require the data to follow a specific distribution and instead compares the relative magnitudes of data points rather than the actual values (Gilbert, 1987).\nThe Mann-Kendall test produces two main results: Kendall‚Äôs Tau (œÑ) and the Kendall Score (S). Kendall‚Äôs Tau (œÑ) is a correlation measure that assesses the strength of association between two variables, ranging between 0 and 1. It is calculated using the formula œÑ = (C - D) / (C + D), where C is the count of concordant pairs and D is the count of discordant pairs. The hypotheses for Kendall‚Äôs Tau are:\n\nNull Hypothesis: œÑ = 0 (No correlation exists.)\nAlternative Hypothesis: œÑ ‚â† 0 (A correlation exists.)\n\nAs described by Kendall M.G. (1975), the Kendall Score (S) starts at zero, with the assumption of no trend. For each data point that is higher than a previous one, S is incremented, and for each point that is lower, S is decremented. A high positive S indicates an increasing trend, while a strongly negative S suggests a decreasing trend. The p-value is used to determine the statistical significance of the trend.\nFor this study, the MannKendall() function from the Kendall package is used to conduct this test. We will apply the Mann-Kendall test to the three key hotspots and three key coldspots identified in our HCSA analysis.\n\n9.5.1 Trend Test of Top 3 Statistically Significant Hotspots\nPreviously, we identified that Amnat Charoen TH37), Bangkok (TH10) and Buri Ram (TH31) are the top provinces with most significant hotspots.\n\nth37_hotspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Amnat Charoen\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\nth10_hotspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Bangkok\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\nth31_hotspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Buri Ram\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\nNext, we can visualise the trend of Gi* values for the three hotspot provinces identified above, namely using the plotly() function as shown.\n\ntop_hotspots &lt;- ggplot() +\n  geom_line(data = th37_hotspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Amnat Charoen Province\")) +\n  geom_line(data = th10_hotspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Bangkok Special Admin Area\")) +\n  geom_line(data = th31_hotspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Buri Ram Province\")) +\n  labs(x = \"Fiscal Year\", y = \"Gi* Value\",\n       title = \"Gi* of Three Most Significant Hotspots From 2017 to 2022\",\n       color = \"Province\")\n\nplotly::ggplotly(top_hotspots)\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt is interesting to see that two provinces (Bangkok and Buri Ram) share a similar upward moving trend from 2017 to 2022 - though the latter experiences some fluctuations. The Gi* values peaked once in 2019, followed by a dip in 2020 (presumably due to COVID-19) and peaked once again in 2021.\nOn the other hand, the Amnat Charoen province experiences a downward moving trend with the biggest drop in Gi* values from 2020 to 2021. This results is particularly surprising since Amnat Charoen started as the most concentrated hotspot for drug activities in 2017. By the end of 2022, we see that Bangkok has even surpassed it in Gi* values, meaning Amnat Charoen had improved drastically in its drug control efforts!\nIt‚Äôs worth noting that these provinces were identified as significant hotspots due to the cumulative count of drug cases. However, just identifying hotspots can overlook the temporal dynamics and shifts in drug clustering over time. If not for the Mann-Kendall trend test, we would have not have realised these provinces have been stable or somewhat better in their control of drug cases, except for Bangkok.\n\n\nWe will now calculate the Kendall‚Äôs tau and Kendall score¬†S¬†for each province using¬†MannKendall()¬†function from¬†Kendall¬†package.\n\nAmnat Charoen ProvinceBangkok Special Admin AreaBuri Ram Province\n\n\nAnmat Charoen shows a weak upward trend in the data (tau = 0.2, S &gt; 0). This is surprising as the graph earlier indicated a downward moving trend in the number of drug cases.\nRecall that we previously decided the threshold for statistically significant results is Gi* &gt; 0 instead of Gi* &gt; 2, since only two hotspots fall under the p-value &lt; 0.05 (Bangkok and Buri Ram). Hence, it is unsurprising that the sl returned is &gt; 0.05, i.e.¬†the test does not find sufficient evidence to claim a clear upward or downward trend in the data.\n\nth37_hotspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n    tau    sl     S     D  varS\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.200 0.707     3  15.0  28.3\n\n\n\n\nThe results shows a very strong downward trend in Bangkok (tau = -0.999, S &lt; 0), and the p-value confirms that this trend is statistically significant. This suggests a significant decrease in the measured number of drug cases over time.\n\nth10_hotspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n    tau      sl     S     D  varS\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -1.00 0.00853   -15  15.0  28.3\n\n\n\n\nThis province showed a strong upward trend (tau = 0.733, S &gt; 0), but the p-value is slightly above 0.05, meaning this trend is not statistically significant at the 5% level. However, it is close to being significant, suggesting some evidence of an increasing trend.\n\nth31_hotspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n    tau     sl     S     D  varS\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.733 0.0603    11  15.0  28.3\n\n\n\n\n\n\n\n9.5.2 Trend Test of Top 3 Statistically Significant Coldspots\nPreviously, we also identified that Chai Nat (TH18), Kamphaeng Phet (TH62) and Nakhon Sawan (TH60) are the top provinces with most significant coldspots.\n\nth18_coldspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Chai Nat\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\nth62_coldspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Kamphaeng Phet\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\nth60_coldspot_data &lt;- gi_stars_year %&gt;%\n                     ungroup() %&gt;%\n                     filter(province_en == \"Nakhon Sawan\") %&gt;%\n                     select(province_en, fiscal_year, gi_star)\n\n\ntop_hotspots &lt;- ggplot() +\n  geom_line(data = th18_coldspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Chai Nat Province\")) +\n  geom_line(data = th62_coldspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Kamphaeng Phet Province\")) +\n  geom_line(data = th60_coldspot_data, mapping = aes(x = fiscal_year, y = gi_star, color = \"Nakhon Sawan Province\")) +\n  labs(x = \"Fiscal Year\", y = \"Gi* Value\",\n       title = \"Gi* of Three Most Significant Coldspots From 2017 to 2022\",\n       color = \"Province\")\n\nplotly::ggplotly(top_hotspots)\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAcross 2017 to 2022, we see a very similar downward trend in Gi* values for the three provinces we identified to be the most significant coldspots for drug cases. In fact, from 2017 to 2018, Kamphaeng Phet and Nakhon Sawan showed weakening of clusters of low drug cases as observed from the rise in Gi* values. This is followed by a dip in Gi* values over time where these provinces had evolved into stronger coldspots with a much lower concentration of drug cases compared to other provinces.\nThe Chai Nat province shares a similar negative trend in Gi* values, suggesting stronger clustering of low drug cases. However, we see a slight increase in Gi* values from 2021.\n\n\nSimilarly, we can calculate the Kendall‚Äôs tau and Kendall score¬†S¬†for each province\n\nChai Nat ProvinceKamphaeng Phet ProvinceNakhon Sawan Province\n\n\nThere is a very strong downward trend in the clustering of low drug cases as observed from tau = -0.866 and S &gt; 0. These results are also statistically significant as seen from sl &lt; 0.05, meaning that there was sufficient evidence in the test to prove this negative trend.\n\nth18_coldspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n     tau     sl     S     D  varS\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.867 0.0242   -13  15.0  28.3\n\n\n\n\nThe results shows a somewhat weak downward trend in Kamphaeng Phet province as seen from tau = -0.466, meaning it is closer to 0 than -1. The S &lt; 0 results just confirms the negative trends observed. However, since sl returned is greater than 0.05, the trends observed here could be due to random chance.\n\nth62_coldspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n     tau    sl     S     D  varS\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.467 0.260    -7  15.0  28.3\n\n\n\n\nThis time, we see a relatively strong negative associations where the tau value returned is -0.733 and S &lt; 0. Additionally, a sl of 0.06 is returned, that is slightly higher than the threshold for statistical significance. This means that the observations from Nakhon Sawan province could also be due to random chance.\n\nth60_coldspot_data %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 √ó 5\n     tau     sl     S     D  varS\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.733 0.0603   -11  15.0  28.3\n\n\n\n\n\n\n\n\n9.6 Performing Emerging Hot Spot Analysis\nLastly, we will perform EHSA analysis by using¬†emerging_hotspot_analysis()¬†of the sfdep package.\n\n\n\n\n\n\nNote\n\n\n\nPlease note thatfor the case of our dataset that spreads across 6 years only, performing EHSA is NOT recommended since our dataset does not meet the minimum period requirement of the emerging_hotspot_analysis() function of minimally 10 periodic intervals in the historical data.\nAdditionally, most literature, such as Edzer Pebesma (2012) and Skochko S et al.¬†(2024), would suggest to keep the length of period at 10-12 intervals, since a longer temporal span can establish more meaningful trends over time. This attempt in performing EHSA using spacetime cubes in this exercise is solely for the purposes of attempting different spatio-temporal app\n\n\nWith that said, I will utilise the spacetime object¬†drug_cases_spt, and the name of the variable of interest¬†total_cases¬†for the.var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Finally,¬†nsim¬†map numbers of simulation to be performed.\nThe method classifies locations into different categories, such as:\n\n\n\n\n\n\n\nPattern Type\nDefinition\n\n\n\n\nNo Pattern Detected\nDoes not fall into any of the hot or cold spot patterns defined below.\n\n\nSporadic Hot Spot\nA statistically significant hot spot for the final time-step interval with a history of also being an on-again and off-again hot spot. Less than 90 percent of the time-step intervals have been statistically significant hot spots and none of the time-step intervals have been statistically cold spots.\n\n\nSporadic Cold Spot\nA statistically significant cold spot for the final time-step interval with a history of also being an on-again and off-again cold spot. Less than 90 percent of the time-step intervals have been statistically significant cold spots and none of the time-step intervals have been statistically significant hot spots.\n\n\nPersistent Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals with no discernible trend in the intensity of clustering of counts over time.\n\n\nNew Cold Spot\nA location that is a statistically significant cold spot for the final time step and has never been a statistically significant cold spot before.\n\n\nConsecutive Cold Spot\nA location with a single uninterrupted run of at least two statistically significant cold spot bins in the final time-step intervals. The location has never been a statistically significant cold spot prior to the final cold spot run and less than 90 percent of all bins are statistically significant cold spots.\n\n\nConsecutive Hot Spot\nA location with a single uninterrupted run of at least two statistically significant hot spot bins in the final time-step intervals. The location has never been a statistically significant hot spot prior to the final hot spot run and less than 90 percent of all bins are statistically significant hot spots.\n\n\n\nThis classification helps in identifying regions that require immediate attention, those where trends are improving or deteriorating, and areas where consistent patterns persist.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = drug_cases_spt,\n  .var = \"total_cases\",\n  k = 1,\n  nsim = 99\n)\n\nthailand_ehsa &lt;- drug_cases_province_year %&gt;%\n  left_join(ehsa,\n            by = join_by(ADM1_PCODE == location)) %&gt;%\n  mutate(label = paste(ADM1_PCODE,province_en))\n\nWe are now ready to plot the bar charts of each category type.\n\nlibrary(gridExtra)\nthailand_ehsa_sig &lt;- thailand_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\n\nplot1 &lt;- ggplot(data = ehsa,\n                aes(y = classification, fill = classification)) +\n  geom_bar(show.legend = FALSE) +\n  labs(title = \"Overall EHSA Classification\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))  # Reducing size and bolding the title\n\nplot2 &lt;- ggplot(data = thailand_ehsa_sig,\n                aes(y = classification, fill = classification)) +\n  geom_bar(show.legend = FALSE) +\n  labs(title = \"Statistically Significant EHSA Classification\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))  # Reducing size and bolding the title\n\ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWhen analysing the overall classification of EHSA concerning drug cases, it appears that the majority of observed patterns do not distinctly categorise areas as hotspots (regions with a high concentration of drug cases) or cold spots (regions with a low concentration). This may suggest that many regions do not experience pronounced issues related to drug activities, or it could indicate potential limitations in the data or the methods used for analysis.\nAdditionally, we can see that sporadic coldspots and consecutive coldspots constitute the highest EHSA class for both overall and statistically significant respectively. These two results are rather contrasting since one shows fluctuations in low drug case offenses while the other suggests consistently low drug cases observed.\nOn the other hand, among the hotspot classifications, sporadic hotspots and consecutive hotspots account for the highest counts in both the overall and statistically significant datasets, respectively. This indicates that certain regions experience occasional peaks in drug cases, while others maintain a more stable pattern of elevated drug activity. These results underscore the complexity of drug offenses in the studied areas, revealing both variability and consistency in patterns of drug-related incidents.\n\n\nWe can also plot these ESHA classes onto our map of Thailand for visualisation of province-level dynamics of drug cases. This plot revealed that there is a consecutive hotspot found in a Southern province (red) which could be due to underlying socio-economic factors causing such high intensity of drug cluster. Interestingly, there is also a conseutive coldspot (purple) found adjacent to a new coldspot (green) province. The proximity of these coldspots implies that effective interventions in one area could have a positive impact on adjacent regions, potentially leading to a broader reduction in drug-related issues.\n\nlibrary(tmap)\npalette &lt;- c(\n  consecutive_coldspot = \"#C9DAF8\",     # Light Purple\n  consecutive_hotspot = \"#D74B32\",    # Orange-Red\n  new_coldspot = \"#D9EAD3\",           # Light Green\n  no_patterns_detected = \"#FFFFFF\",    # White\n  persistent_coldspot = \"#4F81BD\",    # Medium Blue\n  sporadic_coldspot = \"#A6C8E0\"      # Light Blue\n)\n\n# Plot with your specifications\ntm_shape(thailand_ehsa) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6) +\ntm_shape(thailand_ehsa_sig) +\n  tm_fill(\"classification\", \n          palette = palette,\n          title = \"ESHA Classification\",\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Emerging Statistically Significant Hotspots &\\n Coldspots of Drug Cases in Thailand\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,  # Increased size for better visibility\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.5,  # Increased for better visibility\n            legend.text.size = 0.4,\n            legend.hist.size = 0.4,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.5, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.4, alpha = 0.1)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02_Part2.html#conclusion-and-reflections",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02_Part2.html#conclusion-and-reflections",
    "title": "Take-home Exercise 2 - Part 2",
    "section": "10. Conclusion and Reflections",
    "text": "10. Conclusion and Reflections\nAs I come to the end of this study, I have successfully uncovered deeper insights into Thailand‚Äôs drug abuse via spatial and spatio-temporal autocorrelation for the period of 2017 to 2022, namely using global Moran‚Äôs I, global Geary‚Äôs C, local Moran‚Äôs I, LISA classifications, local Getis-Ord Gi* and emerging hot spot analysis.\nThis study reveals a compelling need for greater intervention for drug control and law enforcement since a majority of high drug case clusters continue remaining high across the years. We also observed a tendency for low-low and low-high clusters to evolve into high-high drug clusters, meaning that the presence of drug rings / networks in neighbour provinces increases the potential of such drug cases spilling over into provinces that were once less affected by drug offenses. This could be due to strong transport networks and the movement of goods and people through these transport routes.\nInterestingly, there were some provinces that are geographically close but exhibited starkly different patterns in drug case distribution, e.g.¬†hotspots and coldspots found adjacent to each other. Such disparities can be due to various factors, e.g.¬†differences in local governance, law enforcement and socio-economic conditions. Though these are beyond the take-home exercise, I believe that including factors (e.g.¬†GDPPC) could act as potentially interesting future analyses.\nThrough this study, it is evident that the analysis of drug cases reveals a complex landscape where sporadic cold spots and consecutive cold spots highlight fluctuations and consistency in low drug offenses, while sporadic and consecutive hotspots indicate varying intensities of drug-related activity across different regions.\nHowever, there is an interconnectedness of drug dynamics across different provinces, where the presence of coldspot regions can influence low drug cases in its surroundings too. Hence, this highlights the importance of coordinated strategies in addressing drug challenges in a national context and the need for greater targeted interventions with a deeper understanding of underlying factors influencing these patterns.\nWith that said, I would like to extend my gratitude to Prof.¬†Kam Tin Seong for his valuable guidance and mentorship throughout this process! :) This exercise requires some intensive computational load, depending on the level of granularity of this analysis. Therefore, it is most recommended to begin the take-home exercise early to make room for processing time and debugging. This marks the end of my study and I thank you for taking the time to read till here! üéâüéâüéâ"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02_Part2.html#references",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex02_Part2.html#references",
    "title": "Take-home Exercise 2 - Part 2",
    "section": "11. References",
    "text": "11. References\n\nCRAN. (2024, September 5). Classes and Methods for Spatio-Temporal Data.\nCrawley, M. J. (2007). The R Book. Wiley.\nHandbook of Spatial Analysis. (2018). Insee. https://www.insee.fr/en/information/3635545¬†\nKanato, M., Sarasiri, R., & Leyatikul, P. (2022). ASEAN Drug Monitoring Report.\nNguyen, K.-B., Choi, J., & Yang, J.-S. (2022). Checkerboard Dropout: A Structured Dropout With Checkerboard Pattern for Convolutional Neural Networks.\nParry, J., & Locke, D. H. (n.d.). The Basics of sfdep. Retrieved September 26, 2024, from https://sfdep.josiahparry.com/articles/basics-of-sfdep¬†\nParry, J., & Locke, D. H. (n.d.). Spacetime and spacetime cubes ‚Ä¢ sfdep. sfdep. Retrieved October 6, 2024, from https://sfdep.josiahparry.com/articles/spacetime-s3.html¬†\nPeay, A. (n.d.). Spatial Regression Analysis Advanced Data Analytics. chrismgentry.github.io. https://chrismgentry.github.io/Spatial-Regression/¬†\nSkochko, S. (2024, September 5). Asthma Hot Spots in New York Before and During the COVID-19 Pandemic. CDC. Retrieved October 8, 2024, from https://www.cdc.gov/pcd/issues/2024/24_0059.htm\nvsp.pnnl.gov. (n.d.). Mann-Kendall Test For Monotonic Trend. Design Trend Mann-Kendall. Retrieved October 6, 2024, from https://vsp.pnnl.gov/help/vsample/design_trend_mann_kendall.htm"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html",
    "title": "Hands-on Exercise 8",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#overview",
    "title": "Hands-on Exercise 8",
    "section": "1. Overview",
    "text": "1. Overview\nIn this hands-on exercise, I will focus on the application of clustering techniques to perform geographical segmentation. There are two major types of techniques:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\nIt is a common practice to use clustering techniques to segment the market or planning area into different zones. In this exercise, we will try to delineate Shan State, Myanmar, into different zones based on multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#getting-started",
    "title": "Hands-on Exercise 8",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n2.1 Install Required Packages\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\n\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\n\n\n\n\n\n\nNote\n\n\n\nNote: With¬†tidyverse, we do not have to install¬†readr,¬†ggplot2¬†and¬†dplyr¬†packages separately. In fact,¬†tidyverse¬†also installs other very useful R packages such as¬†tidyr.\n\n\n\n\n2.2 Import Data and Set Up Folders\n\n\n\n\n\nTwo data sets I will be using in this study are as follows:\n\nMyanmar Township Boundary Data (i.e.¬†myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\nBoth data sets are download from Myanmar Information Management Unit (MIMU)\n\n2.2.1 Importing Aspatial Data\nInfoComm variables extracted from The 2014 Myanmar Population and Housing Census Myanmar at the township level. The attribute data set is called ict. It is saved in R‚Äôs tibble data.frame format.\n\nict &lt;- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\nRows: 55 Columns: 11\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): District Pcode, District Name, Township Pcode, Township Name\ndbl (7): Total households, Radio, Television, Land line phone, Mobile phone,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar.\n\n# Let's inspect\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe attribute data set is called ict as shown above. It is saved in R‚Äôs tibble data.frame format. The code chunk below reveal the summary statistics of ict data.frame.\n\n\n\n\n2.2.2 Importing Geospatial Data\nMyanmar township boundaries are downloaded from Myanmar Information Management Unit (MIMU). The geospatial data set is called myanmar_township_boundaries. It is saved in ESRI shapefile format. Let‚Äôs import it into R environment by using the¬†st_read()¬†function of¬†sf.\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex8\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nThe imported township boundary object is called¬†shan_sf. It is saved in¬†simple feature data.frame¬†format. We can view the content of the newly created¬†shan_sf¬†simple features data.frame by using the code chunk below.\n\n# Let's inspect\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe output of the shan_sf sf dataframe shows a multipolygon geometric attribute with 55 features and 6 fields of WGS84 universal geodatic CRS.\n\n\n\n\n\n2.3 Data Preparation\nWe will now derive new variables from the ict data set. Since the unit of measurements for each variables are the number of households, it creates bias towards townships with larger population. To mitigate this bias, we will derive new variables that are the proportion of households with Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.\nThe new variables are calculated per 1000 households. The new variables are named RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR. The new variables are then renamed to more intuitive names. The derived data set is called ict_derived.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) |&gt;\n  mutate(`TV_PR` = `Television`/`Total households`*1000) |&gt;\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) |&gt;\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) |&gt;\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) |&gt;\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) |&gt;\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 8",
    "section": "3. Exploratory Data Analysis (EDA)",
    "text": "3. Exploratory Data Analysis (EDA)\n\n3.1 Statistical Graphics\nNow we will conduct univariate analysis to understand the distribution of each variable. We will use histograms to visualise the distribution of each variable. We will use boxplots as well to detect if there are outliers.\n\n\nPlot original variables\nradio_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nradio_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\ntv_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `TV`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`TV`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\nllphone_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`LLPHONE`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\nmphone_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`MPHONE`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\ncomputer_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`COMPUTER`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\ninternet_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`INTERNET`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\nggarrange(radio_hist, radio_box,\n          tv_hist, tv_box,\n          llphone_hist, llphone_box,\n          mphone_hist, mphone_box,\n          computer_hist, computer_box,\n          internet_hist, internet_box,\n          ncol = 2, \n          nrow = 6)\n\n\n\n\n\n\n\n\n\nAs you can see, most of the variables are right-skewed. Luckily, we have addressed this issue in the previous section. Let‚Äôs now plot the newly derived variables.\n\n\nPlot new variables\nradio_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nradio_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\ntv_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`TV_PR`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\nllphone_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`LLPHONE_PR`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\nmphone_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`MPHONE_PR`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\ncomputer_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`COMPUTER_PR`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\ninternet_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`INTERNET_PR`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\nggarrange(radio_hist, radio_box,\n          tv_hist, tv_box,\n          llphone_hist, llphone_box,\n          mphone_hist, mphone_box,\n          computer_hist, computer_box,\n          internet_hist, internet_box,\n          ncol = 2, \n          nrow = 6)\n\n\n\n\n\n\n\n\n\n\n\n3.2 Choropleth\n\n3.2.1 Joinning geospatial data with aspatial data\nBefore we can proceed with the choropleth mapping, we need to join the geospatial data with the aspatial data. We will use the left_join function from the dplyr package to join the two data sets. The join key is the township code. The joined data set is called shan_sf.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"data/rds/shan_sf.rds\")\n\n\n\n3.2.2 Choropleth Mapping\n\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e.¬†TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#correlation-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#correlation-analysis",
    "title": "Hands-on Exercise 8",
    "section": "4. Correlation Analysis",
    "text": "4. Correlation Analysis\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#hierarchy-cluster-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#hierarchy-cluster-analysis",
    "title": "Hands-on Exercise 8",
    "section": "5. Hierarchy Cluster Analysis",
    "text": "5. Hierarchy Cluster Analysis\nIn this section, we will perform hierarchical cluster analysis to segment the townships in Shan State into different zones based on the derived ICT measures. The analysis consists of four major steps:\n\n5.1 Extracting clustering variables\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\nNow, we will delete the TS.x field by using the code chunk below.\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n5.2 Data Standardisation\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\n\n5.2.1 Min-Max standardisation\nIn the code chunk below,¬†normalize()¬†of¬†heatmaply¬†package is used to stadardisation the clustering variables by using Min-Max method. The¬†summary()¬†is then used to display the summary statistics of the standardised clustering variables.\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\n5.2.2 Z-score standardisation\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\n\n\n\n\n\n\nObservations\n\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\nNote: describe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation.\nWarning: Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.\n\n\n\n\n5.2.3 Visualising the standardised clustering variables\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled¬†Radio_PR¬†field.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\nIt looks like RADIO_PR is already normally distributed.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n5.3 Computing proximity matrix\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\nThe code chunk below can then be used to list the content of¬†proxmat¬†for visual inspection.\n\nproxmat\n\n\n\n5.4 Computing hierarchical clustering\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\nWe can then plot the tree by using¬†plot()¬†of R Graphics as shown in the code chunk below.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n5.5 Selecting the optimal clustering algorithm\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward‚Äôs method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward‚Äôs method will be used.\n\n\n5.6 Determining Optimal Clusters\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\n\n5.7 Gap Statistic Method\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n5.8 Interpreting the dendrograms\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt‚Äôs also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\n\n\n5.9 Visually-driven hierarchical clustering analysis\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\n5.9.1 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\n5.9.2 Plotting interactive cluster heatmap using heatmaply()\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n\n\n\n\n5.9 Mapping the clusters formed\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nNext,¬†qtm()¬†of¬†tmap¬†package is used to plot the choropleth map showing the cluster formed.\n\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#spatially-constrained-clustering-skater-approach",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#spatially-constrained-clustering-skater-approach",
    "title": "Hands-on Exercise 8",
    "section": "6. Spatially Constrained Clustering: SKATER approach",
    "text": "6. Spatially Constrained Clustering: SKATER approach\nIn this section, you will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\n6.1 Converting into SpatialPolygonsDataFrame\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nshan_sp &lt;- as_Spatial(shan_sf)\n\n\n\n6.2 Computing Neighbour List\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list.\n\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\nplot(shan_sp, \n     border=grey(0.5))\nplot(shan.nb, \n     coordinates(shan_sp), \n     col=\"blue\", \n     add=TRUE)\n\nNote that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n6.3 Computing minimum spanning tree\n\n6.3.1 Calculating edge costs\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardized.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\n6.4 Computing minimum spanning tree\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nshan.mst &lt;- mstree(shan.w)\n\nAfter computing the MST, we can check its class and dimension by using the code chunk below.\n\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\n\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nNote that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nWe can display the content of shan.mst by using head() as shown in the code chunk below.\n\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   54   48  47.79331\n[2,]   54   17 109.08506\n[3,]   54   45 127.42203\n[4,]   45   52 146.78891\n[5,]   52   13 110.55197\n[6,]   13   28  92.79567\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbor list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(shan_sp, border=gray(.5))\nplot.mst(shan.mst, \n         coordinates(shan_sp), \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n6.5 Computing spatially constrained clusters using SKATER method\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e.¬†not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nWe can check the cluster assignment by using the conde chunk below.\n\nccs6 &lt;- clust6$groups\nccs6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can find out how many observations are in each cluster by means of the table command. Parenthetially, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.\n\ntable(ccs6)\n\nccs6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nplot(shan_sp, border=gray(.5))\nplot(clust6, \n     coordinates(shan_sp), \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n6.6 Visualising the clusters in choropleth map\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\n\n\n\n\nFor easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#spatially-constrained-clustering-clustgeo-method",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#spatially-constrained-clustering-clustgeo-method",
    "title": "Hands-on Exercise 8",
    "section": "7. Spatially Constrained Clustering: ClustGeo Method",
    "text": "7. Spatially Constrained Clustering: ClustGeo Method\nIn this section, you will gain hands-on experience on using functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\n7.1 A short note about ClustGeo package\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\n7.2 Ward-like hierarchical clustering: ClustGeo\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e.¬†an object obtained with the function dist(). For sample code chunk, please refer to 5.7.6 Computing proximity matrix\n\n7.2.1 Mapping the clusters formed\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\n7.3 Spatially Constrained Hierarchical Clustering\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nNext,¬†cutree()¬†is used to derive the cluster objecct.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\nqtm(shan_sf_Gcluster, \"CLUSTER\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#visual-interpretation-of-clusters",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8.html#visual-interpretation-of-clusters",
    "title": "Hands-on Exercise 8",
    "section": "7.4 Visual Interpretation of Clusters",
    "text": "7.4 Visual Interpretation of Clusters\n\n7.4.1 Visualising individual clustering variable\nCode chunk below is used to reveal the distribution of a clustering variable (i.e RADIO_PR) by cluster.\n\nggplot(data = shan_sf_ngeo_cluster,\n       aes(x = CLUSTER, y = RADIO_PR)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.\n\n\n7.4.2 Multivariate Visualisation\nPast studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 √ó 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ‚Ñπ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe resulting shan_sf_ngeo_cluster is a tibble of 6 rows and 6 columns containing the newly added mean calculations based on the unique clusers of 1, 2, 3, 4, 5 and 6."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html",
    "title": "Hands-on Exercise 9",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#overview",
    "title": "Hands-on Exercise 9",
    "section": "1. Overview",
    "text": "1. Overview\nIn this hands-on exercise, I will learn more on performing how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n1.1 Learning Outcome\nBy the end of this hands-on exercise, I aim to accomplish the following learning:\n\nto convert GIS polygon data into R‚Äôs simple feature data.frame by using appropriate functions of sf package of R;\nto convert simple feature data.frame into R‚Äôs SpatialPolygonDataFrame object by using appropriate sf of package of R;\nto perform custer analysis by using hclust() of Base R;\nto perform spatially constrained cluster analysis using skater() of Base R; and\nto visualise the analysis output by using ggplot2 and tmap package.\n\n\n\n1.2 Installing Required Packages\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\n\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\n\n\n\n\n\n\nNote\n\n\n\nWith¬†tidyverse, we do not have to install¬†readr,¬†ggplot2¬†and¬†dplyr¬†packages separately. In fact,¬†tidyverse¬†also installs other very useful R packages such as¬†tidyr.\n\n\n\n\n1.3 Preparing the Datasets\n\nTwo data sets will be used in this study. They are:\n\nMyanmar Township Boundary Data (i.e.¬†myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\nBoth data sets are download from Myanmar Information Management Unit (MIMU)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#getting-started",
    "title": "Hands-on Exercise 9",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n2.1 Install Required Packages\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\n\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\n\n\n\n\n\n\nNote\n\n\n\nNote: With¬†tidyverse, we do not have to install¬†readr,¬†ggplot2¬†and¬†dplyr¬†packages separately. In fact,¬†tidyverse¬†also installs other very useful R packages such as¬†tidyr.\n\n\n\n\n2.2 Import Data and Set Up Folders\n\n\n\n\n\nTwo data sets I will be using in this study are as follows:\n\nMyanmar Township Boundary Data (i.e.¬†myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\nBoth data sets are download from Myanmar Information Management Unit (MIMU)\n\n2.2.1 Importing Aspatial Data\nInfoComm variables extracted from The 2014 Myanmar Population and Housing Census Myanmar at the township level. The attribute data set is called ict. It is saved in R‚Äôs tibble data.frame format.\n\nict &lt;- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\nRows: 55 Columns: 11\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): District Pcode, District Name, Township Pcode, Township Name\ndbl (7): Total households, Radio, Television, Land line phone, Mobile phone,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar.\n\n# Let's inspect\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe attribute data set is called ict as shown above. It is saved in R‚Äôs tibble data.frame format. The code chunk below reveal the summary statistics of ict data.frame.\n\n\n\n\n2.2.2 Importing Geospatial Data\nMyanmar township boundaries are downloaded from Myanmar Information Management Unit (MIMU). The geospatial data set is called myanmar_township_boundaries. It is saved in ESRI shapefile format. Let‚Äôs import it into R environment by using the¬†st_read()¬†function of¬†sf.\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex9\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nThe imported township boundary object is called¬†shan_sf. It is saved in¬†simple feature data.frame¬†format. We can view the content of the newly created¬†shan_sf¬†simple features data.frame by using the code chunk below.\n\n# Let's inspect\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe output of the shan_sf sf dataframe shows a multipolygon geometric attribute with 55 features and 6 fields of WGS84 universal geodatic CRS.\n\n\n\n\n\n2.3 Data Preparation\nWe will now derive new variables from the ict data set. Since the unit of measurements for each variables are the number of households, it creates bias towards townships with larger population. To mitigate this bias, we will derive new variables that are the proportion of households with Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.\nThe new variables are calculated per 1000 households. The new variables are named RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR. The new variables are then renamed to more intuitive names. The derived data set is called ict_derived.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) |&gt;\n  mutate(`TV_PR` = `Television`/`Total households`*1000) |&gt;\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) |&gt;\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) |&gt;\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) |&gt;\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) |&gt;\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 9",
    "section": "3. Exploratory Data Analysis (EDA)",
    "text": "3. Exploratory Data Analysis (EDA)\n\n3.1 Statistical Graphics\nNow we will conduct univariate analysis to understand the distribution of each variable. We can plot the distribution of the variables (i.e.¬†Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nHistogram is useful to identify the overall distribution of the data values (i.e.¬†left skew, right skew or normal distribution)\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\n\n\n\n\n\n\nWe will use boxplots as well to detect if there are outliers.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\n\n\n\n\nNext, we will also plotting the distribution of the newly derived variables (i.e.¬†Radio penetration rate) by using the code chunk below.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\n\n\n\n\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\n\n\n\n\nAs you can see, most of the variables are right-skewed. Luckily, we have addressed this issue in the previous section. Let‚Äôs now plot the newly derived variables.\n\n\nPlot new variables\nradio_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nradio_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\ntv_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`TV_PR`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\nllphone_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`LLPHONE_PR`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\nmphone_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`MPHONE_PR`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\ncomputer_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`COMPUTER_PR`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\ninternet_hist &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet_box &lt;- ggplot(data=ict_derived, \n                    aes(x=`INTERNET_PR`)) +\n  geom_boxplot(color=\"black\",\n               fill=\"light blue\")\n\nggarrange(radio_hist, radio_box,\n          tv_hist, tv_box,\n          llphone_hist, llphone_box,\n          mphone_hist, mphone_box,\n          computer_hist, computer_box,\n          internet_hist, internet_box,\n          ncol = 2, \n          nrow = 6)\n\n\n\n\n\n\n\n\n\nNext, the¬†ggarrange()¬†function of¬†ggpubr¬†package is used to group these histograms together.\n\nggarrange(radio_hist, tv_hist, llphone_hist, mphone_hist, computer_hist, internet_hist, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\n\n\n\n\n\n\n3.2 Choropleth\n\n3.2.1 Joinning geospatial data with aspatial data\nBefore we can proceed with the choropleth mapping, we need to join the geospatial data with the aspatial data. We will use the left_join function from the dplyr package to join the two data sets. The join key is the township code. The joined data set is called shan_sf.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"data/rds/shan_sf.rds\")\n\n\n\n3.2.2 Choropleth Mapping\n\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e.¬†TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#correlation-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#correlation-analysis",
    "title": "Hands-on Exercise 9",
    "section": "4. Correlation Analysis",
    "text": "4. Correlation Analysis\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#hierarchy-cluster-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#hierarchy-cluster-analysis",
    "title": "Hands-on Exercise 9",
    "section": "5. Hierarchy Cluster Analysis",
    "text": "5. Hierarchy Cluster Analysis\nIn this section, we will perform hierarchical cluster analysis to segment the townships in Shan State into different zones based on the derived ICT measures. The analysis consists of four major steps:\n\n5.1 Extracting clustering variables\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\nNow, we will delete the TS.x field by using the code chunk below.\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n5.2 Data Standardisation\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\n\n5.2.1 Min-Max standardisation\nIn the code chunk below,¬†normalize()¬†of¬†heatmaply¬†package is used to stadardisation the clustering variables by using Min-Max method. The¬†summary()¬†is then used to display the summary statistics of the standardised clustering variables.\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\n5.2.2 Z-score standardisation\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\n\n\n\n\n\n\nObservations\n\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\nNote: describe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation.\nWarning: Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.\n\n\n\n\n5.2.3 Visualising the standardised clustering variables\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled¬†Radio_PR¬†field.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\nIt looks like RADIO_PR is already normally distributed.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n5.3 Computing proximity matrix\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\nThe code chunk below can then be used to list the content of¬†proxmat¬†for visual inspection.\n\nproxmat\n\n             Mongmit   Pindaya   Ywangan  Pinlaung    Mabein     Kalaw\nPindaya    171.86828                                                  \nYwangan    381.88259 257.31610                                        \nPinlaung    57.46286 208.63519 400.05492                              \nMabein     263.37099 313.45776 529.14689 312.66966                    \nKalaw      160.05997 302.51785 499.53297 181.96406 198.14085          \nPekon       59.61977 117.91580 336.50410  94.61225 282.26877 211.91531\nLawksawk   140.11550 204.32952 432.16535 192.57320 130.36525 140.01101\nNawnghkio   89.07103 180.64047 377.87702 139.27495 204.63154 127.74787\nKyaukme    144.02475 311.01487 505.89191 139.67966 264.88283  79.42225\nMuse       563.01629 704.11252 899.44137 571.58335 453.27410 412.46033\nLaihka     141.87227 298.61288 491.83321 101.10150 345.00222 197.34633\nMongnai    115.86190 258.49346 422.71934  64.52387 358.86053 200.34668\nMawkmai    434.92968 437.99577 397.03752 398.11227 693.24602 562.59200\nKutkai      97.61092 212.81775 360.11861  78.07733 340.55064 204.93018\nMongton    192.67961 283.35574 361.23257 163.42143 425.16902 267.87522\nMongyai    256.72744 287.41816 333.12853 220.56339 516.40426 386.74701\nMongkaing  503.61965 481.71125 364.98429 476.29056 747.17454 625.24500\nLashio     251.29457 398.98167 602.17475 262.51735 231.28227 106.69059\nMongpan    193.32063 335.72896 483.68125 192.78316 301.52942 114.69105\nMatman     401.25041 354.39039 255.22031 382.40610 637.53975 537.63884\nTachileik  529.63213 635.51774 807.44220 555.01039 365.32538 373.64459\nNarphan    406.15714 474.50209 452.95769 371.26895 630.34312 463.53759\nMongkhet   349.45980 391.74783 408.97731 305.86058 610.30557 465.52013\nHsipaw     118.18050 245.98884 388.63147  76.55260 366.42787 212.36711\nMonghsat   214.20854 314.71506 432.98028 160.44703 470.48135 317.96188\nMongmao    242.54541 402.21719 542.85957 217.58854 384.91867 195.18913\nNansang    104.91839 275.44246 472.77637  85.49572 287.92364 124.30500\nLaukkaing  568.27732 726.85355 908.82520 563.81750 520.67373 427.77791\nPangsang   272.67383 428.24958 556.82263 244.47146 418.54016 224.03998\nNamtu      179.62251 225.40822 444.66868 170.04533 366.16094 307.27427\nMonghpyak  177.76325 221.30579 367.44835 222.20020 212.69450 167.08436\nKonkyan    403.39082 500.86933 528.12533 365.44693 613.51206 444.75859\nMongping   265.12574 310.64850 337.94020 229.75261 518.16310 375.64739\nHopong     136.93111 223.06050 352.85844  98.14855 398.00917 264.16294\nNyaungshwe  99.38590 216.52463 407.11649 138.12050 210.21337  95.66782\nHsihseng   131.49728 172.00796 342.91035 111.61846 381.20187 287.11074\nMongla     384.30076 549.42389 728.16301 372.59678 406.09124 260.26411\nHseni      189.37188 337.98982 534.44679 204.47572 213.61240  38.52842\nKunlong    224.12169 355.47066 531.63089 194.76257 396.61508 273.01375\nHopang     281.05362 443.26362 596.19312 265.96924 368.55167 185.14704\nNamhkan    386.02794 543.81859 714.43173 382.78835 379.56035 246.39577\nKengtung   246.45691 385.68322 573.23173 263.48638 219.47071  88.29335\nLangkho    164.26299 323.28133 507.78892 168.44228 253.84371  67.19580\nMonghsu    109.15790 198.35391 340.42789  80.86834 367.19820 237.34578\nTaunggyi   399.84278 503.75471 697.98323 429.54386 226.24011 252.26066\nPangwaun   381.51246 512.13162 580.13146 356.37963 523.44632 338.35194\nKyethi     202.92551 175.54012 287.29358 189.47065 442.07679 360.17247\nLoilen     145.48666 293.61143 469.51621  91.56527 375.06406 217.19877\nManton     430.64070 402.42888 306.16379 405.83081 674.01120 560.16577\nMongyang   309.51302 475.93982 630.71590 286.03834 411.88352 233.56349\nKunhing    173.50424 318.23811 449.67218 141.58836 375.82140 197.63683\nMongyawng  214.21738 332.92193 570.56521 235.55497 193.49994 173.43078\nTangyan    195.92520 208.43740 324.77002 169.50567 448.59948 348.06617\nNamhsan    237.78494 228.41073 286.16305 214.33352 488.33873 385.88676\n               Pekon  Lawksawk Nawnghkio   Kyaukme      Muse    Laihka\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk   157.51129                                                  \nNawnghkio  113.15370  90.82891                                        \nKyaukme    202.12206 186.29066 157.04230                              \nMuse       614.56144 510.13288 533.68806 434.75768                    \nLaihka     182.23667 246.74469 211.88187 128.24979 526.65211          \nMongnai    151.60031 241.71260 182.21245 142.45669 571.97975 100.53457\nMawkmai    416.00669 567.52693 495.15047 512.02846 926.93007 429.96554\nKutkai     114.98048 224.64646 147.44053 170.93318 592.90743 144.67198\nMongton    208.14888 311.07742 225.81118 229.28509 634.71074 212.07320\nMongyai    242.52301 391.26989 319.57938 339.27780 763.91399 264.13364\nMongkaing  480.23965 625.18712 546.69447 586.05094 995.66496 522.96309\nLashio     303.80011 220.75270 230.55346 129.95255 313.15288 238.64533\nMongpan    243.30037 228.54223 172.84425 110.37831 447.49969 210.76951\nMatman     368.25761 515.39711 444.05061 505.52285 929.11283 443.25453\nTachileik  573.39528 441.82621 470.45533 429.15493 221.19950 549.08985\nNarphan    416.84901 523.69580 435.59661 420.30003 770.40234 392.32592\nMongkhet   342.08722 487.41102 414.10280 409.03553 816.44931 324.97428\nHsipaw     145.37542 249.35081 176.09570 163.95741 591.03355 128.42987\nMonghsat   225.64279 352.31496 289.83220 253.25370 663.76026 158.93517\nMongmao    293.70625 314.64777 257.76465 146.09228 451.82530 185.99082\nNansang    160.37607 188.78869 151.13185  60.32773 489.35308  78.78999\nLaukkaing  624.82399 548.83928 552.65554 428.74978 149.26996 507.39700\nPangsang   321.81214 345.91486 287.10769 175.35273 460.24292 214.19291\nNamtu      165.02707 260.95300 257.52713 270.87277 659.16927 185.86794\nMonghpyak  190.93173 142.31691  93.03711 217.64419 539.43485 293.22640\nKonkyan    421.48797 520.31264 439.34272 393.79911 704.86973 351.75354\nMongping   259.68288 396.47081 316.14719 330.28984 744.44948 272.82761\nHopong     138.86577 274.91604 204.88286 218.84211 648.68011 157.48857\nNyaungshwe 139.31874 104.17830  43.26545 126.50414 505.88581 201.71653\nHsihseng   105.30573 257.11202 209.88026 250.27059 677.66886 175.89761\nMongla     441.20998 393.18472 381.40808 241.58966 256.80556 315.93218\nHseni      243.98001 171.50398 164.05304  81.20593 381.30567 204.49010\nKunlong    249.36301 318.30406 285.04608 215.63037 547.24297 122.68682\nHopang     336.38582 321.16462 279.84188 154.91633 377.44407 230.78652\nNamhkan    442.77120 379.41126 367.33575 247.81990 238.67060 342.43665\nKengtung   297.67761 209.38215 208.29647 136.23356 330.08211 258.23950\nLangkho    219.21623 190.30257 156.51662  51.67279 413.64173 160.94435\nMonghsu    113.84636 242.04063 170.09168 200.77712 633.21624 163.28926\nTaunggyi   440.66133 304.96838 344.79200 312.60547 250.81471 425.36916\nPangwaun   423.81347 453.02765 381.67478 308.31407 541.97887 351.78203\nKyethi     162.43575 317.74604 267.21607 328.14177 757.16745 255.83275\nLoilen     181.94596 265.29318 219.26405 146.92675 560.43400  59.69478\nManton     403.82131 551.13000 475.77296 522.86003 941.49778 458.30232\nMongyang   363.58788 363.37684 323.32123 188.59489 389.59919 229.71502\nKunhing    213.46379 278.68953 206.15773 145.00266 533.00162 142.03682\nMongyawng  248.43910 179.07229 220.61209 181.55295 422.37358 211.99976\nTangyan    167.79937 323.14701 269.07880 306.78359 736.93741 224.29176\nNamhsan    207.16559 362.84062 299.74967 347.85944 778.52971 273.79672\n             Mongnai   Mawkmai    Kutkai   Mongton   Mongyai Mongkaing\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai    374.50873                                                  \nKutkai      91.15307 364.95519                                        \nMongton    131.67061 313.35220 107.06341                              \nMongyai    203.23607 178.70499 188.94166 159.79790                    \nMongkaing  456.00842 133.29995 428.96133 365.50032 262.84016          \nLashio     270.86983 638.60773 289.82513 347.11584 466.36472 708.65819\nMongpan    178.09554 509.99632 185.18173 200.31803 346.39710 563.56780\nMatman     376.33870 147.83545 340.86349 303.04574 186.95158 135.51424\nTachileik  563.95232 919.38755 568.99109 608.76740 750.29555 967.14087\nNarphan    329.31700 273.75350 314.27683 215.97925 248.82845 285.65085\nMongkhet   275.76855 115.58388 273.91673 223.22828 104.98924 222.60577\nHsipaw      52.68195 351.34601  51.46282  90.69766 177.33790 423.77868\nMonghsat   125.25968 275.09705 154.32012 150.98053 127.35225 375.60376\nMongmao    188.29603 485.52853 204.69232 206.57001 335.61300 552.31959\nNansang     92.79567 462.41938 130.04549 199.58124 288.55962 542.16609\nLaukkaing  551.56800 882.51110 580.38112 604.66190 732.68347 954.11795\nPangsang   204.25746 484.14757 228.33583 210.77938 343.30638 548.40662\nNamtu      209.35473 427.95451 225.28268 308.71751 278.02761 525.04057\nMonghpyak  253.26470 536.71695 206.61627 258.04282 370.01575 568.21089\nKonkyan    328.82831 339.01411 310.60810 248.25265 287.87384 380.92091\nMongping   202.99615 194.31049 182.75266 119.86993  65.38727 257.18572\nHopong      91.53795 302.84362  73.45899 106.21031 124.62791 379.37916\nNyaungshwe 169.63695 502.99026 152.15482 219.72196 327.13541 557.32112\nHsihseng   142.36728 329.29477 128.21054 194.64317 162.27126 411.59788\nMongla     354.10985 686.88950 388.40984 411.06668 535.28615 761.48327\nHseni      216.81639 582.53670 229.37894 286.75945 408.23212 648.04408\nKunlong    202.92529 446.53763 204.54010 270.02165 299.36066 539.91284\nHopang     243.00945 561.24281 263.31986 273.50305 408.73288 626.17673\nNamhkan    370.05669 706.47792 392.48568 414.53594 550.62819 771.39688\nKengtung   272.28711 632.54638 279.19573 329.38387 460.39706 692.74693\nLangkho    174.67678 531.08019 180.51419 236.70878 358.95672 597.42714\nMonghsu     84.11238 332.07962  62.60859 107.04894 154.86049 400.71816\nTaunggyi   448.55282 810.74692 450.33382 508.40925 635.94105 866.21117\nPangwaun   312.13429 500.68857 321.80465 257.50434 394.07696 536.95736\nKyethi     210.50453 278.85535 184.23422 222.52947 137.79420 352.06533\nLoilen      58.41263 388.73386 131.56529 176.16001 224.79239 482.18190\nManton     391.54062 109.08779 361.82684 310.20581 195.59882  81.75337\nMongyang   260.39387 558.83162 285.33223 295.60023 414.31237 631.91325\nKunhing    110.55197 398.43973 108.84990 114.03609 238.99570 465.03971\nMongyawng  275.77546 620.04321 281.03383 375.22688 445.78964 700.98284\nTangyan    180.37471 262.66006 166.61820 198.88460 109.08506 348.56123\nNamhsan    218.10003 215.19289 191.32762 196.76188  77.35900 288.66231\n              Lashio   Mongpan    Matman Tachileik   Narphan  Mongkhet\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan    172.33279                                                  \nMatman     628.11049 494.81014                                        \nTachileik  311.95286 411.03849 890.12935                              \nNarphan    525.63854 371.13393 312.05193 760.29566                    \nMongkhet   534.44463 412.17123 203.02855 820.50164 217.28718          \nHsipaw     290.86435 179.52054 344.45451 576.18780 295.40170 253.80950\nMonghsat   377.86793 283.30992 313.59911 677.09508 278.21548 167.98445\nMongmao    214.23677 131.59966 501.59903 472.95568 331.42618 375.35820\nNansang    184.47950 144.77393 458.06573 486.77266 398.13308 360.99219\nLaukkaing  334.65738 435.58047 903.72094 325.06329 708.82887 769.06406\nPangsang   236.72516 140.23910 506.29940 481.31907 316.30314 375.58139\nNamtu      365.88437 352.91394 416.65397 659.56458 494.36143 355.99713\nMonghpyak  262.09281 187.85699 470.46845 444.04411 448.40651 462.63265\nKonkyan    485.51312 365.87588 392.40306 730.92980 158.82353 254.24424\nMongping   454.52548 318.47482 201.65224 727.08969 188.64567 113.80917\nHopong     345.31042 239.43845 291.84351 632.45718 294.40441 212.99485\nNyaungshwe 201.58191 137.29734 460.91883 445.81335 427.94086 417.08639\nHsihseng   369.00833 295.87811 304.02806 658.87060 377.52977 256.70338\nMongla     179.95877 253.20001 708.17595 347.33155 531.46949 574.40292\nHseni       79.41836 120.66550 564.64051 354.90063 474.12297 481.88406\nKunlong    295.23103 288.03320 468.27436 595.70536 413.07823 341.68641\nHopang     170.63913 135.62913 573.55355 403.82035 397.85908 451.51070\nNamhkan    173.27153 240.34131 715.42102 295.91660 536.85519 596.19944\nKengtung    59.85893 142.21554 613.01033 295.90429 505.40025 531.35998\nLangkho    115.18145  94.98486 518.86151 402.33622 420.65204 428.08061\nMonghsu    325.71557 216.25326 308.13805 605.02113 311.92379 247.73318\nTaunggyi   195.14541 319.81385 778.45810 150.84117 684.20905 712.80752\nPangwaun   362.45608 232.52209 523.43600 540.60474 264.64997 407.02947\nKyethi     447.10266 358.89620 233.83079 728.87329 374.90376 233.25039\nLoilen     268.92310 207.25000 406.56282 573.75476 354.79137 284.76895\nManton     646.66493 507.96808  59.52318 910.23039 280.26395 181.33894\nMongyang   209.33700 194.93467 585.61776 448.79027 401.39475 445.40621\nKunhing    255.10832 137.85278 403.66587 532.26397 281.62645 292.49814\nMongyawng  172.70139 275.15989 601.80824 432.10118 572.76394 522.91815\nTangyan    429.84475 340.39128 242.78233 719.84066 348.84991 201.49393\nNamhsan    472.04024 364.77086 180.09747 754.03913 316.54695 170.90848\n              Hsipaw  Monghsat   Mongmao   Nansang Laukkaing  Pangsang\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat   121.78922                                                  \nMongmao    185.99483 247.17708                                        \nNansang    120.24428 201.92690 164.99494                              \nLaukkaing  569.06099 626.44910 404.00848 480.60074                    \nPangsang   205.04337 256.37933  57.60801 193.36162 408.04016          \nNamtu      229.44658 231.78673 365.03882 217.61884 664.06286 392.97391\nMonghpyak  237.67919 356.84917 291.88846 227.52638 565.84279 315.11651\nKonkyan    296.74316 268.25060 281.87425 374.70456 635.92043 274.81900\nMongping   168.92101 140.95392 305.57166 287.36626 708.13447 308.33123\nHopong      62.86179 100.45714 244.16253 167.66291 628.48557 261.51075\nNyaungshwe 169.92664 286.37238 230.45003 131.18943 520.24345 257.77823\nHsihseng   136.54610 153.49551 311.98001 193.53779 670.74564 335.52974\nMongla     373.47509 429.00536 216.24705 289.45119 202.55831 217.88123\nHseni      231.48538 331.22632 184.67099 136.45492 391.74585 214.66375\nKunlong    205.10051 202.31862 224.43391 183.01388 521.88657 258.49342\nHopang     248.72536 317.64824  78.29342 196.47091 331.67199  92.57672\nNamhkan    382.79302 455.10875 223.32205 302.89487 196.46063 231.38484\nKengtung   284.08582 383.72138 207.58055 193.67980 351.48520 229.85484\nLangkho    183.05109 279.52329 134.50170  99.39859 410.41270 167.65920\nMonghsu     58.55724 137.24737 242.43599 153.59962 619.01766 260.52971\nTaunggyi   462.31183 562.88102 387.33906 365.04897 345.98041 405.59730\nPangwaun   298.12447 343.53898 187.40057 326.12960 470.63605 157.48757\nKyethi     195.17677 190.50609 377.89657 273.02385 749.99415 396.89963\nLoilen      98.04789 118.65144 190.26490  94.23028 535.57527 207.94433\nManton     359.60008 317.15603 503.79786 476.55544 907.38406 504.75214\nMongyang   267.10497 312.64797  91.06281 218.49285 326.19219 108.37735\nKunhing     90.77517 165.38834 103.91040 128.20940 500.41640 123.18870\nMongyawng  294.70967 364.40429 296.40789 191.11990 454.80044 336.16703\nTangyan    167.69794 144.59626 347.14183 249.70235 722.40954 364.76893\nNamhsan    194.47928 169.56962 371.71448 294.16284 760.45960 385.65526\n               Namtu Monghpyak   Konkyan  Mongping    Hopong Nyaungshwe\nPindaya                                                                \nYwangan                                                                \nPinlaung                                                               \nMabein                                                                 \nKalaw                                                                  \nPekon                                                                  \nLawksawk                                                               \nNawnghkio                                                              \nKyaukme                                                                \nMuse                                                                   \nLaihka                                                                 \nMongnai                                                                \nMawkmai                                                                \nKutkai                                                                 \nMongton                                                                \nMongyai                                                                \nMongkaing                                                              \nLashio                                                                 \nMongpan                                                                \nMatman                                                                 \nTachileik                                                              \nNarphan                                                                \nMongkhet                                                               \nHsipaw                                                                 \nMonghsat                                                               \nMongmao                                                                \nNansang                                                                \nLaukkaing                                                              \nPangsang                                                               \nNamtu                                                                  \nMonghpyak  346.57799                                                   \nKonkyan    478.37690 463.39594                                         \nMongping   321.66441 354.76537 242.02901                               \nHopong     206.82668 267.95563 304.49287 134.00139                     \nNyaungshwe 271.41464 103.97300 432.35040 319.32583 209.32532           \nHsihseng   131.89940 285.37627 383.49700 199.64389  91.65458  225.80242\nMongla     483.49434 408.03397 468.09747 512.61580 432.31105  347.60273\nHseni      327.41448 200.26876 448.84563 395.58453 286.41193  130.86310\nKunlong    233.60474 357.44661 329.11433 309.05385 219.06817  285.13095\nHopang     408.24516 304.26577 348.18522 379.27212 309.77356  247.19891\nNamhkan    506.32466 379.50202 481.59596 523.74815 444.13246  333.32428\nKengtung   385.33554 221.47613 474.82621 442.80821 340.47382  177.75714\nLangkho    305.03473 200.27496 386.95022 343.96455 239.63685  128.26577\nMonghsu    209.64684 232.17823 331.72187 158.90478  43.40665  173.82799\nTaunggyi   518.72748 334.17439 650.56905 621.53039 513.76415  325.09619\nPangwaun   517.03554 381.95144 263.97576 340.37881 346.00673  352.92324\nKyethi     186.90932 328.16234 400.10989 187.43974 136.49038  288.06872\nLoilen     194.24075 296.99681 334.19820 231.99959 124.74445  206.40432\nManton     448.58230 502.20840 366.66876 200.48082 310.58885  488.79874\nMongyang   413.26052 358.17599 329.39338 387.80686 323.35704  294.29500\nKunhing    296.43996 250.74435 253.74202 212.59619 145.15617  189.97131\nMongyawng  262.24331 285.56475 522.38580 455.59190 326.59925  218.12104\nTangyan    178.69483 335.26416 367.46064 161.67411 106.82328  284.14692\nNamhsan    240.95555 352.70492 352.20115 130.23777 132.70541  315.91750\n            Hsihseng    Mongla     Hseni   Kunlong    Hopang   Namhkan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla     478.66210                                                  \nHseni      312.74375 226.82048                                        \nKunlong    231.85967 346.46200 276.19175                              \nHopang     370.01334 147.02444 162.80878 271.34451                    \nNamhkan    492.09476  77.21355 212.11323 375.73885 146.18632          \nKengtung   370.72441 202.45004  66.12817 317.14187 164.29921 175.63015\nLangkho    276.27441 229.01675  66.66133 224.52741 134.24847 224.40029\nMonghsu     97.82470 424.51868 262.28462 239.89665 301.84458 431.32637\nTaunggyi   528.14240 297.09863 238.19389 471.29032 329.95252 257.29147\nPangwaun   433.06326 319.18643 330.70182 392.45403 206.98364 310.44067\nKyethi      84.04049 556.02500 388.33498 298.55859 440.48114 567.86202\nLoilen     158.84853 338.67408 227.10984 166.53599 242.89326 364.90647\nManton     334.87758 712.51416 584.63341 479.76855 577.52046 721.86149\nMongyang   382.59743 146.66661 210.19929 247.22785  69.25859 167.72448\nKunhing    220.15490 306.47566 206.47448 193.77551 172.96164 314.92119\nMongyawng  309.51462 315.57550 173.86004 240.39800 290.51360 321.21112\nTangyan     70.27241 526.80849 373.07575 268.07983 412.22167 542.64078\nNamhsan    125.74240 564.02740 411.96125 310.40560 440.51555 576.42717\n            Kengtung   Langkho   Monghsu  Taunggyi  Pangwaun    Kyethi\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho    107.16213                                                  \nMonghsu    316.91914 221.84918                                        \nTaunggyi   186.28225 288.27478 486.91951                              \nPangwaun   337.48335 295.38434 343.38498 497.61245                    \nKyethi     444.26274 350.91512 146.61572 599.57407 476.62610          \nLoilen     282.22935 184.10672 131.55208 455.91617 331.69981 232.32965\nManton     631.99123 535.95620 330.76503 803.08034 510.79265 272.03299\nMongyang   217.08047 175.35413 323.95988 374.58247 225.25026 453.86726\nKunhing    245.95083 146.38284 146.78891 429.98509 229.09986 278.95182\nMongyawng  203.87199 186.11584 312.85089 287.73864 475.33116 387.71518\nTangyan    429.95076 332.02048 127.42203 592.65262 447.05580  47.79331\nNamhsan    466.20497 368.20978 153.22576 631.49232 448.58030  68.67929\n              Loilen    Manton  Mongyang   Kunhing Mongyawng   Tangyan\nPindaya                                                               \nYwangan                                                               \nPinlaung                                                              \nMabein                                                                \nKalaw                                                                 \nPekon                                                                 \nLawksawk                                                              \nNawnghkio                                                             \nKyaukme                                                               \nMuse                                                                  \nLaihka                                                                \nMongnai                                                               \nMawkmai                                                               \nKutkai                                                                \nMongton                                                               \nMongyai                                                               \nMongkaing                                                             \nLashio                                                                \nMongpan                                                               \nMatman                                                                \nTachileik                                                             \nNarphan                                                               \nMongkhet                                                              \nHsipaw                                                                \nMonghsat                                                              \nMongmao                                                               \nNansang                                                               \nLaukkaing                                                             \nPangsang                                                              \nNamtu                                                                 \nMonghpyak                                                             \nKonkyan                                                               \nMongping                                                              \nHopong                                                                \nNyaungshwe                                                            \nHsihseng                                                              \nMongla                                                                \nHseni                                                                 \nKunlong                                                               \nHopang                                                                \nNamhkan                                                               \nKengtung                                                              \nLangkho                                                               \nMonghsu                                                               \nTaunggyi                                                              \nPangwaun                                                              \nKyethi                                                                \nLoilen                                                                \nManton     419.06087                                                  \nMongyang   246.76592 585.70558                                        \nKunhing    130.39336 410.49230 188.89405                              \nMongyawng  261.75211 629.43339 304.21734 295.35984                    \nTangyan    196.60826 271.82672 421.06366 249.74161 377.52279          \nNamhsan    242.15271 210.48485 450.97869 270.79121 430.02019  63.67613\n\n\n\n\n5.4 Computing hierarchical clustering\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\nWe can then plot the tree by using¬†plot()¬†of R Graphics as shown in the code chunk below.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n5.5 Selecting the optimal clustering algorithm\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward‚Äôs method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward‚Äôs method will be used.\n\n\n5.6 Determining Optimal Clusters\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\n\n5.7 Gap Statistic Method\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n5.8 Interpreting the dendrograms\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt‚Äôs also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\n\n\n5.9 Visually-driven hierarchical clustering analysis\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\n5.9.1 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\n5.9.2 Plotting interactive cluster heatmap using heatmaply()\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n\n\n\n\n\n\n\n\n5.10 Mapping the clusters formed\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nNext,¬†qtm()¬†of¬†tmap¬†package is used to plot the choropleth map showing the cluster formed.\n\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#spatially-constrained-clustering-skater-approach",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#spatially-constrained-clustering-skater-approach",
    "title": "Hands-on Exercise 9",
    "section": "6. Spatially Constrained Clustering: SKATER approach",
    "text": "6. Spatially Constrained Clustering: SKATER approach\nIn this section, you will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\n6.1 Converting into SpatialPolygonsDataFrame\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nshan_sp &lt;- as_Spatial(shan_sf)\n\n\n\n6.2 Computing Neighbour List\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list.\n\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\ncoords &lt;- st_coordinates(\n  st_centroid(st_geometry(shan_sf)))\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\nplot(shan.nb,\n     coords, \n     col=\"blue\", \n     add=TRUE)\n\n\nNote that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n6.3 Computing minimum spanning tree\n\n6.3.1 Calculating edge costs\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardized.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\n6.4 Computing minimum spanning tree\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nshan.mst &lt;- mstree(shan.w)\n\nAfter computing the MST, we can check its class and dimension by using the code chunk below.\n\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\n\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nNote that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nWe can display the content of shan.mst by using head() as shown in the code chunk below.\n\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   54   48  47.79331\n[2,]   54   17 109.08506\n[3,]   54   45 127.42203\n[4,]   45   52 146.78891\n[5,]   52   13 110.55197\n[6,]   13   28  92.79567\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbor list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(shan_sp, border=gray(.5))\nplot.mst(shan.mst, \n         coordinates(shan_sp), \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n6.5 Computing spatially constrained clusters using SKATER method\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e.¬†not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nWe can check the cluster assignment by using the conde chunk below.\n\nccs6 &lt;- clust6$groups\nccs6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can find out how many observations are in each cluster by means of the table command. Parenthetially, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.\n\ntable(ccs6)\n\nccs6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nlibrary(sp)\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot(clust6, \n     coordinates(shan_sp), \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\n\n\n\n\n\n\n\n\n\n\n\n6.6 Visualising the clusters in choropleth map\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\n\n\n\n\nFor easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#spatially-constrained-clustering-clustgeo-method",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#spatially-constrained-clustering-clustgeo-method",
    "title": "Hands-on Exercise 9",
    "section": "7. Spatially Constrained Clustering: ClustGeo Method",
    "text": "7. Spatially Constrained Clustering: ClustGeo Method\nIn this section, you will gain hands-on experience on using functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\n7.1 A short note about ClustGeo package\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\n7.2 Ward-like hierarchical clustering: ClustGeo\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e.¬†an object obtained with the function dist(). For sample code chunk, please refer to 5.7.6 Computing proximity matrix\n\n7.2.1 Mapping the clusters formed\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\n7.3 Spatially Constrained Hierarchical Clustering\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nNext,¬†cutree()¬†is used to derive the cluster objecct.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\nqtm(shan_sf_Gcluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n7.4 Visual Interpretation of Clusters\n\n7.4.1 Visualising individual clustering variable\nCode chunk below is used to reveal the distribution of a clustering variable (i.e RADIO_PR) by cluster.\n\nggplot(data = shan_sf_ngeo_cluster,\n       aes(x = CLUSTER, y = RADIO_PR)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.\n\n\n7.4.2 Multivariate Visualisation\nPast studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 √ó 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ‚Ñπ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe resulting shan_sf_ngeo_cluster is a tibble of 6 rows and 6 columns containing the newly added mean calculations based on the unique clusers of 1, 2, 3, 4, 5 and 6."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#visual-interpretation-of-clusters",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#visual-interpretation-of-clusters",
    "title": "Hands-on Exercise 9",
    "section": "7.4 Visual Interpretation of Clusters",
    "text": "7.4 Visual Interpretation of Clusters\n\n7.4.1 Visualising individual clustering variable\nCode chunk below is used to reveal the distribution of a clustering variable (i.e RADIO_PR) by cluster.\n\nggplot(data = shan_sf_ngeo_cluster,\n       aes(x = CLUSTER, y = RADIO_PR)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.\n\n\n7.4.2 Multivariate Visualisation\nPast studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 √ó 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ‚Ñπ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe resulting shan_sf_ngeo_cluster is a tibble of 6 rows and 6 columns containing the newly added mean calculations based on the unique clusers of 1, 2, 3, 4, 5 and 6."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#import-data-and-set-up-folders",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9.html#import-data-and-set-up-folders",
    "title": "Hands-on Exercise 9",
    "section": "2. Import Data and Set Up Folders",
    "text": "2. Import Data and Set Up Folders\n\n2.1 Importing Aspatial Data\nInfoComm variables extracted from The 2014 Myanmar Population and Housing Census Myanmar at the township level. The attribute data set is called ict. It is saved in R‚Äôs tibble data.frame format.\n\nict &lt;- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\nRows: 55 Columns: 11\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): District Pcode, District Name, Township Pcode, Township Name\ndbl (7): Total households, Radio, Television, Land line phone, Mobile phone,...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar.\n\n# Let's inspect\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe attribute data set is called ict as shown above. It is saved in R‚Äôs tibble data.frame format. The code chunk below reveal the summary statistics of ict data.frame.\n\n\n\n\n2.2 Importing Geospatial Data\nMyanmar township boundaries are downloaded from Myanmar Information Management Unit (MIMU). The geospatial data set is called myanmar_township_boundaries. It is saved in ESRI shapefile format. Let‚Äôs import it into R environment by using the¬†st_read()¬†function of¬†sf.\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex9\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nThe imported township boundary object is called¬†shan_sf. It is saved in¬†simple feature data.frame¬†format. We can view the content of the newly created¬†shan_sf¬†simple features data.frame by using the code chunk below.\n\n# Let's inspect\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe output of the shan_sf sf dataframe shows a multipolygon geometric attribute with 55 features and 6 fields of WGS84 universal geodatic CRS.\n\n\n\n\n2.3 Data Preparation\nWe will now derive new variables from the ict data set. Since the unit of measurements for each variables are the number of households, it creates bias towards townships with larger population. To mitigate this bias, we will derive new variables that are the proportion of households with Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.\nThe new variables are calculated per 1000 households. The new variables are named RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR. The new variables are then renamed to more intuitive names. The derived data set is called ict_derived.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`)\n\n\nsummary(ict_derived)\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html",
    "title": "In-class Exercise 9",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#overview",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#overview",
    "title": "In-class Exercise 9",
    "section": "1. Overview",
    "text": "1. Overview\nIn this hands-on exercise, I will learn more on performing how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n1.1 Learning Outcome\nBy the end of this hands-on exercise, I aim to accomplish the following learning:\n\nto convert GIS polygon data into R‚Äôs simple feature data.frame by using appropriate functions of sf package of R;\nto convert simple feature data.frame into R‚Äôs SpatialPolygonDataFrame object by using appropriate sf of package of R;\nto perform custer analysis by using hclust() of Base R;\nto perform spatially constrained cluster analysis using skater() of Base R; and\nto visualise the analysis output by using ggplot2 and tmap package.\n\n\n\n1.2 Installing Required Packages\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\n\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\n\n\n\n\n\n\nNote\n\n\n\nWith¬†tidyverse, we do not have to install¬†readr,¬†ggplot2¬†and¬†dplyr¬†packages separately. In fact,¬†tidyverse¬†also installs other very useful R packages such as¬†tidyr.\n\n\n\n\n1.3 Preparing the Datasets\nTwo data sets will be used in this study. They are:\n\nMyanmar Township Boundary Data (i.e.¬†myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\nBoth data sets are download from Myanmar Information Management Unit (MIMU)\n\n\n2.2 Import Data and Set Up Folders\nTwo data sets I will be using in this study are as follows:\n\nShan_sf.Rds: Myanmar Township Boundary Data (i.e.¬†myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan_ICT.Rds: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\nshan_sf_cluster.Rds: this variable combines the shan_ict and shan_sf dataframes which leads to some redundant variables like dt.x\n\nBoth data sets are download from Myanmar Information Management Unit (MIMU)\n\nshan_ict &lt;- readRDS(file = \"data/rds/shan_ict.Rds\")\nshan_sf &lt;- readRDS(file = \"data/rds/shan_sf.Rds\")\nHsan_sf_cluster &lt;- readRDS(file = \"data/rds/shan_sf_cluster.Rds\")\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar.\n\n# Let's inspect\nglimpse(Hsan_sf_cluster)\n\nRows: 55\nColumns: 19\n$ ST            &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (S‚Ä¶\n$ ST_PCODE      &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR01‚Ä¶\n$ TS            &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"‚Ä¶\n$ TS_PCODE      &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR‚Ä¶\n$ TT_HOUSEHOLDS &lt;dbl&gt; 13652, 17544, 18348, 25504, 8632, 41341, 20084, 25957, 3‚Ä¶\n$ RADIO         &lt;dbl&gt; 3907, 7324, 8890, 5908, 3880, 11607, 6399, 10048, 11257,‚Ä¶\n$ TV            &lt;dbl&gt; 7565, 8862, 4781, 13816, 6117, 25285, 10762, 16353, 1765‚Ä¶\n$ LLPHONE       &lt;dbl&gt; 482, 348, 219, 728, 628, 1739, 800, 818, 1239, 1123, 310‚Ä¶\n$ MPHONE        &lt;dbl&gt; 3559, 2849, 2207, 6363, 3389, 16900, 4315, 8321, 10409, ‚Ä¶\n$ COMPUTER      &lt;dbl&gt; 166, 226, 81, 351, 142, 1225, 381, 565, 508, 878, 2028, ‚Ä¶\n$ INTERNET      &lt;dbl&gt; 321, 136, 152, 737, 165, 1741, 316, 556, 1216, 936, 2020‚Ä¶\n$ RADIO_PR      &lt;dbl&gt; 286.18517, 417.46466, 484.52147, 231.64994, 449.49027, 2‚Ä¶\n$ TV_PR         &lt;dbl&gt; 554.1313, 505.1300, 260.5734, 541.7189, 708.6423, 611.62‚Ä¶\n$ LLPHONE_PR    &lt;dbl&gt; 35.306182, 19.835841, 11.935906, 28.544542, 72.752549, 4‚Ä¶\n$ MPHONE_PR     &lt;dbl&gt; 260.69440, 162.39170, 120.28559, 249.49028, 392.60890, 4‚Ä¶\n$ COMPUTER_PR   &lt;dbl&gt; 12.159391, 12.881897, 4.414650, 13.762547, 16.450417, 29‚Ä¶\n$ INTERNET_PR   &lt;dbl&gt; 23.513038, 7.751938, 8.284282, 28.897428, 19.114921, 42.‚Ä¶\n$ CLUSTER       &lt;chr&gt; \"1\", \"1\", \"2\", \"1\", \"3\", \"3\", \"1\", \"3\", \"3\", \"3\", \"4\", \"‚Ä¶\n$ geometry      &lt;MULTIPOLYGON [¬∞]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGO‚Ä¶\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe resulting output of the shan_ict_cluster variabble includes attribute data as well as geometry variables which consists of multipolygons. This simple feature output has a total of 55 rows and 19 columns, which has been tidied for our in-class exercise."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#conventional-hierarchical-clustering",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#conventional-hierarchical-clustering",
    "title": "In-class Exercise 9",
    "section": "2. Conventional Hierarchical Clustering",
    "text": "2. Conventional Hierarchical Clustering\n\nHierarchical ClusteringAppend to geospatial dataThe DendrogramCluster Map\n\n\nTake note that the hclust() function must first be used before the k value can be selected. In this case, hclust_ward is not a simple output data, but a hierarchical clustering object class. We have used k = 6 to output the number of groups based on our analysis of the optimal k-value in the chart\n\nproxmat &lt;- dist(shan_ict, method = \"euclidean\")\nhclust_ward &lt;- hclust(proxmat, method = \"ward.D\")\ngroups &lt;- as.factor(cutree(hclust_ward, k = 6))\n\n*Recall our analysis of the optimal k-value as discovered in Hands-on Exercise 9\n\n\n\nWe do not use left_join() since we don‚Äôt want to sort the sequence. Beisdes using as.matrix(), we can also convert it into a data table or tibble format. Note that using variables like TS.x¬†is not user-friendly and should be renamed as shown. We will also drop columns not relevant to us, e.g.¬†columns 3 to 4, and 7 to 9.\n\nshan_sf_cluster &lt;- cbind(shan_sf,\n                         as.matrix(groups)) %&gt;%\n  rename('CLUSTER' = 'as.matrix.groups.') %&gt;%\n  select(-c(3:4, 7:9)) %&gt;%\n  rename(TS = TS.x)\n\n\n\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, k = 6, border = 2:5)\n\n\n\n\n\n\n\n\n\n\nIn Chapter 2 of Prof Kam‚Äôs book and our in-class ex 2, we used 1) qtm() and 2) call tm.shape() and other functions like tm_fill(). However, we will use the QTM() function of the tmap package which serves as a fast way of generating the map for us.\nNote that the default number of colours the QTM() function will classify to is 5 so any dataset with &gt;5 categories should manually include the colours we need.\n\nqtm(shan_sf_cluster, \"CLUSTER\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#spatially-constrained-clustering",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#spatially-constrained-clustering",
    "title": "In-class Exercise 9",
    "section": "3. Spatially Constrained Clustering",
    "text": "3. Spatially Constrained Clustering\nIf the clustering values are close (attribute) and they are goegraphically close in terms of proximity, we form spatially constrained clustering. This form of clustering attempts to create more spatial homogeneity by using two methods‚Ä¶\n\nSkater: a hard spatial classification method\nClustGeo: a soft spatial classification method\n\n\n3.1 SKATER Algorithm (Spatial ‚Äôk‚Äôluster Analysis by Tree Edge Removal)\nBy generating the clustering results, we assign a weight by adding the attribute value to the edges of each node, which aims to refine the clusters through rounds of iterations.\n\n3.1.1 Computing Nearest Neighbours\nTo build our minimum spanning tree, we first calculate the number of neighbours each node has.\n\nshan.nb &lt;- poly2nb(shan_sf)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that we do not need to convert the data to sp since the two previous versions of SPDEP can accept sf variables!\n\n\n\n\n3.1.2 Visualise the Neighbours\nHere we plot the nodes and their connecting edges to adjacent neighbours. It is worth noting that not all plots are relevant for us - Prof Kam suggests to think about what your end-users want to see and be selective of what your app should expose :)\n\ncoords &lt;- st_coordinates(\n  st_centroid(shan_sf))\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\nplot(shan.nb,\n     coords, \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\n\n\n\n\n\n\n3.1.3 Computing Minimum Spanning Tree\n1) Calculate edge costs\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\n2) Incorporating these costs into a weights object\n\nshan.w &lt;- nb2listw(shan.nb,\n                   lcosts,\n                   style = 'B')\n\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n3) Computing MST\nThe mstree() function is of the spdep package which combines the weight matrix and proximity matrix together.\n\nshan.mst &lt;- mstree(shan.w)\n\nAfter computing the MST, we can check its class and dimension by using the code chunk below. It should return a list-object data format\n\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   26   34 140.95392\n[2,]   34   24 113.80917\n[3,]   34   16 119.86993\n[4,]   16   13 131.67061\n[5,]   13   28  92.79567\n[6,]   28   12  78.78999\n\n\n4) Visualise the MST\n\nplot(st_geometry(shan_sf), border=gray(.5))\nplot.mst(shan.mst, \n         coords,\n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n\n\n\n\n5) Computing Spatially Constrained Clusters\nNext, we will compute the spatially constrained cluster using¬†skater()¬†of¬†spdep¬†package.\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe result of the¬†skater()¬†is an object of class¬†skater. We can examine its contents by using the code chunk below. As seen below, this is a list of 8 list items.\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 45 55 52 37 34 16 25 54 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 45 37 34 34 52 25 16 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nWe are also able to check the cluster assignment.\n\nccs6 &lt;- clust6$groups\nccs6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\n\n\nWe can use as.factor() to encode our numerical data accordingly. Note that the cluster numbers will be sorted in ascending format by default as 1,2,3‚Ä¶5.\n\nlibrary(sp)\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot(clust6, \n     coords, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1], coords[id2,\n: \"add\" is not a graphical parameter\n\n\n\n\n\n\n\n\n\nLastly, let us plot the map using the qtm() function of the spdep package.\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\n3.2 ClustGeo Method\nThis is a soft classification where the user can manipulate and select specific interactions they desire, e.g.¬†spatial interactions, where the user can choose specify a certain range.\n\n3.2.1 Compute spatial distance matrix\nThe first step we should take is to utilise the st_distance() function of the sf package to compute the distance matrix.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\n\n\n\n\n\n\nNote\n\n\n\nThe fieldnames will be the destination while the row numbers are numbers. Hence, we keep the distance matri. Notice that as.dist() is used to convert the data frame into matrix.\n\n\n\n\n3.2.2 Plotting Cluster Graphs\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below. The 0.1 breaks interval will output a graph that will create intervals of 0.1\n\ncr &lt;- choicealpha(proxmat, distmat, \n                  range.alpha = seq(0, 1, 0.1), \n                  K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 Saving ClustGeo Output\nYou will want to save the output and await for user to click a button on the app before the app exposes the updated charts.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\ngroups &lt;- as.factor(cutree(clustG, k=6))\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_Gcluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n3.2.4 Multivariate Visualisation\nggarray() is an extension of ggplot(). By using a ssimilar y-axis and facet-ing these plots in a row, we can easily visualise how each cluster might differ by ICT variables.\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI made a realisation that we should use a set.seed() function to ensure that the cluster numbers do not change since it will inherently be generated differently for each run of the codes. E.g. cluster 1 can become cluster 2‚Äôs patterns the next time we run the codes."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#overview",
    "title": "Hands-on Exercise 10",
    "section": "1. Overview",
    "text": "1. Overview\nTo begin with, geographically weighted regression (GWR)¬†is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable).\nIn this hands-on exercise, I will be building¬†hedonic pricing¬†models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.\n\n1.1 Installing Required Packages\nThe R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, sfdep)\n\nInstalling package into 'C:/Users/Samantha/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\nalso installing the dependency 'glue'\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4:\n  cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4/PACKAGES'\n\n\npackage 'glue' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'glue'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\Samantha\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\glue\\libs\\x64\\glue.dll\nto C:\\Users\\Samantha\\AppData\\Local\\R\\win-library\\4.4\\glue\\libs\\x64\\glue.dll:\nPermission denied\n\n\nWarning: restored 'glue'\n\n\npackage 'gtsummary' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Samantha\\AppData\\Local\\Temp\\Rtmp06xofg\\downloaded_packages\n\n\n\ngtsummary installed\n\n\nWarning in pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, : Failed to install/load:\ngtsummary\n\n\n\n\n1.2 Importing the Datasets\n\n1.2.1 Aspatial Data\nThe¬†condo_resale_2015¬†is in csv file format. The codes chunk below uses¬†read_csv()¬†function of¬†readr¬†package to import¬†condo_resale_2015¬†into R as a tibble data frame called¬†condo_resale.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly. By using glimpse(), we can observe that it returns 1,436 rows and 23 columns.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,‚Ä¶\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,‚Ä¶\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3‚Ä¶\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320‚Ä¶\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,‚Ä¶\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,‚Ä¶\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402‚Ä¶\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0‚Ä¶\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121‚Ä¶\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,‚Ä¶\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0‚Ä¶\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0‚Ä¶\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528‚Ä¶\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116‚Ä¶\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709‚Ä¶\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709‚Ä¶\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307‚Ä¶\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581‚Ä¶\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0‚Ä¶\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3‚Ä¶\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n\n\nNext,¬†summary()¬†of base R is used to display the summary statistics of¬†cond_resale¬†tibble data frame.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n1.2.2 Geospatial Data\nThe geospatial data used in this hands-on exercise is MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format consisting of URA Master Plan 2014‚Äôs planning subzone boundaries.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex10\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe geometry data in this shapefile consists of polygon features which are used to represent these geographic boundaries. We can also see that the GIS data is in the svy21 projected coordinates system.\n\n\n\n\n\n1.3 Data Wrangling\n\n1.3.1 Aspatial Data\nWe first need to convert the condo_resale dibble data frame into a sf object. We will also need to convert the projection from WSG84 into SVY21, which is the projection used in Singapore.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)\n\nNext, head() is used to list the content of condo_resale.sf object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 √ó 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ‚Ñπ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\n\n\n\n\n\n\nObservations\n\n\n\nNotice that st_transform() of sf package has successfully helped us to convert the geometric coordinates from wgs84 (i.e.¬†crs:4326) to svy21 (i.e.¬†crs=3414).\n\n\n\n\n1.3.2 Geospatial Data\nSimilar to the aspatial data, we need to convert the projection from WSG84 into SVY21.\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\n\nAfter transforming the projection metadata, we can indeed see that the projection has been transformed to 3414¬†by using¬†st_crs()¬†of¬†sf¬†package.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNext, we can reveal the extent of¬†mpsz_svy21¬†by using¬†st_bbox()¬†of sf package.\n\nst_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 10",
    "section": "2. Exploratory Data Analysis (EDA)",
    "text": "2. Exploratory Data Analysis (EDA)\n\n2.1 Using Statistical Graphics\nWe will first plot the distribution of the selling price of the condominiums by using the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can observe that the distribution of the selling price is right-skewed, with a long tail to the right. This skewed distribution is typical of real estate prices, where most of the properties are sold at a lower price, with a few sold at a much higher price.\n\n\nHowever, working with the raw selling price can be problematic, especially when the distribution is skewed. We can transform the selling price using the natural logarithm to make the distribution more symmetric.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nNow let‚Äôs plot the distribution of the log-transformed selling price, i.e.¬†LOG_SELLING_PRICE.\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the distrbiution above, we see that the distribution of the log-transformed selling price is more symmetric compared to the raw selling price. This transformation will be useful when we calibrate the hedonic pricing model.\n\n\n\n\n2.2 Multiple Histogram Plots distribution of variables\nWe will now draw a small multiples of histograms to visualise the distribution of the independent variables in the hedonic pricing model. The code below will create 12 histograms. Then, ggarrange() is used to arrange the histograms in a 3x4 grid.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n\n\n\n\n2.3 Drawing Statistical Point Map\nLastly, we will draw a statistical point map to visualise the distribution of the log-transformed selling price of the condominiums in 2015. The code below will create a statistical point map using the tmap package.\n\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\n\n\nmpsz_svy21 &lt;- st_make_valid(mpsz_svy21)\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21) +\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style = \"quantile\") +\n  tm_view(set.zoom.limits = c(11, 14))\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#hedonic-pricing-model",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#hedonic-pricing-model",
    "title": "Hands-on Exercise 10",
    "section": "3. Hedonic Pricing Model",
    "text": "3. Hedonic Pricing Model\n\n3.1 Simple Linear Regression\nWe will first set the baseline model using simple linear regression (SLR). The SLR model will be used to estimate the relationship between the log-transformed selling price and the area of the condominium.\nThe code below will calibrate the SLR model for SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nThe functions summary() and anova() are used to obtain the summary statistics and the ANOVA table of the SLR model, respectively.\n\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the output report above, SELLING_PRICE can be explained using the formula:\n\\[ y = -258121.1 + 14719x1 \\]\nThe R-squared value of 0.4518 indicates that the area of the condominium can explain 45% of the variation in the selling price.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\n\n\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot‚Äôs geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can tell that there are a few outliers with relatively higher selling prices in the scatterplot.\n\n\n3.2 Multiple Linear Regression\n\n3.2.1 Visualizing the relationships of the independent variables\nBefore we begin calibrating the multiple linear regression (MLR) model, we will first visualise the relationships between the independent variables to identify any multicollinearity issues. The code below will create a correlation matrix of the independent variables.\nTo identify the pattern in the matrix, we also need to consider the order of the variables. There are four methods:\n\nThe ‚ÄúAOE‚Äù method is used to order the variables based on the average of the absolute off-diagonal correlations.\nThe ‚ÄúFPC‚Äù method is used to order the variables based on the first principal component.\nThe ‚Äúhclust‚Äù method is used to order the variables based on the hierarchical clustering.\nThe ‚Äúalphabet‚Äù method is used to order the variables based on the alphabetical order.\n\nWe will use the ‚ÄúAOE‚Äù method in this example.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the matrix above, we can clearly see that Freehold is highly correlated to LEASE_99YEAR. This is expected as the two variables are related to the tenure of the property. We will need to remove one of the variables to avoid multicollinearity issues. In this case, we will remove LEASE_99YEAR from the hedonic pricing model.\n\n\n\n\n\n3.3 Calibrating the Multiple Linear Regression Model\nThe code chunk below using¬†lm()¬†to calibrate the multiple linear regression model.\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.4 Preparing Publication Quality Table: olsrr method\nWe can infer that not all the independent variables are significant in explaining the variation in the selling price. To identify the significant variables, we can use the olsrr package to obtain the summary statistics of the MLR model. The olsrr package provides a comprehensive summary statistics of the MLR model, including the ANOVA table, the coefficients, the R-squared value, and the p-values of the estimates.\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     751998.679 \nR-Squared                    0.651       MSE                571471422208.591 \nAdj. R-Squared               0.647       Coef. Var                    43.168 \nPred R-Squared               0.638       AIC                       42966.758 \nMAE                     414819.628       SBC                       43051.072 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\n3.5 Preparing Publication Quality Table: gtsummary method\nWe can also use the gtsummary package to obtain the summary statistics of the MLR model in an elegant and flexible way.\nIn the code chunk below,¬†tbl_regression()¬†is used to create a well formatted regression report.\n\ntbl_regression(condo.mlr1, intercept = TRUE)\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using¬†add_glance_table()¬†or adding as a table source note by using¬†add_glance_source_note()¬†as shown in the code chunk below.\n\nlibrary(dplyr)\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %&gt;% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n3.5.1 Checking for multicolinearity\nWe can also use the olsrr package to check for multicollinearity issues in the MLR model. The olsrr package provides the variance inflation factor (VIF) and the tolerance of the independent variables. The VIF measures the extent of multicollinearity in the model, while the tolerance measures the proportion of the variance of an independent variable that is not explained by the other independent variables.\nIt provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nIn the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n3.5.2 Test for Non-Linearity\nIt is also important to test for non-linearity in the model. The¬†ols_plot_resid_fit()¬†of¬†olsrr package is used to test for non-linearity in the model.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\n\n\n\n\nFrom the figure above, we can tell that the residuals are randomly scattered around the zero line. This indicates that there are no signs of non-linearity in the model.\n\n1) Test for Normality Assumption\nLastly, we will test for the normality assumption of the residuals. The¬†ols_plot_resid_hist()¬†of¬†olsrr package is used to test for the normality assumption of the residuals.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\n\n\n\n\nThe figure above shows that the residuals are normally distributed, which is a key assumption of the multiple linear regression model.\nAlso, if you want to use a formal statistical test method, you can use the¬†ols_test_normality()¬†of¬†olsrr package to test for the normality assumption of the residuals.\n\nols_test_normality(condo.mlr1)\n\nWarning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): ties should not be\npresent for the one-sample Kolmogorov-Smirnov test\n\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n2) Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nVariable(s) \"MLR_RES\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, the Moran‚Äôs I test will be performed\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe summary table above reveals that the p-value of the Moran‚Äôs I test is way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\nSince the Global Moran‚Äôs I = 0.1424418 is greater than 0, we can infer that there is sign of positive spatial autocorrelation."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Hands-on Exercise 10",
    "section": "4. Building Hedonic Pricing Models using GWmodel",
    "text": "4. Building Hedonic Pricing Models using GWmodel\n\n4.1 Building Fixed Bandwidth GWR Model\n\n4.1.1 Computing the Bandwidth\nThe first step in calibrating the GWR model is to compute the bandwidth. The bandwidth is a critical parameter in the GWR model as it determines the number of observations that will be used to calibrate the local regression model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are several methods to compute the bandwidth, they are: CV cross-validation approach and AIC corrected (AICc) approach. In this example, we will use the cross-validation (CV) method to compute the bandwidth. The CV method is a robust method to compute the bandwidth as it minimizes the prediction error of the GWR model.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3405 metres. This means that the GWR model will use the observations within 971.3405 metres to calibrate the local regression model. Metres is used as the unit of measurement because the data is projected in SVY21.\n\n\n4.1.2 Calibrating the GWR Model\nThe next step is to calibrate the GWR model using the recommended bandwidth. The code chunk below will calibrate the GWR model using the recommended bandwidth.\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\nThe output is saved in a list of class ‚Äúgwrm‚Äù. The code below can be used to display the model output.\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-22 18:31:13.865109 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2024-10-22 18:31:14.738371 \n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the global multiple linear regression model of 42967.1. This indicates that the GWR model is a better model to explain the variation in the selling price.\n\n\n\n4.2 Building Adaptive Bandwidth GWR Model\n\n4.2.1 Computing the Bandwidth\nUnlike the fixed bandwidth GWR model, the adaptive bandwidth GWR model does not require the bandwidth to be computed. Instead, the bandwidth is computed for each observation based on the number of observations within a certain distance. The code chunk below will calibrate the adaptive bandwidth GWR model. To do this, we need to set the adaptive argument to TRUE.\n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\n4.2.2 Calibrating the GWR Model\nThe code chunk below will calibrate the GWR model using the recommended bandwidth.\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\nSimilarly, we can also display the model output.\n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-22 18:31:21.146594 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-10-22 18:31:22.472292 \n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\n4.3 Visualizing GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local colinearity. In the presence of strong local colinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its ‚Äúdata‚Äù slot in an object called SDF of the output list.\n\n\n4.4 Converting SDF into sf data.frame\nTo visualize the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\n\ncondo_resale.sf.adaptive.svy21 &lt;- st_transform(condo_resale.sf.adaptive, 3414)\ncondo_resale.sf.adaptive.svy21  \n\nSimple feature collection with 1436 features and 51 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14940.85 ymin: 24765.67 xmax: 43352.45 ymax: 48382.81\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n    Intercept  AREA_SQM        AGE  PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n1   2050011.7  9561.892  -9514.634 -120681.9      319266.92       -393417.79\n2   1633128.2 16576.853 -58185.479 -149434.2      441102.18        325188.74\n3   3433608.2 13091.861 -26707.386 -259397.8     -120116.82        535855.81\n4    234358.9 20730.601 -93308.988 2426853.7      480825.28        314783.72\n5   2285804.9  6722.836 -17608.018 -316835.5       90764.78       -137384.61\n6  -3568877.4  6039.581 -26535.592  327306.1     -152531.19       -700392.85\n7  -2874842.4 16843.575 -59166.727 -983577.2     -177810.50       -122384.02\n8   2038086.0  6905.135 -17681.897 -285076.6       70259.40        -96012.78\n9   1718478.4  9580.703 -14401.128  105803.4     -657698.02       -123276.00\n10  3457054.0 14072.011 -31579.884 -234895.4       79961.45        548581.04\n   PROX_URA_GROWTH_AREA    PROX_MRT  PROX_PARK PROX_PRIMARY_SCH\n1            -159980.20  -299742.96 -172104.47        242668.03\n2            -142290.39 -2510522.23  523379.72       1106830.66\n3            -253621.21  -936853.28  209099.85        571462.33\n4           -2679297.89 -2039479.50 -759153.26       3127477.21\n5             303714.81   -44567.05  -10284.62         30413.56\n6             -28051.25   733566.47 1511488.92        320878.23\n7            1397676.38 -2745430.34  710114.74       1786570.95\n8             269368.71   -14552.99   73533.34         53359.73\n9            -361974.72  -476785.32 -132067.59        -40128.92\n10           -150024.38 -1503835.53  574155.47        108996.67\n   PROX_SHOPPING_MALL PROX_BUS_STOP  NO_Of_UNITS FAMILY_FRIENDLY  FREEHOLD\n1          300881.390     1210615.4  104.8290640       -9075.370  303955.6\n2          -87693.378     1843587.2 -288.3441183      310074.664  396221.3\n3         -126732.712     1411924.9   -9.5532945        5949.746  168821.7\n4          -29593.342     7225577.5 -161.3551620     1556178.531 1212515.6\n5           -7490.586      677577.0   42.2659674       58986.951  328175.2\n6          258583.881     1086012.6 -214.3671271      201992.641  471873.1\n7         -384251.210     5094060.5   -0.9212521      359659.512  408871.9\n8          -39634.902      735767.1   30.1741069       55602.506  347075.0\n9          276718.757     2815772.4  675.1615559      -30453.297  503872.8\n10        -454726.822     2123557.0  -21.3044311     -100935.586  213324.6\n         y    yhat    residual CV_Score Stud_residual Intercept_SE AREA_SQM_SE\n1  3000000 2886532   113468.16        0    0.38207013     516105.5    823.2860\n2  3880000 3466801   413198.52        0    1.01433140     488083.5    825.2380\n3  3325000 3616527  -291527.20        0   -0.83780678     963711.4    988.2240\n4  4250000 5435482 -1185481.63        0   -2.84614670     444185.5    617.4007\n5  1400000 1388166    11834.26        0    0.03404453    2119620.6   1376.2778\n6  1320000 1516702  -196701.94        0   -0.72065800   28572883.7   2348.0091\n7  3410000 3266881   143118.77        0    0.41291992     679546.6    893.5893\n8  1420000 1431955   -11955.27        0   -0.03033109    2217773.1   1415.2604\n9  2025000 1832799   192200.83        0    0.52018109     814281.8    943.8434\n10 2550000 2223364   326635.53        0    1.10559735    2410252.0   1271.4073\n      AGE_SE PROX_CBD_SE PROX_CHILDCARE_SE PROX_ELDERLYCARE_SE\n1   5889.782    37411.22          319111.1           120633.34\n2   6226.916    23615.06          299705.3            84546.69\n3   6510.236    56103.77          349128.5           129687.07\n4   6010.511   469337.41          304965.2           127150.69\n5   8180.361   410644.47          698720.6           327371.55\n6  14601.909  5272846.47         1141599.8          1653002.19\n7   8970.629   346164.20          530101.1           148598.71\n8   8661.309   438035.69          742532.8           399221.05\n9  11791.208    89148.35          704630.7           329683.30\n10  9941.980   173532.77          500976.2           281876.74\n   PROX_URA_GROWTH_AREA_SE PROX_MRT_SE PROX_PARK_SE PROX_PRIMARY_SCH_SE\n1                 56207.39    185181.3     205499.6            152400.7\n2                 76956.50    281133.9     229358.7            165150.7\n3                 95774.60    275483.7     314124.3            196662.6\n4                470762.12    279877.1     227249.4            240878.9\n5                474339.56    363830.0     364580.9            249087.7\n6               5496627.21    730453.2    1741712.0            683265.5\n7                371692.97    375511.9     297400.9            344602.8\n8                517977.91    423155.4     440984.4            261251.2\n9                153436.22    285325.4     304998.4            278258.5\n10               239182.57    571355.7     599131.8            331284.8\n   PROX_SHOPPING_MALL_SE PROX_BUS_STOP_SE NO_Of_UNITS_SE FAMILY_FRIENDLY_SE\n1               109268.8         600668.6       218.1258           131474.7\n2                98906.8         410222.1       208.9410           114989.1\n3               119913.3         464156.7       210.9828           146607.2\n4               177104.1         562810.8       361.7767           108726.6\n5               301032.9         740922.4       299.5034           160663.7\n6              2931208.6        1418333.3       602.5571           331727.0\n7               249969.5         821236.4       532.1978           129241.2\n8               351634.0         775038.4       338.6777           171895.1\n9               289872.7         850095.5       439.9037           220223.4\n10              265529.7         631399.2       259.0169           189125.5\n   FREEHOLD_SE Intercept_TV AREA_SQM_TV     AGE_TV PROX_CBD_TV\n1     115954.0    3.9720784   11.614302  -1.615447 -3.22582173\n2     130110.0    3.3460017   20.087361  -9.344188 -6.32792021\n3     141031.5    3.5629010   13.247868  -4.102368 -4.62353528\n4     138239.1    0.5276150   33.577223 -15.524302  5.17080808\n5     210641.1    1.0784029    4.884795  -2.152474 -0.77155660\n6     374347.3   -0.1249043    2.572214  -1.817269  0.06207388\n7     182216.9   -4.2305303   18.849348  -6.595605 -2.84136028\n8     216649.4    0.9189786    4.879056  -2.041481 -0.65080678\n9     220473.7    2.1104224   10.150733  -1.221345  1.18682383\n10    206346.2    1.4343123   11.068059  -3.176418 -1.35360852\n   PROX_CHILDCARE_TV PROX_ELDERLYCARE_TV PROX_URA_GROWTH_AREA_TV PROX_MRT_TV\n1         1.00048819          -3.2612693            -2.846248368 -1.61864578\n2         1.47178634           3.8462625            -1.848971738 -8.92998600\n3        -0.34404755           4.1319138            -2.648105057 -3.40075727\n4         1.57665606           2.4756745            -5.691404992 -7.28705261\n5         0.12990138          -0.4196596             0.640289855 -0.12249416\n6        -0.13361179          -0.4237096            -0.005103357  1.00426206\n7        -0.33542751          -0.8235874             3.760298131 -7.31116712\n8         0.09462126          -0.2405003             0.520038994 -0.03439159\n9        -0.93339393          -0.3739225            -2.359121712 -1.67102293\n10        0.15961128           1.9461735            -0.627237944 -2.63204802\n   PROX_PARK_TV PROX_PRIMARY_SCH_TV PROX_SHOPPING_MALL_TV PROX_BUS_STOP_TV\n1   -0.83749312           1.5923022            2.75358842        2.0154464\n2    2.28192684           6.7019454           -0.88662640        4.4941192\n3    0.66565951           2.9058009           -1.05686949        3.0419145\n4   -3.34061770          12.9836105           -0.16709578       12.8383775\n5   -0.02820944           0.1220998           -0.02488294        0.9145046\n6    0.86781794           0.4696245            0.08821750        0.7656963\n7    2.38773567           5.1844351           -1.53719231        6.2029165\n8    0.16674816           0.2042469           -0.11271635        0.9493299\n9   -0.43301073          -0.1442145            0.95462153        3.3123012\n10   0.95831249           0.3290120           -1.71252687        3.3632555\n   NO_Of_UNITS_TV FAMILY_FRIENDLY_TV FREEHOLD_TV  Local_R2\n1     0.480589953        -0.06902748    2.621347 0.8846744\n2    -1.380026395         2.69655779    3.045280 0.8899773\n3    -0.045279967         0.04058290    1.197050 0.8947007\n4    -0.446007570        14.31276425    8.771149 0.9073605\n5     0.141120178         0.36714544    1.557983 0.9510057\n6    -0.355762335         0.60891234    1.260522 0.9247586\n7    -0.001731033         2.78285441    2.243875 0.8310458\n8     0.089093858         0.32346758    1.602012 0.9463936\n9     1.534793921        -0.13828365    2.285410 0.8380365\n10   -0.082251138        -0.53369623    1.033819 0.9080753\n                    geometry\n1  POINT (22085.12 29951.54)\n2   POINT (25656.84 34546.2)\n3   POINT (23963.99 32890.8)\n4  POINT (27044.28 32319.77)\n5  POINT (41042.56 33743.64)\n6   POINT (39717.04 32943.1)\n7   POINT (28419.1 33513.37)\n8  POINT (40763.57 33879.61)\n9  POINT (23595.63 28884.78)\n10 POINT (24586.56 33194.31)\n\ngwr.adaptive.output &lt;- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive &lt;- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472‚Ä¶\n$ SELLING_PRICE           &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1‚Ä¶\n$ AREA_SQM                &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 1‚Ä¶\n$ AGE                     &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,‚Ä¶\n$ PROX_CBD                &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783‚Ä¶\n$ PROX_CHILDCARE          &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543‚Ä¶\n$ PROX_ELDERLYCARE        &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.‚Ä¶\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.4106‚Ä¶\n$ PROX_HAWKER_MARKET      &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969‚Ä¶\n$ PROX_KINDERGARTEN       &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076‚Ä¶\n$ PROX_MRT                &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.‚Ä¶\n$ PROX_PARK               &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.‚Ä¶\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.‚Ä¶\n$ PROX_TOP_PRIMARY_SCH    &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.‚Ä¶\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.‚Ä¶\n$ PROX_SUPERMARKET        &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.‚Ä¶\n$ PROX_BUS_STOP           &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340‚Ä¶\n$ NO_Of_UNITS             &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34‚Ä¶\n$ FAMILY_FRIENDLY         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0‚Ä¶\n$ FREEHOLD                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1‚Ä¶\n$ LEASEHOLD_99YR          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ LOG_SELLING_PRICE       &lt;dbl&gt; 14.91412, 15.17135, 15.01698, 15.26243, 14.151‚Ä¶\n$ MLR_RES                 &lt;dbl&gt; -1489099.55, 415494.57, 194129.69, 1088992.71,‚Ä¶\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,‚Ä¶\n$ AREA_SQM.1              &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672‚Ä¶\n$ AGE.1                   &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,‚Ä¶\n$ PROX_CBD.1              &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66‚Ä¶\n$ PROX_CHILDCARE.1        &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28‚Ä¶\n$ PROX_ELDERLYCARE.1      &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72‚Ä¶\n$ PROX_URA_GROWTH_AREA.1  &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929‚Ä¶\n$ PROX_MRT.1              &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.‚Ä¶\n$ PROX_PARK.1             &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, ‚Ä¶\n$ PROX_PRIMARY_SCH.1      &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, ‚Ä¶\n$ PROX_SHOPPING_MALL.1    &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34‚Ä¶\n$ PROX_BUS_STOP.1         &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51‚Ä¶\n$ NO_Of_UNITS.1           &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35‚Ä¶\n$ FAMILY_FRIENDLY.1       &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, ‚Ä¶\n$ FREEHOLD.1              &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3‚Ä¶\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1‚Ä¶\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13‚Ä¶\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,‚Ä¶\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146‚Ä¶\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962‚Ä¶\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2‚Ä¶\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3‚Ä¶\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064‚Ä¶\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720‚Ä¶\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327‚Ä¶\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433‚Ä¶\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830‚Ä¶\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580‚Ä¶\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087‚Ä¶\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.‚Ä¶\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922‚Ä¶\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50‚Ä¶\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16‚Ä¶\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641‚Ä¶\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.‚Ä¶\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.‚Ä¶\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301‚Ä¶\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080‚Ä¶\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766‚Ä¶\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745‚Ä¶\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6‚Ä¶\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870‚Ä¶\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617‚Ä¶\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104‚Ä¶\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16‚Ä¶\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0‚Ä¶\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44‚Ä¶\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764‚Ä¶\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.‚Ä¶\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.‚Ä¶\n$ coords.x1               &lt;dbl&gt; 22085.12, 25656.84, 23963.99, 27044.28, 41042.‚Ä¶\n$ coords.x2               &lt;dbl&gt; 29951.54, 34546.20, 32890.80, 32319.77, 33743.‚Ä¶\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.‚Ä¶\n\n\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n4.5 Visualizing local R2\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\n\n\n4.6 Visualizing coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nLastly, we can also visualize the GWR output by URA planning region. The code chunk below will visualize the local R2 of the GWR output for the Central region.\n\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html",
    "href": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html",
    "title": "Hands-on Exercise 11",
    "section": "",
    "text": "In this hands-on execise, I will delve into predictive modelling which leverages statistical learning or machine learning techniques to predict outcomes, i.e.¬†to predict the likelihood of an event happening in the future. Additionally, I will also be using a set of known outcome and predictors (i.e.¬†a variables) to calibrate the predictive models.\nGeospatial predictive modelling is conceptually rooted in the principle that the occurrences of events being modeled are limited in distribution. When geographically referenced data are used, occurrences of events are neither uniform nor random in distriabution over space. There are geospatial factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences.\n\nBy the end of this hands-on exercise, I would have learnt to build predictive model by using geographical random forest method and acquire these skills:\n\npreparing training and test data sets by using appropriate data sampling methods,\ncalibrating predictive models by using both geospatial statistical learning and machine learning methods,\ncomparing and selecting the best model for predicting the future outcome,\npredicting the future outcomes by using the best model calibrated.\n\n\n\n\nThe set of codes below are used to install our required packages for this exercise.\n\nA list called packages will be created and will consists of all the R packages required to accomplish this exercise.\nCheck if R packages on package have been installed in R and if not, they will be installed.\nAfter all the R packages have been installed, they will be loaded.\n\n\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, Metrics, tidyverse)\n\n\n\n\n\nAspatial dataset:\n\nHDB Resale data: a list of HDB resale transacted prices in Singapore from Jan 2017 onwards. It is in csv format which can be downloaded from Data.gov.sg.\n\nGeospatial dataset:\n\nMP14_SUBZONE_WEB_PL: a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg\n\nLocational factors with geographic coordinates:\n\nDownloaded from Data.gov.sg.\n\nEldercare data is a list of eldercare in Singapore. It is in shapefile format.\nHawker Centre data is a list of hawker centres in Singapore. It is in geojson format.\nParks data is a list of parks in Singapore. It is in geojson format.\nSupermarket data is a list of supermarkets in Singapore. It is in geojson format.\nCHAS clinics data is a list of CHAS clinics in Singapore. It is in geojson format.\nChildcare service data is a list of childcare services in Singapore. It is in geojson format.\nKindergartens data is a list of kindergartens in Singapore. It is in geojson format.\n\nDownloaded from Datamall.lta.gov.sg.\n\nMRT data is a list of MRT/LRT stations in Singapore with the station names and codes. It is in shapefile format.\nBus stops data is a list of bus stops in Singapore. It is in shapefile format.\n\n\nLocational factors without geographic coordinates:\n\nDownloaded from Data.gov.sg.\n\nPrimary school data is extracted from the list on General information of schools from data.gov portal. It is in csv format.\n\nRetrieved/Scraped from other sources\n\nCBD coordinates obtained from Google.\nShopping malls data is a list of Shopping malls in Singapore obtained from Wikipedia.\nGood primary schools is a list of primary schools that are ordered in ranking in terms of popularity and this can be found at Local Salary Forum."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#overview",
    "title": "Hands-on Exercise 11",
    "section": "",
    "text": "In this hands-on execise, I will delve into predictive modelling which leverages statistical learning or machine learning techniques to predict outcomes, i.e.¬†to predict the likelihood of an event happening in the future. Additionally, I will also be using a set of known outcome and predictors (i.e.¬†a variables) to calibrate the predictive models.\nGeospatial predictive modelling is conceptually rooted in the principle that the occurrences of events being modeled are limited in distribution. When geographically referenced data are used, occurrences of events are neither uniform nor random in distriabution over space. There are geospatial factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences.\n\nBy the end of this hands-on exercise, I would have learnt to build predictive model by using geographical random forest method and acquire these skills:\n\npreparing training and test data sets by using appropriate data sampling methods,\ncalibrating predictive models by using both geospatial statistical learning and machine learning methods,\ncomparing and selecting the best model for predicting the future outcome,\npredicting the future outcomes by using the best model calibrated.\n\n\n\n\nThe set of codes below are used to install our required packages for this exercise.\n\nA list called packages will be created and will consists of all the R packages required to accomplish this exercise.\nCheck if R packages on package have been installed in R and if not, they will be installed.\nAfter all the R packages have been installed, they will be loaded.\n\n\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, Metrics, tidyverse)\n\n\n\n\n\nAspatial dataset:\n\nHDB Resale data: a list of HDB resale transacted prices in Singapore from Jan 2017 onwards. It is in csv format which can be downloaded from Data.gov.sg.\n\nGeospatial dataset:\n\nMP14_SUBZONE_WEB_PL: a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg\n\nLocational factors with geographic coordinates:\n\nDownloaded from Data.gov.sg.\n\nEldercare data is a list of eldercare in Singapore. It is in shapefile format.\nHawker Centre data is a list of hawker centres in Singapore. It is in geojson format.\nParks data is a list of parks in Singapore. It is in geojson format.\nSupermarket data is a list of supermarkets in Singapore. It is in geojson format.\nCHAS clinics data is a list of CHAS clinics in Singapore. It is in geojson format.\nChildcare service data is a list of childcare services in Singapore. It is in geojson format.\nKindergartens data is a list of kindergartens in Singapore. It is in geojson format.\n\nDownloaded from Datamall.lta.gov.sg.\n\nMRT data is a list of MRT/LRT stations in Singapore with the station names and codes. It is in shapefile format.\nBus stops data is a list of bus stops in Singapore. It is in shapefile format.\n\n\nLocational factors without geographic coordinates:\n\nDownloaded from Data.gov.sg.\n\nPrimary school data is extracted from the list on General information of schools from data.gov portal. It is in csv format.\n\nRetrieved/Scraped from other sources\n\nCBD coordinates obtained from Google.\nShopping malls data is a list of Shopping malls in Singapore obtained from Wikipedia.\nGood primary schools is a list of primary schools that are ordered in ranking in terms of popularity and this can be found at Local Salary Forum."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#data-preprocessing",
    "href": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#data-preprocessing",
    "title": "Hands-on Exercise 11",
    "section": "2. Data Preprocessing",
    "text": "2. Data Preprocessing\n\n2.1 Reading data file to rds\nFirstly, let us read the input data set, in which we will assign it to a mdata variable. This outputs a simple feature data frame.\n\nmdata &lt;- read_rds(\"data/aspatial/mdata.rds\")\n\n\n\n2.2 Data Sampling\nNext, we will split the entire data into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\nset.seed(1234)\nresale_split &lt;- initial_split(mdata, \n                              prop = 6.5/10,)\ntrain_data &lt;- training(resale_split)\ntest_data &lt;- testing(resale_split)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#computing-correlation-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#computing-correlation-matrix",
    "title": "Hands-on Exercise 11",
    "section": "3. Computing Correlation Matrix",
    "text": "3. Computing Correlation Matrix\nBefore loading the predictors into a predictive model, it is always a good practice to use correlation matrix to examine if there is sign of multicolinearity.\n\nmdata_nogeo &lt;- mdata %&gt;%\n  st_drop_geometry()\ncorrplot::corrplot(cor(mdata_nogeo[, 2:17]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#retrieving-the-stored-data",
    "href": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#retrieving-the-stored-data",
    "title": "Hands-on Exercise 11",
    "section": "4. Retrieving the Stored Data",
    "text": "4. Retrieving the Stored Data\n\nwrite_rds(train_data, \"data/model/train_data.rds\")\nwrite_rds(test_data, \"data/model/test_data.rds\")\n\n\ntrain_data &lt;- read_rds(\"data/model/train_data.rds\")\ntest_data &lt;- read_rds(\"data/model/test_data.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#building-a-non-spatial-multiple-linear-regression",
    "href": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#building-a-non-spatial-multiple-linear-regression",
    "title": "Hands-on Exercise 11",
    "section": "5. Building a non-spatial multiple linear regression",
    "text": "5. Building a non-spatial multiple linear regression\n\nprice_mlr &lt;- lm(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data)\nsummary(price_mlr)\n\n\nCall:\nlm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + \n    PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + \n    PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\nfloor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\nstorey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\nremaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\nPROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\nPROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\nPROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\nPROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\nPROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\nPROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\nPROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\nWITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\nWITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\nWITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\nWITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61650 on 10320 degrees of freedom\nMultiple R-squared:  0.7373,    Adjusted R-squared:  0.737 \nF-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#gwr-predictive-method",
    "href": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#gwr-predictive-method",
    "title": "Hands-on Exercise 11",
    "section": "6. GWR Predictive Method",
    "text": "6. GWR Predictive Method\nIn this section, you will learn how to calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.\n\n6.1 Converting the sf data.frame to SpatialPointDataFrame\n\ntrain_data_sp &lt;- as_Spatial(train_data)\ntrain_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 10335 \nextent      : 11597.31, 42623.63, 28217.39, 48741.06  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,          PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       218000,             74,            1,                  555, 0.999393538715878, 1.98943787433087e-08, 0.0333358643817954, 0.0220407324774434, 0.0441643212802781, 0.0652540365486641,                0, 6.20621206270077e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1186888,            133,           17,                 1164,  19.6500691667807,     3.30163731686804,   2.86763031236184,   2.13060636038504,   2.41313695915468,   10.6223726149914, 2.27100643784442,    0.808332738794272,     1.57131703651196,                        7,                    20, ... \n\n\n\n\n6.2 Computing adaptive bandwidth\nNext, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used.\n\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                   storey_order + remaining_lease_mths +\n                   PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                   PROX_MRT + PROX_PARK + PROX_MALL + \n                   PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                   WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                   WITHIN_1KM_PRISCH,\n                   data=train_data_sp, \n                   approach=\"CV\",\n                   kernel=\"gaussian\",\n                   adaptive=TRUE,\n                   longlat=FALSE)\n\nTake a cup of tea and have a break, it will take a few minutes.\n          -----A kind suggestion from GWmodel development group\nAdaptive bandwidth: 6395 CV score: 3.60536e+13 \nAdaptive bandwidth: 3960 CV score: 3.320316e+13 \nAdaptive bandwidth: 2455 CV score: 2.928339e+13 \nAdaptive bandwidth: 1524 CV score: 2.550957e+13 \nAdaptive bandwidth: 950 CV score: 1.95632e+13 \nAdaptive bandwidth: 593 CV score: 1.58347e+13 \nAdaptive bandwidth: 375 CV score: 1.310042e+13 \nAdaptive bandwidth: 237 CV score: 1.113152e+13 \nAdaptive bandwidth: 155 CV score: 9.572037e+12 \nAdaptive bandwidth: 101 CV score: 8.457003e+12 \nAdaptive bandwidth: 71 CV score: 7.605058e+12 \nAdaptive bandwidth: 49 CV score: 6.965752e+12 \nAdaptive bandwidth: 38 CV score: 8.249935e+12 \nAdaptive bandwidth: 58 CV score: 7.275234e+12 \nAdaptive bandwidth: 45 CV score: 6.871439e+12 \nAdaptive bandwidth: 41 CV score: 6.7928e+12 \nAdaptive bandwidth: 40 CV score: 6.780447e+12 \nAdaptive bandwidth: 38 CV score: 8.249935e+12 \nAdaptive bandwidth: 40 CV score: 6.780447e+12 \n\n\nThe result shows that 40 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.\n\nwrite_rds(bw_adaptive, \"data/model/bw_adaptive.rds\")\n\n\n\n6.3 Constructing the adaptive bandwidth GWR model\nFirst, let us call the save bandwidth by using the code chunk below.\n\nbw_adaptive &lt;- read_rds(\"data/model/bw_adaptive.rds\")\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\ngwr_adaptive &lt;- gwr.basic(formula = resale_price ~\n                            floor_area_sqm + storey_order +\n                            remaining_lease_mths + PROX_CBD + \n                            PROX_ELDERLYCARE + PROX_HAWKER +\n                            PROX_MRT + PROX_PARK + PROX_MALL + \n                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                            WITHIN_1KM_PRISCH,\n                          data=train_data_sp,\n                          bw=bw_adaptive, \n                          kernel='gaussian', \n                          adaptive=TRUE,\n                          longlat=FALSE)\n\nThe code chunk below will be used to save the model in rds format for future use.\n\nwrite_rds(gwr_adaptive, \"data/model/gwr_adaptive.rds\")\n\n\n\n6.4 Retrieve gwr output object\nThe code chunk below will be used to retrieve the save gwr model object.\n\ngwr_adaptive &lt;- read_rds(\"data/model/gwr_adaptive.rds\")\n\nThe code below can be used to display the model output.\n\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-23 17:50:43.724956 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + storey_order + \n    remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n    PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + \n    WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, \n    data = train_data_sp, bw = bw_adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm storey_order remaining_lease_mths PROX_CBD PROX_ELDERLYCARE PROX_HAWKER PROX_MRT PROX_PARK PROX_MALL PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN WITHIN_350M_CHILDCARE WITHIN_350M_BUS WITHIN_1KM_PRISCH\n   Number of data points: 10335\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-205193  -39120   -1930   36545  472355 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)              107601.073  10601.261  10.150  &lt; 2e-16 ***\n   floor_area_sqm             2780.698     90.579  30.699  &lt; 2e-16 ***\n   storey_order              14299.298    339.115  42.167  &lt; 2e-16 ***\n   remaining_lease_mths        344.490      4.592  75.027  &lt; 2e-16 ***\n   PROX_CBD                 -16930.196    201.254 -84.124  &lt; 2e-16 ***\n   PROX_ELDERLYCARE         -14441.025    994.867 -14.516  &lt; 2e-16 ***\n   PROX_HAWKER              -19265.648   1273.597 -15.127  &lt; 2e-16 ***\n   PROX_MRT                 -32564.272   1744.232 -18.670  &lt; 2e-16 ***\n   PROX_PARK                 -5712.625   1483.885  -3.850 0.000119 ***\n   PROX_MALL                -14717.388   2007.818  -7.330 2.47e-13 ***\n   PROX_SUPERMARKET         -26881.938   4189.624  -6.416 1.46e-10 ***\n   WITHIN_350M_KINDERGARTEN   8520.472    632.812  13.464  &lt; 2e-16 ***\n   WITHIN_350M_CHILDCARE     -4510.650    354.015 -12.741  &lt; 2e-16 ***\n   WITHIN_350M_BUS             813.493    222.574   3.655 0.000259 ***\n   WITHIN_1KM_PRISCH         -8010.834    491.512 -16.298  &lt; 2e-16 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 61650 on 10320 degrees of freedom\n   Multiple R-squared: 0.7373\n   Adjusted R-squared: 0.737 \n   F-statistic:  2069 on 14 and 10320 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 3.922202e+13\n   Sigma(hat): 61610.08\n   AIC:  257320.2\n   AICc:  257320.3\n   BIC:  247249\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 40 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -3.2478e+08 -4.7727e+05 -8.3004e+03  5.5025e+05\n   floor_area_sqm           -2.8714e+04  1.4475e+03  2.3011e+03  3.3900e+03\n   storey_order              3.3186e+03  8.5899e+03  1.0826e+04  1.3397e+04\n   remaining_lease_mths     -1.4431e+03  2.6063e+02  3.9048e+02  5.2865e+02\n   PROX_CBD                 -1.0837e+07 -5.7697e+04 -1.3787e+04  2.6552e+04\n   PROX_ELDERLYCARE         -3.2195e+07 -4.0643e+04  1.0562e+04  6.1054e+04\n   PROX_HAWKER              -2.3985e+08 -5.1365e+04  3.0026e+03  6.4287e+04\n   PROX_MRT                 -1.1632e+07 -1.0488e+05 -4.9373e+04  5.1037e+03\n   PROX_PARK                -6.5961e+06 -4.8671e+04 -8.8128e+02  5.3498e+04\n   PROX_MALL                -1.8112e+07 -7.4238e+04 -1.3982e+04  4.9779e+04\n   PROX_SUPERMARKET         -4.5761e+06 -6.3461e+04 -1.7429e+04  3.5616e+04\n   WITHIN_350M_KINDERGARTEN -4.1823e+05 -6.0040e+03  9.0209e+01  4.7127e+03\n   WITHIN_350M_CHILDCARE    -1.0273e+05 -2.2375e+03  2.6668e+02  2.6388e+03\n   WITHIN_350M_BUS          -1.1757e+05 -1.4719e+03  1.1626e+02  1.7584e+03\n   WITHIN_1KM_PRISCH        -6.6465e+05 -5.5959e+03  2.6916e+02  5.7500e+03\n                                  Max.\n   Intercept                1.6493e+08\n   floor_area_sqm           5.0907e+04\n   storey_order             2.9537e+04\n   remaining_lease_mths     1.8119e+03\n   PROX_CBD                 2.2411e+07\n   PROX_ELDERLYCARE         8.2444e+07\n   PROX_HAWKER              5.9654e+06\n   PROX_MRT                 2.0189e+08\n   PROX_PARK                1.5188e+07\n   PROX_MALL                1.0443e+07\n   PROX_SUPERMARKET         3.8330e+06\n   WITHIN_350M_KINDERGARTEN 6.6799e+05\n   WITHIN_350M_CHILDCARE    1.0802e+05\n   WITHIN_350M_BUS          3.7313e+04\n   WITHIN_1KM_PRISCH        5.0231e+05\n   ************************Diagnostic information*************************\n   Number of data points: 10335 \n   Effective number of parameters (2trace(S) - trace(S'S)): 1730.101 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 8604.899 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 238871.9 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 237036.9 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 238209.1 \n   Residual sum of squares: 4.829191e+12 \n   R-square value:  0.967657 \n   Adjusted R-square value:  0.9611534 \n\n   ***********************************************************************\n   Program stops at: 2024-10-23 17:53:04.390397 \n\n\n\n\n6.5 Converting the test data from sf data.frame to SpatialPointDataFrame\n\ntest_data_sp &lt;- test_data |&gt;\n  as_Spatial()\ntest_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 5566 \nextent      : 11597.31, 42623.63, 28287.8, 48669.59  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       : resale_price, floor_area_sqm, storey_order, remaining_lease_mths,         PROX_CBD,     PROX_ELDERLYCARE,        PROX_HAWKER,           PROX_MRT,          PROX_PARK,   PROX_GOOD_PRISCH,        PROX_MALL,            PROX_CHAS,     PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, ... \nmin values  :       230888,             74,            1,                  546, 1.00583660772922, 3.34897933104965e-07, 0.0474019664161957, 0.0414043955932523, 0.0502664084494264, 0.0907500295577619,                0, 4.55547870890763e-09, 1.21715176356525e-07,                        0,                     0, ... \nmax values  :      1050000,            138,           14,                 1151,  19.632402730488,     3.30163731686804,   2.83106651960209,   2.13060636038504,   2.41313695915468,   10.6169590126272, 2.26056404492346,     0.79249074802552,     1.53786629004208,                        7,                    16, ... \n\n\n\n\n6.6 Computing adaptive bandwidth for the test data\n\ngwr_bw_test_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=test_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\nThe CV shows that 25 is the most optimal number of neighbor.\n\nwrite_rds(gwr_bw_test_adaptive, \"data/model/gwr_bw_test_adaptive.rds\")\n\n\n\n6.7 Computing predicted values of the test data\n\ngwr_bw_test_adaptive = read_rds(\"data/model/gwr_bw_test_adaptive.rds\")\n\n\ngwr_test_predictions &lt;- gwr.predict(resale_price ~\n                           floor_area_sqm + storey_order +\n                           remaining_lease_mths + PROX_CBD +\n                           PROX_ELDERLYCARE + PROX_HAWKER +\n                           PROX_MRT + PROX_PARK + PROX_MALL +\n                           PROX_SUPERMARKET + \n                           WITHIN_350M_KINDERGARTEN +\n                           WITHIN_350M_CHILDCARE + \n                           WITHIN_350M_BUS + \n                           WITHIN_1KM_PRISCH, \n                         data=train_data_sp,  \n                         predictdata=test_data_sp, \n                         bw=gwr_bw_adaptive,  \n                         kernel=\"gaussian\", \n                         adaptive=TRUE, \n                         longlat=FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#preparing-coordinates-data",
    "href": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#preparing-coordinates-data",
    "title": "Hands-on Exercise 11",
    "section": "7. Preparing coordinates data",
    "text": "7. Preparing coordinates data\n\n7.1 Extracting coordinates data\nThe code chunk below extract the x,y coordinates of the full, training and test data sets.\n\ncoords &lt;- st_coordinates(mdata)\ncoords_train &lt;- st_coordinates(train_data)\ncoords_test &lt;- st_coordinates(test_data)\n\nBefore continue, we write all the output into rds for future used.\n\nwrite_rds(coords_train, \"data/model/coords_train.rds\")\nwrite_rds(coords_test, \"data/model/coords_test.rds\")\n\n\ncoords_train &lt;- read_rds(\"data/model/coords_train.rds\")\ncoords_test &lt;- read_rds(\"data/model/coords_test.rds\")\n\n\n\n7.2 Dropping the geometry column\nFirst, we will drop geometry column of the sf data.frame by using st_drop_geometry() of sf package.\n\ntrain_data &lt;- train_data |&gt;\n  st_drop_geometry()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#calibrating-random-forest-model",
    "href": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#calibrating-random-forest-model",
    "title": "Hands-on Exercise 11",
    "section": "8. Calibrating Random Forest Model",
    "text": "8. Calibrating Random Forest Model\nNow, we will train a random forest regressor to predict HDB resale price using the ranger package.\n\nset.seed(1234)\nrf &lt;- ranger(resale_price ~ floor_area_sqm + storey_order + \n               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + \n               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n               WITHIN_1KM_PRISCH,\n             data=train_data)\n\n\nwrite_rds(rf, \"data/model/rf.rds\")\n\n\nrf &lt;- read_rds(\"data/model/rf.rds\")\nrf\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       728602496 \nR squared (OOB):                  0.9495728"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#calibrating-geographical-random-forest-model",
    "href": "Hands-on_Ex/Hands-on_Ex12/Hands-on_Ex12.html#calibrating-geographical-random-forest-model",
    "title": "Hands-on Exercise 11",
    "section": "9. Calibrating Geographical Random Forest Model",
    "text": "9. Calibrating Geographical Random Forest Model\nIn this section, you will learn how to calibrate a model to predict HDB resale price by using grf() of SpatialML package.\n\n9.1 Calibrating using training data\nThe code chunk below calibrate a geographic ranform forest model by using grf() of SpatialML package.\n\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + storey_order +\n                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +\n                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\n\nNumber of Observations: 10335\n\n\nNumber of Independent Variables: 14\n\n\nKernel: Adaptive\nNeightbours: 55\n\n\n\n--------------- Global ML Model Summary ---------------\n\n\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data, num.trees = 500, mtry = 4, importance = \"impurity\",      num.threads = NULL) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             4 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       700081018 \nR squared (OOB):                  0.9515468 \n\n\n\nImportance:\n\n\n          floor_area_sqm             storey_order     remaining_lease_mths \n            7.376510e+12             1.413229e+13             2.991844e+13 \n                PROX_CBD         PROX_ELDERLYCARE              PROX_HAWKER \n            5.312697e+13             7.017513e+12             5.506719e+12 \n                PROX_MRT                PROX_PARK                PROX_MALL \n            7.446857e+12             4.825986e+12             4.173165e+12 \n        PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN    WITHIN_350M_CHILDCARE \n            2.879598e+12             1.028775e+12             1.701318e+12 \n         WITHIN_350M_BUS        WITHIN_1KM_PRISCH \n            1.564038e+12             7.214027e+12 \n\n\n\nMean Square Error (Not OOB): 173279991.32\n\n\nR-squared (Not OOB) %: 98.801\n\n\nAIC (Not OOB): 196089.283\n\n\nAICc (Not OOB): 196089.33\n\n\n\n--------------- Local Model Summary ---------------\n\n\n\nResiduals OOB:\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-236112.0  -13033.7     444.4     593.8   14831.5  358041.7 \n\n\n\nResiduals Predicted (Not OOB):\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-79279.83  -3510.70     54.56     50.98   3909.85  83074.08 \n\n\n\nLocal Variable Importance:\n\n\n                               Min          Max        Mean         StD\nfloor_area_sqm                   0 401562922035 18210850992 41426270899\nstorey_order             302736445 243728744368 16368419468 23620589843\nremaining_lease_mths     696564138 546463600727 34119912443 70328183398\nPROX_CBD                  55173040 382484896335 12154563393 29293290548\nPROX_ELDERLYCARE          45182031 344081962746 10597657883 24546405941\nPROX_HAWKER               43516026 342597797419 10551807020 23408387903\nPROX_MRT                  54234551 299075025906  9873129985 21055852211\nPROX_PARK                 49919822 322633843469  9353956995 19517077658\nPROX_MALL                 43296133 433263607933 11247374493 27537334970\nPROX_SUPERMARKET          52665827 417310417234 10802122271 26572460731\nWITHIN_350M_KINDERGARTEN         0 186468064682  2848177740 12928886968\nWITHIN_350M_CHILDCARE            0 255236737234  5526292324 18109971102\nWITHIN_350M_BUS                  0 193828795378  4747552546 11886064288\nWITHIN_1KM_PRISCH                0 178360608427  1778262602  7163381668\n\n\n\nMean squared error (OOB): 930426169.333\n\n\nR-squared (OOB) %: 93.56\n\n\nAIC (OOB): 213459.669\n\n\nAICc (OOB): 213459.716\n\n\nMean squared error Predicted (Not OOB): 73859413.696\n\n\nR-squared Predicted (Not OOB) %: 99.489\n\n\nAIC Predicted (Not OOB): 187276.161\n\n\nAICc Predicted (Not OOB): 187276.208\n\n\n\nCalculation time (in seconds): 8.3196\n\n\nLet‚Äôs save the model output by using the code chunk below.\n\nwrite_rds(gwRF_adaptive, \"data/model/gwRF_adaptive.rds\")\n\nThe code chunk below can be used to retrieve the save model in future.\n\ngwRF_adaptive &lt;- read_rds(\"data/model/gwRF_adaptive.rds\")\n\n\n\n9.2 Predicting by using test data\n\n9.2.1 Preparing the test data\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\ntest_data &lt;- cbind(test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\n\n\n9.2.2 Predicting with test data\nNext, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.\n\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                           test_data, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\nBefore moving on, let us save the output into rds file for future use.\n\nGRF_pred &lt;- write_rds(gwRF_pred, \"data/model/GRF_pred.rds\")\n\n\n\n9.2.3 Converting the predicting output into a data frame\nThe output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.\n\nGRF_pred &lt;- read_rds(\"data/model/GRF_pred.rds\")\nGRF_pred_df &lt;- as.data.frame(GRF_pred)\n\nIn the code chunk below,¬†cbind()¬†is used to append the predicted values onto test_datathe\n\ntest_data_p &lt;- cbind(test_data, GRF_pred_df)\n\n\nwrite_rds(test_data_p, \"data/model/test_data_p.rds\")\n\n\n\n\n9.3 Calculating Root Mean Square Error\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.\n\ntest_data_p = read_rds(\"data/model/test_data_p.rds\")\n\n\nrmse(test_data_p$resale_price, \n     test_data_p$GRF_pred)\n\n[1] 27302.9\n\n\n\n\n9.4 Visualising the predicted values\nAlternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\ntest_data_p &lt;- test_data_p |&gt; select(1:19)\nggplot(data = test_data_p,\n       aes(x = GRF_pred,\n           y = resale_price)) +\n  geom_point()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "In this Take-Home Exercise, I will be leveraging the Grab Posisi dataset to analyse Jakarta‚Äôs growing need for on-demand ride-hailing services like Grab, given its increasing pace of urbanisation, particularly in areas with high Points of Interests (POIs).\nFeel free to move over to my team‚Äôs Netlify site for a more in-depth understanding of our project scope and our progress: https://is415-projectgrab-g2.netlify.app/\nWith that said, let‚Äôs begin with my contributions to the project. In this exercise, I will ddelve into two aspects of the project - exploratory data analysis (EDA) and exploratory spatial data analysis (ESDA) using Local Moran‚Äôs I and LISA classificationon the Grab dataset. My goal is to experiment with as many charts as possible, e.g.¬†heat maps, flow maps, choropleths, to answer critical questions that will be useful in informing us of ride-hailing demand, traffic hotspots, and movement patterns around key locations in Jakarta. Eventually, the most useful charts will be selected in the final project."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#traffic-volume-trends",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#traffic-volume-trends",
    "title": "Take-home Exercise 3",
    "section": "4.1 Traffic Volume & Trends",
    "text": "4.1 Traffic Volume & Trends\n\n4.1.1 Distribution of trips originating from each district\n\nPlot/Graph: Bar chart\nAnalysis/Conclusion: Identify districts with the highest and lowest traffic volumes. These are areas of high trip demand or under-service. Look for disparities that might suggest uneven service distribution.\n\n\ndistrict_counts &lt;- trips %&gt;%\n  count(origin_district) %&gt;%\n  rename(total_trips = n)\n\nggplot(district_counts, aes(x = reorder(origin_district, -total_trips), y = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue2\") +\n  theme_gray() +\n  labs(title = \"Total Number of Grab Trips per District\",\n       x = \"District\",\n       y = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"), # Center and style the title\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\n4.1.2 Top origins and destinations for ride-hailing trips\n\nPlot/Graph: bar chart\nAnalysis/Conclusion: Identify major traffic corridors between districts. This can show the flow between key origin-destination pairs, helping to highlight important commuter routes.\n\n\n# Calculate top 5 districts for origin_district\ntop_5_origin_districts &lt;- trips %&gt;%\n  count(origin_district) %&gt;%\n  rename(total_trips = n) %&gt;%\n  arrange(desc(total_trips)) %&gt;%\n  slice(1:5)\n\n# Calculate top 5 districts for destination_district\ntop_5_destination_districts &lt;- trips %&gt;%\n  count(destination_district) %&gt;%\n  rename(total_trips = n) %&gt;%\n  arrange(desc(total_trips)) %&gt;%\n  slice(1:5)\n\n# Plot for Top 5 Origin Districts\np1 &lt;- ggplot(top_5_origin_districts, aes(y = reorder(origin_district, total_trips), x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  theme_gray() +\n  labs(title = \"Most Popular Origin Districts\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n# Plot for Top 5 Destination Districts\np2 &lt;- ggplot(top_5_destination_districts, aes(y = reorder(destination_district, total_trips), x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"pink2\") +\n  theme_gray() +\n  labs(title = \"Most Popular Destination Districts\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n# Display the plots side by side\np1 + p2\n\n\n\n\n\n\n\n\n\n\n4.1.3 Least popular origins and destinations for ride-hailing trips\n\nPlot/Graph: bar chart\nAnalysis/Conclusion: Identify major traffic corridors between districts. This can show the flow between key origin-destination pairs, helping to highlight important commuter routes.\n\nCount trips by origin district and destination district, and arrange in descending order\nIdentify the bottom 5 origin districts and destination districts\nSet factor levels for ordered display and plot using geom_bar() of the ggplot2 package.\n\norigin_counts &lt;- trips %&gt;%\n  count(origin_district) %&gt;%\n  rename(total_trips = n) %&gt;%\n  arrange(desc(total_trips))\n\nbottom_5_origin_districts &lt;- origin_counts %&gt;%\n  slice_tail(n = 5)\n\ndestination_counts &lt;- trips %&gt;%\n  count(destination_district) %&gt;%\n  rename(total_trips = n) %&gt;%\n  arrange(desc(total_trips))\n\nbottom_5_destination_districts &lt;- destination_counts %&gt;%\n  slice_tail(n = 5)\n\nbottom_5_origin_districts$origin_district &lt;- factor(bottom_5_origin_districts$origin_district, \n                                                     levels = bottom_5_origin_districts$origin_district)\n\nbottom_5_destination_districts$destination_district &lt;- factor(bottom_5_destination_districts$destination_district, \n                                                               levels = bottom_5_destination_districts$destination_district)\n\np1 &lt;- ggplot(bottom_5_origin_districts, aes(y = origin_district, x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  theme_gray() +\n  labs(title = \"Least Popular Origin Districts\", y = \"District\", x = \"Number of Trips\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"))\n\np2 &lt;- ggplot(bottom_5_destination_districts, aes(y = destination_district, x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"pink2\") +\n  theme_gray() +\n  labs(title = \"Least Popular Destination Districts\", y = \"District\", x = \"Number of Trips\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"))\n\n# Display the plots side by side\np1 + p2\n\n\n\n\n\n\n\n\n\n\n4.1.4 Distribution of Trips Throughout the week\nCount the number of trips for each day of the week and driving mode using functions from dplyr package.\nConvert ‚Äòorigin_day‚Äô to a factor with specific order\nUse geom_bar() from ggplot2 package to plot\n\nweekly_trips &lt;- trips %&gt;%\n  group_by(origin_day, driving_mode) %&gt;%\n  summarise(total_trips = n(), .groups = 'drop') %&gt;%\n  arrange(match(origin_day, c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\")))\n\nweekly_trips$origin_day &lt;- factor(weekly_trips$origin_day, \n                                   levels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"))\n\nggplot(weekly_trips, aes(x = origin_day, y = total_trips, fill = driving_mode)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +  # Stacked bar chart\n  labs(title = \"Number of Trips Throughout the Week by Driving Mode\",\n       x = \"Day of the Week\",\n       y = \"Number of Trips\") +\n  scale_fill_manual(values = c(\"car\" = \"lightblue\", \"motorcycle\" = \"pink2\")) +  # Custom colors\n  theme_gray() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")) +\n  geom_text(aes(label = total_trips), \n            position = position_stack(vjust = 0.5), \n            color = \"black\", \n            size = 4, \n            show.legend = FALSE)  \n\n\n\n\n\n\n\n\n\n\n4.1.4 Distribution of number of trips throughout the day\n\nPlot/Graph: Line graph with time of day on the x-axis\nAnalysis/Conclusion: Detect peak traffic times. This can inform resource allocation for drivers or surge pricing strategies during peak periods.\n\nBy using the geom_bar function, we can plot the count of Grab trips taken for each hour of a day. I have converted ‚Äòorigin_hour‚Äô to a factor to maintain order in the plot.\n\nhourly_trips &lt;- trips %&gt;%\n  group_by(origin_hour) %&gt;%\n  summarise(number_of_trips = n(), .groups = 'drop') %&gt;%\n  arrange(origin_hour)\n\nhourly_trips$origin_hour &lt;- factor(hourly_trips$origin_hour, \n                                    levels = sort(unique(trips$origin_hour)))\n\nggplot(hourly_trips, aes(x = origin_hour, y = number_of_trips)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue3\") + \n  labs(title = \"Distribution of Number of Trips Throughout the Day\",\n       x = \"Hour of the Day\",\n       y = \"Number of Trips\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text()\n  ) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\n\nAdditionally, we can also aggegate the hours into morning, afternoon, evening and midnight, including whether they are peak or lull periods, as such. This allows for easier interpretability of the data charts. We will use this new hour_category column next.\n\ntrips &lt;- trips %&gt;%\n  mutate(hour_category = case_when(\n    origin_hour %in% 1:3 ~ \"midnight_peak\",\n    origin_hour %in% 4:6 ~ \"midnight_lull\",\n    origin_hour %in% 7:9 ~ \"morning_peak\",\n    origin_hour %in% 10:12 ~ \"morning_lull\",\n    origin_hour %in% 13:15 ~ \"afternoon_peak\",\n    origin_hour %in% 16:18 ~ \"afternoon_lull\",\n    origin_hour %in% 19:21 ~ \"evening_peak\",\n    origin_hour %in% c(22:24,0) ~ \"evening_lull\"\n  )) %&gt;%\n  mutate(hour_category = factor(hour_category, \n                                 levels = c(\"midnight_peak\", \"midnight_lull\", \n                                            \"morning_peak\", \"morning_lull\", \n                                            \"afternoon_peak\", \"afternoon_lull\", \n                                            \"evening_peak\", \"evening_lull\")))\n\nAs shown, ‚Ä¶\n\nhourly_trips &lt;- trips %&gt;%\n  group_by(hour_category) %&gt;%\n  summarise(number_of_trips = n(), .groups = 'drop')\n\n# Plot the distribution of number of trips throughout the day\nggplot(hourly_trips, aes(x = hour_category, y = number_of_trips)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue3\") +\n  labs(title = \"Distribution of Number of Trips By Hour Category\",\n       x = \"Hour Category\",\n       y = \"Number of Trips\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\n\n\n\n4.1.5 Distribution of Trip Distance\n1) By Hour Categories\nHere, we randomly sample 2000 trip data to plot the violin plot of total_distance_km against hour categories. This will allow better readability of the data points since plotting the entire dataset will result in a chunk of dots that do not offer insights into the spread of trip distance.\n\nsampled_trips &lt;- trips %&gt;%\n  sample_n(2000)\n\nggplot(sampled_trips, aes(x = hour_category, y = total_distance_km, fill = hour_category)) +\n  geom_violin(trim = FALSE, alpha = 0.6) +\n  geom_jitter(width = 0.2, size = 0.5, alpha = 0.3, color = \"black\") +  \n  labs(title = \"Distribution of Trip Distance (km) by Hour Category\",\n       x = \"Hour Category\",\n       y = \"Total Distance (km)\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)  # Tilt x-axis labels\n  ) +\n  scale_fill_manual(values = c(\"lightblue\", \"lightcoral\", \"lightgreen\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\", \"lightgoldenrod\")) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\n\n2) By Driving Mode\nNext, we can use the geom_density() function to create density plots that shows us a smoothed version of the trip distance (in kilometres), further drilled down by the mode of driving (car vs motorcycle).\n\nAnalysis/Conclusion: Understand the relative popularity of different driving mode. Motorbikes may dominate short trips, while cars are preferred for longer journeys.\n\n\nggplot(trips, aes(x = total_distance_km, fill = driving_mode)) +\n  geom_density(alpha = 0.8) +\n  labs(title = \"Trip Distance (km) by Driving Mode\",\n       x = \"Total Distance (km)\",\n       y = \"Density\") +\n  theme_gray() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")) +\n  scale_fill_manual(values = c(\"lightblue\", \"pink2\"))\n\n\n\n\n\n\n\n\n\n\n4.1.5 Distribution of Trip Duration\n1) By Hour Categories\nSimilar to the distribution of trip distance, by using geom_violin() from ggplot2 package, we can visualise the spread of trip duration (in minutes) from origin to destination, based on each hour category.\n\nsampled_trips &lt;- trips %&gt;%\n  sample_n(2000)\n\nggplot(sampled_trips, aes(x = hour_category, y = total_duration_minutes, fill = hour_category)) +\n  geom_violin(trim = FALSE, alpha = 0.6) +\n  geom_jitter(width = 0.2, size = 0.5, alpha = 0.3, color = \"black\") +  \n  labs(title = \"Distribution of Trip Duration (minutes) by Hour Category\",\n       x = \"Hour Category\",\n       y = \"Total Duration (minutes)\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)  # Tilt x-axis labels\n  ) +\n  scale_fill_manual(values = c(\"lightblue\", \"lightcoral\", \"lightgreen\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\", \"lightgoldenrod\")) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\n\n2) By Driving Mode\nWe can also use the total_duration_minutes column of the trips data to plot the distribution of trip duration across driving mode using a violin plot.\n\nAnalysis/Conclusion: Understand the relative popularity of different driving mode. Motorbikes may dominate short trips, while cars are preferred for longer journeys.\n\n\nggplot(trips, aes(x = total_duration_minutes, fill = driving_mode)) +\n  geom_density(alpha = 0.8) +\n  labs(title = \"Trip Duration (minutes) by Driving Mode\",\n       x = \"Total Duration (minutes)\",\n       y = \"Density\") +\n  theme_gray() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")) +\n  scale_fill_manual(values = c(\"lightblue\", \"pink2\"))\n\n\n\n\n\n\n\n\n\n\n4.1.6 Are there seasonal or day-of-week patterns in ride-hailing demand? Do certain days consistently have higher demand?\n\nPlot/Graph: Heatmap with days of the week and times of day\nAnalysis/Conclusion: Identify if specific days or seasons see higher demand. Use this insight to optimize fleet size and availability.\n\nNow, let‚Äôs generate a heatmap displaying the distribution of trips across different days of the week (using the origin_day column) and the specified hour categories, with the fill color indicating the number of trips. The days are ordered from Monday to Sunday, making it easier to interpret the data.\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Create a new summary data frame using hour_category\nheatmap_data &lt;- trips %&gt;%\n  group_by(origin_day, hour_category) %&gt;%\n  summarise(number_of_trips = n(), .groups = 'drop') %&gt;%\n  arrange(origin_day, hour_category)\n\n# Convert origin_day to a factor with specific order\nheatmap_data$origin_day &lt;- factor(heatmap_data$origin_day, \n                                   levels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \n                                              \"Friday\", \"Saturday\", \"Sunday\"))  # Specify the order\n\n# Convert hour_category to a factor for better ordering\nheatmap_data$hour_category &lt;- factor(heatmap_data$hour_category, \n                                      levels = c(\"midnight_peak\", \"midnight_lull\", \n                                                 \"morning_peak\", \"morning_lull\", \n                                                 \"afternoon_peak\", \"afternoon_lull\", \n                                                 \"evening_peak\", \"evening_lull\")) \n\n# Create the heatmap\nggplot(heatmap_data, aes(x = hour_category, y = origin_day)) +\n  geom_tile(aes(fill = number_of_trips), color = \"white\") +\n  geom_text(aes(label = number_of_trips), color = \"black\", size = 2.5) +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\", name = \"Number of Trips\") +\n  labs(title = \"Heatmap of Number of Trips by Day and Hour Category\",\n       x = \"Hour Category\",\n       y = \"Day\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1) \n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#impact-of-time-and-weather",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#impact-of-time-and-weather",
    "title": "Take-home Exercise 3",
    "section": "4.2 Impact of Time and Weather",
    "text": "4.2 Impact of Time and Weather\n\n4.2.1 How does the volume of trips change with weather conditions (rain vs.¬†no rain)?\n\nPlot/Graph: Boxplot comparing rainy vs.¬†non-rainy days\nAnalysis/Conclusion: Understand how adverse weather affects overall trip volume. This can reveal if people use ride-hailing more or less in different weather conditions.\n\n\n\n4.2.2 How do traffic patterns vary across different times of the day (e.g., morning peak vs.¬†evening peak)?\n\nPlot/Graph: Line chart comparing different periods of the day (morning vs.¬†evening)\nAnalysis/Conclusion: Compare morning and evening peaks to optimize driver deployment during the busiest times.\n\n\n\n4.2.3 Do weather conditions affect trip demand differently for motorbikes vs.¬†cars?\n\nPlot/Graph: Side-by-side boxplots for cars and motorbikes\nAnalysis/Conclusion: Analyze how weather influences the use of motorbikes versus cars. Motorbike demand may drop significantly during rain."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#point-of-interest-poi-analysis",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#point-of-interest-poi-analysis",
    "title": "Take-home Exercise 3",
    "section": "4.4 Point of Interest (POI) Analysis",
    "text": "4.4 Point of Interest (POI) Analysis\n\n4.4.1 Distribution of POIs Across Districts\nWe can visualise the distribution of POIs across each district using the district_dest augmented dataframe we created previously. With that said, we can observe that the top three highest number POIs are found in the districts Kebayoran Baru, Setia Budi and Grogol Petamburan.\n\n# Plot the distribution of POIs across districts\nggplot(district_dest, aes(x = reorder(district, num_of_pois), y = num_of_pois)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue3\") +\n  labs(title = \"Distribution of POIs Across Districts\",\n       x = \"District\",\n       y = \"Number of POIs\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\nNext, let us use the pois_num dataframe created earlier. I will plot a heatmap plot using the geom_tile() function of the ggplot2 package to understand how the number of trips demanded vary by district and POI category. We can evidently see that districts like Danau Sunter have low to no POIs, with only essentials (e.g.¬†clinics, veteranarians) found in the district. However, in districts that may be more populated or urbanised, we see a higher number of POIs for the office businesses, restaurants and food, and shops as marked by the darker shades of blue below.\n\n# Count the number of POIs by district\npois_num_cat &lt;- pois %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(district, category) %&gt;%\n  summarise(num_of_pois = n(), .groups = 'drop')\n\nheatmap_plot &lt;- ggplot(pois_num_cat, aes(x = category, y = district, fill = num_of_pois)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\n  labs(\n    title = \"Heatmap of Points of Interest by District and Category\",\n    x = \"POI Category\",\n    y = \"District\",\n    fill = \"Number of POIs\"\n  ) +\n  theme_minimal() + # Clean theme\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\") \n  )\n\nprint(heatmap_plot)\n\n\n\n\n\n\n\n\n\n\n4.4.2 Demand of Ride-hailing Services by POI\nTo prepare the dataset for the analysis involving POIs, I will augment the jakarta_pois.rds dataset by aggregating the trips data from the trips simple dataframe. Restaurants, office businesses and shops seem to dominate the POIs available in Jakarta, while cultural attractions and recreation entertainment suggests to be less available.\n\ncategory_counts &lt;- pois %&gt;%\n  group_by(category) %&gt;%\n  summarise(num_of_pois = n()) %&gt;%\n  arrange(desc(num_of_pois))\n\nggplot(category_counts, aes(x = reorder(category, -num_of_pois), y = num_of_pois, fill = num_of_pois)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = num_of_pois), vjust = -0.5, size = 4) +\n  labs(\n    title = \"Distribution of POIs Across Jakarta\",\n    x = \"Category\",\n    y = \"Number of POIs\"\n  ) +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1, color = \"grey30\"),\n    legend.position = \"none\"\n  ) +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") \n\n\n\n\n\n\n\n\nWe can also plot the number of POIs discovered across each POI category type by using a radar chart. Here, I am employing the radarchart() function of the fmsb package.\n\n\nCode\ncategory_counts &lt;- pois %&gt;%\n  group_by(category) %&gt;%\n  summarise(num_of_pois = n()) %&gt;%\n  arrange(desc(num_of_pois))\n\nmax_value &lt;- max(category_counts$num_of_pois)\nmin_value &lt;- 0\n\nradar_data &lt;- rbind(\n  rep(max_value, nrow(category_counts)),\n  rep(min_value, nrow(category_counts)),\n  t(category_counts$num_of_pois)\n)\n\nradar_data &lt;- as.data.frame(radar_data)\ncolnames(radar_data) &lt;- category_counts$category\n\nradarchart(\n  radar_data,\n  axistype = 1,\n  pcol = rgb(0.2, 0.4, 0.4, 0.6),  # Line color\n  pfcol = rgb(0.2, 0.2, 0.2, 0.2),  # Fill color\n  plwd = 2,  # Line width\n  cglcol = \"lightblue3\",  # Grid line color\n  cglty = 1,  # Grid line type\n  axislabcol = \"black\",  # Axis label color\n  caxislabels = seq(0, max_value, length.out = 5),  # Axis labels\n  cglwd = 1.2,  # Grid line width\n  vlcex = 1  # Category label size\n)\n\ntitle(\"Distribution of POIs Across Jakarta\", cex.main = 1.4)\n\n\n\n\n\n\n\n\n\nNext, I will go deeper into finding out how many Grab trips were taken to these destinations at the district level. It can be observed that a larger number of POIs (pink) within a district lead to similarly higher number of Grab trips taken (blue) to the district.\n\n# Reshape the data for plotting\ndistrict_long &lt;- district_dest %&gt;%\n  pivot_longer(cols = c(num_of_trips, num_of_pois), \n               names_to = \"metric\", \n               values_to = \"count\")\n\n# Plot the line charts\nggplot(district_long, aes(x = district, y = count, group = metric, color = metric)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Trends in Number of Trips and Availability of POIs by District\",\n    x = \"District\",\n    y = \"Number of Trips\",\n    color = \"Metric\"\n  ) +\n  scale_color_manual(values = c(\"num_of_trips\" = \"lightblue3\", \"num_of_pois\" = \"pink3\")) + \n  theme_gray() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(size = 16, hjust = 0.5, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nThe scatterplot below indicates that more Grab trips are taken for smaller average distances travelled from the passenger‚Äôs origin, suggesting most trips are bounded within shorter distances. This is surprising since I would have assumed a higher demand for Grab services for when the destination is far from the passenger‚Äôs origin.\nIn addition, we can also observe that destinations with a higher number of POI (darker shades of blue) tend to result in more Grab trips taken. This means that passengers are more likely to take a trip to districts in Jakarta that offer more POIs.\n\ndistrict_pois_5 &lt;- district_dest %&gt;%\n  mutate(pois_category = cut(num_of_pois, \n                              breaks = 5, \n                              labels = c(\"100\", \"200\", \"300\", \"400\", \"500\"), \n                              include.lowest = TRUE))\n\ncolor_palette &lt;- c(\"100\" = \"#A3C6E4\",   # Light blue\n                   \"200\" = \"#76A4D6\",      # Medium light blue\n                   \"300\" = \"#4A83C6\",   # Medium blue\n                   \"400\" = \"#1F5DA0\",     # Dark blue\n                   \"500\" = \"#003C71\") # Darker blue\n\nggplot(district_pois_5, aes(x = num_of_trips, y = avg_distance_km, size = num_of_pois, color = pois_category)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Correlation of Number of Trips and Average Distance by Number of POIs\",\n    x = \"Number of Trips\",\n    y = \"Average Distance (km)\",\n    size = \"Number of POIs\",\n    color = \"Number of POIs\"  # Renamed here\n  ) +\n  scale_size(range = c(1, 6)) +  # Adjust point size range\n  scale_color_manual(values = color_palette) +  # Apply custom color palette\n  theme_gray() +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5, face = \"bold\"),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n4.4.3 Correlation of Number of Trips and POIs\nWe can observe a positive correlation between the number of trips taken to a destination and the availability of POIs in the destination district based on the linear regression model below. This means that more trips are demanded to the destination when there are more POIs available, typically these districts might be within the city with more amenities for people to eat, shop and work.\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Calculate the correlation coefficient\ncorrelation_result &lt;- cor(district_dest$num_of_trips, district_dest$num_of_pois)\n\n# Create a scatter plot with a regression line\nggplot(district_dest, aes(x = num_of_pois, y = num_of_trips)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +  # Points\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +  # Linear regression line\n  labs(\n    title = \"Correlation between Number of Trips and Number of POIs\",\n    x = \"Number of POIs\",\n    y = \"Number of Trips\"\n  ) +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\")\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Print the correlation result\nprint(paste(\"Correlation between Number of Trips and Number of POIs:\", round(correlation_result, 2)))\n\n[1] \"Correlation between Number of Trips and Number of POIs: 0.8\"\n\n\nBy including the average distance traveled, we can see that fewer passengers tend to take ride-hailing services like Grab for when the average distance of trips are shorter. For district destinations with higher number of POIs available, trip distances tend to be shorter too, which could mean that passengers likely reside near to these POIs.\n\ndistrict_dist_6 &lt;- district_dest %&gt;%\n  mutate(distance_category = cut(avg_distance_km, \n                                  breaks = 6, \n                                  labels = c(\"5.0\", \"5.5\", \"6.0\", \"6.5\", \"7.0\", \"7.5\"), \n                                  include.lowest = TRUE))\n\ndistance_color_palette &lt;- c(\"5.0\" = \"#A3C6E4\",   # Light blue\n                             \"5.5\" = \"#76A4D6\",   # Medium light blue\n                             \"6.0\" = \"#4A83C6\",   # Medium blue\n                             \"6.5\" = \"#1F5DA0\",   # Dark blue\n                             \"7.0\" = \"#003C71\",   # Darker blue\n                             \"7.5\" = \"midnightblue\")  # Deepest blue\n\ndistrict_dist_6 &lt;- district_dist_6 %&gt;%\n  mutate(distance_category = factor(distance_category))\n\nggplot(district_dist_6, aes(x = num_of_trips, y = num_of_pois)) +\n  geom_point(aes(color = distance_category, size = distance_category), alpha = 0.7) + \n  labs(\n    title = \"Relationship between Number of Trips and Number of POIs by Average Distance\",\n    x = \"Number of Trips\",\n    y = \"Number of POIs\",\n    size = \"Average Distance Category (km)\",  \n    color = \"Average Distance Category (km)\"  \n  ) +\n  scale_size_manual(values = c(1, 2, 3, 4, 5, 6)) +  \n  scale_color_manual(values = distance_color_palette) + \n  theme_gray() +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n4.4.3 Traffic Volumes By Time of Day and POI Type\nNext, i will want to understand traffic volumes by Hour Category and POI Category. Before that, I will create a new dataframe district_hour_pois to aggregate the 8 hour categories into 4 instead, namely morning, afternoon, evening and midnight.\n\n\nCode\n# Define time-of-day categories based on hour_category labels\ntrips_num &lt;- trips %&gt;%\n  group_by(destination_district, hour_category) %&gt;%\n  summarise(num_of_trips = n(), .groups = 'drop') %&gt;%\n  rename(district = destination_district) %&gt;%\n  mutate(\n    district = tolower(district),\n    # Group hour_category into broader time-of-day groups\n    time_of_day = case_when(\n      hour_category %in% c(\"morning_peak\", \"morning_lull\") ~ \"morning\",\n      hour_category %in% c(\"afternoon_peak\", \"afternoon_lull\") ~ \"afternoon\",\n      hour_category %in% c(\"evening_peak\", \"evening_lull\") ~ \"evening\",\n      hour_category %in% c(\"midnight_peak\", \"midnight_lull\") ~ \"midnight\",\n      TRUE ~ \"other\"\n    ),\n    # Set specific ordering for time_of_day\n    time_of_day = factor(time_of_day, levels = c(\"morning\", \"afternoon\", \"evening\", \"midnight\"))\n  ) %&gt;%\n  # Group by district and new time_of_day categories, summing num_of_trips\n  group_by(district, time_of_day) %&gt;%\n  summarise(num_of_trips = sum(num_of_trips), .groups = 'drop')\n\n# Count the number of POIs by district & category\npois_num_cat &lt;- pois %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(district, category) %&gt;%\n  summarise(num_of_pois = n(), .groups = 'drop')\n\n# Combine the two datasets by district\ndistrict_hour_pois &lt;- trips_num %&gt;%\n  inner_join(pois_num_cat, by = \"district\", relationship = \"many-to-many\") %&gt;%\n  arrange(district, time_of_day)\n\n# View the combined dataset\nhead(district_hour_pois)\n\n\n# A tibble: 6 √ó 5\n  district time_of_day num_of_trips category                 num_of_pois\n  &lt;chr&gt;    &lt;fct&gt;              &lt;int&gt; &lt;chr&gt;                          &lt;int&gt;\n1 cakung   morning              357 Cultural_Attractions               1\n2 cakung   morning              357 Essentials                        12\n3 cakung   morning              357 Facilities_Services               16\n4 cakung   morning              357 Offices_Business                  18\n5 cakung   morning              357 Recreation_Entertainment           2\n6 cakung   morning              357 Restaurants_Food                   3\n\n\nAcross all POI types, districts offering restaurants and shops tend to garner the highest demand for Grab services. In constrast, recreation entertainment showed to be least demanded by Grab users.\n\nIn the morning, Grab services are most demanded to destinations that offer facilities services, shops and essentials, where commuters are likely headed to work or settle household errands. This trend can also be observed in the afternoon.\nWhen it comes to evening time, the number of trips taken to restaurants are seen to rise likely since commuters are done with the day and are headed for dinner. It is also surprising that Grab transport are also highly active during midnight (12am - 6pm), namely where destinations are facilities services, shops and restaurants. This could be due to commuters ending work late or heading out for work in the early morning.\n\n\naggregated_pois &lt;- district_hour_pois %&gt;%\n  group_by(category, time_of_day) %&gt;%\n  summarise(num_of_trips = sum(num_of_trips, na.rm = TRUE), .groups = \"drop\")  \n\nggplot(aggregated_pois, aes(x = category, y = num_of_trips, fill = time_of_day)) +\n  geom_bar(stat = \"identity\") + \n  geom_text(aes(label = num_of_trips), \n            position = position_stack(vjust = 0.5),  \n            color = \"black\", \n            size = 3.5) +\n  labs(\n    title = \"Number of POIs Visited Using Grab by Time of Day\",\n    x = \"POI Category\",\n    y = \"Number of POIs\"\n  ) +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"), \n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) + \n  scale_fill_brewer(palette = \"Blues\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#demand-distribution-segmentation",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#demand-distribution-segmentation",
    "title": "Take-home Exercise 3",
    "section": "4.4 Demand Distribution & Segmentation",
    "text": "4.4 Demand Distribution & Segmentation\n\n4.4.1 What are the busiest districts for motorbikes vs.¬†cars?\n\nPlot/Graph: Side-by-side choropleth maps for motorbike and car trips\nAnalysis/Conclusion: Analyze which areas favor motorbikes over cars. This can inform fleet allocation based on geographic preferences.\n\n\n\n4.4.2 How do trips originating from residential vs.¬†commercial areas compare in volume?\n\nPlot/Graph: Grouped bar chart comparing residential and commercial areas\nAnalysis/Conclusion: Observe differences in traffic demand. Residential areas may see high morning traffic, while commercial hubs are busier in the evening.\n\n\n\n4.4.3 What is the average trip duration (in distance) between districts, and does it vary by vehicle type or time of day?\n\nPlot/Graph: Boxplot for distance by vehicle type and time of day\nAnalysis/Conclusion: Identify whether car trips tend to cover longer distances and how trip length changes across different periods."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#temporal-trends",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#temporal-trends",
    "title": "Take-home Exercise 3",
    "section": "4.2 Temporal Trends",
    "text": "4.2 Temporal Trends\n\n4.2.1 Distribution of Trips Throughout the Week\nIt will also be interesting to investigate how the demand for Grab trips changes based on time, in particular by day of week. Here, I counted the number of trips for each day of the week and driving mode by using functions from the dplyr package.\nWe plot the geom_bar() from ggplot2 package here. We can see that weekends have slightly fewer number of trips taken than during the weekdays. Additionally, the choice between motorcycle or car is almost equal, with a marginally higher number of motorcycle trips taken across all days. Out of all days of the week, Thursdays have the highest number of trips taken which coincides with a school and work day, while Sunday which coincides with a rest day showed the least number of trips.\n\nweekly_trips &lt;- trips %&gt;%\n  group_by(origin_day, driving_mode) %&gt;%\n  summarise(total_trips = n(), .groups = 'drop') %&gt;%\n  arrange(match(origin_day, c(\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\")))\n\nweekly_trips$origin_day &lt;- factor(weekly_trips$origin_day, \n                                   levels = c(\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"))\n\nggplot(weekly_trips, aes(x = origin_day, y = total_trips, fill = driving_mode)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +  # Stacked bar chart\n  labs(title = \"Number of Trips Throughout the Week by Driving Mode\",\n       x = \"Day of the Week\",\n       y = \"Number of Trips\") +\n  scale_fill_manual(values = c(\"car\" = \"lightblue\", \"motorcycle\" = \"pink2\")) +  # Custom colors\n  theme_gray() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")) +\n  geom_text(aes(label = total_trips), \n            position = position_stack(vjust = 0.5), \n            color = \"black\", \n            size = 4, \n            show.legend = FALSE)  \n\n\n\n\n\n\n\n\n\n\n4.2.2 Distribution of number of trips throughout the Day\n1) By Individual Hours\nBy using the geom_bar function, we can plot the count of Grab trips taken for each hour of a day. The peak traffic time appears to be at 10am while the quietest period is at 7pm in the evening. We cna also see the the demand for traffic rising from 12am onward which then declines in number of Grab trips past 10am.\n\nhourly_trips &lt;- trips %&gt;%\n  group_by(origin_hour) %&gt;%\n  summarise(number_of_trips = n(), .groups = 'drop') %&gt;%\n  arrange(origin_hour)\n\nhourly_trips$origin_hour &lt;- factor(hourly_trips$origin_hour, \n                                    levels = sort(unique(trips$origin_hour)))\n\nggplot(hourly_trips, aes(x = origin_hour, y = number_of_trips, group = 1)) +  # Set group = 1\n  geom_line(color = \"lightblue3\", linewidth = 1) + \n  geom_point(color = \"lightblue3\", size = 2) + \n  geom_text(aes(label = number_of_trips), vjust = -0.5, color = \"black\", size = 3) +\n  labs(title = \"Distribution of Number of Trips Throughout the Day\",\n       x = \"Hour of the Day\",\n       y = \"Number of Trips\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)  # Rotate x-axis text for readability\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n2) By Aggregated Hour Category\nAdditionally, we can also aggregate the hour_category column into morning, afternoon, evening and midnight, including whether they are peak or lull periods, as such. This allows for easier interpretability of the data charts.\n\ntrips &lt;- trips %&gt;%\n  mutate(hour_category = case_when(\n    origin_hour %in% 1:3 ~ \"midnight_peak\",\n    origin_hour %in% 4:6 ~ \"midnight_lull\",\n    origin_hour %in% 7:9 ~ \"morning_peak\",\n    origin_hour %in% 10:12 ~ \"morning_lull\",\n    origin_hour %in% 13:15 ~ \"afternoon_peak\",\n    origin_hour %in% 16:18 ~ \"afternoon_lull\",\n    origin_hour %in% 19:21 ~ \"evening_peak\",\n    origin_hour %in% c(22:24,0) ~ \"evening_lull\"\n  )) %&gt;%\n  mutate(hour_category = factor(hour_category, \n                                 levels = c(\"midnight_peak\", \"midnight_lull\", \n                                            \"morning_peak\", \"morning_lull\", \n                                            \"afternoon_peak\", \"afternoon_lull\", \n                                            \"evening_peak\", \"evening_lull\")))\n\nAs shown, the morning peak hours of 6am to 9am have the highest number of Grab trips while the evening peak periods of 6pm to 9pm have the least number of trips taken.\n\nhourly_trips &lt;- trips %&gt;% \n  group_by(hour_category) %&gt;% \n  summarise(number_of_trips = n(), .groups = 'drop')\n\nhourly_trips$hour_category &lt;- factor(hourly_trips$hour_category, \n                                     levels = unique(hourly_trips$hour_category))\n\nggplot(hourly_trips, aes(x = hour_category, y = number_of_trips, group = 1)) +\n  geom_line(color = \"lightblue3\", linewidth = 1) + \n  geom_point(color = \"lightblue3\", size = 2) +\n  geom_text(aes(label = number_of_trips), vjust = -0.5, color = \"black\", size = 3.5) +\n  labs(title = \"Distribution of Number of Trips By Hour Category\",\n       x = \"Hour Category\",\n       y = \"Number of Trips\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n4.2.3 Distribution of Trip Duration\n1) By Time of Day\nSimilar to the distribution of trip distance, by using geom_violin() from ggplot2 package, we can visualise the spread of trip duration (in minutes) from origin to destination, based on each hour category. Here, we randomly sample 2000 trip data to enable more readability since plotting the entire dataset will result in a chunk of dots that do not offer insights into the spread of trip distance.\nAcross each hour category, the total duration spent on Grab journeys tend to be similar in their median values. Perhaps, we see the most outliers in trip distances for trips taken during the morning lull and evening lull where extremely long trip distances are observed.\n\nsampled_trips &lt;- trips %&gt;%\n  sample_n(2000)\n\nggplot(sampled_trips, aes(x = hour_category, y = total_duration_minutes, fill = hour_category)) +\n  geom_violin(trim = FALSE, alpha = 0.6, draw_quantiles = c(\"0.5\")) +\n  geom_jitter(width = 0.2, size = 0.5, alpha = 0.2, color = \"black\") +  \n  labs(title = \"Distribution of Trip Duration (minutes) by Hour Category\",\n       x = \"Hour Category\",\n       y = \"Total Duration (minutes)\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)  # Tilt x-axis labels\n  ) +\n  scale_fill_manual(values = c(\"lightblue\", \"lightcoral\", \"green3\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\", \"lightgoldenrod\")) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\n\nWhen I remove outliers in total_duration_minutes that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR, we can once again observe that the median duration taken is the same across all time of day.\n\n# Calculate IQR and filter outliers in total_duration_minutes\nfiltered_trips &lt;- trips %&gt;%\n  filter(between(total_duration_minutes, \n                 quantile(total_duration_minutes, 0.25) - 1.5 * IQR(total_duration_minutes), \n                 quantile(total_duration_minutes, 0.75) + 1.5 * IQR(total_duration_minutes))) %&gt;%\n  sample_n(2000)\n\n# Plot without outliers\nggplot(filtered_trips, aes(x = hour_category, y = total_duration_minutes, fill = hour_category)) +\n  geom_violin(trim = FALSE, alpha = 0.6, draw_quantiles = c(\"0.5\")) +\n  geom_jitter(width = 0.2, size = 0.5, alpha = 0.2, color = \"black\") +  \n  labs(title = \"Distribution of Trip Duration (minutes) by Hour Category - No Outliers\",\n       x = \"Hour Category\",\n       y = \"Total Duration (minutes)\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)  # Tilt x-axis labels\n  ) +\n  scale_fill_manual(values = c(\"lightblue\", \"lightcoral\", \"green3\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\", \"lightgoldenrod\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n2) By Day of Week\nIn terms of day of the week, Mondays, Tuesdays and Wednesdays tend to have a slightly shorter trip duration than the rest of the days of the week.\n\n# Sample 2000 trips\nsampled_trips &lt;- trips %&gt;%\n  sample_n(2000)\n\n# Reorder origin_day as a factor with levels from Monday to Sunday\nsampled_trips$origin_day &lt;- factor(sampled_trips$origin_day, \n                                    levels = c(\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"))\n\n# Create the violin plot\nggplot(sampled_trips, aes(x = origin_day, y = total_duration_minutes, fill = origin_day)) +\n  geom_violin(trim = FALSE, alpha = 0.6, draw_quantiles = c(\"0.5\")) +\n  geom_jitter(width = 0.2, size = 0.5, alpha = 0.2, color = \"black\") +  \n  labs(title = \"Distribution of Trip Duration (minutes) by Day of Week\",\n       x = \"Day of Week\",\n       y = \"Trip Duration (minutes)\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)  # Tilt x-axis labels\n  ) +\n  scale_fill_manual(values = c(\"lightblue\", \"lightcoral\", \"green3\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWe have a clearer view of the spread of trip duration when we remove all outliers. We can see that trips taken on Wednesday tend to be shorter journeys than other days of the week.\n\n# Remove outliers based on IQR and then sample 2000 trips\nfiltered_trips &lt;- trips %&gt;%\n  filter(between(total_duration_minutes, \n                 quantile(total_duration_minutes, 0.25) - 1.5 * IQR(total_duration_minutes), \n                 quantile(total_duration_minutes, 0.75) + 1.5 * IQR(total_duration_minutes))) %&gt;%\n  sample_n(2000)\n\n# Reorder origin_day as a factor with levels from Monday to Sunday\nfiltered_trips$origin_day &lt;- factor(filtered_trips$origin_day, \n                                    levels = c(\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"))\n\n# Create the violin plot without outliers\nggplot(filtered_trips, aes(x = origin_day, y = total_duration_minutes, fill = origin_day)) +\n  geom_violin(trim = FALSE, alpha = 0.6, draw_quantiles = c(\"0.5\")) +\n  geom_jitter(width = 0.2, size = 0.5, alpha = 0.2, color = \"black\") +  \n  labs(title = \"Distribution of Trip Duration (minutes) by Day of Week - No Outliers\",\n       x = \"Day of Week\",\n       y = \"Trip Duration (minutes)\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)  # Tilt x-axis labels\n  ) +\n  scale_fill_manual(values = c(\"lightblue\", \"lightcoral\", \"green3\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n3) By Driving Mode\nWe can also use the driving_mode column of the trips data to plot the distribution of trip duration. We can see that motorcycles are slighty more preferred for shorter trip duration than cars,though cars are almost equally as demanded. In terms of longer journeys which involve more minutes, taking the car is more preferred as a mode of driving.\n\nggplot(trips, aes(x = total_duration_minutes, fill = driving_mode)) +\n  geom_density(alpha = 0.8) +\n  labs(title = \"Trip Duration (minutes) by Driving Mode (All Trips)\",\n       x = \"Total Duration (minutes)\",\n       y = \"Density\") +\n  theme_gray() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")) +\n  scale_fill_manual(values = c(\"lightblue\", \"pink2\"))\n\n\n\n\n\n\n\n\nWe can also observe the distribution of trip duration within Jakarta only, meaning we filter out all origin and destinations that are discovered to be outside of Jakarta. Likewise, cars are slightly more preferred than motorcycles for longer trip duration.\n\nggplot(trips_inside_jakarta, aes(x = total_duration_minutes, fill = driving_mode)) +\n  geom_density(alpha = 0.8) +\n  labs(title = \"Trip Duration (minutes) by Driving Mode (Within Jakarta)\",\n       x = \"Total Duration (minutes)\",\n       y = \"Density\") +\n  theme_gray() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")) +\n  scale_fill_manual(values = c(\"lightblue\", \"pink2\"))\n\n\n\n\n\n\n\n\n\n\n4.2.4 Distribution of Trip Distance\n1) By Time of Day\nAgain, we randomly sample 2000 trip data to plot the violin plot of total_distance_km against hour categories. We can observe that longer distances are traveled during the afternoon lull and evening peak periods on average which could be attributed to high traffic volumes or jams on the road.\n\nsampled_trips &lt;- trips %&gt;%\n  sample_n(2000)\n\nggplot(sampled_trips, aes(x = hour_category, y = total_distance_km, fill = hour_category)) +\n  geom_violin(trim = FALSE, alpha = 0.6, draw_quantiles = \"0.5\") +\n  geom_jitter(width = 0.2, size = 0.5, alpha = 0.2, color = \"black\") +  \n  labs(title = \"Distribution of Trip Distance (km) by Hour Category\",\n       x = \"Hour Category\",\n       y = \"Total Distance (km)\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)  # Tilt x-axis labels\n  ) +\n  scale_fill_manual(values = c(\"lightblue\", \"lightcoral\", \"green3\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\", \"lightgoldenrod\")) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\n\n2) By Day of Week\nThe chart suggests that trips taken on Saturday and Sunday have the highest average distance travelled, meaning that people are more willing to travel to destinations further away from where they stay on these days.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Sample 2000 trips\nsampled_trips &lt;- trips %&gt;%\n  sample_n(2000)\n\n# Reorder origin_day as a factor with levels from Monday to Sunday\nsampled_trips$origin_day &lt;- factor(sampled_trips$origin_day, \n                                    levels = c(\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"))\n\n# Create the violin plot\nggplot(sampled_trips, aes(x = origin_day, y = total_distance_km, fill = origin_day)) +\n  geom_violin(trim = FALSE, alpha = 0.6, draw_quantiles = \"0.5\") +\n  geom_jitter(width = 0.2, size = 0.5, alpha = 0.2, color = \"black\") +  \n  labs(title = \"Distribution of Trip Distance (km) by Day of Week\",\n       x = \"Day of Week\",\n       y = \"Total Distance (km)\") +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)  # Tilt x-axis labels\n  ) +\n  scale_fill_manual(values = c(\"lightblue\", \"lightcoral\", \"green3\", \"lightpink\", \"lightyellow\", \"lightgray\", \"lightcyan\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n4.2.5 Day of Week and Hourly Patterns in Ride-hailing Demand\nNow, let‚Äôs generate a heatmap displaying the distribution of trips across different days of the week (using the origin_day column) and the specified hour categories, with the fill color indicating the number of trips. The days are ordered from Monday to Sunday, making it easier to interpret the data.\nWe can observe the highest number of trips taken during both morning peak and lull periods on Thursdays, as well as, Saturday midnight lull periods. On the other hand, Monday and Tuesday evening peaks have the lowest number of trips.\n\n# Create a new summary data frame using hour_category\nheatmap_data &lt;- trips %&gt;%\n  group_by(origin_day, hour_category) %&gt;%\n  summarise(number_of_trips = n(), .groups = 'drop') %&gt;%\n  arrange(origin_day, hour_category)\n\n# Convert origin_day to a factor with specific order\nheatmap_data$origin_day &lt;- factor(heatmap_data$origin_day, \n                                   levels = c(\"monday\", \"tuesday\", \"wednesday\", \"thursday\",\n                                              \"friday\", \"saturday\", \"sunday\"))\n\n# Convert hour_category to a factor for better ordering\nheatmap_data$hour_category &lt;- factor(heatmap_data$hour_category, \n                                      levels = c(\"midnight_peak\", \"midnight_lull\", \n                                                 \"morning_peak\", \"morning_lull\", \n                                                 \"afternoon_peak\", \"afternoon_lull\", \n                                                 \"evening_peak\", \"evening_lull\")) \n\n# Create the heatmap\nggplot(heatmap_data, aes(x = hour_category, y = origin_day)) +\n  geom_tile(aes(fill = number_of_trips), color = \"white\") +\n  geom_text(aes(label = number_of_trips), color = \"black\", size = 4) +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\", name = \"Number of Trips\") +\n  labs(title = \"Heatmap of Number of Trips by Day and Hour Category\",\n       x = \"Hour Category\",\n       y = \"Day\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1) \n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#spatial-distribution-of-ride-hailing-traffic",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#spatial-distribution-of-ride-hailing-traffic",
    "title": "Take-home Exercise 3",
    "section": "5.1 Spatial Distribution of Ride-Hailing Traffic",
    "text": "5.1 Spatial Distribution of Ride-Hailing Traffic\n\n5.1.1 What is the spatial distribution of trips originating from different districts across Jakarta?\n\nPlot/Graph: Choropleth map\nAnalysis/Conclusion: Observe areas of high and low trip origins. These insights can help in identifying underserved regions or overly concentrated areas.\n\n\n\n5.1.2 How do origin-destination flows vary spatially across the city (e.g., desire lines or flow maps)?\n\nPlot/Graph: Desire line map (spatial flow map)\nAnalysis/Conclusion: Highlight major commuting or movement corridors. High flow between districts suggests popular commuting routes.\n\n\n\n5.1.3 Are there specific clusters of high ride-hailing demand that form around particular locations (e.g., central business districts, residential areas)?\n\nPlot/Graph: Kernel density map\nAnalysis/Conclusion: Detect spatial clusters of high demand. Business districts might be heavy traffic zones during office hours."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#spatial-correlation-hotspot-detection",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#spatial-correlation-hotspot-detection",
    "title": "Take-home Exercise 3",
    "section": "5.2 Spatial Correlation & Hotspot Detection",
    "text": "5.2 Spatial Correlation & Hotspot Detection\n\n5.2.1 Are there spatial patterns or clusters of high ride-hailing traffic (using techniques like spatial autocorrelation or hotspot analysis)?\n\nPlot/Graph: Hotspot map or Moran‚Äôs I for spatial autocorrelation\nAnalysis/Conclusion: Identify areas where traffic demand is spatially concentrated. Hotspots can inform traffic management strategies.\n\n\n\n5.2.2 Where are the spatial hotspots of traffic congestion or high demand for rides?\n\nPlot/Graph: Heatmap or kernel density map\nAnalysis/Conclusion: Analyze where demand is so high that congestion becomes an issue. Look for recurring patterns to adjust traffic flow or driver distribution."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#spatial-relationships-with-points-of-interest",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#spatial-relationships-with-points-of-interest",
    "title": "Take-home Exercise 3",
    "section": "5.3 Spatial Relationships with Points of Interest",
    "text": "5.3 Spatial Relationships with Points of Interest\n\n5.3.1 How does the distance from key POIs (e.g., schools, hospitals, malls, offices) impact ride-hailing traffic patterns in surrounding districts?\n\nPlot/Graph: Bubble map with distance from POIs\nAnalysis/Conclusion: Correlate traffic density with proximity to POIs. Shorter distances from key locations might lead to higher demand.\n\n\n\n5.3.2 Are there spatial relationships between POIs and the volume of trips originating or terminating nearby?\n\nPlot/Graph: Heatmap for POIs vs.¬†trip volumes\nAnalysis/Conclusion: Understand how certain POIs contribute to trip volume. This is important for prioritizing service areas."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#impact-of-weather-on-spatial-patterns",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#impact-of-weather-on-spatial-patterns",
    "title": "Take-home Exercise 3",
    "section": "5.4 Impact of Weather on Spatial Patterns",
    "text": "5.4 Impact of Weather on Spatial Patterns\n\n5.4.1 How does rainy weather influence spatial traffic patterns? Do people travel more or less between certain districts during rain?\n\nPlot/Graph: Comparative maps for rainy vs.¬†non-rainy days\nAnalysis/Conclusion: Compare spatial flows under different weather conditions. Some areas may see drops in traffic due to adverse weather."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#what-are-the-major-spatial-flow-patterns-origin-to-destination-across-different-villages-in-jakarta",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#what-are-the-major-spatial-flow-patterns-origin-to-destination-across-different-villages-in-jakarta",
    "title": "Take-home Exercise 3",
    "section": "6.1 What are the major spatial flow patterns (origin to destination) across different villages in Jakarta?",
    "text": "6.1 What are the major spatial flow patterns (origin to destination) across different villages in Jakarta?\n\nPlot/Graph: Flow map or desire line map\nAnalysis/Conclusion: Analyze directional flows to understand commuter or travel behavior between regions."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#do-spatial-flow-patterns-change-depending-on-the-vehicle-type-or-time-of-day",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#do-spatial-flow-patterns-change-depending-on-the-vehicle-type-or-time-of-day",
    "title": "Take-home Exercise 3",
    "section": "6.3 Do spatial flow patterns change depending on the vehicle type or time of day?",
    "text": "6.3 Do spatial flow patterns change depending on the vehicle type or time of day?\n\nPlot/Graph: Comparative flow maps for motorbikes and cars\nAnalysis/Conclusion: Identify different flow patterns for vehicle types, potentially revealing different uses of motorbikes versus cars."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#what-are-the-major-spatial-flow-patterns-origin-to-destination-across-different-districts-in-jakarta",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#what-are-the-major-spatial-flow-patterns-origin-to-destination-across-different-districts-in-jakarta",
    "title": "Take-home Exercise 3",
    "section": "What are the major spatial flow patterns (origin to destination) across different districts in Jakarta?",
    "text": "What are the major spatial flow patterns (origin to destination) across different districts in Jakarta?\n\nPlot/Graph: Flow map or desire line map\nAnalysis/Conclusion: Analyze directional flows to understand commuter or travel behavior between regions."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#impact-of-weather",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#impact-of-weather",
    "title": "Take-home Exercise 3",
    "section": "4.3 Impact of Weather",
    "text": "4.3 Impact of Weather\n\n4.3.1 Volume of Trips per District based on Weather Conditions (Rain or No Rain)\nWe can visualise that the number of trips taken when there is no rain outweights the number of trips for rainy weather conditins rather significantly across all districts. This is good news for Grab since their ride-hailing services are still as demanded for not just during trip origins where rain might be seen as an inconvenience, but also during regular weather conditions where there is no rain.\n\ndistrict_origin_long &lt;- district_origin %&gt;%\n  select(district, not_rain, rain) %&gt;%\n  pivot_longer(cols = c(not_rain, rain), names_to = \"weather_type\", values_to = \"trip_count\")\n\n# Plot the data\nggplot(district_origin_long, aes(x = district, y = trip_count, fill = weather_type)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  labs(\n    title = \"Trip Counts by District and Weather Type\",\n    x = \"District\",\n    y = \"Number of Trips\",\n    fill = \"Weather Type\"\n  ) +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.text.x = element_text(angle = 90, hjust = 1) \n  ) +\n  scale_fill_manual(values = c(\"not_rain\" = \"skyblue\", \"rain\" = \"steelblue\"))\n\n\n\n\n\n\n\n\n\n\n4.3.2 Volume of Trips based on Specific Weather Sub-Categories\nThe most number of trips were taken when the weather condition for the origin was ‚Äòbroken clouds‚Äô, ‚Äòscattered clouds‚Äô and ‚Äòovercast clouds‚Äô which is Jakarta‚Äôs most common weather condition throughout the year, with occasional wet weathers. This indicates that Grab services are as demanded even when there is no rain. In fact, we can notice that origin trips with light rain has more demand than trips found having heavy rain.\n\n# Summing trip counts by weather condition\nweather_summary &lt;- district_origin %&gt;%\n  summarise(across(starts_with(\"broken clouds\"):starts_with(\"heavy rain\"), sum)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"weather_condition\", values_to = \"trip_count\")\n\n# Bar chart\nggplot(weather_summary, aes(x = reorder(weather_condition, -trip_count), y = trip_count)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue3\") +\n  geom_text(aes(label = trip_count), vjust = -0.5, size = 4) +\n  labs(\n    title = \"Number of Trips by Weather Condition\",\n    x = \"Weather Condition\",\n    y = \"Number of Trips\"\n  ) +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1) \n  )\n\n\n\n\n\n\n\n\n\n\n4.3.3 Impact of Trip Duration on Trip Volume by Weather Conditions (Rain or No Rain)\nThe analysis reveals that trips with shorter durations (measured in minutes) tend to have higher demand, with a greater number of trips recorded for shorter journeys compared to longer ones. This trend persists across different weather conditions, including both rainy and non-rainy weather.\nThis pattern suggests that weather conditions‚Äîwhether rainy or no rain‚Äîdo not significantly impact commuter behavior in relation to trip length. In other words, commuters are inclined to travel shorter distances more frequently, regardless of weather, while longer trips are less frequent under all weather conditions.\n\ndistrict_long &lt;- district_origin %&gt;%\n  pivot_longer(\n    cols = c(\"rain\",\"not_rain\"),\n    names_to = \"weather_condition\",\n    values_to = \"trip_count\"\n  )\n\nggplot(district_long, aes(x = avg_duration_minutes, y = trip_count, color = weather_condition)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method=\"lm\", se = TRUE) +\n  facet_wrap(~ weather_condition, scales = \"free_y\") +\n  scale_color_viridis_d(option = \"plasma\") +  \n  labs(\n    title = \"Relationship Between Average Trip Duration and Trip Count by Weather Condition\",\n    x = \"Average Trip Duration (minutes)\",\n    y = \"Number of Trips\"\n  ) +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    legend.position = \"none\",  \n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Impact of Trip Duration on Trip Volume by Weather Sub-Categories\nNext, I will further drill down the rain and not_rain weather categories by its sub-categories. Interestingly, trip origins that are under heavy rain conditions tend to somewhat accumulate more trips especially for journeys that are longer in duration (minutes). This means that commuters likely expect for the heavy rain to last for a long time and would hence, result in a greater demand for Grab services when there is heavy rain. In contrast, start of trips with light rain shows the reverse trend where less trips are demanded for when duration of trip is longer.\n\ndistrict_long &lt;- district_origin %&gt;%\n  pivot_longer(\n    cols = c(\"broken clouds\", \"scattered clouds\", \"light rain\", \"overcast clouds\", \n             \"moderate rain\", \"haze\", \"fog\", \"few clouds\", \"heavy rain\"),\n    names_to = \"weather_condition\",\n    values_to = \"trip_count\"\n  )\n\nggplot(district_long, aes(x = avg_duration_minutes, y = trip_count, color = weather_condition)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method=\"lm\", se = TRUE) +\n  facet_wrap(~ weather_condition, scales = \"free_y\") +\n  scale_color_viridis_d(option = \"plasma\") +  \n  labs(\n    title = \"Relationship Between Average Trip Duration and Trip Count by Weather Condition\",\n    x = \"Average Trip Duration (minutes)\",\n    y = \"Number of Trips\"\n  ) +\n  theme_gray() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    legend.position = \"none\",  \n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 Impact of Weather on Trip Demand By Motorcycles and Cars\n\n\nPrepare data by aggregating district, weather & driving mode\ntrips_origin_weather_vehicle &lt;- trips %&gt;%\n  filter(origin_weather_description %in% weather_descriptions) %&gt;%\n  group_by(origin_district, origin_weather_description, driving_mode) %&gt;%\n  summarise(num_of_trips = n(), .groups = 'drop') %&gt;%\n  # Create a new column for each combination of weather and driving mode\n  pivot_wider(\n    names_from = c(origin_weather_description, driving_mode),\n    values_from = num_of_trips,\n    values_fill = list(num_of_trips = 0)\n  ) %&gt;%\n  rename(district = origin_district) %&gt;%\n  mutate(district = tolower(district))\n\n# The rest remains the same\ncategory_counts &lt;- trips %&gt;%\n  group_by(origin_district, origin_weather_description_category) %&gt;%\n  summarise(total_category_count = n(), .groups = 'drop') %&gt;%\n  pivot_wider(\n    names_from = origin_weather_description_category,\n    values_from = total_category_count,\n    values_fill = list(total_category_count = 0)\n  ) %&gt;%\n  rename(district = origin_district) %&gt;%\n  mutate(district = tolower(district))\n\n# Join the dataframes\ntrips_origin_weather_vehicle &lt;- trips_origin_weather_vehicle %&gt;%\n  left_join(category_counts, by = \"district\")\n\n# Prepare data by gathering columns into long format\nplot_data &lt;- trips_origin_weather_vehicle %&gt;%\n  select(district, contains(\"car\"), contains(\"motorcycle\")) %&gt;%\n  pivot_longer(\n    cols = -district,\n    names_to = c(\"weather\", \"driving_mode\"),\n    names_sep = \"_\",\n    values_to = \"num_of_trips\"\n  ) %&gt;%\n  filter(driving_mode %in% c(\"car\", \"motorcycle\")) %&gt;%\n  group_by(weather, driving_mode) %&gt;%\n  summarise(total_trips = sum(num_of_trips, na.rm = TRUE), .groups = 'drop')\n\nhead(plot_data)\n\n\n# A tibble: 6 √ó 3\n  weather       driving_mode total_trips\n  &lt;chr&gt;         &lt;chr&gt;              &lt;int&gt;\n1 broken clouds car                10433\n2 broken clouds motorcycle          8925\n3 few clouds    car                    6\n4 few clouds    motorcycle             8\n5 fog           car                   74\n6 fog           motorcycle           131\n\n\nThe demand for cars seem to be higher than motorcycles during trip origins with light rain and overcast clouds. This could be attributed to safety concerns of using motorcycles on slippery roads. Ironically, the trips taken during periods of heavy rain seem to be dominated by motorcycles than cars. Therefore, we cannot make much conclusions of how the weather impacts the mode of driving, but we can see that weather does minimal impact on commuter‚Äôs decision of their preferred vehicle.\n\nggplot(plot_data, aes(x = weather, y = total_trips, fill = driving_mode)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(\n    aes(label = total_trips),\n    position = position_dodge(width = 0.9),\n    vjust = -0.5, color = \"black\", size = 3.5\n  ) +\n  scale_fill_manual(values = c(\"car\" = \"skyblue\", \"motorcycle\" = \"salmon\")) +\n  labs(\n    title = \"Comparison of Car and Motorcycle Trips Across Different Weather Conditions\",\n    x = \"Weather Condition\",\n    y = \"Total Number of Trips\",\n    fill = \"Driving Mode\"\n  ) +\n  theme_gray() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    legend.position = \"top\"\n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#flow-from-origin-to-top-10-destinations",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#flow-from-origin-to-top-10-destinations",
    "title": "Take-home Exercise 3",
    "section": "6.1 Flow from Origin to Top 10 Destinations",
    "text": "6.1 Flow from Origin to Top 10 Destinations\nStep 1: Identify the top 10 destination districts\nStep 2: Filter the data to include only top 10 destinations\nStep 3: Plot the alluvial diagram using geom_alluvium() from the ggalluvial package\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggalluvial)\n\ntop_10_destinations &lt;- trips %&gt;%\n  count(destination_district) %&gt;%\n  top_n(10, wt = n) %&gt;%\n  pull(destination_district)\n\nflow_data &lt;- trips %&gt;%\n  filter(destination_district %in% top_10_destinations) %&gt;%\n  count(origin_district, destination_district) %&gt;%\n  rename(count = n)\n\nggplot(flow_data, aes(axis1 = origin_district, axis2 = destination_district, y = count)) +\n  geom_alluvium(aes(fill = destination_district)) +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  labs(title = \"Flow from Origin to Top 10 Destination Districts\", \n       x = \"Districts\", \n       y = \"Number of Trips\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"))\n\nOR‚Ä¶ top 5 destinations\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggalluvial)\n\n# Step 1: Identify the top 5 destination districts\ntop_5_destinations &lt;- trips %&gt;%\n  count(destination_district) %&gt;%\n  top_n(5, wt = n) %&gt;%\n  pull(destination_district)\n\n# Step 2: Filter the data to include only top 5 destinations\nflow_data &lt;- trips %&gt;%\n  filter(destination_district %in% top_5_destinations) %&gt;%\n  count(origin_district, destination_district) %&gt;%\n  rename(count = n)\n\n# Step 3: Plot the alluvial diagram\nggplot(flow_data, aes(axis1 = origin_district, axis2 = destination_district, y = count)) +\n  geom_alluvium(aes(fill = destination_district)) +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  labs(title = \"Flow from Origin to Top 5 Destination Districts\", \n       x = \"Districts\", \n       y = \"Number of Trips\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"))\n\nWarning in to_lodes_form(data = data, axes = axis_ind, discern =\nparams$discern): Some strata appear at multiple axes.\nWarning in to_lodes_form(data = data, axes = axis_ind, discern =\nparams$discern): Some strata appear at multiple axes.\nWarning in to_lodes_form(data = data, axes = axis_ind, discern =\nparams$discern): Some strata appear at multiple axes."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#traffic-trends-demand-distribution",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#traffic-trends-demand-distribution",
    "title": "Take-home Exercise 3",
    "section": "4.1 Traffic Trends & Demand Distribution",
    "text": "4.1 Traffic Trends & Demand Distribution\n\n4.1.1 Distribution of Trips Origin and Destination By District\nIn terms of trip origins, districts such as Setia Budi, Grogol Petamburan, and Kebayoran Baru have the highest traffic volumes moving out of the district, indicating areas of high trip demand. In terms of trip destinations, districts such as Tanah A bang and Setia Budi are most popular for Grab trips. In contrast, districts like Johar Baru, Dananu Sunter DII, and Danau Sunter experience the lowest traffic volumes, suggesting potential areas of under-service or potentially low demand for Grab services.\n\n# Count trips for origin districts\norigin_counts &lt;- trips %&gt;%\n  count(origin_district) %&gt;%\n  rename(total_trips = n)\n\n# Count trips for destination districts\ndestination_counts &lt;- trips %&gt;%\n  count(destination_district) %&gt;%\n  rename(total_trips = n)\n\n# Create a bar plot for origin districts\np1 &lt;- ggplot(origin_counts, aes(x = reorder(origin_district, -total_trips), y = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue2\") +\n  theme_gray() +\n  labs(title = \"Total Number of Grab Trips per Origin District\",\n       x = \"Origin District\",\n       y = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"), # Center and style the title\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n# Create a bar plot for destination districts\np2 &lt;- ggplot(destination_counts, aes(x = reorder(destination_district, -total_trips), y = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"pink2\") +\n  theme_gray() +\n  labs(title = \"Total Number of Grab Trips per Destination District\",\n       x = \"Destination District\",\n       y = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"), # Center and style the title\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\np1 / p2 \n\n\n\n\n\n\n\n\nWe can also inspect how the population size of each district influences the number of trips taken to and from each district. Generally, districts with a high number of trips per capita from origin also displayed more frequent trips taken into the district.\nInitially, there are districts with the highest trips per capita but in reality, they do NOT correspond to high number of trips (previous chart). For instance, Danau Sunter DII and Danau Sunter are districts with the lowest number of trips but since they also have a population of 0, resulting in a seemingly high number of trips per capita. Hence, we remove trip data from these two districts since it is not possible to divide a value by 0!\n\n\nPrepare data\ndestination_data &lt;- district_dest %&gt;%\n  select(district, population_count, num_of_trips) %&gt;%\n  filter(district != \"Danau Sunter Dll\" & district != \"Danau Sunter\") %&gt;%\n  mutate(trips_per_capita_dest = num_of_trips / population_count) %&gt;%\n  rename(district = district)\n\norigin_data &lt;- district_origin %&gt;%\n  select(district, population_count, num_of_trips) %&gt;%\n  filter(district != \"Danau Sunter Dll\" & district != \"Danau Sunter\") %&gt;%\n  mutate(trips_per_capita_origin = num_of_trips / population_count) %&gt;%\n  rename(district = district)\n\ncombined_data &lt;- destination_data %&gt;%\n  select(district, trips_per_capita = trips_per_capita_dest) %&gt;%\n  mutate(trip_type = \"Destination\") %&gt;%\n  bind_rows(\n    origin_data %&gt;%\n      select(district, trips_per_capita = trips_per_capita_origin) %&gt;%\n      mutate(trip_type = \"Origin\")\n  )\n\n\n\n# Create the line plot for trips per capita with specified colors and theme\nggplot(combined_data, aes(x = district, y = trips_per_capita, color = trip_type, group = trip_type)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Number of Trips Per Capita by District\",\n    x = \"District\",\n    y = \"Trips Per Capita\"\n  ) +\n  theme_gray() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5), \n    axis.text.x = element_text(angle = 90, hjust = 1)\n  ) +\n  scale_color_manual(values = c(\"Origin\" = \"lightblue3\", \"Destination\" = \"pink3\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nInstead, we can just separate the number of trips and the population size as such. Generally, we do not see any trends, meaning that the population size has no effect on the number of trips taken.\n\n\nPrepare data\ndistrict_data &lt;- district_dest %&gt;%\n  select(district, population_count, num_of_trips) %&gt;%\n  rename(trips = num_of_trips)\n\ndistrict_data &lt;- district_data %&gt;%\n  bind_rows(\n    district_origin %&gt;%\n      select(district, population_count, num_of_trips) %&gt;%\n      rename(trips = num_of_trips)\n  ) %&gt;%\n  group_by(district) %&gt;%\n  summarise(\n    population_count = mean(population_count, na.rm = TRUE),\n    total_trips = mean(trips, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(population_count))\n\n\n\nggplot(district_data) +\n  geom_line(aes(x = reorder(district, population_count), y = total_trips, group = 1, color = \"Average Trips Taken\"), size = 1.2) +  # Updated label\n  geom_line(aes(x = reorder(district, population_count), y = population_count * max(total_trips) / max(population_count), group = 2, color = \"Population Count\"), size = 1.2) +\n  geom_point(aes(x = reorder(district, population_count), y = total_trips, color = \"Average Trips Taken\"), size = 3) +  # Updated label\n  geom_point(aes(x = reorder(district, population_count), y = population_count * max(total_trips) / max(population_count), color = \"Population Count\"), size = 3) +\n  labs(\n    title = \"Mean Number of Trips and Population Count by District\",\n    x = \"District\",\n    y = \"Mean Number of Trips\",\n    color = \"Legend\"\n  ) +\n  theme_gray() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),  # Center, bold, size 16\n    axis.text.x = element_text(angle = 90, hjust = 1),\n    legend.position = \"top\"\n  ) +\n  scale_color_manual(values = c(\"Average Trips Taken\" = \"lightblue3\", \"Population Count\" = \"pink3\")) +  # Updated color mapping\n  scale_y_continuous(\n    sec.axis = sec_axis(~ . * max(district_data$population_count) / max(district_data$total_trips), name = \"Population Count\")  # Secondary axis\n  )\n\n\n\n\n\n\n\n\n\n\n4.1.2 Top Origins and Destinations for Ride-hailing Trips\nNext, we will prepare the top 5 districts with the most number of Grab trips based on trip origin and trip destinations.\n\n# Calculate top 5 districts for origin_district\ntop_5_origin_districts &lt;- trips %&gt;%\n  count(origin_district) %&gt;%\n  rename(total_trips = n) %&gt;%\n  arrange(desc(total_trips)) %&gt;%\n  slice(1:5)\n\n# Calculate top 5 districts for destination_district\ntop_5_destination_districts &lt;- trips %&gt;%\n  count(destination_district) %&gt;%\n  rename(total_trips = n) %&gt;%\n  arrange(desc(total_trips)) %&gt;%\n  slice(1:5)\n\nBy plotting a bar chart, we can identify districts with the most demand for Grab services for both in-flow and out-flow of Grab trips. In particular, we can see that districts with most Grab trips originating from it corresponds with the highest number of Grab trips arriving into it (e.g.¬†Setia Budi).\nSuch alignment suggests that these districts function as key hubs of movement within the city, likely due to high-density residential, commercial, or mixed-use areas.\n\n# Plot for Top 5 Origin Districts\np1 &lt;- ggplot(top_5_origin_districts, aes(y = reorder(origin_district, total_trips), x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  theme_gray() +\n  labs(title = \"Most Popular Origin Districts\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n# Plot for Top 5 Destination Districts\np2 &lt;- ggplot(top_5_destination_districts, aes(y = reorder(destination_district, total_trips), x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"pink2\") +\n  theme_gray() +\n  labs(title = \"Most Popular Destination Districts\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n4.1.3 Least popular origins and destinations for ride-hailing trips\nWe can also gather origin and destination districts with the least trips by arranging the total_trips in descending order and filtering districts with the lowest 5 number of trips.\n\n\nPrepare data\norig_count &lt;- trips %&gt;%\n  count(origin_district) %&gt;%\n  rename(trips = n) %&gt;%\n  arrange(trips)\n\ndest_count &lt;- trips %&gt;%\n  count(destination_district) %&gt;%\n  rename(trips = n) %&gt;%\n  arrange(trips)\n\nbottom_5_orig &lt;- orig_count %&gt;%\n  slice_head(n = 5)\n\nbottom_5_dest &lt;- dest_count %&gt;%\n  slice_head(n = 5)\n\nbottom_5_orig$origin_district &lt;- factor(bottom_5_orig$origin_district, \n                                        levels = bottom_5_orig$origin_district)\nbottom_5_dest$destination_district &lt;- factor(bottom_5_dest$destination_district, \n                                             levels = bottom_5_dest$destination_district)\n\n\nHere I plot the geom_bar() of the ggplot2 package. The results aligns with what we mentioned earlier, where the same origin districts with the least out-flow of Grab trips also happen to be districts with the least in-flow of Grab trips.\n\np1 &lt;- ggplot(bottom_5_orig, aes(y = origin_district, x = trips)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  labs(title = \"Least Popular Origin Districts\", y = \"District\", x = \"Trips\") +\n  theme_gray() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"))\n\np2 &lt;- ggplot(bottom_5_dest, aes(y = destination_district, x = trips)) +\n  geom_bar(stat = \"identity\", fill = \"pink2\") +\n  labs(title = \"Least Popular Destination Districts\", y = \"District\", x = \"Trips\") +\n  theme_gray() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"))\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n4.1.4 Popularity of Districts Based on Driving Mode\nWhen using the geom_density() function to create density plots, we can see a smoothed version of the trip distance (in kilometres), further drilled down by the mode of driving (car vs motorcycle). There is almost no difference in relative distance travelled by cars or motorcycles.\nFor longer distances traveled, cars are slightly more preferredthan motorcycles, while for shorter trip distances, both driving modes are equally demanded.Thus, we only see some instances where cars are preferred for when journeys are longer.\n\nggplot(trips, aes(x = total_distance_km, fill = driving_mode)) +\n  geom_density(alpha = 0.8) +\n  labs(title = \"Trip Distance (km) by Driving Mode (All Trips)\",\n       x = \"Total Distance (km)\",\n       y = \"Density\") +\n  theme_gray() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")) +\n  scale_fill_manual(values = c(\"lightblue\", \"pink2\"))\n\n\n\n\n\n\n\n\nThis is the adjusted plot when we completely remove all trips with origins and destinations outside of Jakarta. The distance of Grab journeys significantly decrease from 80km to ~20km, and observations of popularity of the driving mode remains the same.\n\ntrips_inside_jakarta &lt;- trips %&gt;%\n  filter(origin_district != 'Outside of Jakarta', destination_district != 'Outside of Jakarta')\n\nggplot(trips_inside_jakarta, aes(x = total_distance_km, fill = driving_mode)) +\n  geom_density(alpha = 0.8) +\n  labs(title = \"Trip Distance (km) by Driving Mode (Within Jakarta)\",\n       x = \"Total Distance (km)\",\n       y = \"Density\") +\n  theme_gray() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")) +\n  scale_fill_manual(values = c(\"lightblue\", \"pink2\"))\n\n\n\n\n\n\n\n\nWe can further break this down into popular origins and destinations for car and motorcycle drivers respectively. It is rather intriguing that both modes of driving have similar top origin and destinations, as well as least popular origin and destinations travelled. Once again, indicating how both vehicle types are equally demanded in Jakarta.\n1) For Car Drivers\n\n# Filter trips for cars only\ncar_trips &lt;- trips %&gt;%\n  filter(driving_mode == 'car')\n\n# Calculate counts for origin districts\norigin_counts &lt;- car_trips %&gt;%\n  count(origin_district) %&gt;%\n  rename(total_trips = n) %&gt;%\n  arrange(desc(total_trips))\n\n# Get top 5 origin districts\ntop_5_origin_districts &lt;- origin_counts %&gt;%\n  slice_head(n = 5)\n\n# Get least 5 origin districts\nbottom_5_origin_districts &lt;- origin_counts %&gt;%\n  slice_tail(n = 5)\n\n# Set factor levels for top and bottom origin districts\ntop_5_origin_districts$origin_district &lt;- factor(top_5_origin_districts$origin_district, \n                                                  levels = top_5_origin_districts$origin_district)\n\nbottom_5_origin_districts$origin_district &lt;- factor(bottom_5_origin_districts$origin_district, \n                                                     levels = bottom_5_origin_districts$origin_district)\n\n# Plot for Top 5 Origin Districts in descending order\np1 &lt;- ggplot(top_5_origin_districts, aes(y = reorder(origin_district, total_trips), x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"green3\") +\n  theme_gray() +\n  labs(title = \"Top 5 Popular Origin Districts (Cars)\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n# Plot for Least 5 Origin Districts\np2 &lt;- ggplot(bottom_5_origin_districts, aes(y = origin_district, x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"lightcoral\") +\n  theme_gray() +\n  labs(title = \"Least 5 Popular Origin Districts (Cars)\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n# Calculate counts for destination districts\ndestination_counts &lt;- car_trips %&gt;%\n  count(destination_district) %&gt;%\n  rename(total_trips = n) %&gt;%\n  arrange(desc(total_trips))\n\n# Get top 5 destination districts\ntop_5_destination_districts &lt;- destination_counts %&gt;%\n  slice_head(n = 5)\n\n# Get least 5 destination districts\nbottom_5_destination_districts &lt;- destination_counts %&gt;%\n  slice_tail(n = 5)\n\n# Set factor levels for top and bottom destination districts\ntop_5_destination_districts$destination_district &lt;- factor(top_5_destination_districts$destination_district, \n                                                             levels = top_5_destination_districts$destination_district)\n\nbottom_5_destination_districts$destination_district &lt;- factor(bottom_5_destination_districts$destination_district, \n                                                               levels = bottom_5_destination_districts$destination_district)\n\n# Plot for Top 5 Destination Districts\np3 &lt;- ggplot(top_5_destination_districts, aes(y = reorder(destination_district, total_trips), x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  theme_gray() +\n  labs(title = \"Top 5 Popular Destination Districts (Cars)\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n# Plot for Least 5 Destination Districts\np4 &lt;- ggplot(bottom_5_destination_districts, aes(y = destination_district, x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"pink2\") +\n  theme_gray() +\n  labs(title = \"Least 5 Popular Destination Districts (Cars)\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n\n\n\n\n\n\n\n2) For Motorcycle Drivers\n\n\nCode\n# Filter trips for motorcycles only\nmotorcycle_trips &lt;- trips %&gt;%\n  filter(driving_mode == 'motorcycle')\n\n# Calculate counts for origin districts\norigin_counts &lt;- motorcycle_trips %&gt;%\n  count(origin_district) %&gt;%\n  rename(total_trips = n) %&gt;%\n  arrange(desc(total_trips))\n\n# Get top 5 origin districts\ntop_5_origin_districts &lt;- origin_counts %&gt;%\n  slice_head(n = 5)\n\n# Get least 5 origin districts\nbottom_5_origin_districts &lt;- origin_counts %&gt;%\n  slice_tail(n = 5)\n\n# Set factor levels for top and bottom origin districts\ntop_5_origin_districts$origin_district &lt;- factor(top_5_origin_districts$origin_district, \n                                                  levels = top_5_origin_districts$origin_district)\n\nbottom_5_origin_districts$origin_district &lt;- factor(bottom_5_origin_districts$origin_district, \n                                                     levels = bottom_5_origin_districts$origin_district)\n\n# Plot for Top 5 Origin Districts in descending order\np1 &lt;- ggplot(top_5_origin_districts, aes(y = reorder(origin_district, total_trips), x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"green3\") +\n  theme_gray() +\n  labs(title = \"Top 5 Popular Origin Districts (Motorcycles)\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n# Plot for Least 5 Origin Districts\np2 &lt;- ggplot(bottom_5_origin_districts, aes(y = origin_district, x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"lightcoral\") +\n  theme_gray() +\n  labs(title = \"Least 5 Popular Origin Districts (Motorcycles)\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n# Calculate counts for destination districts\ndestination_counts &lt;- motorcycle_trips %&gt;%\n  count(destination_district) %&gt;%\n  rename(total_trips = n) %&gt;%\n  arrange(desc(total_trips))\n\n# Get top 5 destination districts\ntop_5_destination_districts &lt;- destination_counts %&gt;%\n  slice_head(n = 5)\n\n# Get least 5 destination districts\nbottom_5_destination_districts &lt;- destination_counts %&gt;%\n  slice_tail(n = 5)\n\n# Set factor levels for top and bottom destination districts\ntop_5_destination_districts$destination_district &lt;- factor(top_5_destination_districts$destination_district, \n                                                             levels = top_5_destination_districts$destination_district)\n\nbottom_5_destination_districts$destination_district &lt;- factor(bottom_5_destination_districts$destination_district, \n                                                               levels = bottom_5_destination_districts$destination_district)\n\n# Plot for Top 5 Destination Districts\np3 &lt;- ggplot(top_5_destination_districts, aes(y = reorder(destination_district, total_trips), x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  theme_gray() +\n  labs(title = \"Top 5 Popular Destination Districts (Motorcycles)\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\n# Plot for Least 5 Destination Districts\np4 &lt;- ggplot(bottom_5_destination_districts, aes(y = destination_district, x = total_trips)) +\n  geom_bar(stat = \"identity\", fill = \"pink2\") +\n  theme_gray() +\n  labs(title = \"Least 5 Popular Destination Districts (Motorcycles)\", y = \"District\", x = \"Number of Trips\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\")\n  )\n\ngrid.arrange(p1, p2, p3, p4, ncol = 2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#installing-required-packages",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#installing-required-packages",
    "title": "Take-home Exercise 3",
    "section": "2.1 Installing Required Packages",
    "text": "2.1 Installing Required Packages\nFirstly, let us begin by loading these requiring libraries into our R environment.\n\ntidyverse : a collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structure.\nggplot2 : for creating advanced visualisations, graphics and maps using the Grammar of Graphics.\npatchwork and gridExtra: for arranging multiple ggplot2 maps beside each other\nggalluvial: allows for building flow diagrams between origin and destination locations, similar to a sankey diagram.\nsf and sfdep: for spatial data handling\nfmsb: for plotting radarcharts in R\ntmap: for creating thematic maps with spatial data, providing custom styles, colors, legends, and interactivity\n\n\npacman::p_load(tidyverse, ggplot2, patchwork, ggalluvial, gridExtra, sf, sfdep, fmsb, tmap)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#importing-datasets",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#importing-datasets",
    "title": "Take-home Exercise 3",
    "section": "2.2 Importing Datasets",
    "text": "2.2 Importing Datasets\nAs mentioned, I will be using the Grab Posisi dataset for Jakarta which contains GPS pings from Grab vehicles, including timestamps, route data, and vehicle type (motorcycle/car).\nIn addition, my team will be delving into the district administration level to offer more meaningful analysis that is still computationally suitable for this project. We have also further augmented the dataset by including data categories such as, POIs, weather, population size.\nDatasets used in this project\n\nJakarta Point of interests (POIs): https://data.humdata.org/dataset/hotosm_idn_points_of_interest\nWeather API https://www.weatherbit.io/\nJakarta Population Density Data: https://storymaps.arcgis.com/stories/36e38ceefab0455eb6059a734381723c\nJakarta Map: https://data.humdata.org/dataset/cod-ab-idn\nGrab Possi: https://engineering.grab.com/grab-posisi\n\n\n2.2.1 Aspatial Data\n\ntrips &lt;- readRDS(\"data/aspatial/trip_data.rds\")\npois &lt;- readRDS(\"data/aspatial/jakarta_pois.rds\")\npopulation &lt;- read_csv(\"data/aspatial/jakarta_township_population.csv\")\n\nRows: 264 Columns: 5\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): province, city, district, township\ndbl (1): population_2019\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe trips tibble dataframe contains 55,995 unique Grab trips based on every 1 second GPS ping. This dataset also includes useful variables such as the driving_mode (car / motorcycle), total_duration_minutes, total_distance_km, location data (origin_district & destination_district) and time data (origin_day & origin_hour). I will be most interested in knowing where a Grab trip ends, i.e.¬†its destination district, across which periods of the day and week.\n\nglimpse(trips)\n\nRows: 55,995\nColumns: 32\n$ trj_id                              &lt;chr&gt; \"1\", \"10000\", \"10002\", \"10003\", \"1‚Ä¶\n$ driving_mode                        &lt;chr&gt; \"car\", \"motorcycle\", \"motorcycle\",‚Ä¶\n$ origin_time                         &lt;dttm&gt; 2019-04-11 14:17:35, 2019-04-16 0‚Ä¶\n$ destination_time                    &lt;dttm&gt; 2019-04-11 14:36:24, 2019-04-16 0‚Ä¶\n$ total_duration_minutes              &lt;dbl&gt; 18.816667, 9.450000, 21.300000, 32‚Ä¶\n$ total_distance_km                   &lt;dbl&gt; 5.943848, 2.809531, 8.040161, 16.8‚Ä¶\n$ average_speed_kmh                   &lt;dbl&gt; 18.952926, 17.838289, 22.648340, 3‚Ä¶\n$ origin_rawlat                       &lt;dbl&gt; -6.197622, -6.248311, -6.249766, -‚Ä¶\n$ origin_rawlng                       &lt;dbl&gt; 106.7690, 106.9304, 106.9682, 106.‚Ä¶\n$ destination_rawlat                  &lt;dbl&gt; -6.239817, -6.229177, -6.235125, -‚Ä¶\n$ destination_rawlng                  &lt;dbl&gt; 106.8019, 106.9470, 106.8970, 106.‚Ä¶\n$ origin_lat                          &lt;dbl&gt; -2939149, -2993318, -3001902, -295‚Ä¶\n$ origin_lng                          &lt;dbl&gt; 13499453, 13554146, 13569122, 1355‚Ä¶\n$ destination_lat                     &lt;dbl&gt; -2962961, -2989069, -2980935, -300‚Ä¶\n$ destination_lng                     &lt;dbl&gt; 13504006, 13564901, 13543389, 1350‚Ä¶\n$ origin_province                     &lt;chr&gt; \"jakarta\", \"jakarta\", \"outside of ‚Ä¶\n$ origin_city                         &lt;chr&gt; \"kota jakarta barat\", \"kota jakart‚Ä¶\n$ origin_district                     &lt;chr&gt; \"kebon jeruk\", \"duren sawit\", \"out‚Ä¶\n$ destination_province                &lt;chr&gt; \"jakarta\", \"outside of jakarta\", \"‚Ä¶\n$ destination_city                    &lt;chr&gt; \"kota jakarta selatan\", \"outside o‚Ä¶\n$ destination_district                &lt;chr&gt; \"kebayoran baru\", \"outside of jaka‚Ä¶\n$ origin_datetime                     &lt;dttm&gt; 2019-04-11 14:17:35, 2019-04-16 0‚Ä¶\n$ destination_datetime                &lt;dttm&gt; 2019-04-11 14:36:24, 2019-04-16 0‚Ä¶\n$ origin_day                          &lt;chr&gt; \"thursday\", \"tuesday\", \"monday\", \"‚Ä¶\n$ origin_hour                         &lt;dbl&gt; 14, 0, 5, 10, 23, 7, 4, 4, 2, 10, ‚Ä¶\n$ destination_day                     &lt;chr&gt; \"thursday\", \"tuesday\", \"monday\", \"‚Ä¶\n$ destination_hour                    &lt;dbl&gt; 14, 1, 6, 11, 23, 7, 4, 4, 4, 10, ‚Ä¶\n$ origin_time_cluster                 &lt;chr&gt; \"afternoon lull\", \"midnight peak\",‚Ä¶\n$ destination_time_cluster            &lt;chr&gt; \"afternoon lull\", \"midnight peak\",‚Ä¶\n$ origin_date                         &lt;date&gt; 2019-04-11, 2019-04-16, 2019-04-0‚Ä¶\n$ origin_weather_description          &lt;chr&gt; \"broken clouds\", \"scattered clouds‚Ä¶\n$ origin_weather_description_category &lt;chr&gt; \"not_rain\", \"not_rain\", \"Outside o‚Ä¶\n\n\n\n\n\n\n\n\nNote\n\n\n\nGrab trips from outside of Jakarta and moving out of Jakarta are attributed to the outer islands of Jakarta, namely the Kepulauan Seribu Regency, which are chains of islands in the North of Jakarta‚Äôs coasts.\n\n\nThis dataset contains all unique 9,685 POIs found within each district of Jakarta. This will give us insights on how Grab is demanded based on the availability of POIs across different districts.\n\nglimpse(pois)\n\nRows: 9,685\nColumns: 11\n$ poi_name &lt;chr&gt; \"TK Sekolah Kemurnian I\", \"PAUD Melati X\", \"Posyandu Cempaka\"‚Ä¶\n$ province &lt;chr&gt; \"jakarta\", \"jakarta\", \"jakarta\", \"jakarta\", \"jakarta\", \"jakar‚Ä¶\n$ city     &lt;chr&gt; \"kota jakarta barat\", \"kota jakarta timur\", \"kota jakarta tim‚Ä¶\n$ district &lt;chr&gt; \"taman sari\", \"pasar rebo\", \"pasar rebo\", \"cilandak\", \"koja\",‚Ä¶\n$ category &lt;chr&gt; \"Facilities_Services\", \"Facilities_Services\", \"Essentials\", \"‚Ä¶\n$ poi_type &lt;chr&gt; \"kindergarten\", \"kindergarten\", \"clinic\", \"kindergarten\", \"co‚Ä¶\n$ geometry &lt;POINT [m]&gt; POINT (13527881 -2927558), POINT (13508901 -3013973), P‚Ä¶\n$ lat_4326 &lt;dbl&gt; -6.146429, -6.334694, -6.331832, -6.313693, -6.124969, -6.340‚Ä¶\n$ lng_4326 &lt;dbl&gt; 106.8132, 106.8634, 106.8456, 106.7959, 106.9196, 106.8066, 1‚Ä¶\n$ lat_5580 &lt;dbl&gt; -2927558, -3013973, -3009066, -2991388, -2940921, -3004218, -‚Ä¶\n$ lng_5580 &lt;dbl&gt; 13527881, 13508901, 13502345, 13486227, 13575632, 13484988, 1‚Ä¶\n\n\nThirdly, the population dataframe contains all population counts in 2019 down to the township level. We will need to perform some data wrangling to summarise it to the district level later.\n\nglimpse(population)\n\nRows: 264\nColumns: 5\n$ province        &lt;chr&gt; \"Jakarta\", \"Jakarta\", \"Jakarta\", \"Jakarta\", \"Jakarta\",‚Ä¶\n$ city            &lt;chr&gt; \"Kota Jakarta Utara\", \"Kota Jakarta Barat\", \"Kota Jaka‚Ä¶\n$ district        &lt;chr&gt; \"Pademangan\", \"Tambora\", \"Kramat Jati\", \"Jatinegara\", ‚Ä¶\n$ township        &lt;chr&gt; \"ancol\", \"angke\", \"bale kambang\", \"bali mester\", \"bamb‚Ä¶\n$ population_2019 &lt;dbl&gt; 4921, 1545, 3774, 2762, 2667, 1724, 2938, 3774, 982, 2‚Ä¶\n\n\n\n\n2.2.2 Geospatial Data\nNext, let‚Äôs proceed with producing the jakarta_district simple feature dataframe consisting of the district boundary of Jakarta by reading from the idn_admbnda_adm3_bps_20200401 shapefile below.\n\n# Step 1: Read the Indonesia administrative boundary shapefile\nindonesia &lt;- st_read(\n  dsn = \"data/geospatial/indon\", \n  layer = \"idn_admbnda_adm3_bps_20200401\"\n)\n\nReading layer `idn_admbnda_adm3_bps_20200401' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Take-home_Ex\\Take-home_Ex3\\data\\geospatial\\indon' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7069 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 95.01079 ymin: -11.00762 xmax: 141.0194 ymax: 6.07693\nGeodetic CRS:  WGS 84\n\n# Step 2: Filter for DKI Jakarta and rename it to \"Jakarta\"\njakarta_district &lt;- indonesia %&gt;%\n  filter(ADM1_EN == \"Dki Jakarta\") %&gt;%\n  mutate(ADM1_EN = \"Jakarta\")\n\n# Step 3: Exclude districts belonging to \"Kepulauan Seribu\"\njakarta_district &lt;- jakarta_district %&gt;%\n  filter(ADM2_EN != \"Kepulauan Seribu\")  # Exclude Kepulauan Seribu\n\n# Step 4: Select only the required columns\njakarta_district &lt;- jakarta_district %&gt;%\n  dplyr::select(ADM1_EN, ADM2_EN, ADM3_EN, geometry)\n\n# Step 5: Rename columns to more meaningful names\njakarta_district &lt;- jakarta_district %&gt;%\n  rename(\n    province = ADM1_EN,\n    city = ADM2_EN,\n    district = ADM3_EN,\n  )\n\n# Step 6: Ensure the CRS is EPSG:4326 (WGS84)\njakarta_district &lt;- jakarta_district %&gt;%\n  st_transform(crs = 5580)  # Transform to WGS84\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: CRS EPSG:5580 is deprecated.\nIts non-deprecated replacement EPSG:6384 will be used instead. To use the\noriginal CRS, set the OSR_USE_NON_DEPRECATED configuration option to NO.\n\njakarta_district &lt;- jakarta_district %&gt;%\n  mutate(across(where(is.character), tolower))\n\njakarta_district_df &lt;- jakarta_district %&gt;%\n  st_drop_geometry()\n\n# Step 7: Simplify the geometry with a smaller tolerance\njakarta_district &lt;- jakarta_district %&gt;%\n  st_simplify(dTolerance = 10.0)  # Smaller tolerance for longitude/latitude data\n\n\nclass(jakarta_district)\n\n[1] \"sf\"         \"data.frame\"\n\n\nNext, we can plot the interactive map of Jakarta and their districts using tmap and OpenStreetMap, which plots all of its 44 districts and their boundaries as shown below.\n\n\nPlot the interactive map using tmap\ntmap_mode(\"view\") \n\ntm_shape(jakarta_district) +  \n  tm_polygons(\n    col = \"district\",         \n    palette = \"Blues\",     \n    border.col = \"black\",     \n    lwd = 0.5,                \n    popup.vars = \"district\"    \n  ) +\n  tm_text(\"district\", size = 0.6, col = \"black\") + \n  tm_basemap(\"OpenStreetMap\")\n\n\n\n\n\n\nPlot the interactive map using tmap\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#categorising-point-of-interests-data",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#categorising-point-of-interests-data",
    "title": "Take-home Exercise 3",
    "section": "3.1 Categorising Point of interests Data",
    "text": "3.1 Categorising Point of interests Data\nI assisted my team mate, Jia Le, with handling some of the data cleaning process of the project. This includes re-categorising the 205 POIs identified for each district into its correct category as the initial categories were previously classified wrongly.\nThere are a total of 9 unique categories that needed some re-categorisation.\n\nFacilities_Services\nEssentials\nOffices_Business\nCultural_Attractions\nRestaurants_Food\nRecreation_Entertainment\nOthers\nShops\nTourism_Spots\n\nFor instance, the ‚Äòtaxi‚Äô POI was initially classified as ‚ÄòOthers‚Äô instead of something more related like ‚ÄòFacilities_Services‚Äô."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#augment-poi-dataset",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#augment-poi-dataset",
    "title": "Take-home Exercise 3",
    "section": "3.2 Augment POI Dataset",
    "text": "3.2 Augment POI Dataset"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html",
    "title": "In-class Exercise 11",
    "section": "",
    "text": "Hands-on Exercise R GWmodel gtsummary GWR\nIn this in-class exercise, we explore new R packages to"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#aggregate-the-population-dataset",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#aggregate-the-population-dataset",
    "title": "Take-home Exercise 3",
    "section": "3.2 Aggregate the Population Dataset",
    "text": "3.2 Aggregate the Population Dataset\nIt is worth noting that we have two lakes that are considered districts here - Danau Sunter and Danau Sunter DII - which explains why they have 0 population size. We also do not have population data for districts outside of Jakarta here.\n\npopulation &lt;- population %&gt;%\n  group_by(district) %&gt;%\n  summarise(population_count = sum(population_2019))\n\n# Inspect\npopulation\n\n# A tibble: 44 √ó 2\n   district         population_count\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 Cakung                      38710\n 2 Cempaka Putih                7440\n 3 Cengkareng                  40470\n 4 Cilandak                    16880\n 5 Cilincing                   37471\n 6 Cipayung                    21336\n 7 Ciracas                     30015\n 8 Danau Sunter                    0\n 9 Danau Sunter Dll                0\n10 Duren Sawit                 38150\n# ‚Ñπ 34 more rows"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#prepare-the-augmented-pois-dataset",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#prepare-the-augmented-pois-dataset",
    "title": "Take-home Exercise 3",
    "section": "3.3 Prepare the Augmented POIs Dataset",
    "text": "3.3 Prepare the Augmented POIs Dataset\nFor a more useful analysis, we can break it down by POI category types and get the count of POIs within each category for each district, as shown in the pois_num tibble below.\n\n# Count the number of POIs by district\npois_num &lt;- pois %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(district) %&gt;%\n  summarise(num_of_pois = n(), .groups = 'drop')\n\n# Inspect\npois_num\n\n# A tibble: 44 √ó 2\n   district         num_of_pois\n   &lt;chr&gt;                  &lt;int&gt;\n 1 cakung                    56\n 2 cempaka putih             38\n 3 cengkareng               298\n 4 cilandak                 242\n 5 cilincing                119\n 6 cipayung                  56\n 7 ciracas                  126\n 8 danau sunter               1\n 9 danau sunter dll           3\n10 duren sawit              173\n# ‚Ñπ 34 more rows"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#preprare-the-augmented-trips-dataset",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#preprare-the-augmented-trips-dataset",
    "title": "Take-home Exercise 3",
    "section": "3.4 Preprare the Augmented Trips Dataset",
    "text": "3.4 Preprare the Augmented Trips Dataset\nI noticed that we have records where destination districts are found outside of Jakarta‚Äôs boundary. For the purpose of analysing Grab trends within Jakarta, I will filter these trip records out.\n\ntrips &lt;- trips %&gt;%\n  filter(destination_district != \"outside of jakarta\" |\n         origin_district != \"outside of jakarta\")\n\n# Inspect\nhead(trips)\n\n# A tibble: 6 √ó 32\n  trj_id driving_mode origin_time         destination_time   \n  &lt;chr&gt;  &lt;chr&gt;        &lt;dttm&gt;              &lt;dttm&gt;             \n1 1      car          2019-04-11 14:17:35 2019-04-11 14:36:24\n2 10000  motorcycle   2019-04-16 00:51:24 2019-04-16 01:00:51\n3 10002  motorcycle   2019-04-08 05:55:41 2019-04-08 06:16:59\n4 10003  car          2019-04-19 10:52:34 2019-04-19 11:24:44\n5 10007  car          2019-04-11 07:05:00 2019-04-11 07:28:56\n6 10012  car          2019-04-20 02:38:54 2019-04-20 04:05:16\n# ‚Ñπ 28 more variables: total_duration_minutes &lt;dbl&gt;, total_distance_km &lt;dbl&gt;,\n#   average_speed_kmh &lt;dbl&gt;, origin_rawlat &lt;dbl&gt;, origin_rawlng &lt;dbl&gt;,\n#   destination_rawlat &lt;dbl&gt;, destination_rawlng &lt;dbl&gt;, origin_lat &lt;dbl&gt;,\n#   origin_lng &lt;dbl&gt;, destination_lat &lt;dbl&gt;, destination_lng &lt;dbl&gt;,\n#   origin_province &lt;chr&gt;, origin_city &lt;chr&gt;, origin_district &lt;chr&gt;,\n#   destination_province &lt;chr&gt;, destination_city &lt;chr&gt;,\n#   destination_district &lt;chr&gt;, origin_datetime &lt;dttm&gt;, ‚Ä¶\n\n\nNext, I will leverage the trips tibble dataframe to group all trips by destination_district and summarise them by number of trips, average duration (minutes) and average distance travelled (km).\n\ntrips_dest &lt;- trips %&gt;%\n  group_by(destination_district) %&gt;%\n  summarise(\n    num_of_trips = n(), \n    avg_duration_minutes = mean(total_duration_minutes, na.rm = TRUE), \n    avg_distance_km = mean(total_distance_km, na.rm = TRUE)  \n  ) %&gt;%\n  rename(district = destination_district) %&gt;%\n  mutate(district = tolower(district))\n\ntrips_origin &lt;- trips %&gt;%\n  group_by(origin_district) %&gt;%\n  summarise(\n    num_of_trips = n(), \n    avg_duration_minutes = mean(total_duration_minutes, na.rm = TRUE), \n    avg_distance_km = mean(total_distance_km, na.rm = TRUE)  \n  ) %&gt;%\n  rename(district = origin_district) %&gt;%\n  mutate(district = tolower(district))\n\nI will also join the pois_num dataframe to trips_dest and trips_origin by the district column so we can append the total number of POIs for each destination district.\n\ntrips_dest_pois &lt;- trips_dest %&gt;%\n  inner_join(pois_num, by = \"district\")\n\ntrips_origin_pois &lt;- trips_origin %&gt;%\n  inner_join(pois_num, by = \"district\")\n\nLet‚Äôs also include the population data from the population dataframe we imported into R.\n\n# For the destination\ndistrict_dest &lt;- trips_dest_pois %&gt;%\n  left_join(population %&gt;% mutate(district = tolower(district)), \n            by = \"district\")\n\ndistrict_dest\n\n# A tibble: 44 √ó 6\n   district        num_of_trips avg_duration_minutes avg_distance_km num_of_pois\n   &lt;chr&gt;                  &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;       &lt;int&gt;\n 1 cakung                   844                 21.5            6.40          56\n 2 cempaka putih            392                 19.1            6.13          38\n 3 cengkareng              1120                 19.9            6.20         298\n 4 cilandak                 905                 20.1            5.78         242\n 5 cilincing                277                 24.4            7.24         119\n 6 cipayung                 552                 22.5            7.08          56\n 7 ciracas                  546                 22.6            7.03         126\n 8 danau sunter              17                 17.3            5.21           1\n 9 danau sunter d‚Ä¶           33                 16.9            5.00           3\n10 duren sawit              877                 21.1            6.32         173\n# ‚Ñπ 34 more rows\n# ‚Ñπ 1 more variable: population_count &lt;dbl&gt;\n\n\n\n# For the origin\ndistrict_origin &lt;- trips_origin_pois %&gt;%\n  left_join(population %&gt;% mutate(district = tolower(district)), \n            by = \"district\")\n\ndistrict_origin\n\n# A tibble: 44 √ó 6\n   district        num_of_trips avg_duration_minutes avg_distance_km num_of_pois\n   &lt;chr&gt;                  &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;       &lt;int&gt;\n 1 cakung                   628                 22.2            6.56          56\n 2 cempaka putih            460                 17.9            5.52          38\n 3 cengkareng               928                 20.2            6.00         298\n 4 cilandak                 830                 20.4            6.09         242\n 5 cilincing                266                 24.7            7.30         119\n 6 cipayung                 481                 22.9            7.09          56\n 7 ciracas                  607                 23.1            7.38         126\n 8 danau sunter               4                 19.4            8.55           1\n 9 danau sunter d‚Ä¶           34                 21.3            7.54           3\n10 duren sawit              891                 21.3            6.39         173\n# ‚Ñπ 34 more rows\n# ‚Ñπ 1 more variable: population_count &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#origin-destination-spatial-flows",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#origin-destination-spatial-flows",
    "title": "Take-home Exercise 3",
    "section": "4.5 Origin-Destination Spatial Flows",
    "text": "4.5 Origin-Destination Spatial Flows\nNext, we can also delve into the spatial flows between origin and destination districts. To do so, I will leverage the geom_alluviam() function of the ggplot2 and ggalluvial packages to map the flows. I will also ientify the top 5 origin / destination districts by doing a filter() on the data as shown.\n\n4.5.1 Flow from All Origins to Top 5 Destinations\nWe can see how complex the spread of origin districts are, where a thicker line connecting the origin and destination resembles a higher demand for Grab. In this case, trips taken outside of Jakarta and moving into the district of Kebayoran Lama are the highest. The reverse is also true; Grab trips moving from Kebayoran Lama to outside of Jakarta are the highest.\n\n# Identify the top 5 destination districts\ntop_5_destinations &lt;- trips %&gt;%\n  count(destination_district) %&gt;%\n  top_n(5, wt = n) %&gt;%\n  pull(destination_district)\n\n# Filter the data to include only top 5 destinations\nflow_data &lt;- trips %&gt;%\n  filter(destination_district %in% top_5_destinations) %&gt;%\n  count(origin_district, destination_district) %&gt;%\n  rename(count = n)\n\n# Plot the alluvial diagram\nggplot(flow_data, aes(axis1 = origin_district, axis2 = destination_district, y = count)) +\n  geom_alluvium(aes(fill = destination_district)) +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  labs(title = \"Flow from Origin to Top 5 Destination Districts\", \n       x = \"Districts\", \n       y = \"Number of Trips\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n4.5.2 Flow Pattern from Top 5 Origins to All Destinations\nAs for the trips moving out from the top 5 origin districts, we see that most trips come from outside of Jakarta, meaning that a substantial flow of Grab traffic comes from outbound of Jakarta. As compared to the previous flow chart, the destinations of these 5 origins seem to spread evenly across all districts of Jakarta. Perhaps, the most popular destination spots for Grab trips are found in Kebayoran Lama, Setia Budi and Tanah Abang.\n\n# Identify the top 5 origin districts based on the number of trips\ntop_5_origins &lt;- trips %&gt;%\n  count(origin_district) %&gt;%\n  top_n(5, wt = n) %&gt;%\n  pull(origin_district)\n\n# Filter the data to include only top 5 origins and their corresponding destination districts\nflow_data &lt;- trips %&gt;%\n  filter(origin_district %in% top_5_origins) %&gt;%\n  count(origin_district, destination_district) %&gt;%\n  rename(count = n)\n\n# Plot the alluvial diagram\nggplot(flow_data, aes(axis1 = origin_district, axis2 = destination_district, y = count)) +\n  geom_alluvium(aes(fill = origin_district)) +  # Use origin_district for fill\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)), size = 4) +\n  labs(title = \"Flow from Top 5 Origin Districts to Destination Districts\", \n       x = \"Districts\", \n       y = \"Number of Trips\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n4.5.3 Flow Patterns from Top 5 Origins to Top 5 Destinations\nI believe we can make the most meaningful analysis when just looking at the top 5 originating and top 5 destination districts where these districts had the highest number of Grab trips taken. We can observe that Setia Budi is a popular origin and destination for Grab services, in fact, many of the trips are found within the district itself.\n\n# Step 1: Identify the top 5 origin districts\ntop_5_origins &lt;- trips %&gt;%\n  count(origin_district) %&gt;%\n  top_n(5, wt = n) %&gt;%\n  pull(origin_district)\n\n# Step 2: Identify the top 5 destination districts\ntop_5_destinations &lt;- trips %&gt;%\n  count(destination_district) %&gt;%\n  top_n(5, wt = n) %&gt;%\n  pull(destination_district)\n\n# Step 3: Filter the trips data for top districts only\nflow_data &lt;- trips %&gt;%\n  filter(origin_district %in% top_5_origins & destination_district %in% top_5_destinations) %&gt;%\n  count(origin_district, destination_district) %&gt;%\n  rename(count = n)\n\n# Step 4: Create a new column to distinguish origins and destinations\nflow_data &lt;- flow_data %&gt;%\n  mutate(district_type = ifelse(origin_district %in% top_5_origins, \"Origin\", \"Destination\"))\n\n# Step 5: Plot the alluvial diagram\nggplot(flow_data, aes(axis1 = origin_district, axis2 = destination_district, y = count)) +\n  geom_alluvium(aes(fill = origin_district)) + \n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)), size = 4, color = \"black\") +\n  labs(title = \"Flow of Trips Between Top 5 Origin and Top 5 Destination Districts in Jakarta\", \n       x = \"Districts\", \n       y = \"Number of Trips\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html",
    "title": "In-class Exercise 11",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#overview",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#overview",
    "title": "In-class Exercise 11",
    "section": "1. Overview",
    "text": "1. Overview\nIn this in-class exercise, we explore new R packages to carry out the calibration of hedonic pricing model for private highrise property using MLR models.\nThe dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.\n\n1.1 Install Required Packages\nThe R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for graphics with details from statistical tests included in the information-rich plots themselves\n\nggstatplot\n\nSpatial data handling\n\nsf. sfdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\n\npacman::p_load(olsrr, ggstatsplot, sf, \n               tmap, tidyverse, gtsummary,\n               performance, see, sfdep)\n\nInstalling package into 'C:/Users/Samantha/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\nalso installing the dependency 'glue'\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4:\n  cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4/PACKAGES'\n\n\npackage 'glue' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'glue'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\Samantha\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\glue\\libs\\x64\\glue.dll\nto C:\\Users\\Samantha\\AppData\\Local\\R\\win-library\\4.4\\glue\\libs\\x64\\glue.dll:\nPermission denied\n\n\nWarning: restored 'glue'\n\n\npackage 'gtsummary' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Samantha\\AppData\\Local\\Temp\\RtmpqOZoqX\\downloaded_packages\n\n\n\ngtsummary installed\n\n\nWarning in pacman::p_load(olsrr, ggstatsplot, sf, tmap, tidyverse, gtsummary, : Failed to install/load:\ngtsummary\n\n\n\n\n1.2 Importing the Data\n\n1.2.1 Aspatial Data\nThe¬†condo_resale_2015¬†is in csv file format. The codes chunk below uses¬†read_csv()¬†function of¬†readr¬†package to import¬†condo_resale_2015¬†into R as a tibble data frame called¬†condo_resale.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly. By using glimpse(), we can observe that it returns 1,436 rows and 23 columns.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,‚Ä¶\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,‚Ä¶\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3‚Ä¶\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320‚Ä¶\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,‚Ä¶\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,‚Ä¶\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402‚Ä¶\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0‚Ä¶\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121‚Ä¶\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,‚Ä¶\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0‚Ä¶\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0‚Ä¶\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528‚Ä¶\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116‚Ä¶\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709‚Ä¶\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709‚Ä¶\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307‚Ä¶\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581‚Ä¶\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0‚Ä¶\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3‚Ä¶\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n\n\nNext,¬†summary()¬†of base R is used to display the summary statistics of¬†cond_resale¬†tibble data frame.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n1.2.2 Geospatial Data\nThe geospatial data used in this hands-on exercise is MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format consisting of URA Master Plan 2014‚Äôs planning subzone boundaries.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex10\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe geometry data in this shapefile consists of polygon features which are used to represent these geographic boundaries. We can also see that the GIS data is in the svy21 projected coordinates system.\n\n\n\n\n\n1.3 Data Wrangling\n\n1.3.1 Aspatial Data\nWe first need to convert the condo_resale dibble data frame into a sf object. We will also need to convert the projection from WSG84 into SVY21, which is the projection used in Singapore.\n\ncondo_resale_sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)\n\nNext, head() is used to list the content of condo_resale.sf object.\n\nhead(condo_resale_sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 √ó 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ‚Ñπ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#correlation-analysis---ggstatsplot-methods",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#correlation-analysis---ggstatsplot-methods",
    "title": "In-class Exercise 11",
    "section": "2. Correlation Analysis - ggstatsplot methods",
    "text": "2. Correlation Analysis - ggstatsplot methods\nCorrelation matrix is an effective graphical method for checking if there are pair independent variables with high correlation. In the code chunk below,¬†ggcorrmat()¬†of¬†ggstatsplot¬†is used to plot the correlation matrix.\n\nggcorrmat(condo_resale[, 5:23])"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#building-a-hedonic-pricing-model-by-using-multiple-linear-regression-method",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#building-a-hedonic-pricing-model-by-using-multiple-linear-regression-method",
    "title": "In-class Exercise 11",
    "section": "3. Building a Hedonic Pricing Model by using Multiple Linear Regression Method",
    "text": "3. Building a Hedonic Pricing Model by using Multiple Linear Regression Method\n\n3.1 Building the MLR regression model\nInstead of using GWR models like we did in Hands-on Exercise 10, let us use the lm() function to produce our MLR model.\n\ncondo_mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + \n                  AGE   + PROX_CBD + PROX_CHILDCARE + \n                  PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n                  PROX_HAWKER_MARKET    + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + \n                  PROX_SUPERMARKET + PROX_BUS_STOP + \n                  NO_Of_UNITS + FAMILY_FRIENDLY + \n                  FREEHOLD + LEASEHOLD_99YR, \n                data=condo_resale_sf)\nsummary(condo_mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD + \n    LEASEHOLD_99YR, data = condo_resale_sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3471036  -286903   -22426   239412 12254549 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           543071.4   136210.9   3.987 7.03e-05 ***\nAREA_SQM               12688.7      370.1  34.283  &lt; 2e-16 ***\nAGE                   -24566.0     2766.0  -8.881  &lt; 2e-16 ***\nPROX_CBD              -78122.0     6791.4 -11.503  &lt; 2e-16 ***\nPROX_CHILDCARE       -333219.0   111020.3  -3.001 0.002734 ** \nPROX_ELDERLYCARE      170950.0    42110.8   4.060 5.19e-05 ***\nPROX_URA_GROWTH_AREA   38507.6    12523.7   3.075 0.002147 ** \nPROX_HAWKER_MARKET     23801.2    29299.9   0.812 0.416739    \nPROX_KINDERGARTEN     144098.0    82738.7   1.742 0.081795 .  \nPROX_MRT             -322775.9    58528.1  -5.515 4.14e-08 ***\nPROX_PARK             564487.9    66563.0   8.481  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      186170.5    65515.2   2.842 0.004553 ** \nPROX_TOP_PRIMARY_SCH    -477.1    20598.0  -0.023 0.981525    \nPROX_SHOPPING_MALL   -207721.5    42855.5  -4.847 1.39e-06 ***\nPROX_SUPERMARKET      -48074.7    77145.3  -0.623 0.533273    \nPROX_BUS_STOP         675755.0   138552.0   4.877 1.20e-06 ***\nNO_Of_UNITS             -216.2       90.3  -2.394 0.016797 *  \nFAMILY_FRIENDLY       142128.3    47055.1   3.020 0.002569 ** \nFREEHOLD              300646.5    77296.5   3.890 0.000105 ***\nLEASEHOLD_99YR        -77137.4    77570.9  -0.994 0.320192    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1416 degrees of freedom\nMultiple R-squared:  0.652, Adjusted R-squared:  0.6474 \nF-statistic: 139.6 on 19 and 1416 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.2 Generating a linear regression report\nWe can utilise the olsrr package to generate a tidy linear regression report.\n\nols_regress(condo_mlr)\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     750537.537 \nR-Squared                    0.652       MSE                571262902261.223 \nAdj. R-Squared               0.647       Coef. Var                    43.160 \nPred R-Squared               0.637       AIC                       42971.173 \nMAE                     412117.987       SBC                       43081.835 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.515738e+15          19        7.977571e+13    139.648    0.0000 \nResidual      8.089083e+14        1416    571262902261.223                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     543071.420    136210.918                   3.987    0.000     275874.535     810268.305 \n            AREA_SQM      12688.669       370.119        0.579     34.283    0.000      11962.627      13414.710 \n                 AGE     -24566.001      2766.041       -0.166     -8.881    0.000     -29991.980     -19140.022 \n            PROX_CBD     -78121.985      6791.377       -0.267    -11.503    0.000     -91444.227     -64799.744 \n      PROX_CHILDCARE    -333219.036    111020.303       -0.087     -3.001    0.003    -551000.984    -115437.089 \n    PROX_ELDERLYCARE     170949.961     42110.748        0.083      4.060    0.000      88343.803     253556.120 \nPROX_URA_GROWTH_AREA      38507.622     12523.661        0.059      3.075    0.002      13940.700      63074.545 \n  PROX_HAWKER_MARKET      23801.197     29299.923        0.019      0.812    0.417     -33674.725      81277.120 \n   PROX_KINDERGARTEN     144097.972     82738.669        0.030      1.742    0.082     -18205.570     306401.514 \n            PROX_MRT    -322775.874     58528.079       -0.123     -5.515    0.000    -437586.937    -207964.811 \n           PROX_PARK     564487.876     66563.011        0.148      8.481    0.000     433915.162     695060.590 \n    PROX_PRIMARY_SCH     186170.524     65515.193        0.072      2.842    0.005      57653.253     314687.795 \nPROX_TOP_PRIMARY_SCH       -477.073     20597.972       -0.001     -0.023    0.982     -40882.894      39928.747 \n  PROX_SHOPPING_MALL    -207721.520     42855.500       -0.109     -4.847    0.000    -291788.613    -123654.427 \n    PROX_SUPERMARKET     -48074.679     77145.257       -0.012     -0.623    0.533    -199405.956     103256.599 \n       PROX_BUS_STOP     675755.044    138551.991        0.133      4.877    0.000     403965.817     947544.272 \n         NO_Of_UNITS       -216.180        90.302       -0.046     -2.394    0.017       -393.320        -39.040 \n     FAMILY_FRIENDLY     142128.272     47055.082        0.056      3.020    0.003      49823.107     234433.438 \n            FREEHOLD     300646.543     77296.529        0.117      3.890    0.000     149018.525     452274.561 \n      LEASEHOLD_99YR     -77137.375     77570.869       -0.030     -0.994    0.320    -229303.551      75028.801 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\nNote\n\n\n\nolsrris a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\n\n\n\n\n3.3 Conducting Variable Selection\nStepwise regression is the step-by-step iterative construction of a regression model that involves the selection of independent variables to be used in a final model. It involves adding or removing potential explanatory variables in succession and testing for statistical significance after each iteration.\n\nols_step_forward_p\nols_step_backward_p\nols_step_both_p\n\n1) Step Forward\n\ncondo_fw_mlr &lt;- ols_step_forward_p(\n  condo_mlr,\n  p_val = 0.05,\n  details = FALSE)\ncondo_fw_mlr\n\n\n                                     Stepwise Summary                                      \n-----------------------------------------------------------------------------------------\nStep    Variable                   AIC          SBC         SBIC         R2       Adj. R2 \n-----------------------------------------------------------------------------------------\n 0      Base Model              44449.068    44459.608    40371.745    0.00000    0.00000 \n 1      AREA_SQM                43587.753    43603.562    39510.883    0.45184    0.45146 \n 2      PROX_CBD                43243.523    43264.602    39167.182    0.56928    0.56868 \n 3      PROX_PARK               43177.691    43204.039    39101.331    0.58915    0.58829 \n 4      FREEHOLD                43125.474    43157.092    39049.179    0.60438    0.60327 \n 5      AGE                     43069.222    43106.109    38993.167    0.62010    0.61878 \n 6      PROX_ELDERLYCARE        43046.515    43088.672    38970.548    0.62659    0.62502 \n 7      PROX_SHOPPING_MALL      43020.990    43068.417    38945.209    0.63367    0.63188 \n 8      PROX_URA_GROWTH_AREA    43009.092    43061.788    38933.407    0.63720    0.63517 \n 9      PROX_MRT                42999.058    43057.024    38923.483    0.64023    0.63796 \n 10     PROX_BUS_STOP           42984.951    43048.186    38909.582    0.64424    0.64175 \n 11     FAMILY_FRIENDLY         42981.085    43049.590    38905.797    0.64569    0.64296 \n 12     NO_Of_UNITS             42975.246    43049.021    38900.092    0.64762    0.64465 \n 13     PROX_CHILDCARE          42971.858    43050.902    38896.812    0.64894    0.64573 \n 14     PROX_PRIMARY_SCH        42966.758    43051.072    38891.872    0.65067    0.64723 \n-----------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     751998.679 \nR-Squared                    0.651       MSE                571471422208.591 \nAdj. R-Squared               0.647       Coef. Var                    43.168 \nPred R-Squared               0.638       AIC                       42966.758 \nMAE                     414819.628       SBC                       43051.072 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n-----------------------------------------------------------------------------------------------------------------\n\n\nLet‚Äôs plot the results out too\n\nplot(condo_fw_mlr)\n\n\n\n\n\n\n\n\n2) Step Backward\n\ncondo_bw_mlr &lt;- ols_step_backward_p(\n  condo_mlr,\n  p_val = 0.05,\n  details = FALSE)\ncondo_bw_mlr\n\n\n                                     Stepwise Summary                                      \n-----------------------------------------------------------------------------------------\nStep    Variable                   AIC          SBC         SBIC         R2       Adj. R2 \n-----------------------------------------------------------------------------------------\n 0      Full Model              42971.173    43081.835    38896.546    0.65203    0.64736 \n 1      PROX_TOP_PRIMARY_SCH    42969.173    43074.565    38894.518    0.65203    0.64761 \n 2      PROX_SUPERMARKET        42967.567    43067.689    38892.873    0.65193    0.64776 \n 3      PROX_HAWKER_MARKET      42966.461    43061.315    38891.719    0.65172    0.64779 \n 4      LEASEHOLD_99YR          42965.558    43055.141    38890.764    0.65145    0.64777 \n 5      PROX_KINDERGARTEN       42966.758    43051.072    38891.872    0.65067    0.64723 \n-----------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     751998.679 \nR-Squared                    0.651       MSE                571471422208.591 \nAdj. R-Squared               0.647       Coef. Var                    43.168 \nPred R-Squared               0.638       AIC                       42966.758 \nMAE                     414819.628       SBC                       43051.072 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\nplot(condo_bw_mlr)\n\n\n\n\n\n\n\n\n3) Step Forward & Backward\n\ncondo_sb_mlr &lt;- ols_step_both_p(\n  condo_mlr,\n  p_val = 0.05,\n  details = FALSE)\ncondo_sb_mlr\n\n\n                                       Stepwise Summary                                        \n---------------------------------------------------------------------------------------------\nStep    Variable                       AIC          SBC         SBIC         R2       Adj. R2 \n---------------------------------------------------------------------------------------------\n 0      Base Model                  44449.068    44459.608    40371.745    0.00000    0.00000 \n 1      AREA_SQM (+)                43587.753    43603.562    39510.883    0.45184    0.45146 \n 2      PROX_CBD (+)                43243.523    43264.602    39167.182    0.56928    0.56868 \n 3      PROX_PARK (+)               43177.691    43204.039    39101.331    0.58915    0.58829 \n 4      FREEHOLD (+)                43125.474    43157.092    39049.179    0.60438    0.60327 \n 5      AGE (+)                     43069.222    43106.109    38993.167    0.62010    0.61878 \n 6      PROX_ELDERLYCARE (+)        43046.515    43088.672    38970.548    0.62659    0.62502 \n 7      PROX_SHOPPING_MALL (+)      43020.990    43068.417    38945.209    0.63367    0.63188 \n 8      PROX_URA_GROWTH_AREA (+)    43009.092    43061.788    38933.407    0.63720    0.63517 \n 9      PROX_MRT (+)                42999.058    43057.024    38923.483    0.64023    0.63796 \n 10     PROX_BUS_STOP (+)           42984.951    43048.186    38909.582    0.64424    0.64175 \n 11     FAMILY_FRIENDLY (+)         42981.085    43049.590    38905.797    0.64569    0.64296 \n 12     NO_Of_UNITS (+)             42975.246    43049.021    38900.092    0.64762    0.64465 \n 13     PROX_CHILDCARE (+)          42971.858    43050.902    38896.812    0.64894    0.64573 \n 14     PROX_PRIMARY_SCH (+)        42966.758    43051.072    38891.872    0.65067    0.64723 \n 15     PROX_KINDERGARTEN (+)       42965.558    43055.141    38890.764    0.65145    0.64777 \n---------------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     751161.087 \nR-Squared                    0.651       MSE                570600646491.086 \nAdj. R-Squared               0.648       Coef. Var                    43.135 \nPred R-Squared               0.638       AIC                       42965.558 \nMAE                     413583.799       SBC                       43055.141 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.514394e+15          15        1.009596e+14    176.936    0.0000 \nResidual      8.102529e+14        1420    570600646491.086                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     459826.675    114616.014                   4.012    0.000     234991.777     684661.574 \n            AREA_SQM      12720.174       368.610        0.581     34.509    0.000      11997.096      13443.252 \n            PROX_CBD     -75676.065      5816.474       -0.258    -13.011    0.000     -87085.870     -64266.259 \n           PROX_PARK     575749.528     65523.382        0.151      8.787    0.000     447216.504     704282.552 \n            FREEHOLD     360203.286     48768.851        0.140      7.386    0.000     264536.552     455870.021 \n                 AGE     -24697.719      2752.751       -0.167     -8.972    0.000     -30097.615     -19297.824 \n    PROX_ELDERLYCARE     182435.081     39910.469        0.088      4.571    0.000     104145.268     260724.893 \n  PROX_SHOPPING_MALL    -224513.955     36588.872       -0.117     -6.136    0.000    -296288.004    -152739.906 \nPROX_URA_GROWTH_AREA      40145.474     11758.824        0.062      3.414    0.001      17078.942      63212.007 \n            PROX_MRT    -311753.202     57670.032       -0.119     -5.406    0.000    -424880.814    -198625.590 \n       PROX_BUS_STOP     711858.014    135420.040        0.140      5.257    0.000     446213.188     977502.840 \n     FAMILY_FRIENDLY     144034.218     46874.683        0.057      3.073    0.002      52083.153     235985.283 \n         NO_Of_UNITS       -236.270        88.032       -0.051     -2.684    0.007       -408.956        -63.583 \n      PROX_CHILDCARE    -336118.857    108331.761       -0.088     -3.103    0.002    -548626.339    -123611.374 \n    PROX_PRIMARY_SCH     162183.897     60202.895        0.063      2.694    0.007      44087.730     280280.063 \n   PROX_KINDERGARTEN     141915.768     79726.155        0.029      1.780    0.075     -14477.927     298309.464 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\nplot(condo_sb_mlr)\n\n\n\n\n\n\n\n\n\n\n3.4 Model Selection\nIn the code chunk below,¬†compare_performance()¬†of¬†performance¬†package is used to compare the performance of the models.\n\nmetric &lt;- compare_performance(condo_mlr, \n                    condo_fw_mlr$model,\n                    condo_bw_mlr$model,\n                    condo_sb_mlr$model)\n\nIn the code chunk below,¬†gsub()¬†is used to tidy the test value in¬†Name¬†field.\n\nmetric$Name &lt;- gsub(\".*\\\\\\\\([a-zA-Z0-9_]+)\\\\\\\\, \\\\\\\\model\\\\\\\\.*\", \"\\\\1\", metric$Name)\n\nIn the code chunk below,¬†plot()¬†of see package is used to plot a radar chart to compare the performance measures of the models.\n\nplot(metric)\n\n\n\n\n\n\n\n\nWe can also visualise model paramters here.\n\nggcoefstats(condo_sb_mlr$model,\n            sort = \"ascending\")\n\nNumber of labels is greater than default palette color count.\n‚Ä¢ Select another color `palette` (and/or `package`).\n\n\n\n\n\n\n\n\n\n\n\n3.5 Visualise Results\n\n3.5.1 Multicollinearity Test\nWe might want to further check for multicollinearity\n\n\nMulticollinearity is a statistical concept where two or more independent variables in a regression model are correlated.\nA statistical technique called the variance inflation factor (VIF) can detect and measure the amount of collinearity in a multiple regression model.\nVIF measures how much the variance of the estimated regression coefficients is inflated as compared to when the predictor variables are not linearly related.\nInterpretation of VIF\n\n&lt; 5: low multicollinearity\n5-10: moderate multicollinearity\n&gt; 10: strong multicollineariy\n\n\n\n\ncheck_collinearity(condo_sb_mlr$model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                 Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n             AREA_SQM 1.15 [1.10, 1.24]         1.07      0.87     [0.81, 0.91]\n             PROX_CBD 1.60 [1.50, 1.73]         1.27      0.62     [0.58, 0.67]\n            PROX_PARK 1.21 [1.15, 1.30]         1.10      0.83     [0.77, 0.87]\n             FREEHOLD 1.46 [1.37, 1.57]         1.21      0.68     [0.64, 0.73]\n                  AGE 1.41 [1.33, 1.52]         1.19      0.71     [0.66, 0.75]\n     PROX_ELDERLYCARE 1.52 [1.42, 1.63]         1.23      0.66     [0.61, 0.70]\n   PROX_SHOPPING_MALL 1.49 [1.40, 1.60]         1.22      0.67     [0.62, 0.72]\n PROX_URA_GROWTH_AREA 1.33 [1.26, 1.43]         1.16      0.75     [0.70, 0.79]\n             PROX_MRT 1.96 [1.83, 2.13]         1.40      0.51     [0.47, 0.55]\n        PROX_BUS_STOP 2.89 [2.66, 3.15]         1.70      0.35     [0.32, 0.38]\n      FAMILY_FRIENDLY 1.38 [1.30, 1.48]         1.18      0.72     [0.67, 0.77]\n          NO_Of_UNITS 1.45 [1.37, 1.56]         1.21      0.69     [0.64, 0.73]\n       PROX_CHILDCARE 3.29 [3.02, 3.59]         1.81      0.30     [0.28, 0.33]\n     PROX_PRIMARY_SCH 2.21 [2.05, 2.40]         1.49      0.45     [0.42, 0.49]\n    PROX_KINDERGARTEN 1.11 [1.06, 1.20]         1.05      0.90     [0.84, 0.94]\n\n\n\nplot(check_collinearity(condo_sb_mlr$model)) +\n  theme(axis.text.x = element_text(\n    angle = 45, hjust = 1))\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 Linearity Assumption Test\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the check_model() of performance package is used to perform linearity assumption test.\n\nout &lt;- plot(check_model(condo_sb_mlr$model, \n                        panel = FALSE))\n\nFor confidence bands, please install `qqplotr`.\n\nout[[2]]\n\n\n\n\n\n\n\n\n\n\n\nFigure above reveals that most of the data points are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear\n\n\n\n\n3.5.3 Normality Assumption Test\nMultiple Linear Regression analysis assumes that¬†the residuals (the differences between observed and predicted values) are normally distributed. This assumption can be assessed by using statistical graphics, or through statistical tests such as the Kolmogorov-Smirnov test.\nCode chunk below uses¬†check_normality¬†of¬†performance¬†package to perform normality assumption test.\n\nplot(check_normality(condo_sb_mlr$model))\n\nFor confidence bands, please install `qqplotr`.\n\n\n\n\n\n\n\n\n\nFigure above reveals that the residual of the multiple linear regression model (i.e.¬†condo.mlr1) is resemble normal distribution.\n\n\n3.5.4 Checking for Outliers\n\noutliers &lt;- check_outliers(condo_sb_mlr$model,\n                           method = \"cook\")\noutliers\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (1).\n- For variable: (Whole model)\n\n\n\nplot(check_outliers(condo_sb_mlr$model,\n                           method = \"cook\"))\n\n\n\n\n\n\n\n\n\n\n3.5.5 Spatial Non-stationary Assumption\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\n\nHo: The residuals are randomly distributed (also known as spatial stationary)\nH1: The residuals are spatially non-stationary\n\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr_output &lt;- as.data.frame(condo_fw_mlr$model$residuals) %&gt;%\n  rename(`FW_MLR_RES` = `condo_fw_mlr$model$residuals`)\n\nNext, we will join the newly created data frame with¬†condo_resale_sf¬†object.\n\ncondo_resale_sf &lt;- cbind(condo_resale_sf, \n                        mlr_output$FW_MLR_RES) %&gt;%\n  rename(`MLR_RES` = `mlr_output.FW_MLR_RES`)\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map. The code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale_sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") \n\nWarning: The shape mpsz is invalid (after reprojection). See sf::st_is_valid\n\n\nVariable(s) \"MLR_RES\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\n\n\n3.5.6 Spatial stationary test\nTo proof that our observation is indeed true, the Moran‚Äôs I test will be performed\nFirst, we will compute the distance-based weight matrix by using¬†dnearneigh()¬†function of¬†spdep.\n\ncondo_resale_sf &lt;- condo_resale_sf %&gt;%\n  mutate(nb = st_knn(geometry, k=6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\nNext,¬†global_moran_perm()¬†of sfdep is used to perform global Moran permutation test.\n\nglobal_moran_perm(condo_resale_sf$MLR_RES, \n                  condo_resale_sf$nb, \n                  condo_resale_sf$wt, \n                  alternative = \"two.sided\", \n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.32254, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe Global Moran‚Äôs I test for residual spatial autocorrelation shows that it‚Äôs p-value is less than the alpha value of 0.05. Hence, we reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.25586 which is greater than 0, we can infer that the residuals resemble cluster distribution."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html",
    "href": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html",
    "title": "Hands-on Exercise 11",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#overview",
    "title": "Hands-on Exercise 11",
    "section": "1. Overview",
    "text": "1. Overview\nTo begin with, geographically weighted regression (GWR)¬†is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable).\nIn this hands-on exercise, I will be building¬†hedonic pricing¬†models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.\n\n1.1 Installing Required Packages\nThe R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, sfdep)\n\nInstalling package into 'C:/Users/Samantha/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\nalso installing the dependency 'glue'\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4:\n  cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4/PACKAGES'\n\n\npackage 'glue' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'glue'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\Samantha\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\glue\\libs\\x64\\glue.dll\nto C:\\Users\\Samantha\\AppData\\Local\\R\\win-library\\4.4\\glue\\libs\\x64\\glue.dll:\nPermission denied\n\n\nWarning: restored 'glue'\n\n\npackage 'gtsummary' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Samantha\\AppData\\Local\\Temp\\RtmpyqfMII\\downloaded_packages\n\n\n\ngtsummary installed\n\n\nWarning in pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, : Failed to install/load:\ngtsummary\n\n\n\n\n1.2 Importing the Datasets\n\n1.2.1 Aspatial Data\nThe¬†condo_resale_2015¬†is in csv file format. The codes chunk below uses¬†read_csv()¬†function of¬†readr¬†package to import¬†condo_resale_2015¬†into R as a tibble data frame called¬†condo_resale.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nRows: 1436 Columns: 23\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (23): LATITUDE, LONGITUDE, POSTCODE, SELLING_PRICE, AREA_SQM, AGE, PROX_...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly. By using glimpse(), we can observe that it returns 1,436 rows and 23 columns.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,‚Ä¶\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,‚Ä¶\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3‚Ä¶\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320‚Ä¶\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,‚Ä¶\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,‚Ä¶\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402‚Ä¶\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0‚Ä¶\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121‚Ä¶\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,‚Ä¶\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0‚Ä¶\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0‚Ä¶\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528‚Ä¶\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116‚Ä¶\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709‚Ä¶\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709‚Ä¶\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307‚Ä¶\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581‚Ä¶\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0‚Ä¶\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3‚Ä¶\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n\n\nNext,¬†summary()¬†of base R is used to display the summary statistics of¬†cond_resale¬†tibble data frame.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n1.2.2 Geospatial Data\nThe geospatial data used in this hands-on exercise is MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format consisting of URA Master Plan 2014‚Äôs planning subzone boundaries.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex11\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe geometry data in this shapefile consists of polygon features which are used to represent these geographic boundaries. We can also see that the GIS data is in the svy21 projected coordinates system.\n\n\n\n\n\n1.3 Data Wrangling\n\n1.3.1 Aspatial Data\nWe first need to convert the condo_resale dibble data frame into a sf object. We will also need to convert the projection from WSG84 into SVY21, which is the projection used in Singapore.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)\n\nNext, head() is used to list the content of condo_resale.sf object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 √ó 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ‚Ñπ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\n\n\n\n\n\n\nObservations\n\n\n\nNotice that st_transform() of sf package has successfully helped us to convert the geometric coordinates from wgs84 (i.e.¬†crs:4326) to svy21 (i.e.¬†crs=3414).\n\n\n\n\n1.3.2 Geospatial Data\nSimilar to the aspatial data, we need to convert the projection from WSG84 into SVY21.\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\n\nAfter transforming the projection metadata, we can indeed see that the projection has been transformed to 3414¬†by using¬†st_crs()¬†of¬†sf¬†package.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNext, we can reveal the extent of¬†mpsz_svy21¬†by using¬†st_bbox()¬†of sf package.\n\nst_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 11",
    "section": "2. Exploratory Data Analysis (EDA)",
    "text": "2. Exploratory Data Analysis (EDA)\n\n2.1 Using Statistical Graphics\nWe will first plot the distribution of the selling price of the condominiums by using the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can observe that the distribution of the selling price is right-skewed, with a long tail to the right. This skewed distribution is typical of real estate prices, where most of the properties are sold at a lower price, with a few sold at a much higher price.\n\n\nHowever, working with the raw selling price can be problematic, especially when the distribution is skewed. We can transform the selling price using the natural logarithm to make the distribution more symmetric.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nNow let‚Äôs plot the distribution of the log-transformed selling price, i.e.¬†LOG_SELLING_PRICE.\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the distrbiution above, we see that the distribution of the log-transformed selling price is more symmetric compared to the raw selling price. This transformation will be useful when we calibrate the hedonic pricing model.\n\n\n\n\n2.2 Multiple Histogram Plots distribution of variables\nWe will now draw a small multiples of histograms to visualise the distribution of the independent variables in the hedonic pricing model. The code below will create 12 histograms. Then, ggarrange() is used to arrange the histograms in a 3x4 grid.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n\n\n\n\n2.3 Drawing Statistical Point Map\nLastly, we will draw a statistical point map to visualise the distribution of the log-transformed selling price of the condominiums in 2015. The code below will create a statistical point map using the tmap package.\n\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\n\n\nmpsz_svy21 &lt;- st_make_valid(mpsz_svy21)\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21) +\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style = \"quantile\") +\n  tm_view(set.zoom.limits = c(11, 14))\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#hedonic-pricing-model",
    "href": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#hedonic-pricing-model",
    "title": "Hands-on Exercise 11",
    "section": "3. Hedonic Pricing Model",
    "text": "3. Hedonic Pricing Model\n\n3.1 Simple Linear Regression\nWe will first set the baseline model using simple linear regression (SLR). The SLR model will be used to estimate the relationship between the log-transformed selling price and the area of the condominium.\nThe code below will calibrate the SLR model for SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nThe functions summary() and anova() are used to obtain the summary statistics and the ANOVA table of the SLR model, respectively.\n\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the output report above, SELLING_PRICE can be explained using the formula:\n\\[ y = -258121.1 + 14719x1 \\]\nThe R-squared value of 0.4518 indicates that the area of the condominium can explain 45% of the variation in the selling price.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\n\n\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot‚Äôs geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can tell that there are a few outliers with relatively higher selling prices in the scatterplot.\n\n\n3.2 Multiple Linear Regression\n\n3.2.1 Visualizing the relationships of the independent variables\nBefore we begin calibrating the multiple linear regression (MLR) model, we will first visualise the relationships between the independent variables to identify any multicollinearity issues. The code below will create a correlation matrix of the independent variables.\nTo identify the pattern in the matrix, we also need to consider the order of the variables. There are four methods:\n\nThe ‚ÄúAOE‚Äù method is used to order the variables based on the average of the absolute off-diagonal correlations.\nThe ‚ÄúFPC‚Äù method is used to order the variables based on the first principal component.\nThe ‚Äúhclust‚Äù method is used to order the variables based on the hierarchical clustering.\nThe ‚Äúalphabet‚Äù method is used to order the variables based on the alphabetical order.\n\nWe will use the ‚ÄúAOE‚Äù method in this example.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom the matrix above, we can clearly see that Freehold is highly correlated to LEASE_99YEAR. This is expected as the two variables are related to the tenure of the property. We will need to remove one of the variables to avoid multicollinearity issues. In this case, we will remove LEASE_99YEAR from the hedonic pricing model.\n\n\n\n\n\n3.3 Calibrating the Multiple Linear Regression Model\nThe code chunk below using¬†lm()¬†to calibrate the multiple linear regression model.\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.4 Preparing Publication Quality Table: olsrr method\nWe can infer that not all the independent variables are significant in explaining the variation in the selling price. To identify the significant variables, we can use the olsrr package to obtain the summary statistics of the MLR model. The olsrr package provides a comprehensive summary statistics of the MLR model, including the ANOVA table, the coefficients, the R-squared value, and the p-values of the estimates.\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     751998.679 \nR-Squared                    0.651       MSE                571471422208.591 \nAdj. R-Squared               0.647       Coef. Var                    43.168 \nPred R-Squared               0.638       AIC                       42966.758 \nMAE                     414819.628       SBC                       43051.072 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\n3.5 Preparing Publication Quality Table: gtsummary method\nWe can also use the gtsummary package to obtain the summary statistics of the MLR model in an elegant and flexible way.\nIn the code chunk below,¬†tbl_regression()¬†is used to create a well formatted regression report.\n\ntbl_regression(condo.mlr1, intercept = TRUE)\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using¬†add_glance_table()¬†or adding as a table source note by using¬†add_glance_source_note()¬†as shown in the code chunk below.\n\nlibrary(dplyr)\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %&gt;% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n3.5.1 Checking for multicolinearity\nWe can also use the olsrr package to check for multicollinearity issues in the MLR model. The olsrr package provides the variance inflation factor (VIF) and the tolerance of the independent variables. The VIF measures the extent of multicollinearity in the model, while the tolerance measures the proportion of the variance of an independent variable that is not explained by the other independent variables.\nIt provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nIn the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n3.5.2 Test for Non-Linearity\nIt is also important to test for non-linearity in the model. The¬†ols_plot_resid_fit()¬†of¬†olsrr package is used to test for non-linearity in the model.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\n\n\n\n\nFrom the figure above, we can tell that the residuals are randomly scattered around the zero line. This indicates that there are no signs of non-linearity in the model.\n\n1) Test for Normality Assumption\nLastly, we will test for the normality assumption of the residuals. The¬†ols_plot_resid_hist()¬†of¬†olsrr package is used to test for the normality assumption of the residuals.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\n\n\n\n\nThe figure above shows that the residuals are normally distributed, which is a key assumption of the multiple linear regression model.\nAlso, if you want to use a formal statistical test method, you can use the¬†ols_test_normality()¬†of¬†olsrr package to test for the normality assumption of the residuals.\n\nols_test_normality(condo.mlr1)\n\nWarning in ks.test.default(y, \"pnorm\", mean(y), sd(y)): ties should not be\npresent for the one-sample Kolmogorov-Smirnov test\n\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n2) Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nVariable(s) \"MLR_RES\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, the Moran‚Äôs I test will be performed\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe summary table above reveals that the p-value of the Moran‚Äôs I test is way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\nSince the Global Moran‚Äôs I = 0.1424418 is greater than 0, we can infer that there is sign of positive spatial autocorrelation."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "Hands-on_Ex/Hands-on_Ex11/Hands-on_Ex11.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Hands-on Exercise 11",
    "section": "4. Building Hedonic Pricing Models using GWmodel",
    "text": "4. Building Hedonic Pricing Models using GWmodel\n\n4.1 Building Fixed Bandwidth GWR Model\n\n4.1.1 Computing the Bandwidth\nThe first step in calibrating the GWR model is to compute the bandwidth. The bandwidth is a critical parameter in the GWR model as it determines the number of observations that will be used to calibrate the local regression model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are several methods to compute the bandwidth, they are: CV cross-validation approach and AIC corrected (AICc) approach. In this example, we will use the cross-validation (CV) method to compute the bandwidth. The CV method is a robust method to compute the bandwidth as it minimizes the prediction error of the GWR model.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3405 metres. This means that the GWR model will use the observations within 971.3405 metres to calibrate the local regression model. Metres is used as the unit of measurement because the data is projected in SVY21.\n\n\n4.1.2 Calibrating the GWR Model\nThe next step is to calibrate the GWR model using the recommended bandwidth. The code chunk below will calibrate the GWR model using the recommended bandwidth.\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\nThe output is saved in a list of class ‚Äúgwrm‚Äù. The code below can be used to display the model output.\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-30 09:52:58.25927 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2024-10-30 09:53:01.900435 \n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the global multiple linear regression model of 42967.1. This indicates that the GWR model is a better model to explain the variation in the selling price.\n\n\n\n4.2 Building Adaptive Bandwidth GWR Model\n\n4.2.1 Computing the Bandwidth\nUnlike the fixed bandwidth GWR model, the adaptive bandwidth GWR model does not require the bandwidth to be computed. Instead, the bandwidth is computed for each observation based on the number of observations within a certain distance. The code chunk below will calibrate the adaptive bandwidth GWR model. To do this, we need to set the adaptive argument to TRUE.\n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\n4.2.2 Calibrating the GWR Model\nThe code chunk below will calibrate the GWR model using the recommended bandwidth.\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\nSimilarly, we can also display the model output.\n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-30 09:53:29.052649 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-10-30 09:53:33.352695 \n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\n4.3 Visualizing GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local colinearity. In the presence of strong local colinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its ‚Äúdata‚Äù slot in an object called SDF of the output list.\n\n\n4.4 Converting SDF into sf data.frame\nTo visualize the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\n\ncondo_resale.sf.adaptive.svy21 &lt;- st_transform(condo_resale.sf.adaptive, 3414)\ncondo_resale.sf.adaptive.svy21  \n\nSimple feature collection with 1436 features and 51 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14940.85 ymin: 24765.67 xmax: 43352.45 ymax: 48382.81\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n    Intercept  AREA_SQM        AGE  PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n1   2050011.7  9561.892  -9514.634 -120681.9      319266.92       -393417.79\n2   1633128.2 16576.853 -58185.479 -149434.2      441102.18        325188.74\n3   3433608.2 13091.861 -26707.386 -259397.8     -120116.82        535855.81\n4    234358.9 20730.601 -93308.988 2426853.7      480825.28        314783.72\n5   2285804.9  6722.836 -17608.018 -316835.5       90764.78       -137384.61\n6  -3568877.4  6039.581 -26535.592  327306.1     -152531.19       -700392.85\n7  -2874842.4 16843.575 -59166.727 -983577.2     -177810.50       -122384.02\n8   2038086.0  6905.135 -17681.897 -285076.6       70259.40        -96012.78\n9   1718478.4  9580.703 -14401.128  105803.4     -657698.02       -123276.00\n10  3457054.0 14072.011 -31579.884 -234895.4       79961.45        548581.04\n   PROX_URA_GROWTH_AREA    PROX_MRT  PROX_PARK PROX_PRIMARY_SCH\n1            -159980.20  -299742.96 -172104.47        242668.03\n2            -142290.39 -2510522.23  523379.72       1106830.66\n3            -253621.21  -936853.28  209099.85        571462.33\n4           -2679297.89 -2039479.50 -759153.26       3127477.21\n5             303714.81   -44567.05  -10284.62         30413.56\n6             -28051.25   733566.47 1511488.92        320878.23\n7            1397676.38 -2745430.34  710114.74       1786570.95\n8             269368.71   -14552.99   73533.34         53359.73\n9            -361974.72  -476785.32 -132067.59        -40128.92\n10           -150024.38 -1503835.53  574155.47        108996.67\n   PROX_SHOPPING_MALL PROX_BUS_STOP  NO_Of_UNITS FAMILY_FRIENDLY  FREEHOLD\n1          300881.390     1210615.4  104.8290640       -9075.370  303955.6\n2          -87693.378     1843587.2 -288.3441183      310074.664  396221.3\n3         -126732.712     1411924.9   -9.5532945        5949.746  168821.7\n4          -29593.342     7225577.5 -161.3551620     1556178.531 1212515.6\n5           -7490.586      677577.0   42.2659674       58986.951  328175.2\n6          258583.881     1086012.6 -214.3671271      201992.641  471873.1\n7         -384251.210     5094060.5   -0.9212521      359659.512  408871.9\n8          -39634.902      735767.1   30.1741069       55602.506  347075.0\n9          276718.757     2815772.4  675.1615559      -30453.297  503872.8\n10        -454726.822     2123557.0  -21.3044311     -100935.586  213324.6\n         y    yhat    residual CV_Score Stud_residual Intercept_SE AREA_SQM_SE\n1  3000000 2886532   113468.16        0    0.38207013     516105.5    823.2860\n2  3880000 3466801   413198.52        0    1.01433140     488083.5    825.2380\n3  3325000 3616527  -291527.20        0   -0.83780678     963711.4    988.2240\n4  4250000 5435482 -1185481.63        0   -2.84614670     444185.5    617.4007\n5  1400000 1388166    11834.26        0    0.03404453    2119620.6   1376.2778\n6  1320000 1516702  -196701.94        0   -0.72065800   28572883.7   2348.0091\n7  3410000 3266881   143118.77        0    0.41291992     679546.6    893.5893\n8  1420000 1431955   -11955.27        0   -0.03033109    2217773.1   1415.2604\n9  2025000 1832799   192200.83        0    0.52018109     814281.8    943.8434\n10 2550000 2223364   326635.53        0    1.10559735    2410252.0   1271.4073\n      AGE_SE PROX_CBD_SE PROX_CHILDCARE_SE PROX_ELDERLYCARE_SE\n1   5889.782    37411.22          319111.1           120633.34\n2   6226.916    23615.06          299705.3            84546.69\n3   6510.236    56103.77          349128.5           129687.07\n4   6010.511   469337.41          304965.2           127150.69\n5   8180.361   410644.47          698720.6           327371.55\n6  14601.909  5272846.47         1141599.8          1653002.19\n7   8970.629   346164.20          530101.1           148598.71\n8   8661.309   438035.69          742532.8           399221.05\n9  11791.208    89148.35          704630.7           329683.30\n10  9941.980   173532.77          500976.2           281876.74\n   PROX_URA_GROWTH_AREA_SE PROX_MRT_SE PROX_PARK_SE PROX_PRIMARY_SCH_SE\n1                 56207.39    185181.3     205499.6            152400.7\n2                 76956.50    281133.9     229358.7            165150.7\n3                 95774.60    275483.7     314124.3            196662.6\n4                470762.12    279877.1     227249.4            240878.9\n5                474339.56    363830.0     364580.9            249087.7\n6               5496627.21    730453.2    1741712.0            683265.5\n7                371692.97    375511.9     297400.9            344602.8\n8                517977.91    423155.4     440984.4            261251.2\n9                153436.22    285325.4     304998.4            278258.5\n10               239182.57    571355.7     599131.8            331284.8\n   PROX_SHOPPING_MALL_SE PROX_BUS_STOP_SE NO_Of_UNITS_SE FAMILY_FRIENDLY_SE\n1               109268.8         600668.6       218.1258           131474.7\n2                98906.8         410222.1       208.9410           114989.1\n3               119913.3         464156.7       210.9828           146607.2\n4               177104.1         562810.8       361.7767           108726.6\n5               301032.9         740922.4       299.5034           160663.7\n6              2931208.6        1418333.3       602.5571           331727.0\n7               249969.5         821236.4       532.1978           129241.2\n8               351634.0         775038.4       338.6777           171895.1\n9               289872.7         850095.5       439.9037           220223.4\n10              265529.7         631399.2       259.0169           189125.5\n   FREEHOLD_SE Intercept_TV AREA_SQM_TV     AGE_TV PROX_CBD_TV\n1     115954.0    3.9720784   11.614302  -1.615447 -3.22582173\n2     130110.0    3.3460017   20.087361  -9.344188 -6.32792021\n3     141031.5    3.5629010   13.247868  -4.102368 -4.62353528\n4     138239.1    0.5276150   33.577223 -15.524302  5.17080808\n5     210641.1    1.0784029    4.884795  -2.152474 -0.77155660\n6     374347.3   -0.1249043    2.572214  -1.817269  0.06207388\n7     182216.9   -4.2305303   18.849348  -6.595605 -2.84136028\n8     216649.4    0.9189786    4.879056  -2.041481 -0.65080678\n9     220473.7    2.1104224   10.150733  -1.221345  1.18682383\n10    206346.2    1.4343123   11.068059  -3.176418 -1.35360852\n   PROX_CHILDCARE_TV PROX_ELDERLYCARE_TV PROX_URA_GROWTH_AREA_TV PROX_MRT_TV\n1         1.00048819          -3.2612693            -2.846248368 -1.61864578\n2         1.47178634           3.8462625            -1.848971738 -8.92998600\n3        -0.34404755           4.1319138            -2.648105057 -3.40075727\n4         1.57665606           2.4756745            -5.691404992 -7.28705261\n5         0.12990138          -0.4196596             0.640289855 -0.12249416\n6        -0.13361179          -0.4237096            -0.005103357  1.00426206\n7        -0.33542751          -0.8235874             3.760298131 -7.31116712\n8         0.09462126          -0.2405003             0.520038994 -0.03439159\n9        -0.93339393          -0.3739225            -2.359121712 -1.67102293\n10        0.15961128           1.9461735            -0.627237944 -2.63204802\n   PROX_PARK_TV PROX_PRIMARY_SCH_TV PROX_SHOPPING_MALL_TV PROX_BUS_STOP_TV\n1   -0.83749312           1.5923022            2.75358842        2.0154464\n2    2.28192684           6.7019454           -0.88662640        4.4941192\n3    0.66565951           2.9058009           -1.05686949        3.0419145\n4   -3.34061770          12.9836105           -0.16709578       12.8383775\n5   -0.02820944           0.1220998           -0.02488294        0.9145046\n6    0.86781794           0.4696245            0.08821750        0.7656963\n7    2.38773567           5.1844351           -1.53719231        6.2029165\n8    0.16674816           0.2042469           -0.11271635        0.9493299\n9   -0.43301073          -0.1442145            0.95462153        3.3123012\n10   0.95831249           0.3290120           -1.71252687        3.3632555\n   NO_Of_UNITS_TV FAMILY_FRIENDLY_TV FREEHOLD_TV  Local_R2\n1     0.480589953        -0.06902748    2.621347 0.8846744\n2    -1.380026395         2.69655779    3.045280 0.8899773\n3    -0.045279967         0.04058290    1.197050 0.8947007\n4    -0.446007570        14.31276425    8.771149 0.9073605\n5     0.141120178         0.36714544    1.557983 0.9510057\n6    -0.355762335         0.60891234    1.260522 0.9247586\n7    -0.001731033         2.78285441    2.243875 0.8310458\n8     0.089093858         0.32346758    1.602012 0.9463936\n9     1.534793921        -0.13828365    2.285410 0.8380365\n10   -0.082251138        -0.53369623    1.033819 0.9080753\n                    geometry\n1  POINT (22085.12 29951.54)\n2   POINT (25656.84 34546.2)\n3   POINT (23963.99 32890.8)\n4  POINT (27044.28 32319.77)\n5  POINT (41042.56 33743.64)\n6   POINT (39717.04 32943.1)\n7   POINT (28419.1 33513.37)\n8  POINT (40763.57 33879.61)\n9  POINT (23595.63 28884.78)\n10 POINT (24586.56 33194.31)\n\ngwr.adaptive.output &lt;- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive &lt;- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472‚Ä¶\n$ SELLING_PRICE           &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1‚Ä¶\n$ AREA_SQM                &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 1‚Ä¶\n$ AGE                     &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,‚Ä¶\n$ PROX_CBD                &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783‚Ä¶\n$ PROX_CHILDCARE          &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543‚Ä¶\n$ PROX_ELDERLYCARE        &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.‚Ä¶\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.4106‚Ä¶\n$ PROX_HAWKER_MARKET      &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969‚Ä¶\n$ PROX_KINDERGARTEN       &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076‚Ä¶\n$ PROX_MRT                &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.‚Ä¶\n$ PROX_PARK               &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.‚Ä¶\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.‚Ä¶\n$ PROX_TOP_PRIMARY_SCH    &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.‚Ä¶\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.‚Ä¶\n$ PROX_SUPERMARKET        &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.‚Ä¶\n$ PROX_BUS_STOP           &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340‚Ä¶\n$ NO_Of_UNITS             &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34‚Ä¶\n$ FAMILY_FRIENDLY         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0‚Ä¶\n$ FREEHOLD                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1‚Ä¶\n$ LEASEHOLD_99YR          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ LOG_SELLING_PRICE       &lt;dbl&gt; 14.91412, 15.17135, 15.01698, 15.26243, 14.151‚Ä¶\n$ MLR_RES                 &lt;dbl&gt; -1489099.55, 415494.57, 194129.69, 1088992.71,‚Ä¶\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,‚Ä¶\n$ AREA_SQM.1              &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672‚Ä¶\n$ AGE.1                   &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,‚Ä¶\n$ PROX_CBD.1              &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66‚Ä¶\n$ PROX_CHILDCARE.1        &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28‚Ä¶\n$ PROX_ELDERLYCARE.1      &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72‚Ä¶\n$ PROX_URA_GROWTH_AREA.1  &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929‚Ä¶\n$ PROX_MRT.1              &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.‚Ä¶\n$ PROX_PARK.1             &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, ‚Ä¶\n$ PROX_PRIMARY_SCH.1      &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, ‚Ä¶\n$ PROX_SHOPPING_MALL.1    &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34‚Ä¶\n$ PROX_BUS_STOP.1         &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51‚Ä¶\n$ NO_Of_UNITS.1           &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35‚Ä¶\n$ FAMILY_FRIENDLY.1       &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, ‚Ä¶\n$ FREEHOLD.1              &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3‚Ä¶\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1‚Ä¶\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13‚Ä¶\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,‚Ä¶\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146‚Ä¶\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962‚Ä¶\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2‚Ä¶\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3‚Ä¶\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064‚Ä¶\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720‚Ä¶\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327‚Ä¶\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433‚Ä¶\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830‚Ä¶\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580‚Ä¶\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087‚Ä¶\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.‚Ä¶\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922‚Ä¶\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50‚Ä¶\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16‚Ä¶\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641‚Ä¶\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.‚Ä¶\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.‚Ä¶\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301‚Ä¶\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080‚Ä¶\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766‚Ä¶\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745‚Ä¶\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6‚Ä¶\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870‚Ä¶\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617‚Ä¶\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104‚Ä¶\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16‚Ä¶\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0‚Ä¶\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44‚Ä¶\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764‚Ä¶\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.‚Ä¶\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.‚Ä¶\n$ coords.x1               &lt;dbl&gt; 22085.12, 25656.84, 23963.99, 27044.28, 41042.‚Ä¶\n$ coords.x2               &lt;dbl&gt; 29951.54, 34546.20, 32890.80, 32319.77, 33743.‚Ä¶\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.‚Ä¶\n\n\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n4.5 Visualizing local R2\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\n\n\n4.6 Visualizing coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n\nLastly, we can also visualize the GWR output by URA planning region. The code chunk below will visualize the local R2 of the GWR output for the Central region.\n\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#categorising-point-of-interests-pois-data",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#categorising-point-of-interests-pois-data",
    "title": "Take-home Exercise 3",
    "section": "3.1 Categorising Point of Interests (POIs) Data",
    "text": "3.1 Categorising Point of Interests (POIs) Data\nI assisted my team mate, Jia Le, with handling some of the data cleaning process of the project. This includes re-categorising the 205 POIs identified for each district into its correct category as the initial categories were previously classified wrongly.\nThere are a total of 9 unique categories that needed some re-categorisation.\n\nFacilities_Services\nEssentials\nOffices_Business\nCultural_Attractions\nRestaurants_Food\nRecreation_Entertainment\nOthers\nShops\nTourism_Spots\n\nFor instance, the ‚Äòtaxi‚Äô POI was initially classified as ‚ÄòOthers‚Äô instead of something more related like ‚ÄòFacilities_Services‚Äô."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#prepare-the-augmented-trips-dataset",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#prepare-the-augmented-trips-dataset",
    "title": "Take-home Exercise 3",
    "section": "3.4 Prepare the Augmented Trips Dataset",
    "text": "3.4 Prepare the Augmented Trips Dataset\nI noticed that we have records where destination districts are found outside of Jakarta‚Äôs boundary. For the purpose of analysing Grab trends within Jakarta, I will filter these trip records out.\n\ntrips &lt;- trips %&gt;%\n  filter(destination_district != \"outside of jakarta\" |\n         origin_district != \"outside of jakarta\")\n\n# Inspect\nhead(trips)\n\n# A tibble: 6 √ó 32\n  trj_id driving_mode origin_time         destination_time   \n  &lt;chr&gt;  &lt;chr&gt;        &lt;dttm&gt;              &lt;dttm&gt;             \n1 1      car          2019-04-11 14:17:35 2019-04-11 14:36:24\n2 10000  motorcycle   2019-04-16 00:51:24 2019-04-16 01:00:51\n3 10002  motorcycle   2019-04-08 05:55:41 2019-04-08 06:16:59\n4 10003  car          2019-04-19 10:52:34 2019-04-19 11:24:44\n5 10007  car          2019-04-11 07:05:00 2019-04-11 07:28:56\n6 10012  car          2019-04-20 02:38:54 2019-04-20 04:05:16\n# ‚Ñπ 28 more variables: total_duration_minutes &lt;dbl&gt;, total_distance_km &lt;dbl&gt;,\n#   average_speed_kmh &lt;dbl&gt;, origin_rawlat &lt;dbl&gt;, origin_rawlng &lt;dbl&gt;,\n#   destination_rawlat &lt;dbl&gt;, destination_rawlng &lt;dbl&gt;, origin_lat &lt;dbl&gt;,\n#   origin_lng &lt;dbl&gt;, destination_lat &lt;dbl&gt;, destination_lng &lt;dbl&gt;,\n#   origin_province &lt;chr&gt;, origin_city &lt;chr&gt;, origin_district &lt;chr&gt;,\n#   destination_province &lt;chr&gt;, destination_city &lt;chr&gt;,\n#   destination_district &lt;chr&gt;, origin_datetime &lt;dttm&gt;, ‚Ä¶\n\n\nNext, I will leverage the trips tibble dataframe to group all trips by destination_district and summarise them by number of trips, average duration (minutes) and average distance travelled (km).\n\ntrips_dest &lt;- trips %&gt;%\n  group_by(destination_district) %&gt;%\n  summarise(\n    num_of_trips = n(), \n    avg_duration_minutes = mean(total_duration_minutes, na.rm = TRUE), \n    avg_distance_km = mean(total_distance_km, na.rm = TRUE)  \n  ) %&gt;%\n  rename(district = destination_district) %&gt;%\n  mutate(district = tolower(district))\n\ntrips_origin &lt;- trips %&gt;%\n  group_by(origin_district) %&gt;%\n  summarise(\n    num_of_trips = n(), \n    avg_duration_minutes = mean(total_duration_minutes, na.rm = TRUE), \n    avg_distance_km = mean(total_distance_km, na.rm = TRUE)\n  ) %&gt;%\n  rename(district = origin_district) %&gt;%\n  mutate(district = tolower(district))\n\nI will also join the pois_num dataframe to trips_dest and trips_origin by the district column so we can append the total number of POIs for each destination district.\n\ntrips_dest_pois &lt;- trips_dest %&gt;%\n  inner_join(pois_num, by = \"district\")\n\ntrips_origin_pois &lt;- trips_origin %&gt;%\n  inner_join(pois_num, by = \"district\")\n\nLet‚Äôs also include the population data from the population dataframe we imported into R.\n\n# For the destination\ndistrict_dest &lt;- trips_dest_pois %&gt;%\n  left_join(population %&gt;% mutate(district = tolower(district)), \n            by = \"district\")\n\ndistrict_dest\n\n# A tibble: 44 √ó 6\n   district        num_of_trips avg_duration_minutes avg_distance_km num_of_pois\n   &lt;chr&gt;                  &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;       &lt;int&gt;\n 1 cakung                   844                 21.5            6.40          56\n 2 cempaka putih            392                 19.1            6.13          38\n 3 cengkareng              1120                 19.9            6.20         298\n 4 cilandak                 905                 20.1            5.78         242\n 5 cilincing                277                 24.4            7.24         119\n 6 cipayung                 552                 22.5            7.08          56\n 7 ciracas                  546                 22.6            7.03         126\n 8 danau sunter              17                 17.3            5.21           1\n 9 danau sunter d‚Ä¶           33                 16.9            5.00           3\n10 duren sawit              877                 21.1            6.32         173\n# ‚Ñπ 34 more rows\n# ‚Ñπ 1 more variable: population_count &lt;dbl&gt;\n\n\n\n# For the origin\ndistrict_origin &lt;- trips_origin_pois %&gt;%\n  left_join(population %&gt;% mutate(district = tolower(district)), \n            by = \"district\")\n\ndistrict_origin\n\n# A tibble: 44 √ó 6\n   district        num_of_trips avg_duration_minutes avg_distance_km num_of_pois\n   &lt;chr&gt;                  &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;       &lt;int&gt;\n 1 cakung                   628                 22.2            6.56          56\n 2 cempaka putih            460                 17.9            5.52          38\n 3 cengkareng               928                 20.2            6.00         298\n 4 cilandak                 830                 20.4            6.09         242\n 5 cilincing                266                 24.7            7.30         119\n 6 cipayung                 481                 22.9            7.09          56\n 7 ciracas                  607                 23.1            7.38         126\n 8 danau sunter               4                 19.4            8.55           1\n 9 danau sunter d‚Ä¶           34                 21.3            7.54           3\n10 duren sawit              891                 21.3            6.39         173\n# ‚Ñπ 34 more rows\n# ‚Ñπ 1 more variable: population_count &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#prepare-the-dataframe-for-weather-by-origin-district",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#prepare-the-dataframe-for-weather-by-origin-district",
    "title": "Take-home Exercise 3",
    "section": "3.5 Prepare the Dataframe for Weather by Origin District",
    "text": "3.5 Prepare the Dataframe for Weather by Origin District\nFor a more aggregated analysis of the weather conditions influencing commuters to pay for Grab services, I will do a count of all number of trips based on each weather description (e.g.¬†fog, haze, heavy rain) and based on each weather category (e.g.¬†rain or no rain). Take note that I will remove all values returned as ‚Äúoutside of jakarta‚Äù since we do not have weather data for trip origins found outside of Jakarta.\n\nweather_descriptions &lt;- c(\n  \"broken clouds\", \"scattered clouds\",\n  \"light rain\", \"overcast clouds\", \"moderate rain\", \n  \"haze\", \"fog\", \"few clouds\", \"heavy rain\"\n)\n\ntrips_origin_weather &lt;- trips %&gt;%\n  filter(origin_weather_description %in% weather_descriptions) %&gt;%\n  group_by(origin_district, origin_weather_description) %&gt;%\n  summarise(num_of_trips = n(), .groups = 'drop') %&gt;%\n  pivot_wider(\n    names_from = origin_weather_description,\n    values_from = num_of_trips,\n    values_fill = list(num_of_trips = 0)\n  ) %&gt;%\n  rename(district = origin_district) %&gt;%\n  mutate(district = tolower(district))\n\ncategory_counts &lt;- trips %&gt;%\n  filter(origin_weather_description %in% weather_descriptions) %&gt;%\n  group_by(origin_district, origin_weather_description_category) %&gt;%\n  summarise(total_category_count = n(), .groups = 'drop') %&gt;%\n  pivot_wider(\n    names_from = origin_weather_description_category,\n    values_from = total_category_count,\n    values_fill = list(total_category_count = 0)\n  ) %&gt;%\n  rename(district = origin_district) %&gt;%\n  mutate(district = tolower(district))\n\ntrips_origin_weather &lt;- trips_origin_weather %&gt;%\n  left_join(category_counts, by = \"district\")\n\ntrips_origin_weather\n\n# A tibble: 44 √ó 12\n   district         `broken clouds`   fog  haze `light rain` `moderate rain`\n   &lt;chr&gt;                      &lt;int&gt; &lt;int&gt; &lt;int&gt;        &lt;int&gt;           &lt;int&gt;\n 1 cakung                       159    18    25           58               4\n 2 cempaka putih                334     0     0           44               2\n 3 cengkareng                   374     0     8           67               4\n 4 cilandak                     228    22    26           76               8\n 5 cilincing                    110     0     0           26               1\n 6 cipayung                     132     9    16           61               3\n 7 ciracas                      156    18    26           64              13\n 8 danau sunter                   3     0     0            0               0\n 9 danau sunter dll              24     0     0            1               0\n10 duren sawit                  225    23    26           80               3\n# ‚Ñπ 34 more rows\n# ‚Ñπ 6 more variables: `scattered clouds` &lt;int&gt;, `overcast clouds` &lt;int&gt;,\n#   `few clouds` &lt;int&gt;, `heavy rain` &lt;int&gt;, not_rain &lt;int&gt;, rain &lt;int&gt;\n\n\nLet‚Äôs do an inner_join() to combine the weather data with our district_origin dataframe.\n\ndistrict_origin &lt;- district_origin %&gt;%\n  left_join(trips_origin_weather, by = \"district\") %&gt;%\n  select(-ends_with(\".x\"), -ends_with(\".y\"))\n\n# Inspect\ndistrict_origin\n\n# A tibble: 44 √ó 17\n   district        num_of_trips avg_duration_minutes avg_distance_km num_of_pois\n   &lt;chr&gt;                  &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;       &lt;int&gt;\n 1 cakung                   628                 22.2            6.56          56\n 2 cempaka putih            460                 17.9            5.52          38\n 3 cengkareng               928                 20.2            6.00         298\n 4 cilandak                 830                 20.4            6.09         242\n 5 cilincing                266                 24.7            7.30         119\n 6 cipayung                 481                 22.9            7.09          56\n 7 ciracas                  607                 23.1            7.38         126\n 8 danau sunter               4                 19.4            8.55           1\n 9 danau sunter d‚Ä¶           34                 21.3            7.54           3\n10 duren sawit              891                 21.3            6.39         173\n# ‚Ñπ 34 more rows\n# ‚Ñπ 12 more variables: population_count &lt;dbl&gt;, `broken clouds` &lt;int&gt;,\n#   fog &lt;int&gt;, haze &lt;int&gt;, `light rain` &lt;int&gt;, `moderate rain` &lt;int&gt;,\n#   `scattered clouds` &lt;int&gt;, `overcast clouds` &lt;int&gt;, `few clouds` &lt;int&gt;,\n#   `heavy rain` &lt;int&gt;, not_rain &lt;int&gt;, rain &lt;int&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#calculate-centroid-of-each-districts-polygon",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#calculate-centroid-of-each-districts-polygon",
    "title": "Take-home Exercise 3",
    "section": "3.6 Calculate Centroid of Each District‚Äôs Polygon",
    "text": "3.6 Calculate Centroid of Each District‚Äôs Polygon\nFinally, I will calculate the centroid of each polygon in jakarta_district and add the latitude and longitude coordinates of these centroids as new columns. We can also remove the province and city columns.\n\nst_centroid(geometry): Calculates the centroid of each polygon in the geometry column of jakarta_district, returning a point that represents the center of each district‚Äôs shape.\ncentroid_lat and centroid_lng:\n\nst_coordinates(centroid)[, 2]: Extracts the latitude (y-coordinate) from each centroid and stores it in centroid_lat.\nst_coordinates(centroid)[, 1]: Extracts the longitude (x-coordinate) from each centroid and stores it in centroid_lng.\n\n\n\njakarta_district_centroid &lt;- jakarta_district %&gt;%\n  mutate(\n    geometry = st_centroid(geometry), \n    centroid_lat = st_coordinates(geometry)[, 2],\n    centroid_lng = st_coordinates(geometry)[, 1]   \n  ) %&gt;%\n  select(district, centroid_lat, centroid_lng, geometry)  # Select the necessary columns\n\n# Display the result\njakarta_district_centroid\n\nSimple feature collection with 44 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 13484360 ymin: -3018643 xmax: 13585050 ymax: -2900311\nProjected CRS: UCS-2000 / Ukraine TM zone 10\nFirst 10 features:\n           district centroid_lat centroid_lng                  geometry\n1            cakung     -2971241     13570750 POINT (13570750 -2971241)\n2     cempaka putih     -2952903     13543000 POINT (13543000 -2952903)\n3        cengkareng     -2914110     13495544 POINT (13495544 -2914110)\n4          cilandak     -2982437     13489551 POINT (13489551 -2982437)\n5         cilincing     -2946831     13585052 POINT (13585052 -2946831)\n6          cipayung     -3018643     13526977 POINT (13526977 -3018643)\n7           ciracas     -3016628     13515117 POINT (13515117 -3016628)\n8      danau sunter     -2938886     13545267 POINT (13545267 -2938886)\n9  danau sunter dll     -2935989     13540105 POINT (13540105 -2935989)\n10      duren sawit     -2984795     13553448 POINT (13553448 -2984795)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#computing-contiguity-neighbours",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#computing-contiguity-neighbours",
    "title": "Take-home Exercise 3",
    "section": "5.1 Computing Contiguity Neighbours",
    "text": "5.1 Computing Contiguity Neighbours\nFirstly, let‚Äôs create a district_geom simple feature dataframe containing the polygon geometry data of each unique district of Jakarta.\n\ndistrict_geom &lt;- jakarta_district %&gt;%\n  select(district, geometry)\n\n# Inspect\ndistrict_geom\n\nSimple feature collection with 44 features and 1 field\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\nFirst 10 features:\n           district                       geometry\n1            cakung POLYGON ((13577740 -2957434...\n2     cempaka putih POLYGON ((13546309 -2954155...\n3        cengkareng POLYGON ((13501268 -2903641...\n4          cilandak POLYGON ((13493440 -2969564...\n5         cilincing POLYGON ((13600325 -2936635...\n6          cipayung POLYGON ((13537176 -3005377...\n7           ciracas POLYGON ((13521824 -2996302...\n8      danau sunter POLYGON ((13546437 -2939299...\n9  danau sunter dll POLYGON ((13541670 -2937824...\n10      duren sawit POLYGON ((13548180 -2972327...\n\n\nNext, we append the geometry column to the existing district_dest dataframe\n\ndistrict_origindistrict_dest\n\n\n\ndistrict_origin &lt;- district_origin %&gt;%\n  left_join(district_geom, by = 'district') %&gt;%\n  st_as_sf()\n\n# Inspect\ndistrict_origin\n\nSimple feature collection with 44 features and 17 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 18\n   district        num_of_trips avg_duration_minutes avg_distance_km num_of_pois\n   &lt;chr&gt;                  &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;       &lt;int&gt;\n 1 cakung                   628                 22.2            6.56          56\n 2 cempaka putih            460                 17.9            5.52          38\n 3 cengkareng               928                 20.2            6.00         298\n 4 cilandak                 830                 20.4            6.09         242\n 5 cilincing                266                 24.7            7.30         119\n 6 cipayung                 481                 22.9            7.09          56\n 7 ciracas                  607                 23.1            7.38         126\n 8 danau sunter               4                 19.4            8.55           1\n 9 danau sunter d‚Ä¶           34                 21.3            7.54           3\n10 duren sawit              891                 21.3            6.39         173\n# ‚Ñπ 34 more rows\n# ‚Ñπ 13 more variables: population_count &lt;dbl&gt;, `broken clouds` &lt;int&gt;,\n#   fog &lt;int&gt;, haze &lt;int&gt;, `light rain` &lt;int&gt;, `moderate rain` &lt;int&gt;,\n#   `scattered clouds` &lt;int&gt;, `overcast clouds` &lt;int&gt;, `few clouds` &lt;int&gt;,\n#   `heavy rain` &lt;int&gt;, not_rain &lt;int&gt;, rain &lt;int&gt;, geometry &lt;POLYGON [m]&gt;\n\n\n\n\n\ndistrict_dest &lt;- district_dest %&gt;%\n  left_join(district_geom, by = 'district') %&gt;%\n  st_as_sf()\n\n# Inspect\ndistrict_dest\n\nSimple feature collection with 44 features and 6 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 7\n   district        num_of_trips avg_duration_minutes avg_distance_km num_of_pois\n   &lt;chr&gt;                  &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;       &lt;int&gt;\n 1 cakung                   844                 21.5            6.40          56\n 2 cempaka putih            392                 19.1            6.13          38\n 3 cengkareng              1120                 19.9            6.20         298\n 4 cilandak                 905                 20.1            5.78         242\n 5 cilincing                277                 24.4            7.24         119\n 6 cipayung                 552                 22.5            7.08          56\n 7 ciracas                  546                 22.6            7.03         126\n 8 danau sunter              17                 17.3            5.21           1\n 9 danau sunter d‚Ä¶           33                 16.9            5.00           3\n10 duren sawit              877                 21.1            6.32         173\n# ‚Ñπ 34 more rows\n# ‚Ñπ 2 more variables: population_count &lt;dbl&gt;, geometry &lt;POLYGON [m]&gt;\n\n\n\n\n\nWe can now derive our¬†neighbour list object¬†by utilising the¬†st_contiguity()¬†function from the¬†sfdep¬†package to create contiguity weight matrices for the study area. Let us configure different queen parameters that I will eventually like users of the ShinyApp to play with.\n\nqueen = TRUE: refers to computing the neighbour list by queen contiguity (neighbours share a common edge and or corner)\nqueen = FALSE: refers to computing the neighbour list by rook contiguity (similar to bishop, where neighbours share a common edge only)\n\n\nQueen - originQueen - destRook - originRook - dest\n\n\nUsing district_origin sf dataframe and queen = TRUE\n\norigin_nb_queen &lt;- st_contiguity(district_origin$geometry, queen=TRUE)\nsummary(origin_nb_queen)\n\nNeighbour list object:\nNumber of regions: 44 \nNumber of nonzero links: 216 \nPercentage nonzero weights: 11.15702 \nAverage number of links: 4.909091 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 1 11  4 12 11  2  2  1 \n1 least connected region:\n16 with 2 links\n1 most connected region:\n21 with 9 links\n\n\n\n\nUsing district_destsf dataframe and queen = TRUE\n\ndest_nb_queen &lt;- st_contiguity(district_dest$geometry, queen=TRUE)\nsummary(dest_nb_queen)\n\nNeighbour list object:\nNumber of regions: 44 \nNumber of nonzero links: 216 \nPercentage nonzero weights: 11.15702 \nAverage number of links: 4.909091 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 1 11  4 12 11  2  2  1 \n1 least connected region:\n16 with 2 links\n1 most connected region:\n21 with 9 links\n\n\n\n\nUsing district_origin sf dataframe and queen = FALSE\n\norigin_nb_rook &lt;- st_contiguity(district_origin$geometry, queen=FALSE)\nsummary(origin_nb_rook)\n\nNeighbour list object:\nNumber of regions: 44 \nNumber of nonzero links: 210 \nPercentage nonzero weights: 10.84711 \nAverage number of links: 4.772727 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 2 10  6 11 11  2  1  1 \n2 least connected regions:\n6 16 with 2 links\n1 most connected region:\n21 with 9 links\n\n\n\n\nUsing district_dest sf dataframe and queen = FALSE\n\ndest_nb_rook &lt;- st_contiguity(district_dest$geometry, queen=FALSE)\nsummary(dest_nb_rook)\n\nNeighbour list object:\nNumber of regions: 44 \nNumber of nonzero links: 210 \nPercentage nonzero weights: 10.84711 \nAverage number of links: 4.772727 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 2 10  6 11 11  2  1  1 \n2 least connected regions:\n6 16 with 2 links\n1 most connected region:\n21 with 9 links"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#computing-row-standardised-weight-matrix",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#computing-row-standardised-weight-matrix",
    "title": "Take-home Exercise 3",
    "section": "5.2 Computing Row-Standardised Weight Matrix",
    "text": "5.2 Computing Row-Standardised Weight Matrix\n\nQueen - originQueen - destRook - originRook - dest\n\n\n\norigin_wt_queen &lt;- st_weights(origin_nb_queen, style = \"W\", allow_zero = TRUE)\n\nWe will mutate the newly created neighbour list object¬†origin_nb_queen and weight matrix¬†origin_wt_queen into our existing¬†district_origin. This results in a newly created object called¬†origin_queen_wm.\n\norigin_queen_wm &lt;- district_origin %&gt;%\n  mutate(nb = origin_nb_queen,\n         wt = origin_wt_queen,\n         .before = 1) \n\n# Inspect\norigin_queen_wm\n\nSimple feature collection with 44 features and 19 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 20\n   nb        wt     district   num_of_trips avg_duration_minutes avg_distance_km\n * &lt;nb&gt;      &lt;list&gt; &lt;chr&gt;             &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;\n 1 &lt;int [4]&gt; &lt;dbl&gt;  cakung              628                 22.2            6.56\n 2 &lt;int [5]&gt; &lt;dbl&gt;  cempaka p‚Ä¶          460                 17.9            5.52\n 3 &lt;int [5]&gt; &lt;dbl&gt;  cengkareng          928                 20.2            6.00\n 4 &lt;int [5]&gt; &lt;dbl&gt;  cilandak            830                 20.4            6.09\n 5 &lt;int [3]&gt; &lt;dbl&gt;  cilincing           266                 24.7            7.30\n 6 &lt;int [3]&gt; &lt;dbl&gt;  cipayung            481                 22.9            7.09\n 7 &lt;int [3]&gt; &lt;dbl&gt;  ciracas             607                 23.1            7.38\n 8 &lt;int [3]&gt; &lt;dbl&gt;  danau sun‚Ä¶            4                 19.4            8.55\n 9 &lt;int [3]&gt; &lt;dbl&gt;  danau sun‚Ä¶           34                 21.3            7.54\n10 &lt;int [3]&gt; &lt;dbl&gt;  duren saw‚Ä¶          891                 21.3            6.39\n# ‚Ñπ 34 more rows\n# ‚Ñπ 14 more variables: num_of_pois &lt;int&gt;, population_count &lt;dbl&gt;,\n#   `broken clouds` &lt;int&gt;, fog &lt;int&gt;, haze &lt;int&gt;, `light rain` &lt;int&gt;,\n#   `moderate rain` &lt;int&gt;, `scattered clouds` &lt;int&gt;, `overcast clouds` &lt;int&gt;,\n#   `few clouds` &lt;int&gt;, `heavy rain` &lt;int&gt;, not_rain &lt;int&gt;, rain &lt;int&gt;,\n#   geometry &lt;POLYGON [m]&gt;\n\n\n\n\n\ndest_wt_queen &lt;- st_weights(dest_nb_queen, style = \"W\", allow_zero = TRUE)\n\nWe will mutate the newly created neighbour list object¬†dest_nb_queen and weight matrix¬†dest_wt_queen into our existing¬†district_dest. This results in a newly created object called¬†dest_queen_wm.\n\ndest_queen_wm &lt;- district_dest %&gt;%\n  mutate(nb = dest_nb_queen,\n         wt = dest_wt_queen,\n         .before = 1) \n\n# Inspect\ndest_queen_wm\n\nSimple feature collection with 44 features and 8 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 9\n   nb        wt     district   num_of_trips avg_duration_minutes avg_distance_km\n * &lt;nb&gt;      &lt;list&gt; &lt;chr&gt;             &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;\n 1 &lt;int [4]&gt; &lt;dbl&gt;  cakung              844                 21.5            6.40\n 2 &lt;int [5]&gt; &lt;dbl&gt;  cempaka p‚Ä¶          392                 19.1            6.13\n 3 &lt;int [5]&gt; &lt;dbl&gt;  cengkareng         1120                 19.9            6.20\n 4 &lt;int [5]&gt; &lt;dbl&gt;  cilandak            905                 20.1            5.78\n 5 &lt;int [3]&gt; &lt;dbl&gt;  cilincing           277                 24.4            7.24\n 6 &lt;int [3]&gt; &lt;dbl&gt;  cipayung            552                 22.5            7.08\n 7 &lt;int [3]&gt; &lt;dbl&gt;  ciracas             546                 22.6            7.03\n 8 &lt;int [3]&gt; &lt;dbl&gt;  danau sun‚Ä¶           17                 17.3            5.21\n 9 &lt;int [3]&gt; &lt;dbl&gt;  danau sun‚Ä¶           33                 16.9            5.00\n10 &lt;int [3]&gt; &lt;dbl&gt;  duren saw‚Ä¶          877                 21.1            6.32\n# ‚Ñπ 34 more rows\n# ‚Ñπ 3 more variables: num_of_pois &lt;int&gt;, population_count &lt;dbl&gt;,\n#   geometry &lt;POLYGON [m]&gt;\n\n\n\n\n\norigin_wt_rook &lt;- st_weights(origin_nb_rook, style = \"W\", allow_zero = TRUE)\n\nWe will mutate the newly created neighbour list object¬†origin_nb_rook and weight matrix¬†origin_wt_rook into our existing¬†district_origin. This results in a newly created object called¬†origin_rook_wm.\n\norigin_rook_wm &lt;- district_origin %&gt;%\n  mutate(nb = origin_nb_rook,\n         wt = origin_wt_rook,\n         .before = 1) \n\n# Inspect\norigin_rook_wm\n\nSimple feature collection with 44 features and 19 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 20\n   nb        wt     district   num_of_trips avg_duration_minutes avg_distance_km\n * &lt;nb&gt;      &lt;list&gt; &lt;chr&gt;             &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;\n 1 &lt;int [4]&gt; &lt;dbl&gt;  cakung              628                 22.2            6.56\n 2 &lt;int [5]&gt; &lt;dbl&gt;  cempaka p‚Ä¶          460                 17.9            5.52\n 3 &lt;int [5]&gt; &lt;dbl&gt;  cengkareng          928                 20.2            6.00\n 4 &lt;int [5]&gt; &lt;dbl&gt;  cilandak            830                 20.4            6.09\n 5 &lt;int [3]&gt; &lt;dbl&gt;  cilincing           266                 24.7            7.30\n 6 &lt;int [2]&gt; &lt;dbl&gt;  cipayung            481                 22.9            7.09\n 7 &lt;int [3]&gt; &lt;dbl&gt;  ciracas             607                 23.1            7.38\n 8 &lt;int [3]&gt; &lt;dbl&gt;  danau sun‚Ä¶            4                 19.4            8.55\n 9 &lt;int [3]&gt; &lt;dbl&gt;  danau sun‚Ä¶           34                 21.3            7.54\n10 &lt;int [3]&gt; &lt;dbl&gt;  duren saw‚Ä¶          891                 21.3            6.39\n# ‚Ñπ 34 more rows\n# ‚Ñπ 14 more variables: num_of_pois &lt;int&gt;, population_count &lt;dbl&gt;,\n#   `broken clouds` &lt;int&gt;, fog &lt;int&gt;, haze &lt;int&gt;, `light rain` &lt;int&gt;,\n#   `moderate rain` &lt;int&gt;, `scattered clouds` &lt;int&gt;, `overcast clouds` &lt;int&gt;,\n#   `few clouds` &lt;int&gt;, `heavy rain` &lt;int&gt;, not_rain &lt;int&gt;, rain &lt;int&gt;,\n#   geometry &lt;POLYGON [m]&gt;\n\n\n\n\n\ndest_wt_rook &lt;- st_weights(dest_nb_rook, style = \"W\", allow_zero = TRUE)\n\nWe will mutate the newly created neighbour list object¬†dest_nb_rook and weight matrix¬†dest_wt_rook into our existing¬†district_dest. This results in a newly created object called¬†dest_rook_wm.\n\ndest_rook_wm &lt;- district_dest %&gt;%\n  mutate(nb = dest_nb_rook,\n         wt = dest_wt_rook,\n         .before = 1) \n\n# Inspect\ndest_rook_wm\n\nSimple feature collection with 44 features and 8 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 9\n   nb        wt     district   num_of_trips avg_duration_minutes avg_distance_km\n * &lt;nb&gt;      &lt;list&gt; &lt;chr&gt;             &lt;int&gt;                &lt;dbl&gt;           &lt;dbl&gt;\n 1 &lt;int [4]&gt; &lt;dbl&gt;  cakung              844                 21.5            6.40\n 2 &lt;int [5]&gt; &lt;dbl&gt;  cempaka p‚Ä¶          392                 19.1            6.13\n 3 &lt;int [5]&gt; &lt;dbl&gt;  cengkareng         1120                 19.9            6.20\n 4 &lt;int [5]&gt; &lt;dbl&gt;  cilandak            905                 20.1            5.78\n 5 &lt;int [3]&gt; &lt;dbl&gt;  cilincing           277                 24.4            7.24\n 6 &lt;int [2]&gt; &lt;dbl&gt;  cipayung            552                 22.5            7.08\n 7 &lt;int [3]&gt; &lt;dbl&gt;  ciracas             546                 22.6            7.03\n 8 &lt;int [3]&gt; &lt;dbl&gt;  danau sun‚Ä¶           17                 17.3            5.21\n 9 &lt;int [3]&gt; &lt;dbl&gt;  danau sun‚Ä¶           33                 16.9            5.00\n10 &lt;int [3]&gt; &lt;dbl&gt;  duren saw‚Ä¶          877                 21.1            6.32\n# ‚Ñπ 34 more rows\n# ‚Ñπ 3 more variables: num_of_pois &lt;int&gt;, population_count &lt;dbl&gt;,\n#   geometry &lt;POLYGON [m]&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#computing-local-morans-ii",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#computing-local-morans-ii",
    "title": "Take-home Exercise 3",
    "section": "5.3 Computing Local Moran‚Äôs Ii",
    "text": "5.3 Computing Local Moran‚Äôs Ii\nLocal Moran‚Äôs Ii is an extension of Global Moran‚Äôs I, designed to identify local clusters and spatial outliers within a dataset. Local Moran‚Äôs Ii provides a measure of autocorrelation at individual locations, identifying where significant clustering or outliers exist.\nLet‚Äôs utilise the¬†local_moran()¬†function of sfdep to handle the computations.\n\nQueen - originQueen - destRook - originRook - dest\n\n\n\nlisa_queen_origin &lt;- origin_queen_wm %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Inspect\nlisa_queen_origin\n\nSimple feature collection with 44 features and 31 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 32\n        ii      eii  var_ii   z_ii  p_ii p_ii_sim p_folded_sim skewness kurtosis\n     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.0386 -0.00252 0.0415   0.202 0.840     1            0.5   -0.645    0.266 \n 2  0.431  -0.107   0.155    1.36  0.172     0.16         0.08  -0.302   -0.0212\n 3  0.235   0.00131 0.0276   1.41  0.159     0.16         0.08   0.0501   0.119 \n 4  0.0846  0.00621 0.00292  1.45  0.147     0.2          0.1    0.313   -0.434 \n 5  0.348  -0.0617  0.570    0.543 0.587     0.58         0.29   0.222   -0.540 \n 6  0.207  -0.0587  0.178    0.630 0.529     0.66         0.33  -0.137   -0.610 \n 7  0.237   0.0113  0.0589   0.930 0.353     0.28         0.14  -0.415    0.226 \n 8  0.379  -0.0606  1.08     0.424 0.671     0.68         0.34  -0.510    1.41  \n 9  0.780   0.0139  0.887    0.814 0.416     0.46         0.23  -0.394    0.535 \n10 -0.0614  0.0253  0.0328  -0.478 0.632     0.76         0.38   0.545   -0.241 \n# ‚Ñπ 34 more rows\n# ‚Ñπ 23 more variables: mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;, nb &lt;nb&gt;,\n#   wt &lt;list&gt;, district &lt;chr&gt;, num_of_trips &lt;int&gt;, avg_duration_minutes &lt;dbl&gt;,\n#   avg_distance_km &lt;dbl&gt;, num_of_pois &lt;int&gt;, population_count &lt;dbl&gt;,\n#   `broken clouds` &lt;int&gt;, fog &lt;int&gt;, haze &lt;int&gt;, `light rain` &lt;int&gt;,\n#   `moderate rain` &lt;int&gt;, `scattered clouds` &lt;int&gt;, `overcast clouds` &lt;int&gt;,\n#   `few clouds` &lt;int&gt;, `heavy rain` &lt;int&gt;, not_rain &lt;int&gt;, rain &lt;int&gt;, ‚Ä¶\n\n\n\n\n\nlisa_queen_dest &lt;- dest_queen_wm %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Inspect\nlisa_queen_dest\n\nSimple feature collection with 44 features and 20 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 21\n        ii      eii  var_ii   z_ii  p_ii p_ii_sim p_folded_sim skewness kurtosis\n     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.0392 -2.44e-3 0.00494 -0.522 0.601     0.6          0.3  -0.171     0.0467\n 2  0.626  -9.73e-4 0.209    1.37  0.170     0.2          0.1  -0.248    -0.461 \n 3  0.518   6.28e-3 0.147    1.33  0.183     0.2          0.1  -0.204    -0.175 \n 4  0.206   1.87e-2 0.0167   1.45  0.147     0.12         0.06  0.00999   0.209 \n 5  0.485   6.73e-3 0.620    0.607 0.544     0.64         0.32  0.262    -1.01  \n 6  0.197   5.23e-3 0.128    0.535 0.593     0.66         0.33 -0.183    -0.268 \n 7  0.335  -1.85e-2 0.131    0.978 0.328     0.4          0.2   0.0627   -0.239 \n 8  0.381  -7.63e-2 1.43     0.382 0.702     0.78         0.39 -0.261    -0.548 \n 9  1.16   -7.81e-2 1.36     1.06  0.290     0.36         0.18 -0.223    -0.361 \n10  0.0351  9.97e-3 0.0131   0.220 0.826     0.78         0.39  0.294    -0.573 \n# ‚Ñπ 34 more rows\n# ‚Ñπ 12 more variables: mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;, nb &lt;nb&gt;,\n#   wt &lt;list&gt;, district &lt;chr&gt;, num_of_trips &lt;int&gt;, avg_duration_minutes &lt;dbl&gt;,\n#   avg_distance_km &lt;dbl&gt;, num_of_pois &lt;int&gt;, population_count &lt;dbl&gt;,\n#   geometry &lt;POLYGON [m]&gt;\n\n\n\n\n\nlisa_rook_origin &lt;- origin_rook_wm %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Inspect\nlisa_rook_origin\n\nSimple feature collection with 44 features and 31 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 32\n        ii      eii  var_ii    z_ii   p_ii p_ii_sim p_folded_sim skewness\n     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.0386  0.0204  0.0355   0.0968 0.923      0.86         0.43 -0.106  \n 2  0.431  -0.00950 0.110    1.33   0.184      0.26         0.13  0.0159 \n 3  0.235   0.0175  0.0264   1.34   0.180      0.26         0.13  0.304  \n 4  0.0846 -0.00390 0.00285  1.66   0.0974     0.14         0.07  0.456  \n 5  0.348  -0.0175  0.580    0.480  0.631      0.72         0.36 -0.347  \n 6  0.370   0.0122  0.303    0.650  0.515      0.54         0.27 -0.574  \n 7  0.237   0.0123  0.0683   0.859  0.390      0.42         0.21  0.120  \n 8  0.379  -0.281   1.02     0.654  0.513      0.48         0.24  0.00454\n 9  0.780  -0.117   0.930    0.931  0.352      0.42         0.21 -0.126  \n10 -0.0614 -0.00587 0.0202  -0.390  0.696      0.76         0.38 -0.0155 \n# ‚Ñπ 34 more rows\n# ‚Ñπ 24 more variables: kurtosis &lt;dbl&gt;, mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;,\n#   nb &lt;nb&gt;, wt &lt;list&gt;, district &lt;chr&gt;, num_of_trips &lt;int&gt;,\n#   avg_duration_minutes &lt;dbl&gt;, avg_distance_km &lt;dbl&gt;, num_of_pois &lt;int&gt;,\n#   population_count &lt;dbl&gt;, `broken clouds` &lt;int&gt;, fog &lt;int&gt;, haze &lt;int&gt;,\n#   `light rain` &lt;int&gt;, `moderate rain` &lt;int&gt;, `scattered clouds` &lt;int&gt;,\n#   `overcast clouds` &lt;int&gt;, `few clouds` &lt;int&gt;, `heavy rain` &lt;int&gt;, ‚Ä¶\n\n\n\n\n\nlisa_rook_dest &lt;- dest_rook_wm %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Inspect\nlisa_rook_dest\n\nSimple feature collection with 44 features and 20 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 21\n        ii       eii  var_ii   z_ii   p_ii p_ii_sim p_folded_sim skewness\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.0392 -0.000721 0.00530 -0.528 0.597      0.58         0.29  -0.0478\n 2  0.626  -0.0807   0.179    1.67  0.0949     0.12         0.06  -0.0211\n 3  0.518   0.0202   0.105    1.53  0.125      0.2          0.1    0.0689\n 4  0.206   0.00772  0.0191   1.44  0.151      0.2          0.1    0.0465\n 5  0.485  -0.00124  0.584    0.636 0.525      0.58         0.29  -0.197 \n 6  0.303  -0.113    0.181    0.978 0.328      0.36         0.18  -0.0266\n 7  0.335  -0.0507   0.139    1.04  0.300      0.28         0.14   0.246 \n 8  0.381   0.00782  1.19     0.342 0.732      0.78         0.39  -0.520 \n 9  1.16   -0.0378   1.06     1.16  0.247      0.3          0.15  -0.0925\n10  0.0351  0.0207   0.0139   0.123 0.902      0.84         0.42   0.385 \n# ‚Ñπ 34 more rows\n# ‚Ñπ 13 more variables: kurtosis &lt;dbl&gt;, mean &lt;fct&gt;, median &lt;fct&gt;, pysal &lt;fct&gt;,\n#   nb &lt;nb&gt;, wt &lt;list&gt;, district &lt;chr&gt;, num_of_trips &lt;int&gt;,\n#   avg_duration_minutes &lt;dbl&gt;, avg_distance_km &lt;dbl&gt;, num_of_pois &lt;int&gt;,\n#   population_count &lt;dbl&gt;, geometry &lt;POLYGON [m]&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#visualising-local-morans-ii",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#visualising-local-morans-ii",
    "title": "Take-home Exercise 3",
    "section": "5.4 Visualising Local Moran‚Äôs Ii",
    "text": "5.4 Visualising Local Moran‚Äôs Ii\nTo ease our analysis, an approach we can take is to plot the local Moran‚Äôs I values across to visualise the observed values across each district. We‚Äôll use a choropleth map from the tmap package to analyse the spatial patterns.\n\nQueen - originQueen - destRook - originRook - dest\n\n\n\ntm_shape(lisa_queen_origin) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",\"green1\",\"orange\",\"red\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"District-Level Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"view\")\nlisa_queen_origin &lt;- lisa_queen_origin %&gt;% \n  mutate(label = paste(\"District:\", district, \"| Local Moran's I:\", round(ii, 3)))\n\ntm_shape(lisa_queen_origin) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\", \"green1\", \"orange\", \"red\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\ntm_shape(lisa_queen_dest) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",\"green1\",\"orange\",\"red\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"District-Level Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_queen_dest &lt;- lisa_queen_dest %&gt;% \n  mutate(label = paste(\"District:\", district, \"| Local Moran's I:\", round(ii, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_queen_dest) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\", \"green1\", \"orange\", \"red\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\ntm_shape(lisa_rook_origin) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",\"green1\",\"orange\",\"red\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"District-Level Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.6,\n            legend.text.size = 0.6,\n            legend.hist.size = 0.6,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_rook_origin &lt;- lisa_rook_origin %&gt;% \n  mutate(label = paste(\"District:\", district, \"| Local Moran's I:\", round(ii, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_rook_origin) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\", \"green1\", \"orange\", \"red\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\ntm_shape(lisa_rook_dest) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",\"green1\",\"orange\",\"red\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"District-Level Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_rook_dest &lt;- lisa_rook_dest %&gt;% \n  mutate(label = paste(\"District:\", district, \"| Local Moran's I:\", round(ii, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_rook_dest) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\", \"green1\", \"orange\", \"red\"),\n          title = \"Local Moran's I\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#visualising-local-morans-ii-p-value",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#visualising-local-morans-ii-p-value",
    "title": "Take-home Exercise 3",
    "section": "5.5 Visualising Local Moran‚Äôs Ii P-value",
    "text": "5.5 Visualising Local Moran‚Äôs Ii P-value\nAs mentioned in the section above, we shall not hastily conclude the clustering results observed. Instead, let us also evaluate whether the observed clustering (high-high or low-low) is¬†statistically significant¬†or¬†could have occurred by chance. Hence, we can derive the p-values from Local Moran‚Äôs I by using the¬†p_ii_sim¬†field to determine statistical signficance across districts.\n\nQueen - originQueen - destRook - originRook - dest\n\n\n\ntm_shape(lisa_queen_origin) +\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"green3\",\"lightyellow\",\"orange\",\"red\"),\n          title = \"p-value\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_queen_origin &lt;- lisa_queen_origin %&gt;% \n  mutate(label = paste(\"District:\", district, \"| P-value:\", round(p_ii_sim, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_queen_origin) +\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"green3\",\"lightyellow\",\"orange\",\"red\"),\n          title = \"P-value\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\ntm_shape(lisa_queen_dest) +\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"green3\",\"lightyellow\",\"orange\",\"red\"),\n          title = \"p-value\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_queen_dest &lt;- lisa_queen_dest %&gt;% \n  mutate(label = paste(\"District:\", district, \"| P-value:\", round(p_ii_sim, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_queen_dest) +\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"green3\",\"lightyellow\",\"orange\",\"red\"),\n          title = \"P-value\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\ntm_shape(lisa_rook_origin) +\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"green3\",\"lightyellow\",\"orange\",\"red\"),\n          title = \"p-value\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_rook_origin &lt;- lisa_rook_origin %&gt;% \n  mutate(label = paste(\"District:\", district, \"| P-value:\", round(p_ii_sim, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_rook_origin) +\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"green3\",\"lightyellow\",\"orange\",\"red\"),\n          title = \"P-value\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\ntm_shape(lisa_rook_dest) +\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"green3\",\"lightyellow\",\"orange\",\"red\"),\n          title = \"p-value\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_rook_dest &lt;- lisa_rook_dest %&gt;% \n  mutate(label = paste(\"District:\", district, \"| P-value:\", round(p_ii_sim, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_rook_dest) +\n  tm_fill(\"p_ii_sim\", \n          palette = c(\"green3\",\"lightyellow\",\"orange\",\"red\"),\n          title = \"P-value\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#visualising-statistically-significant-local-morans-ii",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#visualising-statistically-significant-local-morans-ii",
    "title": "Take-home Exercise 3",
    "section": "5.6 Visualising Statistically Significant Local Moran‚Äôs Ii",
    "text": "5.6 Visualising Statistically Significant Local Moran‚Äôs Ii\nWith that said, I would like to switch our focus to districts that display statistically significant local¬†Moran‚Äôs I¬†values. To execute this, I will attempt to¬†remove¬†all local¬†Moran‚Äôs I¬†values with¬†p-values greater than 0.05. Subsequently, I will use the¬†tmap¬†function to plot the choropleth of statistically significant local spatial autocorrelation on the map of Jakarta.\n\nQueen - originQueen - destRook - originRook - dest\n\n\n\nlisa_queen_origin_sig &lt;- lisa_queen_origin  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa_queen_origin)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_queen_origin_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",'green3',\"green3\",\"lightyellow\",\"orange\",\"orange4\",\"red\"),\n          title = \"Local Moran's I (p-value &lt; 0.05)\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_queen_origin_sig &lt;- lisa_queen_origin_sig %&gt;% \n  mutate(label = paste(\"District:\", district, \"| Local Moran's I:\", round(ii, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_queen_origin) +\n  tm_polygons(id = \"\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_queen_origin_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",'green3',\"green3\",\"lightyellow\",\"orange\",\"orange4\",\"red\"),\n          title = \"Local Moran's I (p-value &lt; 0.05)\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\nlisa_queen_dest_sig &lt;- lisa_queen_dest  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa_queen_dest)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_queen_dest_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",'green3',\"green3\",\"lightyellow\",\"orange\",\"orange4\",\"red\"),\n          title = \"Local Moran's I (p-value &lt; 0.05)\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_queen_dest_sig &lt;- lisa_queen_dest_sig %&gt;% \n  mutate(label = paste(\"District:\", district, \"| Local Moran's I:\", round(ii, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_queen_dest) +\n  tm_polygons(id = \"\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_queen_dest_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",'green3',\"green3\",\"lightyellow\",\"orange\",\"orange4\",\"red\"),\n          title = \"Local Moran's I (p-value &lt; 0.05)\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\nlisa_queen_origin_sig &lt;- lisa_queen_origin  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa_queen_origin)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_queen_origin_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",'green3',\"green3\",\"lightyellow\",\"orange\",\"orange4\",\"red\"),\n          title = \"Local Moran's I (p-value &lt; 0.05)\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_queen_origin_sig &lt;- lisa_queen_origin_sig %&gt;% \n  mutate(label = paste(\"District:\", district, \"| Local Moran's I:\", round(ii, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_queen_origin) +\n  tm_polygons(id = \"\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_queen_origin_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",'green3',\"green3\",\"lightyellow\",\"orange\",\"orange4\",\"red\"),\n          title = \"Local Moran's I (p-value &lt; 0.05)\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\nlisa_rook_dest_sig &lt;- lisa_rook_dest  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntm_shape(lisa_rook_dest)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_rook_dest_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",'green3',\"green3\",\"lightyellow\",\"orange\",\"orange4\",\"red\"),\n          title = \"Local Moran's I (p-value &lt; 0.05)\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.2) +\n  tm_borders(col = \"black\", alpha = 0.5) +\n  tm_layout(main.title = \"Statistically Significant Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            asp = 1.2,\n            frame = TRUE) +\n  tm_compass(type = \"8star\", text.size = 0.7, size = 3, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(labels.size = 0.6, alpha = 0.1)\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_rook_dest_sig &lt;- lisa_rook_dest_sig %&gt;% \n  mutate(label = paste(\"District:\", district, \"| Local Moran's I:\", round(ii, 3)))\n\ntmap_mode(\"view\")\ntm_shape(lisa_rook_dest) +\n  tm_polygons(id = \"\") +\n  tm_borders(col = \"black\", alpha = 0.6)+\ntm_shape(lisa_rook_dest_sig) +\n  tm_fill(\"ii\", \n          palette = c(\"#B3EBF2\",'green3',\"green3\",\"lightyellow\",\"orange\",\"orange4\",\"red\"),\n          title = \"Local Moran's I (p-value &lt; 0.05)\",\n          midpoint = NA,\n          id = \"label\"\n  ) +\n  tm_borders(col = \"black\", alpha = 0.5) \n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#lisa-classification-categories",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#lisa-classification-categories",
    "title": "Take-home Exercise 3",
    "section": "5.7 LISA Classification Categories",
    "text": "5.7 LISA Classification Categories\nA LISA (Local Indicators of Spatial Association) map is a visualisation tool suitable for our spatial analysis in illustrating the results of Local Moran‚Äôs I. By doing so, we can identify spatial patterns, clusters, and outliers in the demand for Grab services by mapping the local statistics for each district in Jakarta.\n\nThese are the key components of the LISA maps we will plot:\n\nHigh-High (HH): Areas with high no. of Grab trips surrounded by other high values (hotspots).\nLow-Low (LL): Areas with low no. of Grab trips surrounded by other low values (cold spots).\nHigh-Low (HL): Areas with high no. of Grab trips surrounded by low values (outliers).\nLow-High (LH): Areas with low no. of Grab trips surrounded by high values (outliers).\n\n\n\n5.7.1 Visualising Overall LISA of Jakarta Study Area\n\nQueen - originQueen - destRook - originRook - dest\n\n\n\n# Let's inspect the 'mean' column\nsummary(lisa_queen_origin$mean)\n\n  Low-Low  High-Low  Low-High High-High \n       18         8         7        11 \n\n\n\n\n\n# Let's inspect the 'mean' column\nsummary(lisa_queen_dest$mean)\n\n  Low-Low  High-Low  Low-High High-High \n       17         7         6        14 \n\n\n\n\n\n# Let's inspect the 'mean' column\nsummary(lisa_rook_origin$mean)\n\n  Low-Low  High-Low  Low-High High-High \n       18         8         7        11 \n\n\n\n\n\n# Let's inspect the 'mean' column\nsummary(lisa_rook_dest$mean)\n\n  Low-Low  High-Low  Low-High High-High \n       17         7         6        14 \n\n\n\n\n\nTo plot our LISA classifications across districts in Jakarta, we‚Äôll leverage the¬†mean¬†category of the output of our¬†lisa¬†calculations, used as a reference point to determine if individual districts have higher or lower values compared to this average. This provides a baseline for detecting these¬†spatial clusters¬†and¬†outliers¬†in the dataset based on the categories mentioned above. We will produce both overall and statistically significant LISA maps below. There is almost no difference in the spread of LISA categories when queen and rook contiguity methods are used.\n\nLISA by Queen ContiguityLISA by Rook Contiguity\n\n\n\n\nCode\nlisa_queen_origin_sig &lt;- lisa_queen_origin %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_origin &lt;- tm_shape(lisa_queen_origin) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Overall LISA Spatial Autocorrelation of Grab Trip Origins in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\nlisa_queen_dest_sig &lt;- lisa_queen_dest %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_dest &lt;- tm_shape(lisa_queen_dest) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Overall LISA Spatial Autocorrelation of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_origin, lisa_cat_dest, asp=1, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_rook_origin_sig &lt;- lisa_rook_origin %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_origin &lt;- tm_shape(lisa_rook_origin) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Overall LISA Spatial Autocorrelation\\nof Grab Trip Origins in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\nlisa_rook_dest_sig &lt;- lisa_rook_dest %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_dest &lt;- tm_shape(lisa_rook_dest) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Overall LISA Spatial Autocorrelation\\nof Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_origin, lisa_cat_dest, asp=1, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also visualise all statistically significant LISA categories where p-value &lt; 0.05.\nWe can see that the queen method leads to fewer high-high categories and more low-high categories than when the rook method was used. It might suggest that the broader neighborhood definitions are causing the high-value districts to interact more with low-value ones, thus affecting the overall clustering results.\n\nStatistically Significant LISA - QueenStatistically Significant LISA - Rook\n\n\n\n\nCode\nlisa_cat_origin_sig &lt;- tm_shape(lisa_queen_origin)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_queen_origin_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Statistically Signifant LISA Map of Grab Trip Origins in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\nlisa_cat_dest_sig &lt;- tm_shape(lisa_queen_dest)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_queen_dest_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Statistically Signifant LISA Map of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_origin_sig, lisa_cat_dest_sig, asp=1, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_cat_origin_sig &lt;- tm_shape(lisa_rook_origin) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_rook_origin_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Statistically Significant LISA Map of Grab Trip Origins in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\nlisa_cat_dest_sig &lt;- tm_shape(lisa_rook_dest) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_rook_dest_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Statistically Significant LISA Map of Grab Trip Destinations in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_origin_sig, lisa_cat_dest_sig, asp=1, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\nHere‚Äôs the interactive version of the static plots above for more readability using tmap_mode('view'). Each district, when hovered above, shows the district name and LISA category, unless it is not statistically significant then it will just show the district name.\n\nQueen - originQueen - destRook - originRook - dest\n\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n\n\n\nCode\n# Filter significant districts\nlisa_queen_origin_sig &lt;- lisa_queen_origin %&gt;%\n  filter(p_ii_sim &lt; 0.05) %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\n# First map: Overall LISA Spatial Autocorrelation\nlisa_cat &lt;- tm_shape(lisa_queen_origin) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"Overall LISA Classification\",\n          id = \"label\") \n\n# Second map: Statistical Significant LISA Map\nlisa_queen_origin_sig &lt;- lisa_queen_origin_sig %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\nlisa_cat_sig &lt;- tm_shape(lisa_queen_origin) +\n  tm_polygons(id = \"\") +  \n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_queen_origin_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"Significant LISA Classification\",\n          id = \"label\")\ntmap_arrange(lisa_cat, lisa_cat_sig, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n\n\n\nCode\n# Filter significant districts\nlisa_queen_dest_sig &lt;- lisa_queen_dest %&gt;%\n  filter(p_ii_sim &lt; 0.05) %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\n# First map: Overall LISA Spatial Autocorrelation\nlisa_cat &lt;- tm_shape(lisa_queen_dest) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"Overall LISA Classification\",\n          id = \"label\") \n\n# Second map: Statistical Significant LISA Map\nlisa_queen_dest_sig &lt;- lisa_queen_dest_sig %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\nlisa_cat_sig &lt;- tm_shape(lisa_queen_dest) +\n  tm_polygons(id = \"\") +  \n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_queen_dest_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"Significant LISA Classification\",\n          id = \"label\")\ntmap_arrange(lisa_cat, lisa_cat_sig, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n\n\n\nCode\n# Filter significant districts\nlisa_rook_origin_sig &lt;- lisa_rook_origin %&gt;%\n  filter(p_ii_sim &lt; 0.05) %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\n# First map: Overall LISA Spatial Autocorrelation\nlisa_cat &lt;- tm_shape(lisa_rook_origin) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"Overall LISA Classification\",\n          id = \"label\") \n\n# Second map: Statistical Significant LISA Map\nlisa_rook_origin_sig &lt;- lisa_rook_origin_sig %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\nlisa_cat_sig &lt;- tm_shape(lisa_rook_origin) +\n  tm_polygons(id = \"\") +  \n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_rook_origin_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"Significant LISA Classification\",\n          id = \"label\")\ntmap_arrange(lisa_cat, lisa_cat_sig, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n\n\n\nCode\n# Filter significant districts\nlisa_rook_dest_sig &lt;- lisa_rook_dest %&gt;%\n  filter(p_ii_sim &lt; 0.05) %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\n# First map: Overall LISA Spatial Autocorrelation\nlisa_cat &lt;- tm_shape(lisa_rook_dest) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"Overall LISA Classification\",\n          id = \"label\") \n\n# Second map: Statistical Significant LISA Map\nlisa_rook_dest_sig &lt;- lisa_rook_dest_sig %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\nlisa_cat_sig &lt;- tm_shape(lisa_rook_dest) +\n  tm_polygons(id = \"\") +  \n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_rook_dest_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"Significant LISA Classification\",\n          id = \"label\")\ntmap_arrange(lisa_cat, lisa_cat_sig, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\n\n\n5.7.2 Visualising LISA of Jakarta Study Area by Driving Mode\nConducting LISA classification of the number of Grab trips taken by driving mode can provide valuable insights into spatial disparities in travel demand across districts, by car and motorcycle. We will prepare the car_origin, car_dest, motorcycle_origin and motorcycle_dest dataframes to store the total number of trips for each origin and destination district of Grab trips. Note that I will remove districts ‚Äòoutside of Jakarta‚Äô since we do not have geometry values for those districts.\n\nCar-focused clusters might highlight districts where road infrastructure, socioeconomic factors, and parking policies influence car travel demand.\nMotorcycle-focused clusters can point to areas where motorcycles are essential for accessibility, affordability, and maneuverability, especially in highly congested or densely populated districts.\nCar-focused outliers: Districts with high car trips surrounded by low-use areas may highlight unique features attracting car users, requiring tailored infrastructure solutions.\nMotorcycle-focused outliers: Areas with high motorcycle use amidst low surrounding usage may indicate critical transit hubs needing efficient drop-off zones and integrated transport solutions.\n\n\nPrepare Origin DataPrepare Destination Data\n\n\n\norigin_driving_mode &lt;- trips %&gt;%\n  filter(origin_district != 'outside of jakarta') %&gt;%\n  group_by(origin_district, driving_mode) %&gt;%\n  summarise(num_of_trips = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = driving_mode, values_from = num_of_trips) %&gt;%\n  rename(district = origin_district)\n\ncar_origin &lt;- origin_driving_mode %&gt;%\n  select(district, car) %&gt;%\n  rename(num_of_trips = car) %&gt;%\n  left_join(district_geom, by = 'district') %&gt;%\n  st_as_sf()\n\nmotorcycle_origin &lt;- origin_driving_mode %&gt;%\n  select(district, motorcycle) %&gt;%\n  rename(num_of_trips = motorcycle) %&gt;%\n  left_join(district_geom, by = 'district') %&gt;%\n  st_as_sf()\n\n# Inspect\ncar_origin\n\nSimple feature collection with 44 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 3\n   district         num_of_trips                                        geometry\n   &lt;chr&gt;                   &lt;int&gt;                                   &lt;POLYGON [m]&gt;\n 1 cakung                    271 ((13577740 -2957434, 13577686 -2957585, 135780‚Ä¶\n 2 cempaka putih             225 ((13546309 -2954155, 13546032 -2954655, 135451‚Ä¶\n 3 cengkareng                438 ((13501268 -2903641, 13501646 -2904344, 135020‚Ä¶\n 4 cilandak                  396 ((13493440 -2969564, 13493591 -2969657, 134936‚Ä¶\n 5 cilincing                 173 ((13600325 -2936635, 13600614 -2937446, 136008‚Ä¶\n 6 cipayung                  214 ((13537176 -3005377, 13537282 -3005431, 135372‚Ä¶\n 7 ciracas                   263 ((13521824 -2996302, 13521925 -2996378, 135223‚Ä¶\n 8 danau sunter                3 ((13546437 -2939299, 13546343 -2939273, 135461‚Ä¶\n 9 danau sunter dll           27 ((13541670 -2937824, 13540217 -2937341, 135396‚Ä¶\n10 duren sawit               362 ((13548180 -2972327, 13548729 -2972607, 135497‚Ä¶\n# ‚Ñπ 34 more rows\n\n\n\n\n\ndest_driving_mode &lt;- trips %&gt;%\n  filter(destination_district != 'outside of jakarta') %&gt;%\n  group_by(destination_district, driving_mode) %&gt;%\n  summarise(num_of_trips = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = driving_mode, values_from = num_of_trips) %&gt;%\n  rename(district = destination_district)\n\ncar_dest &lt;- dest_driving_mode %&gt;%\n  select(district, car) %&gt;%\n  rename(num_of_trips = car) %&gt;%\n  left_join(district_geom, by = 'district') %&gt;%\n  st_as_sf()\n\nmotorcycle_dest &lt;- dest_driving_mode %&gt;%\n  select(district, motorcycle) %&gt;%\n  rename(num_of_trips = motorcycle) %&gt;%\n  left_join(district_geom, by = 'district') %&gt;%\n  st_as_sf()\n\n# Inspect\ncar_dest\n\nSimple feature collection with 44 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 3\n   district         num_of_trips                                        geometry\n   &lt;chr&gt;                   &lt;int&gt;                                   &lt;POLYGON [m]&gt;\n 1 cakung                    336 ((13577740 -2957434, 13577686 -2957585, 135780‚Ä¶\n 2 cempaka putih             209 ((13546309 -2954155, 13546032 -2954655, 135451‚Ä¶\n 3 cengkareng                540 ((13501268 -2903641, 13501646 -2904344, 135020‚Ä¶\n 4 cilandak                  413 ((13493440 -2969564, 13493591 -2969657, 134936‚Ä¶\n 5 cilincing                 146 ((13600325 -2936635, 13600614 -2937446, 136008‚Ä¶\n 6 cipayung                  284 ((13537176 -3005377, 13537282 -3005431, 135372‚Ä¶\n 7 ciracas                   252 ((13521824 -2996302, 13521925 -2996378, 135223‚Ä¶\n 8 danau sunter               12 ((13546437 -2939299, 13546343 -2939273, 135461‚Ä¶\n 9 danau sunter dll           21 ((13541670 -2937824, 13540217 -2937341, 135396‚Ä¶\n10 duren sawit               364 ((13548180 -2972327, 13548729 -2972607, 135497‚Ä¶\n# ‚Ñπ 34 more rows\n\n\n\n\n\nNext, I prepare all the required data which goes down to the granularity of origin/destination, queen/rook and car/motorcycle. For instance, queen - origin - car will only focus on cars moving from the start of trip, using the queen contiguity method to locate its neighbours.\n\nQueen - origin - carQueen - dest - carRook - origin - carRook - dest - carQueen - origin - motorcycleQueen - dest - motorcycleRook - origin - motorcycleRook - dest - motorcycle\n\n\n\ncar_origin_nb_queen &lt;- st_contiguity(car_origin$geometry, queen=TRUE)\n\ncar_origin_wt_queen &lt;- st_weights(car_origin_nb_queen, style = \"W\", allow_zero = TRUE)\n\ncar_origin_wm_queen &lt;- car_origin %&gt;%\n  mutate(nb = car_origin_nb_queen,\n         wt = car_origin_wt_queen,\n         .before = 1) \n\nlisa_queen_origin_car &lt;- car_origin_wm_queen %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\ncar_dest_nb_queen &lt;- st_contiguity(car_dest$geometry, queen = TRUE)\n\ncar_dest_wt_queen &lt;- st_weights(car_dest_nb_queen, style = \"W\", allow_zero = TRUE)\n\ncar_dest_wm_queen &lt;- car_dest %&gt;%\n  mutate(nb = car_dest_nb_queen,\n         wt = car_dest_wt_queen,\n         .before = 1) \n\nlisa_queen_dest_car &lt;- car_dest_wm_queen %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\ncar_origin_nb_rook &lt;- st_contiguity(car_origin$geometry, queen=FALSE)\n\ncar_origin_wt_rook &lt;- st_weights(car_origin_nb_rook, style = \"W\", allow_zero = TRUE)\n\ncar_origin_wm_rook &lt;- car_origin %&gt;%\n  mutate(nb = car_origin_nb_rook,\n         wt = car_origin_wt_rook,\n         .before = 1) \n\nlisa_rook_origin_car &lt;- car_origin_wm_rook %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\ncar_dest_nb_rook &lt;- st_contiguity(car_dest$geometry, queen = FALSE)\n\ncar_dest_wt_rook &lt;- st_weights(car_dest_nb_rook, style = \"W\", allow_zero = TRUE)\n\ncar_dest_wm_rook &lt;- car_dest %&gt;%\n  mutate(nb = car_dest_nb_rook,\n         wt = car_dest_wt_rook,\n         .before = 1) \n\nlisa_rook_dest_car &lt;- car_dest_wm_rook %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nmotorcycle_origin_nb_queen &lt;- st_contiguity(motorcycle_origin$geometry, queen=TRUE)\n\nmotorcycle_origin_wt_queen &lt;- st_weights(motorcycle_origin_nb_queen, style = \"W\", allow_zero = TRUE)\n\nmotorcycle_origin_wm_queen &lt;- motorcycle_origin %&gt;%\n  mutate(nb = motorcycle_origin_nb_queen,\n         wt = motorcycle_origin_wt_queen,\n         .before = 1) \n\nlisa_queen_origin_motorcycle &lt;- motorcycle_origin_wm_queen %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nmotorcycle_dest_nb_queen &lt;- st_contiguity(motorcycle_dest$geometry, queen = TRUE)\n\nmotorcycle_dest_wt_queen &lt;- st_weights(motorcycle_dest_nb_queen, style = \"W\", allow_zero = TRUE)\n\nmotorcycle_dest_wm_queen &lt;- motorcycle_dest %&gt;%\n  mutate(nb = motorcycle_dest_nb_queen,\n         wt = motorcycle_dest_wt_queen,\n         .before = 1) \n\nlisa_queen_dest_motorcycle &lt;- motorcycle_dest_wm_queen %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nmotorcycle_origin_nb_rook &lt;- st_contiguity(motorcycle_origin$geometry, queen=FALSE)\n\nmotorcycle_origin_wt_rook &lt;- st_weights(motorcycle_origin_nb_rook, style = \"W\", allow_zero = TRUE)\n\nmotorcycle_origin_wm_rook &lt;- motorcycle_origin %&gt;%\n  mutate(nb = motorcycle_origin_nb_rook,\n         wt = motorcycle_origin_wt_rook,\n         .before = 1) \n\nlisa_rook_origin_motorcycle &lt;- motorcycle_origin_wm_rook %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nmotorcycle_dest_nb_rook &lt;- st_contiguity(motorcycle_dest$geometry, queen = FALSE)\n\nmotorcycle_dest_wt_rook &lt;- st_weights(motorcycle_dest_nb_rook, style = \"W\", allow_zero = TRUE)\n\nmotorcycle_dest_wm_rook &lt;- motorcycle_dest %&gt;%\n  mutate(nb = motorcycle_dest_nb_rook,\n         wt = motorcycle_dest_wt_rook,\n         .before = 1) \n\nlisa_rook_dest_motorcycle &lt;- motorcycle_dest_wm_rook %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nNow, we can visualise how spatial clusterings of Grab trips form based on driving modes, namely by car and motorcycle. We will only be looking at statistically significant LISA values i.e.¬†p-value &lt; 0.05.\n\nQueen: Origin Car vs Origin MotorcycleQueen: Dest Car vs Dest MotorcycleRook: Origin Car vs Origin MotorcycleRook: Dest Car vs Dest Motorcycle\n\n\n\n\nCode\nlisa_queen_origin_car_sig &lt;- lisa_queen_origin_car %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_queen_origin_motorcycle_sig &lt;- lisa_queen_origin_motorcycle %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_origin_car &lt;- tm_shape(lisa_queen_origin_car) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_queen_origin_car_sig) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Origins in Jakarta by Car\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\nlisa_cat_origin_motorcycle &lt;- tm_shape(lisa_queen_origin_motorcycle) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_queen_origin_motorcycle_sig) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Origins in Jakarta by Motorcycle\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_origin_car, lisa_cat_origin_motorcycle, asp=1, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_queen_dest_car_sig &lt;- lisa_queen_dest_car %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_queen_dest_motorcycle_sig &lt;- lisa_queen_dest_motorcycle %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_dest_car &lt;- tm_shape(lisa_queen_dest_car) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_queen_dest_car_sig) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Destinations in Jakarta by Car\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\nlisa_cat_dest_motorcycle &lt;- tm_shape(lisa_queen_dest_motorcycle) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_queen_dest_motorcycle_sig) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Destinations in Jakarta by Motorcycle\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_dest_car, lisa_cat_dest_motorcycle, asp=1, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_rook_origin_car_sig &lt;- lisa_rook_origin_car %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_rook_origin_motorcycle_sig &lt;- lisa_rook_origin_motorcycle %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_origin_car &lt;- tm_shape(lisa_rook_origin_car) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_rook_origin_car_sig) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Origins in Jakarta by Car\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\nlisa_cat_origin_motorcycle &lt;- tm_shape(lisa_rook_origin_motorcycle) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_rook_origin_motorcycle_sig) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Origins in Jakarta by Motorcycle\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_origin_car, lisa_cat_origin_motorcycle, asp=1, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_rook_dest_car_sig &lt;- lisa_rook_dest_car %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_rook_dest_motorcycle_sig &lt;- lisa_rook_dest_motorcycle %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_dest_car &lt;- tm_shape(lisa_rook_dest_car) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_rook_dest_car_sig) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Destinations in Jakarta by Car\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\nlisa_cat_dest_motorcycle &lt;- tm_shape(lisa_rook_dest_motorcycle) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_rook_dest_motorcycle_sig) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Destinations in Jakarta by Motorcycle\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_dest_car, lisa_cat_dest_motorcycle, asp=1, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.7.3 Visualising LISA of Jakarta Study Area by Weather (Rain / No Rain)\nNext, we will analyse how the number of Grab trips in one district may influence clusters of Grab trips surrounding it. It is worth noting that we have only acquired weather data for trip origins since we are most interested in determining how the current weather influence commuters in deciding to book a Grab.\n\nRain clusters: Districts with low trip counts during rain reflect a lack of reliance on ride-hailing services, possibly due to insufficient demand or alternative transport options.\nNo-rain clusters: Areas with high trip counts in dry conditions indicate robust travel demand and may reflect socioeconomic activity or tourism, necessitating infrastructure support to accommodate high volumes.\nRain outliers: Districts with low trip counts in rainy conditions surrounded by high usage may indicate barriers to accessing ride-hailing services, suggesting opportunities for improving transport accessibility.\nNo-rain outliers: Areas with high trip volumes in dry conditions surrounded by low-use districts may indicate popularity as a destination, warranting improved connectivity and transport services.\n\n\ntrip_weather &lt;- trips %&gt;%\n  select(origin_district, origin_weather_description_category) %&gt;%\n  filter(origin_weather_description_category != 'Outside of Jakarta') %&gt;%\n  group_by(origin_district, origin_weather_description_category) %&gt;%\n  summarise(num_of_trips = n(), .groups = 'drop') %&gt;%  \n  pivot_wider(names_from = origin_weather_description_category, \n              values_from = num_of_trips, values_fill = list(num_of_trips = 0)) %&gt;%\n  rename(district = origin_district)\n\nno_rain_origin &lt;- trip_weather %&gt;%\n  select(district, not_rain) %&gt;%\n  rename(num_of_trips = not_rain) %&gt;%\n  left_join(district_geom, by = 'district') %&gt;%\n  st_as_sf()\n\nrain_origin &lt;- trip_weather %&gt;%\n  select(district, rain) %&gt;%\n  rename(num_of_trips = rain) %&gt;%\n  left_join(district_geom, by = 'district') %&gt;%\n  st_as_sf()\n\n# Inspect\nno_rain_origin\n\nSimple feature collection with 44 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 3\n   district         num_of_trips                                        geometry\n   &lt;chr&gt;                   &lt;int&gt;                                   &lt;POLYGON [m]&gt;\n 1 cakung                    509 ((13577740 -2957434, 13577686 -2957585, 135780‚Ä¶\n 2 cempaka putih             398 ((13546309 -2954155, 13546032 -2954655, 135451‚Ä¶\n 3 cengkareng                799 ((13501268 -2903641, 13501646 -2904344, 135020‚Ä¶\n 4 cilandak                  678 ((13493440 -2969564, 13493591 -2969657, 134936‚Ä¶\n 5 cilincing                 215 ((13600325 -2936635, 13600614 -2937446, 136008‚Ä¶\n 6 cipayung                  371 ((13537176 -3005377, 13537282 -3005431, 135372‚Ä¶\n 7 ciracas                   465 ((13521824 -2996302, 13521925 -2996378, 135223‚Ä¶\n 8 danau sunter                4 ((13546437 -2939299, 13546343 -2939273, 135461‚Ä¶\n 9 danau sunter dll           31 ((13541670 -2937824, 13540217 -2937341, 135396‚Ä¶\n10 duren sawit               739 ((13548180 -2972327, 13548729 -2972607, 135497‚Ä¶\n# ‚Ñπ 34 more rows\n\n\nLikewise, I prepare all the required data which goes down to the granularity of origin, queen/rook and rain/no rain For instance, queen - origin - rain will only focus on cars moving from the start of trip, in wet weather condition, using the queen contiguity method to locate its neighbours.\n\nQueen - origin - rainRook - origin - rainQueen - origin - no rainRook - origin - no rain\n\n\n\nrain_origin_nb_queen &lt;- st_contiguity(rain_origin$geometry, queen=TRUE)\n\nrain_origin_wt_queen &lt;- st_weights(rain_origin_nb_queen, style = \"W\", allow_zero = TRUE)\n\nrain_origin_wm_queen &lt;- rain_origin %&gt;%\n  mutate(nb = rain_origin_nb_queen,\n         wt = rain_origin_wt_queen,\n         .before = 1) \n\nlisa_queen_rain &lt;- rain_origin_wm_queen %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nrain_origin_nb_rook &lt;- st_contiguity(rain_origin$geometry, queen=FALSE)\n\nrain_origin_wt_rook &lt;- st_weights(rain_origin_nb_rook, style = \"W\", allow_zero = TRUE)\n\nrain_origin_wm_rook &lt;- rain_origin %&gt;%\n  mutate(nb = rain_origin_nb_rook,\n         wt = rain_origin_wt_rook,\n         .before = 1) \n\nlisa_rook_rain &lt;- rain_origin_wm_rook %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nno_rain_origin_nb_queen &lt;- st_contiguity(no_rain_origin$geometry, queen=TRUE)\n\nno_rain_origin_wt_queen &lt;- st_weights(no_rain_origin_nb_queen, style = \"W\", allow_zero = TRUE)\n\nno_rain_origin_wm_queen &lt;- no_rain_origin %&gt;%\n  mutate(nb = no_rain_origin_nb_queen,\n         wt = no_rain_origin_wt_queen,\n         .before = 1) \n\nlisa_queen_no_rain &lt;- no_rain_origin_wm_queen %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nno_rain_origin_nb_rook &lt;- st_contiguity(no_rain_origin$geometry, queen=FALSE)\n\nno_rain_origin_wt_rook &lt;- st_weights(no_rain_origin_nb_rook, style = \"W\", allow_zero = TRUE)\n\nno_rain_origin_wm_rook &lt;- no_rain_origin %&gt;%\n  mutate(nb = no_rain_origin_nb_rook,\n         wt = no_rain_origin_wt_rook,\n         .before = 1) \n\nlisa_rook_no_rain &lt;- no_rain_origin_wm_rook %&gt;% \n  mutate(local_moran = local_moran(num_of_trips, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nNow, let‚Äôs visualise how spatial clustering of Grab trips form based on weather (rain/no rain), namely by car and motorcycle. We will only be looking at statistically significant LISA values i.e.¬†p-value &lt; 0.05.\n\nQueen: Origin Rain vs Origin No RainRook: Origin Rain vs Origin No Rain\n\n\n\n\nCode\nlisa_queen_rain_sig &lt;- lisa_queen_rain %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_queen_no_rain_sig &lt;- lisa_queen_no_rain %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_queen_rain &lt;- tm_shape(lisa_queen_rain) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_queen_rain_sig) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Origins in Jakarta (Rain)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\nlisa_cat_queen_no_rain &lt;- tm_shape(lisa_queen_no_rain) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_queen_no_rain_sig) +\n  tm_polygons(\"mean\", \n          # blue, orange, green, red\n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Origins in Jakarta (No Rain)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_queen_rain, lisa_cat_queen_no_rain, asp=1, nrow=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_rook_rain_sig &lt;- lisa_rook_rain %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_rook_no_rain_sig &lt;- lisa_rook_no_rain %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_rook_rain &lt;- tm_shape(lisa_rook_rain) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_rook_rain_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Origins in Jakarta (Rain)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\nlisa_cat_rook_no_rain &lt;- tm_shape(lisa_rook_no_rain) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_rook_no_rain_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"LISA Spatial Autocorrelation of Grab Trip Origins in Jakarta (No Rain)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_rook_rain, lisa_cat_rook_no_rain, asp = 1, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.7.4 Visualising LISA of Jakarta Study Area by Number of POIs\nNext, I will now perform LISA classification analysis on the number of Points of Interest (POIs) in each district. Analysing POIs with LISA can yield interesting spatial patterns.\n\nHigh-High clusters might reveal regions with high concentrations of POIs, possibly indicating commercial or tourist hubs.\nLow-Low clusters could indicate underdeveloped or residential areas.\nHigh-Low and Low-High outliers could be of interest for urban planners or business analysts to identify potential areas for investment or improvement.\n\nLet‚Äôs quickly prepare all the required data which goes down to the granularity of queen and rook methods to locate its neighbours. For this case, I will not be categorising the data by origin and destination since the distribution of POIs is uniform across districts, making it unnecessary to differentiate based on origin and destination.\n\n# Add geometry to pois_num\npois_num_sf &lt;- pois_num %&gt;%\n  left_join(district_geom, by = 'district') %&gt;%\n  st_as_sf()\n\n# Inspect\npois_num_sf\n\nSimple feature collection with 44 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 13472330 ymin: -3037137 xmax: 13602610 ymax: -2881872\nProjected CRS: UCS-2000 / Ukraine TM zone 10\n# A tibble: 44 √ó 3\n   district         num_of_pois                                         geometry\n   &lt;chr&gt;                  &lt;int&gt;                                    &lt;POLYGON [m]&gt;\n 1 cakung                    56 ((13577740 -2957434, 13577686 -2957585, 1357803‚Ä¶\n 2 cempaka putih             38 ((13546309 -2954155, 13546032 -2954655, 1354519‚Ä¶\n 3 cengkareng               298 ((13501268 -2903641, 13501646 -2904344, 1350205‚Ä¶\n 4 cilandak                 242 ((13493440 -2969564, 13493591 -2969657, 1349367‚Ä¶\n 5 cilincing                119 ((13600325 -2936635, 13600614 -2937446, 1360082‚Ä¶\n 6 cipayung                  56 ((13537176 -3005377, 13537282 -3005431, 1353726‚Ä¶\n 7 ciracas                  126 ((13521824 -2996302, 13521925 -2996378, 1352232‚Ä¶\n 8 danau sunter               1 ((13546437 -2939299, 13546343 -2939273, 1354615‚Ä¶\n 9 danau sunter dll           3 ((13541670 -2937824, 13540217 -2937341, 1353960‚Ä¶\n10 duren sawit              173 ((13548180 -2972327, 13548729 -2972607, 1354977‚Ä¶\n# ‚Ñπ 34 more rows\n\n\n\nQueenRook\n\n\n\npois_nb_queen &lt;- st_contiguity(pois_num_sf$geometry, queen=TRUE)\n\npois_wt_queen &lt;- st_weights(pois_nb_queen, style = \"W\", allow_zero = TRUE)\n\npois_wm_queen &lt;- pois_num_sf %&gt;%\n  mutate(nb = pois_nb_queen,\n         wt = pois_wt_queen,\n         .before = 1) \n\nlisa_queen_pois &lt;- pois_wm_queen %&gt;% \n  mutate(local_moran = local_moran(num_of_pois, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\npois_nb_rook &lt;- st_contiguity(pois_num_sf$geometry, queen=FALSE)\n\npois_wt_rook &lt;- st_weights(pois_nb_rook, style = \"W\", allow_zero = TRUE)\n\npois_wm_rook &lt;- pois_num_sf %&gt;%\n  mutate(nb = pois_nb_rook,\n         wt = pois_wt_rook,\n         .before = 1) \n\nlisa_rook_pois &lt;- pois_wm_rook %&gt;% \n  mutate(local_moran = local_moran(num_of_pois, nb, wt, \n                                   zero.policy = TRUE, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\nNext, we can visualise the LISA categories associated to each district via plotting it on a choropleth map below.\n\nOverall LISA - QueenOverall LISA - Rook\n\n\n\n\nCode\nlisa_cat_queen_pois &lt;- tm_shape(lisa_queen_pois)+\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Overall LISA Map of Points of Interests in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_queen_pois, asp=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_cat_rook_pois &lt;- tm_shape(lisa_rook_pois) +\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_polygons(\"mean\", \n              palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n              title = \"LISA Classification\",\n              midpoint = NA,\n              legend.hist = TRUE, \n              legend.is.portrait = TRUE,\n              legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Overall LISA Map of Points of Interests in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_rook_pois, asp = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can filter the statistically significant LISA observations (p-value &gt; 0.05) and plot them on a map as shown.\n\nStatistically Significant LISA - QueenStatistically Significant LISA - Rook\n\n\n\n\nCode\nlisa_queen_pois_sig &lt;- lisa_queen_pois %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_queen_pois_sig &lt;- tm_shape(lisa_queen_pois)+\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6)+\n  tm_shape(lisa_queen_pois_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\",\"green3\", \"#d21b1c\"),\n          title = \"LISA Classification\",\n          midpoint = NA,\n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Statistically Signifant LISA Map of Points of Interests in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_queen_pois_sig, asp=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlisa_rook_pois_sig &lt;- lisa_rook_pois %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_cat_rook_pois_sig &lt;- tm_shape(lisa_rook_pois) +\n  tm_polygons() +\n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_rook_pois_sig) +\n  tm_polygons(\"mean\", \n              palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n              title = \"LISA Classification\",\n              midpoint = NA,\n              legend.hist = TRUE, \n              legend.is.portrait = TRUE,\n              legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Statistically Significant LISA Map of Points of Interests in Jakarta\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            main.title.fontface = \"bold\",\n            legend.title.size = 0.8,\n            legend.text.size = 0.8,\n            legend.hist.size = 0.8,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\",\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", text.size = 1, size = 2, position = c(\"RIGHT\", \"TOP\")) +\n  tm_scale_bar(position = c(\"LEFT\", \"BOTTOM\"), text.size = 0.5) +\n  tm_grid(alpha = 0.1)\n\ntmap_arrange(lisa_cat_rook_pois_sig, asp = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs also visualise the interactive version of the both overall and statistically significant LISA plots for both the queen and rook contiguity method.\n\nInteractive Map of LISA - QueenInteractive Map of LISA - Rook\n\n\n\n\nCode\ntmap_mode(\"view\")\n# Filter significant districts\nlisa_queen_pois &lt;- lisa_queen_pois %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\n# First map: Overall LISA Spatial Autocorrelation\nlisa_cat &lt;- tm_shape(lisa_queen_pois) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"Overall LISA Classification\",\n          id = \"label\") \n\n# Second map: Statistical Significant LISA Map\nlisa_queen_pois_sig &lt;- lisa_queen_pois_sig %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\nlisa_cat_sig &lt;- tm_shape(lisa_queen_pois) +\n  tm_polygons(id = \"\") +  \n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_queen_pois_sig) +\n  tm_polygons(\"mean\", \n          palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n          title = \"Significant LISA Classification\",\n          id = \"label\")\ntmap_arrange(lisa_cat, lisa_cat_sig, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n\n\nCode\ntmap_mode(\"view\")\n# Filter significant districts\nlisa_rook_pois &lt;- lisa_rook_pois %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\n# First map: Overall LISA Spatial Autocorrelation\nlisa_cat &lt;- tm_shape(lisa_rook_pois) +\n  tm_polygons(\"mean\", \n              palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n              title = \"Overall LISA Classification\",\n              id = \"label\") \n\n# Second map: Statistical Significant LISA Map\nlisa_rook_pois_sig &lt;- lisa_rook_pois_sig %&gt;%\n  mutate(label = paste(\"District:\", district, \"| \", mean))\n\nlisa_cat_sig &lt;- tm_shape(lisa_rook_pois) +\n  tm_polygons(id = \"\") +  \n  tm_borders(col = \"black\", alpha = 0.6) +\n  tm_shape(lisa_rook_pois_sig) +\n  tm_polygons(\"mean\", \n              palette = c(\"lightblue1\", \"#ec9a64\", \"green3\", \"#d21b1c\"),\n              title = \"Significant LISA Classification\",\n              id = \"label\")\n\ntmap_arrange(lisa_cat, lisa_cat_sig, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")"
  }
]