[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "Hey there! 👋 I’m Samantha Foo, a Year 3 SMU student pursuing a Bachelor in Information Systems (Business Analytics) with a 2nd Major in Data Science & Analytics. I’m a huge data enthusiast, a FOOdie and I enjoy exploring new places/cities!\nJoin me on this data odyssey as I conquer Geospatial Analytics! 🌱\n\n\n\n\n\nAug 16, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 1\nGeospatial Wrangling with R\n\n\n\n\n\n\nAug 22, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 2\nChoropleth Mapping and GeoVisualisation with R\n\n\n\n\nAug 27, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 3\n1st and 2nd Order Spatial Point Patterns Analysis Methods\n\n\n\n\nSep 10, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 5\nSpatial Weights and Applications\n\n\n\n\nSep 18, 2024\nFoo Jia Yi Samantha\n\nHands-on Exercise 6\nGlobal and Local Measures of Spatial Autocorrelation\n\n\n\n\nAug 19, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 1\nSet up RStudio and Create a Quarto Document\n\n\n\n\nAug 26, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 2\nImporting Libraries and Data into R, and Performing Data Wrangling\n\n\n\n\nSep 2, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 3\nCovering Errors in Hands-on Exercise 3 and Importing ACLED Datasets\n\n\n\n\nSep 10, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 4\nExploring Spatio-Temporal Kernel Density Estimation\n\n\n\n\nSep 18, 2024\nFoo Jia Yi Samantha\n\nIn-class Exercise 5\nExploring Geographically Weighted Models"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "Hey there! 👋 I’m Samantha Foo, a Year 3 SMU student pursuing a Bachelor in Information Systems (Business Analytics) with a 2nd Major in Data Science & Analytics. I’m a huge data enthusiast, a FOOdie and I enjoy exploring new places/cities!\nJoin me on this data odyssey as I conquer Geospatial Analytics! 🌱"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "University of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "IS415: Geospatial Analytics and Application",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nIn this hands-on exercise, I will be performing geospatial data science tasks in R by using the sf and tidyverse R packages. By the end of this hands-on exercise, I would have acquired the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#lets-set-up",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#lets-set-up",
    "title": "Hands-on Exercise 1",
    "section": "2. Let’s Set Up!",
    "text": "2. Let’s Set Up!\n\n2.1 Data Acquisition\nData are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data. With that said, I have extracted the data sets from the following four sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\nI will be tapping on these vastly available, public data from the government and private sectors for future exercises ahead!\n\n\n\n2.2 Set Up the Folders\nThis is the file structure for containing the data files that I have extracted in the previous step. The Hands-on_Ex1 folder consists of a data sub-folder, and is further separated by the geospatial and aspatial folders.\n\n\n\n\n\n\n\n2.3 Installing R Packages\nIn this exercise, I will be using these two R packages\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nWith that said, I installed the required packages using the code chunk below.\n\npacman::p_load(sf, tidyverse)\n\n\np_load is a function of the pacman package that is used to install and load sf and tidyverse packages into our R environment."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data-into-r",
    "title": "Hands-on Exercise 1",
    "section": "3. Importing Geospatial Data into R",
    "text": "3. Importing Geospatial Data into R\nIn this section, I will import the following geospatial data into R by using st_read() of the sf package:\n\nMP14_SUBZONE_WEB_PL: a polygon feature layer in ESRI shapefile format,\nCyclingPath: a line feature layer in ESRI shapefile format, and\nPreSchool: a point feature layer in kml file format.\n\n\n3.1 Importing Polygon Feature Data in .shp Format\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile (.shp) into R as a polygon feature data frame.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n🔎 Observations: the mpsz simple feature data frame contains 323 multipolygon features, 15 fields and is in the SVY21 projected coordinates system.\n\n\n💡 Note: dsn defines folder path and layer defines file name (AKA a shapefile, no need any extension like .shp)\n\n\n\n3.2 Importing Polyline Feature Data in .shp Format\nThe code chunk below imports CyclingPath shapefile (.shp) into R as a polyline feature data frame.\n\ncyclingpath = st_read(dsn = \"data/geospatial\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3138 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42644.17 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\n🔎 Observations: the cyclingpath linestring feature data frame contains 3138 features and 2 fields and it is in the SVY21 projected coordinates system.\n\n\n\n3.3 Importing GIS Point Feature Data in .kml Format\nThe code chunk below imports PreSchoolsLocation.kml kml format into R as a point feature data frame.\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n🔎 Observations: the PreSchoolsLocation.kml point feature data frame contains 2290 point features, 2 fields and is in the WGS84 projected coordinates"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-aspatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-aspatial-data-into-r",
    "title": "Hands-on Exercise 1",
    "section": "4. Importing Aspatial Data into R",
    "text": "4. Importing Aspatial Data into R\n\n4.1 Importing Aspatial Data\nNotice that the listings data set is in csv file format. Instead of st_read(), we’ll use read_csv() from the readr package to import listings.csv.\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nRows: 3540 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (6): name, host_name, neighbourhood_group, neighbourhood, room_type, l...\ndbl  (11): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nThis outputs an R object called listings which is a tibble data frame.\n\nLet’s take a peak into our listings tibble data frame.\n\nglimpse(listings)\n\nRows: 3,540\nColumns: 18\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Ensuite Room (Room 1 & 2) near EXPO\", …\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ latitude                       &lt;dbl&gt; 1.34537, 1.34754, 1.34531, 1.29015, 1.2…\n$ longitude                      &lt;dbl&gt; 103.9589, 103.9596, 103.9610, 103.8081,…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; NA, 80, 80, 50, 50, NA, 85, 65, 45, 54,…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 180, 180, 92, 92, 180, 180,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 60,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0.11, 0.08, 0.8…\n$ calculated_host_listings_count &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49, 49, 6, 7, 7,…\n$ availability_365               &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 365, 0, 0, 365,…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n\n\n\n🔎 Observations: there are 3540 rows and 18 columns (not features and fields like in our simple data feature frame!)\n\n\n💡 Note: we’ll be using the latitude and longitude fields in the next phase. These fields appear to be adopting the WGS84 geographic coordinate system.\n\n\n\n4.2 Converting Aspatial Data\nNext, we’ll convert listing (a non-geospatial tabular data frame) into a simple feature data frame by using st_as_sf() from the sf package.\n\n💡 Note: a non-simple feature data frame will simply not have a “geometry” column. Use class(listings) as a simple test - if it outputs data.frame, tbl_df, tbl, etc and no sf, then it’s not a simple feature data frame!\n\n\nlistings_sf &lt;- st_as_sf(listings, coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;% st_transform(crs = 3414)\n\n\ncoordscrs%&gt;%\n\n\nIndicates the column name of the x-coordinates, followed by that of the y-coordinates.\n\n\nIndicates the coordinates system in epsg format (more info: epsg.io)\n\nEPSG: 4326 is WGS84 Geographic Coordinate System\nEPSG: 3414 is Singapore SVY21 Projected Coordinate System\n\n\n\nTo nest st_transform() and transform the newly created simple feature data frame into SVY21 Projected Coordinate System\n\n\n\nThis gives us the new simple feature data frame, listings_sf:\n\nglimpse(listings_sf)\n\nRows: 3,540\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Ensuite Room (Room 1 & 2) near EXPO\", …\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; NA, 80, 80, 50, 50, NA, 85, 65, 45, 54,…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 180, 180, 92, 92, 180, 180,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 60,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0.11, 0.08, 0.8…\n$ calculated_host_listings_count &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49, 49, 6, 7, 7,…\n$ availability_365               &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 365, 0, 0, 365,…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\n\n🔎 Observations:\nNotice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been removed from the data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#exploring-contents-of-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#exploring-contents-of-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 1",
    "section": "5. Exploring Contents of a Simple Feature Data Frame",
    "text": "5. Exploring Contents of a Simple Feature Data Frame\nThere are 3 ways to explore the contents of a simple feature data frame like mpsz!\n\n5.1 Using st_geometry()\nThe sf data.frame contains a geometry column, that is a list of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry().\n\n# Retrieve geometry column\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\n🔎 Observations: This MP14_SUBZONE_WEB_PL file consists of 323 features, consisting of multipolygon features.\n\n\n\n5.2 Using glimpse()\nWe use glimpse() from the dplyr package to understand the data type of each fields.\nE.g. FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are in double-precision values.\n\n# Get data types\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\n🔎 Observations: This MP14_SUBZONE_WEB_PL file consists of 323 rows and 16 columns with datatypes ranging from integers, characters, date and doubles.\n\n\n\n5.3 Using head()\nWe use head() from the base R package to get the full information of the feature object mpsz. The n value indicates the no. of rows.\n\n# Display top 5 rows of the feature object\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n🔎 Observations: We can notice that the MP14_SUBZONE_WEB_PL file consists of 5 features and 15 fields, with the top 5 rows per column."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1",
    "section": "6. Plotting the Geospatial Data",
    "text": "6. Plotting the Geospatial Data\nIn geospatial data science, looking at feature information is not sufficient. We are also interested in visualising the geospatial features of the sf object, in which plot() will help with that.\n\n# Plot multi-plot of all attributes\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above.\n\nWe can, however, choose to plot the geometry only as such:\n\n# Plot the geometry only\nplot(st_geometry(mpsz))\n\n\n\n\n\n\n\n\nOr, plot the sf object using a specific attribute\n\n# Plot a specific attribute\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\n\n💡 Note: plot() is meant for plotting the geospatial object at a high level. For high cartographic quality plot, other R package such as tmap should be used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#working-with-projection",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#working-with-projection",
    "title": "Hands-on Exercise 1",
    "section": "7. Working with Projection",
    "text": "7. Working with Projection\nWhat is “map projection”?: it is an important property of geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, I project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n7.1 Assigning EPSG code to a simple feature data frame\nDefine “ESPG code”: a unique identifier to represent coordinate systems.\nCommon issues when importing geospatial data into R : the coordinate system of the source data are either…\n\nMissing (such as due to missing .proj for ESRI shapefile)\nWrongly assigned\n\nTo check the coordinate system of mpsz simple feature data frame, I’ll use st_crs() from the sf package.\n* crs = Coordinate Reference System\n\n# Check coordinate system\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n🔎 Observations Notice the mpsz data is a SVY21 projected coordinate system. However, the ESPG code is wrongly indicated as 9001 in the last few lines. The correct ESPG code for SVY21 should be 3114. Thus, we’ll assign the correct code as such.\n\n\n# Assign new ESPG code\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n# Check that crs has been updated to 3414\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n7.2 Converting Data from Geographic to Projected Coordinate System\nRecall that the geographic coordinate system (e.g., WGS84) is not appropriate for analyses that involve distance/area. Hence, it’s common for us to transform the original data to a projected coordinate system.\nLet’s take a look at the preschool simple feature data frame. It shows that it is in the WGS84 coordinate system, i.e., geographic coordinate system.\n\n# Transform projection\nst_geometry(preschool)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT Z (103.8072 1.299333 0)\n\n\nPOINT Z (103.826 1.312839 0)\n\n\nPOINT Z (103.8409 1.348843 0)\n\n\nPOINT Z (103.8048 1.435024 0)\n\n\nPOINT Z (103.839 1.33315 0)\n\n\nNow, we’ll transform preschool’s coordinate system from geographic (WGS84) to projected (SVY21).\n\npreschool3414 &lt;- st_transform(preschool, crs = 3414)\n\n\n🔎 Observations: Notice that the last row shows “Projected CRS” now"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 1",
    "section": "8. Geoprocessing with sf Package",
    "text": "8. Geoprocessing with sf Package\nBesides providing functions to handling geospatial data (i.e. importing, exporting, assigning projection, transforming projection etc), sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, I perform two commonly-used geoprocessing functions, namely buffering and point in polygon count.\n\n8.1 Buffering\n📝The scenario: The authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on both sides of the current cycling path. You are tasked to determine the extend of the land needed to be acquired and their total area.\n💡The solution:\nFirstly, st_buffer() of the sf package is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, dist = 5, nQuadSegs = 30)\n\n\nA higher nQuadSegs value results in a smoother and more accurate circular buffer. The default is 30.\n\nThis is followed by calculating the area of the buffers\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n2218855 [m^2]\n\n\n\n\n8.2 Point-in-polygon count\n📝The scenario: A pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\n💡The solution:\nFirstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate the no. of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, top_n() of the dplyr package is used.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nNext, I calculate the density of pre-school by planning subzone. I used st_area() of the sf package to derive the area of each planning subzone.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%   st_area()\n\nNext, I used mutate()of the dplyr package to compute the density by using the code chunk below.\n\nmpsz3414 &lt;- mpsz3414 %&gt;% mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 1",
    "section": "9. Exploratory Data Analysis (EDA)",
    "text": "9. Exploratory Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, I will tap on ggplot2() functions to create functional yet transparent statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\n\n\nAlthough the syntax is very easy to use, the output is currently far from meeting publication quality. Furthermore, hist() function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2() functions will be used.\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n\n\nUsing ggplot2 method, I plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "title": "Hands-on Exercise 2",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nThematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices.\nGeovisualisation works by providing graphical ideation to render a place, phenomenon or a process.\nIn this hands-on exercise, I will learn how to plot functional and truthful chloropleth maps by using the tmap R package. The output of this exercise should look like thisL"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#lets-set-up",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#lets-set-up",
    "title": "Hands-on Exercise 2",
    "section": "2. Let’s Set Up!",
    "text": "2. Let’s Set Up!\n\n2.1 Importing Libraries into R\nIn this hands-on exercise, the key R package use is tmap package in R, alongside these four other R packages:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package. Hence, we will only need to install the tidyverse package.\n\nNow, let’s install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n2.2 Download Data and Set Up Folders\nWe will be using two data sets to create the choropleth maps\n1) Master Plan 2014 Subzone Boundary (Web): geospatial data consisting of the geographical boundary of Singapore at the planning subzone level.\n📅 The data is based on URA Master Plan 2014.\n📁 ESRI shapefile format (i.e. MP14_SUBZONE_WEB_PL)\n🔗 Can be downloaded at data.gov.sg\n2) Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling: aspatial data file. Although it does not contain any coordinates values, the PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n📅 June 2011-2020\n📁 csv format (i.e. respopagesextod2011to2020.csv)\n🔗 Can be downloaded at Department of Statistics, Singapore\nThis is the file structure for containing the data files that I have extracted.\n\n\n\n2.3 Importing Data into R\n\n2.3.1 Importing Geospatial Data into R\nNow, we’ll use the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n# Import shapefile\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n# Inspect shapefile\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n🔎 Observations: The MP14_SUBZONE_WEB_PL data set consists of 323 features and 15 fields made up of multipolygon features.\n\n\n\n2.3.2 Importing Aspatial (Attribute) Data into R\nFor aspatial datasets like respopagsex2011to2020.csv, we will import into Rstudio using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n🔎 Observations: The respopagsex2011to2020.csv data follows the SVY21 projected coordinate which contains 984656 rows and 7 columns"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#data-preparation-and-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#data-preparation-and-wrangling",
    "title": "Hands-on Exercise 2",
    "section": "3. Data Preparation and Wrangling",
    "text": "3. Data Preparation and Wrangling\nBefore a thematic map can be prepared, we will need to prepare a data table with values from 2020 which includes these variables:\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n3.1 Data Wrangling\nIn order to carry out necessary data wrangling and transformation, the following functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n🔎 Observations: Notice that we have filtered our population data from 2020 and successfully grouped them by PA, SZ and AG which sums up the population within each category. I’ve also summed up the rows for ECONOMY ACTIVE, AGED and TOTAL, and created a new DEPENDENCY column which takes the sum of YOUNG and AGED, and then divide that sum by the value of ECONOMY ACTIVE.\n\n\n\n3.2 Joining Geospatial Data and Attribute Data\nBefore we can perform the georelational join, we are required to convert the values in PA and SZ fields to uppercase to ensure consistency with the uppercase values in SUBZONE_N and PLN_AREA_N.\nHence, we will standardise the data values in these two fields.\n\n# Convert to uppercase\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 2",
    "section": "4. Choropleth Mapping Geospatial Data Using tmap",
    "text": "4. Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors.\n📖 Scenario: A social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements, i.e. tm_shape()\n\n\n4.1 Method 1: Plotting a Choropleth Map quickly using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\n# Plot choropleth map using qtm()\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n💡 Note:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n4.2 Method 2: Plotting a Choropleth Map quickly using tm_shape()\n\n# Plot choropleth map using tmap's elements\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nStep 1: Drawing a Base Map Using tm_shape()\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\n\ntm_shape(mpsz_pop2020) + tm_polygons()\n\n\n\n\n\n\n\n\n\n💡 Note: tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\n\nStep 2: Drawing a Choropleth Map Using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020) + tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nStep 3: Drawing a Choropleth Map Using tm_fill() and tm_border()\nFirstly, we will try to draw a choropleth map by using tm_fill() alone.\n\n💡 Note: tm_polygons() is a wrapper of tm_fill() and tm_border()\ntm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\n\n\n# Add fill\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown below.\n\n# Add boundary\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n💡 Note: Notice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). Default alpha value is 1.\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width (default is 1)\nlty = border line type (default is “solid”)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#data-classification-methods-of-tmap",
    "title": "Hands-on Exercise 2",
    "section": "5. Data Classification Methods of tmap",
    "text": "5. Data Classification Methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\n\ntmap provides a total ten data classification methods, namely:\n\nfixed,\nsd,\nequal,\npretty (default),\nquantile,\nkmeans,\nhclust,\nbclust,\nfisher,\njenks.\n\n\n\n5.1 Plotting Choropleth Maps with Built-in Classification Methods\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used. The code chunks below uses 5 classes where, n = 5.\n\n💡 There are 10 types of styles: jenks, equal, fixed, sd, pretty (default), quantile, kmeans, fisher, hclust and bclust\n\n\n1) jenks\nFirstly, we’ll use the jenks style method. It is known as natural breaks and is based on natural groupings inherent in the data. Data is clustered into groups that minimise the within-group variance and maximise the between-group variance.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n2) equal\nNext, we will try equal data classification method. This creates a more even distribution as shown.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n💡 Note: not compatible for data that are highly-skewed or with one or two large outliers\n\n\n\n3) sd\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"sd\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n💡 Note: Should only use if the distribution resembles a normal distribution (bell-curve)!\n\n\n\n4) kmeans\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"kmeans\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n5) fisher\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"fisher\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n💡 Note: At a glance, using Fisher and KMeans lead to similar visualisations.\n\n\n\n4) hclust\nhclust is hierarchical clustering used to create a hierarchy of clusters based on their similarity. Each data point starts as an individual cluster and then progressively merges or splits clusters until a stopping criterion is met.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"hclust\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n5) bclust\nbclust is bagged clustering which creates multiple subsets of the original dataset through resampling. Each subset is then used to train an individual clustering model, and the final cluster assignments are obtained by combining the results from all models.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"bclust\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\n\n\n\n5.2 Plotting Choropleth Maps with Custom Breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument in tm_fill().\n\n💡 Note: in tmap, the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\n\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nLooking at the summary statistics, the break point can be set to 0.60, 0.70, 0.80, and 0.90. The minimum and maximum breaks must also be included, which are 0 and 100 respectively.\n\n# Using this information, we will now proceed to plot the choropleth map.\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          palette=\"plasma\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\n\n\n💡 Observations: the legend has now been categorised according to the breaks vector, c(0, 0.60, 0.70, 0.80, 0.90, 1.00).\n\n\n\n5.3 Customising Colour Schemes\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\nTo change the colour, we assign the preferred colour to the palette argument of tm_fill() as shown below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"plasma\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nAdd a “-” prefix to reverse the colour shading.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#controlling-and-customising-map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#controlling-and-customising-map-layouts",
    "title": "Hands-on Exercise 2",
    "section": "6. Controlling and Customising Map Layouts",
    "text": "6. Controlling and Customising Map Layouts\n\n6.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            #legend.height = 0.45, \n            #legend.width = 0.35,\n            legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n6.2 Map Style\nThe layout of the map can also be adjusted using tmap_style(). E.g. Classic\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n💡 Observations: the classic tmap style creates a border with double lines, the colours used are more muted and neutral, and the font has been changed to something more elegant\n\n\n\n6.3 Cartographic Furniture\ntmap also provides arguments to draw other important map elements like compass, scale bar and grid lines.\nTo add compass, scale and gridlines, pay attention to how tm_compass(), tm_scale_bar() and tm_grid() are used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n6.4 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred as facet maps, comprise of many adjacent maps. These facets enable easier visualisation of how spatial relationships change with respect to another variable. Such as, time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nMethod 1: By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n💡 Observations: two choropleth maps been generated to represent the Young and Aged demographics respectively.\n\nAdditionally, the style and palette arguments can be adjusted accordingly.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nMethod 2: By defining a group-by variable in tm_facets()\ntm_facets() can help to group categorical data like regions and subzone areas such that the generated facet maps will zoom in to the specified variable.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated\n\n\n\n\n\n\n\n\n\n\n💡 Observations: we have generated 5 different choropleth maps that represent the 5 unique regions found in the REGION_N data variable!\n\n\n\nMethod 3: By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"viridis\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"plasma\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n💡 Observations: as compared to the charts generated in Method 1, writing two tm_shape() functions allows us to create two separate choropleth maps produced as seen above."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#mappping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#mappping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-on Exercise 2",
    "section": "7. Mappping Spatial Object Meeting a Selection Criterion",
    "text": "7. Mappping Spatial Object Meeting a Selection Criterion\nMap outputs can also be targeted by using selection functions to meet the selection criterion. For example, we have selected the central region and DEPENDENCY column to plot.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n💡 Note: In order to only display data from the Central Region, we need to filter the mpsz_pop2020 data frame via mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ]"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex1/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#getting-started",
    "title": "Hands-on Exercise 2",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nThematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices.\nGeovisualisation works by providing graphical ideation to render a place, phenomenon or a process.\nIn this hands-on exercise, I will learn how to plot functional and truthful chloropleth maps by using the tmap R package. The output of this exercise should look like thisL"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#lets-set-up",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#lets-set-up",
    "title": "Hands-on Exercise 2",
    "section": "2. Let’s Set Up!",
    "text": "2. Let’s Set Up!\n\n2.1 Importing Libraries into R\nIn this hands-on exercise, the key R package use is tmap package in R, alongside these four other R packages:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package. Hence, we will only need to install the tidyverse package.\n\nNow, let’s install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n2.2 Download Data and Set Up Folders\nWe will be using two data sets to create the choropleth maps\n1) Master Plan 2014 Subzone Boundary (Web): geospatial data consisting of the geographical boundary of Singapore at the planning subzone level.\n📅 The data is based on URA Master Plan 2014.\n📁 ESRI shapefile format (i.e. MP14_SUBZONE_WEB_PL)\n🔗 Can be downloaded at data.gov.sg\n2) Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling: aspatial data file. Although it does not contain any coordinates values, the PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n📅 June 2011-2020\n📁 csv format (i.e. respopagesextod2011to2020.csv)\n🔗 Can be downloaded at Department of Statistics, Singapore\nThis is the file structure for containing the data files that I have extracted in the previous step.\n\n\n\n2.3 Importing Data into R\n\n2.3.1 Importing Geospatial Data into R\nNow, we’ll use the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n# Import shapefile\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n# Inspect shapefile\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n🔎 Observations: The MP14_SUBZONE_WEB_PL data set consists of 323 features and 15 fields made up of multipolygon features.\n\n\n\n2.3.2 Importing Aspatial (Attribute) Data into R\nFor aspatial datasets like respopagsex2011to2020.csv, we will import into Rstudio using read_csv() function of readr package.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n🔎 Observations: The respopagsex2011to2020.csv data follows the SVY21 projected coordinate which contains 984656 rows and 7 columns"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#data-preparation-and-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#data-preparation-and-wrangling",
    "title": "Hands-on Exercise 2",
    "section": "3. Data Preparation and Wrangling",
    "text": "3. Data Preparation and Wrangling\nBefore a thematic map can be prepared, we will need to prepare a data table with values from 2020 which includes these variables:\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n3.1 Data Wrangling\nIn order to carry out necessary data wrangling and transformation, the following functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n🔎 Observations: Notice that we have filtered our population data from 2020 and successfully grouped them by PA, SZ and AG which sums up the population within each category. I’ve also summed up the rows for ECONOMY ACTIVE, AGED and TOTAL, and created a new DEPENDENCY column which takes the sum of YOUNG and AGED, and then divide that sum by the value of ECONOMY ACTIVE.\n\n\n\n3.2 Joining Geospatial Data and Attribute Data\nBefore we can perform the georelational join, we are required to convert the values in PA and SZ fields to uppercase to ensure consistency with the uppercase values in SUBZONE_N and PLN_AREA_N.\nHence, we will standardise the data values in these two fields.\n\n# Convert to uppercase\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 2",
    "section": "4. Choropleth Mapping Geospatial Data Using tmap",
    "text": "4. Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors.\n📖 Scenario: A social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n4.1 Method 1: Plotting a Choropleth Map quickly using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n💡 Note:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n4.2 Method 2: Plotting a Choropleth Map quickly using tmap’s elements\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n4.3 Drawing a Base Map Using tm_shape()\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\n\ntm_shape(mpsz_pop2020) + tm_polygons()\n\n\n\n\n\n\n\n\n\n💡 Note: tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\n\n4.4 Drawing a Choropleth Map Using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020) + tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n4.5 Drawing a Choropleth Map Using tm_fill() and tm_border()\nFirstly, we will try to draw a choropleth map by using tm_fill() alone.\n\n💡 Note: tm_polygons() is a wrapper of tm_fill() and tm_border()\ntm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\n\n\n# Add fill\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown below.\n\n# Add boundary\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n💡 Note: Notice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). Default alpha value is 1.\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width (default is 1)\nlty = border line type (default is “solid”)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#data-classification-methods-of-tmap",
    "title": "Hands-on Exercise 2",
    "section": "5. Data Classification Methods of tmap",
    "text": "5. Data Classification Methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\n\ntmap provides a total ten data classification methods, namely:\n\nfixed,\nsd,\nequal,\npretty (default),\nquantile,\nkmeans,\nhclust,\nbclust,\nfisher,\njenks.\n\n\n\n5.1 Plotting Choropleth Maps with Built-in Classification Methods\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used. The code chunks below uses 5 classes where, n = 5.\n\n💡 There are 10 types of styles: jenks, equal, fixed, sd, pretty (default), quantile, kmeans, fisher, hclust and bclust\n\n\n1) jenks\nFirstly, we’ll use the jenks style method. It is known as natural breaks and is based on natural groupings inherent in the data. Data is clustered into groups that minimise the within-group variance and maximises the between-group variance.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n2) equal\nNext, we will try equal data classification method. This creates a more even distribution as shown.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          palette = \"plasma\",\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n3) sd\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"sd\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n4) kmeans\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"kmeans\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n5) fisher\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"fisher\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n💡 Note: At a glance, using Fisher and KMeans lead to similar visualisations.\n\n\n\n4) hclust\nhclust is hierarchical clustering used to create a hierarchy of clusters based on their similarity. Each data point starts as an individual cluster and then progressively merges or splits clusters until a stopping criterion is met.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"hclust\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n5) bclust\nbclust is bagged clustering which creates multiple subsets of the original dataset through resampling. Each subset is then used to train an individual clustering model, and the final cluster assignments are obtained by combining the results from all models.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n=5,\n          style=\"bclust\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\n\n\n\n5.2 Plotting Choropleth Maps with Custom Breaks\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nLooking at the summary statistics, the break point can be set to 0.60, 0.70, 0.80, and 0.90. The minimum and maximum breaks must also be included, which are 0 adn 100 respectively. These would translate to the breaks vector, c(0, 0.60, 0.70, 0.80, 0.90, 1.00).\n\n# Using this information, we will now proceed to plot the choropleth map.\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          palette=\"plasma\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\n\n\n\n5.3 Customising Colour Schemes\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\nTo change the colour, we assign the preferred colour to the palette argument of tm_fill() as shown below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"plasma\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nAdd a “-” prefix to reverse the colour shading.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#controlling-and-customising-map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#controlling-and-customising-map-layouts",
    "title": "Hands-on Exercise 2",
    "section": "6. Controlling and Customising Map Layouts",
    "text": "6. Controlling and Customising Map Layouts\n\n6.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            #legend.height = 0.45, \n            #legend.width = 0.35,\n            legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n6.2 Map Style\nThe layout of the map can also be adjusted using tmap_style(). E.g. Classic\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n6.3 Cartographic Furniture\ntmap also provides arguments to draw other important map elements like compass, scale bar and grid lines.\nTo add compass, scale and gridlines, pay attention to how tm_compass(), tm_scale_bar() and tm_grid() are used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n6.4 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred as facet maps, comprise of many adjacent maps. These facets enable easier visualisation of how spatial relationships change with respect to another variable. Such as, time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nMethod 1: By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\nAdditionally, the style and palette arguments can be adjusted accordingly.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nMethod 2: By defining a group-by variable in tm_facets()\ntm_facets() can help to group categorical data like regions and subzone areas such that the generated facet maps will zoom in to the specified variable.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated\n\n\n\n\n\n\n\n\n\n\n\nMethod 3: By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"viridis\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"plasma\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Test.html#mappping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex2/Test.html#mappping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-on Exercise 2",
    "section": "7. Mappping Spatial Object Meeting a Selection Criterion",
    "text": "7. Mappping Spatial Object Meeting a Selection Criterion\nMap outputs can also be targeted by using selection functions to meet the selection criterion. For example, we have selected the central region and DEPENDENCY column to plot.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"plasma\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex1/data/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-libraries-into-r",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-libraries-into-r",
    "title": "In-class Exercise 2",
    "section": "1. Importing Libraries into R",
    "text": "1. Importing Libraries into R\nIn this exercise, we will use the following R packages:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\ntidyverse for tidying data (https://tidyr.tidyverse.org/)\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-data-sets-into-r",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-data-sets-into-r",
    "title": "In-class Exercise 2",
    "section": "2. Importing Data Sets into R",
    "text": "2. Importing Data Sets into R\nWe will first import the three geospatial data sets into R using st_read() of the sf package.\n\n2.1 Importing Polygon Feature Data in .shp Format\n\n# Import shapefile\nmpsz14_shp &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n# Retrieve geometry column\nst_geometry(mpsz14_shp)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\n# Check class\nclass(mpsz14_shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\n\n\n2.2 Importing Polygon Feature Data in .kml Format\n\n# Import KML file\n#mpsz14_kml &lt;- st_read(\"data/geospatial/MP14_SUBZONE_WEB_PL.kml\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-polygon-feature-data-in-.shp-format",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-polygon-feature-data-in-.shp-format",
    "title": "In-class Exercise 2",
    "section": "2.1 Importing Polygon Feature Data in .shp Format",
    "text": "2.1 Importing Polygon Feature Data in .shp Format\n\n# Import shapefile\nmpsz14_shp &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n# Import KML file\nst_write(mpsz14_shp, \n         \"data/geospatial/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)\n\nWarning in CPL_write_ogr(obj, dsn, layer, driver,\nas.character(dataset_options), : GDAL Error 4: Unable to open\ndata/geospatial/MP14_SUBZONE_WEB_PL.kml to obtain file list.\n\n\nDeleting source `data/geospatial/MP14_SUBZONE_WEB_PL.kml' failed\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/geospatial/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n\nThere are 3 ways to explore the contents of a simple feature data frame like mpsz!\n\n1) Using st_geometry()\nThe sf data.frame contains a geometry column, that is a list of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry().\n\n# Retrieve geometry column \nst_geometry(mpsz14_shp)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\n🔎 Observations: This MP14_SUBZONE_WEB_PL file consists of 323 features, consisting of multipolygon features.\n\n\n\n2) Using glimpse()\nWe use glimpse() from the dplyr package to understand the data type of each fields.\nE.g. FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are in double-precision values.\n\n# Get data types \nglimpse(mpsz14_shp)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\n🔎 Observations: This MP14_SUBZONE_WEB_PL file consists of 323 rows and 16 columns with datatypes ranging from integers, characters, date and doubles.\n\n\n\n3) Using head()\nWe use head() from the base R package to get the full information of the feature object mpsz. The n value indicates the no. of rows.\n\n# Display top 5 rows of the feature object \nhead(mpsz14_shp, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n🔎 Observations: We can notice that the MP14_SUBZONE_WEB_PL file consists of 5 features and 15 fields, with the top 5 rows per column."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#using-glimpse",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#using-glimpse",
    "title": "In-class Exercise 2",
    "section": "2) Using glimpse()",
    "text": "2) Using glimpse()\nWe use glimpse() from the dplyr package to understand the data type of each fields.\nE.g. FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are in double-precision values.\n\n# Get data types \nglimpse(mpsz14_shp)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\n🔎 Observations: This MP14_SUBZONE_WEB_PL file consists of 323 rows and 16 columns with datatypes ranging from integers, characters, date and doubles.\n\n\n3) Using head()\nWe use head() from the base R package to get the full information of the feature object mpsz. The n value indicates the no. of rows.\n\n# Display top 5 rows of the feature object \nhead(mpsz14_shp, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n🔎 Observations: We can notice that the MP14_SUBZONE_WEB_PL file consists of 5 features and 15 fields, with the top 5 rows per column.\n\n\n\n2.2 Importing Polygon Feature Data in .kml Format\n\n# Import KML file\n#mpsz14_kml &lt;- st_read(\"data/geospatial/MP14_SUBZONE_WEB_PL.kml\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-polygon-feature-data-in-.kml-format",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-polygon-feature-data-in-.kml-format",
    "title": "In-class Exercise 2",
    "section": "2.2 Importing Polygon Feature Data in .kml Format",
    "text": "2.2 Importing Polygon Feature Data in .kml Format\n\n💡 Note: delete_dsn = TRUE will help delete the original data before rendering it"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-polygon-data-in-.shp-format",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-polygon-data-in-.shp-format",
    "title": "In-class Exercise 2",
    "section": "2.1 Importing MP14_SUBZONE_WEB_PL (Web) Polygon Data in .shp Format",
    "text": "2.1 Importing MP14_SUBZONE_WEB_PL (Web) Polygon Data in .shp Format\n\n# Import shapefile\nmpsz14_shp &lt;- st_read(dsn = \"data\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThere are 3 ways to explore the contents of a simple feature data frame like mpsz!\n\n1) Using st_geometry()\nThe sf data.frame contains a geometry column, that is a list of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry().\n\n# Retrieve geometry column \nst_geometry(mpsz14_shp)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\n🔎 Observations: This MP14_SUBZONE_WEB_PL file consists of 323 features, consisting of multipolygon features.\n\n\n\n2) Using glimpse()\nWe use glimpse() from the dplyr package to understand the data type of each fields.\nE.g. FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are in double-precision values.\n\n# Get data types \nglimpse(mpsz14_shp)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\n🔎 Observations: This MP14_SUBZONE_WEB_PL file consists of 323 rows and 16 columns with datatypes ranging from integers, characters, date and doubles.\n\n\n\n3) Using head()\nWe use head() from the base R package to get the full information of the feature object mpsz. The n value indicates the no. of rows.\n\n# Display top 5 rows of the feature object \nhead(mpsz14_shp, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n🔎 Observations: We can notice that the MP14_SUBZONE_WEB_PL file consists of 5 features and 15 fields, with the top 5 rows per column."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-polygon-data-in-.kml-format",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-polygon-data-in-.kml-format",
    "title": "In-class Exercise 2",
    "section": "2.2 Importing MP14_SUBZONE_WEB_PL (Web) Polygon Data in .kml Format",
    "text": "2.2 Importing MP14_SUBZONE_WEB_PL (Web) Polygon Data in .kml Format\nWe use the below code chunk to export mpsz14_shp sf data.frame into kml file which saves the file into our data folder.\n\n# Convert .shp file into .kml\nst_write(mpsz14_shp, \n         \"data/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)\n\nDeleting source `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n# Import KML file\nmpsz14_kml = st_read(\"data/MP14_SUBZONE_WEB_PL.kml\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data\\MP14_SUBZONE_WEB_PL.kml' \n  using driver `KML'\nSimple feature collection with 323 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n# Display top 5 rows of the feature object \nhead(mpsz14_kml, n=5)  \n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.8142 ymin: 1.272838 xmax: 103.8725 ymax: 1.291523\nGeodetic CRS:  WGS 84\n  Name Description                       geometry\n1                  MULTIPOLYGON (((103.8647 1....\n2                  MULTIPOLYGON (((103.8431 1....\n3                  MULTIPOLYGON (((103.8507 1....\n4                  MULTIPOLYGON (((103.8255 1....\n5                  MULTIPOLYGON (((103.8194 1....\n\n\n\n💡 Note: delete_dsn = TRUE will help delete the original data before rendering it"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp19_subzone_web_pl-no-sea-.shp-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp19_subzone_web_pl-no-sea-.shp-data",
    "title": "In-class Exercise 2",
    "section": "2.3 Importing MP19_SUBZONE_WEB_PL (No Sea) .shp Data",
    "text": "2.3 Importing MP19_SUBZONE_WEB_PL (No Sea) .shp Data\n\n# Import shapefile\nmpsz19_shp &lt;- st_read(dsn = \"data\", layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n🔎 Observations: We can notice that the data file consists of 332 features and 6 fields, and follows the WGS64 coordinate system. Here we can notice it uses the `ESRI Shapefile’ driver."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp19_subzone_web_pl-no-sea-.kml-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp19_subzone_web_pl-no-sea-.kml-data",
    "title": "In-class Exercise 2",
    "section": "2.4 Importing MP19_SUBZONE_WEB_PL (No Sea) .kml Data",
    "text": "2.4 Importing MP19_SUBZONE_WEB_PL (No Sea) .kml Data\n\n# Convert .shp file into .kml\nst_write(mpsz19_shp, \n         \"data/MP19_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)\n\nDeleting source `data/MP19_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP19_SUBZONE_WEB_PL' to data source \n  `data/MP19_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 332 features with 6 fields and geometry type Multi Polygon.\n\n# Import KML file\nmpsz19_kml = st_read(\"data/MP19_SUBZONE_WEB_PL.kml\")\n\nReading layer `MP19_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data\\MP19_SUBZONE_WEB_PL.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n# Display top 5 rows of the feature object \nhead(mpsz19_kml, n=5)  \n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6537 ymin: 1.216215 xmax: 103.8811 ymax: 1.29742\nGeodetic CRS:  WGS 84\n  Name Description                       geometry\n1                  MULTIPOLYGON (((103.8802 1....\n2                  MULTIPOLYGON (((103.8376 1....\n3                  MULTIPOLYGON (((103.8341 1....\n4                  MULTIPOLYGON (((103.7125 1....\n5                  MULTIPOLYGON (((103.8472 1....\n\n\n\n🔎 Observations: We can notice that the datafile also consists of 332 features and 6 fields, and follows the WGS64 coordinate system, but it uses the kml driver accordingly."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-.shp-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-.shp-data",
    "title": "In-class Exercise 2",
    "section": "2.1 Importing MP14_SUBZONE_WEB_PL (Web) .shp Data",
    "text": "2.1 Importing MP14_SUBZONE_WEB_PL (Web) .shp Data\n\n# Import shapefile\nmpsz14_shp &lt;- st_read(dsn = \"data\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThere are 3 ways to explore the contents of a simple feature data frame like mpsz!\n\n1) Using st_geometry()\nThe sf data.frame contains a geometry column, that is a list of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry().\n\n# Retrieve geometry column \nst_geometry(mpsz14_shp)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\n🔎 Observations: This MP14_SUBZONE_WEB_PL file consists of 323 features, consisting of multipolygon features.\n\n\n\n2) Using glimpse()\nWe use glimpse() from the dplyr package to understand the data type of each fields.\nE.g. FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are in double-precision values.\n\n# Get data types \nglimpse(mpsz14_shp)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\n🔎 Observations: This MP14_SUBZONE_WEB_PL file consists of 323 rows and 16 columns with datatypes ranging from integers, characters, date and doubles.\n\n\n\n3) Using head()\nWe use head() from the base R package to get the full information of the feature object mpsz. The n value indicates the no. of rows. I will use thiis method for this in-class exercise.\n\n# Display top 5 rows of the feature object \nhead(mpsz14_shp, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n🔎 Observations: We can notice that the MP14_SUBZONE_WEB_PL file consists of 5 features and 15 fields, with the top 5 rows per column."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-.kml-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-mp14_subzone_web_pl-web-.kml-data",
    "title": "In-class Exercise 2",
    "section": "2.2 Importing MP14_SUBZONE_WEB_PL (Web) .kml Data",
    "text": "2.2 Importing MP14_SUBZONE_WEB_PL (Web) .kml Data\nWe use the below code chunk to export mpsz14_shp sf data.frame into kml file which saves the file into our data folder.\n\n# Convert .shp file into .kml\nst_write(mpsz14_shp, \n         \"data/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)\n\nDeleting source `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting layer `MP14_SUBZONE_WEB_PL' to data source \n  `data/MP14_SUBZONE_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n# Import KML file\nmpsz14_kml = st_read(\"data/MP14_SUBZONE_WEB_PL.kml\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex2\\data\\MP14_SUBZONE_WEB_PL.kml' \n  using driver `KML'\nSimple feature collection with 323 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n# Display top 5 rows of the feature object \nhead(mpsz14_kml, n=5)  \n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.8142 ymin: 1.272838 xmax: 103.8725 ymax: 1.291523\nGeodetic CRS:  WGS 84\n  Name Description                       geometry\n1                  MULTIPOLYGON (((103.8647 1....\n2                  MULTIPOLYGON (((103.8431 1....\n3                  MULTIPOLYGON (((103.8507 1....\n4                  MULTIPOLYGON (((103.8255 1....\n5                  MULTIPOLYGON (((103.8194 1....\n\n\n\n💡 Note: delete_dsn = TRUE will help delete the original data before rendering it"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#in-class-exercise-objectives",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#in-class-exercise-objectives",
    "title": "In-class Exercise 1",
    "section": "In-class Exercise Objectives",
    "text": "In-class Exercise Objectives\nIn this week’s in-class Exercise 1, I explored the setting up of our RStudio and installed all R tools required for this IS314 Geospatial Analytics and Application module."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#steps-taken-in-this-exercise",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#steps-taken-in-this-exercise",
    "title": "In-class Exercise 1",
    "section": "Steps Taken in this Exercise",
    "text": "Steps Taken in this Exercise\nTo create this exercise file, I first created a sub-folder under the In-class_Ex folder and named it In-class_Ex1 and created this Quarto document by executing the following steps:\n\nClick “File” tab\nClick “New File”\nUnder “New File”, select “Quarto Document”\n\n\n🔎 Observations: A new file is added in File Explorer with a .qmd file format. You will need to save the file and label it a name for the file to render and load.\n\nNext, I created the header of this document by adding YAML code that can be indicated using back-ticks ``` &lt;add code here&gt; ```. Within the YAML code, I indicated the document title, writer, publishing date and modifcation date as displayed accordingly in this webpage."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex2/data/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "title": "Hands-on Exercise 3",
    "section": "1. Getting Started",
    "text": "1. Getting Started\nIn this exercise, I will be exploring the basic methods of spatial point pattern analysis - split into two parts.\n\nPart 1: [1st Order Spatial Point Patterns Analysis]\nPart 2: [2nd Order Spatial Point Patterns Analysis]\n\nIn particular, I will be using the spatstat package for this exercise.\n\n💡 What’s spatstat? the spatstat package is a comprehensive package for the analysis of spatial point patterns. It is a very powerful package, but it is also very complex. We will only be using a small subset of the functionality of the package. (More info can be found on this spatstat website)\n\nThe goal of this exercise is to discover the spatial point processes of childecare centres in Singapore by answering the following questions:\n\nAre the childcare centres in Singapore randomly distributed throughout the country?\nIf no, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#lets-set-up",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#lets-set-up",
    "title": "Hands-on Exercise 3",
    "section": "2. Let’s Set Up!",
    "text": "2. Let’s Set Up!\n\n2.1 Importing Libraries into R\nIn this exercise, we will use the following R packages:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nNow, let’s install and load these packages in RStudio.\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse)\n\n\n\n2.2 Download Data and Set Up Folders\nWe will use 3 data sets for this exercise:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format.\n\nThis is the file structure for containing the data files that I have extracted."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#import-data-sets-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#import-data-sets-into-r",
    "title": "Hands-on Exercise 3",
    "section": "3. Import Data Sets into R",
    "text": "3. Import Data Sets into R\nWe will first import the three geospatial data sets into R using st_read() of the sf package.\n\nchildcare_sf &lt;- st_read(\"data/aspatial/child-care-services-geojson.geojson\") %&gt;% st_transform(crs = 3414) \n\nReading layer `child-care-services-geojson' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex3\\data\\aspatial\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nsg_sf &lt;- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\")  \n\nReading layer `CostalOutline' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 3",
    "section": "4. Geospatial Data Wrangling",
    "text": "4. Geospatial Data Wrangling\n\n4.1 Standardising Coordinate Systems\nBefore we proceed, let’s check if the geospatial data sets are projected in the same projection system.\n\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n💡 Observations: Notice that sg_sf and mpsz_sf is in the SVY21 coordinate system format, but their EPSG code is wrongly indicated as 9001, instead of 3414.\n\nLet’s assign the correct ESPG code to mpsz_sf and sg_sf simple feature data frames:\n\nsg_sf &lt;- st_transform(sg_sf, 3414)\nmpsz_sf &lt;- st_transform(mpsz_sf, 3414)\n\n\n\n4.2 Mapping the Geospatial Data Sets\nNext, let’s map the geospatial data sets to show their spatial patterns.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_sf) +\n  qtm(childcare_sf)\n\n\n\n\n\n\n\n\n\n💡 Observations: We can see that all the geospatial layers are within the same map extend. This shows that their referencing system and coordinate values are referred to similar spatial context. This is very important in any geospatial analysis.\n\nAlternatively, we can also prepare a pin map by using the code chunk below.\n\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n💡 Note: remember to switch back to plot mode after the interactive map as each interactive mode will consume a connection. It is also advised to avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\ntmap_mode('plot')\n\ntmap mode set to plotting\n\n\n\n\n4.3 Converting the Simple Features to sp’s Spatial* Class\nWhen we convert childcare_sf geojson data to a Spatial class, we can observe below that the childcare_sf data is still stored in the Description attribute and has not been fully utilised. In particular, it is in a HTML format\n\nchildcare &lt;- as_Spatial(childcare_sf)\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\nAs such, let us transform this data to make it more meaningful and easier for us to read.\n\nlibrary(xml2)\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nchildcare_validity &lt;- st_is_valid(childcare_sf)\nchildcare_invalid &lt;- which(!childcare_validity)\nif (length(childcare_invalid) &gt; 0) {\n  print(\"ChildCare Invalid!\")\n  print(childcare_sf[childcare_invalid, ])\n} else {\n  print(\"it's valid!\")\n}\n\n[1] \"it's valid!\"\n\n# Ensure the geometry column is preserved\ngeometry_column &lt;- st_geometry(childcare_sf)\nparse_description &lt;- function(html_string) {\n  html &lt;- read_html(html_string)\n  html &lt;- html %&gt;% html_nodes(\"tr\") %&gt;% .[!grepl(\"Attributes\", .)]\n  headers &lt;- html %&gt;% html_nodes(\"th\") %&gt;% html_text(trim = TRUE)\n  values &lt;- html %&gt;% html_nodes(\"td\") %&gt;% html_text(trim = TRUE)\n  \n  # Handle cases where the number of headers and values don't match\n  if (length(headers) != length(values)) {\n    max_length &lt;- max(length(headers), length(values))\n    headers &lt;- c(headers, rep(\"ExtraHeader\", max_length - length(headers)))\n    values &lt;- c(values, rep(\"NULL\", max_length - length(values)))\n  }\n  \n  setNames(values, headers)\n}\n\n# Apply parsing function, unnest the description fields, and remove the original 'Description' column\nchildcare_sf &lt;- childcare_sf %&gt;% \n  mutate(Description_parsed = map(Description, parse_description)) %&gt;%\n  unnest_wider(Description_parsed) %&gt;%\n  select(-Description)  # Remove the original 'Description' column\n\n# Overwrite the 'Name' column with the 'LANDYADDRESSPOINT' column values\nchildcare_sf &lt;- childcare_sf %&gt;%\n  mutate(Name = NAME)  # Overwrite 'Name' with 'LANDYADDRESSPOINT'\n\n# Replace empty strings or NA across all columns with \"NULL\"\nchildcare_sf &lt;- childcare_sf %&gt;%\n  mutate(across(!geometry, ~ ifelse(is.na(.) | . == \"\", \"NULL\", .)))\n\n# Reassign the geometry to the dataframe\nst_geometry(childcare_sf) &lt;- geometry_column\n# Ensure it's still an sf object\nclass(childcare_sf)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe will now convert the sf geospatial data frames to sp Spatial* class and display the information of these three Spatial* classes.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 16\nnames       :                    Name, ADDRESSBLOCKHOUSENUMBER, ADDRESSBUILDINGNAME, ADDRESSPOSTALCODE,                                                                       ADDRESSSTREETNAME, ADDRESSTYPE,         DESCRIPTION, HYPERLINK, LANDXADDRESSPOINT, LANDYADDRESSPOINT,                    NAME, PHOTOURL, ADDRESSFLOORNUMBER,          INC_CRC,     FMEL_UPD_D, ... \nmin values  :    3-IN-1 FAMILY CENTRE,                    NULL,                NULL,            018989,                                                  1 & 3, Stratton Road, SINGAPORE 806787,        NULL, Child Care Services,      NULL,                 0,                 0,    3-IN-1 FAMILY CENTRE,     NULL,               NULL, 00A958622500BF89, 20200812221033, ... \nmax values  : ZEE SCHOOLHOUSE PTE LTD,                    NULL,                NULL,            829646, UPPER BASEMENT LEVEL, WEST WING, TERMINAL 1, SINGAPORE CHANGI AIRPORT, SINGAPORE 819642,        NULL,                NULL,      NULL,                 0,                 0, ZEE SCHOOLHOUSE PTE LTD,     NULL,               NULL, FFCFA88A8CE5665A, 20200826094036, ... \n\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\n\n💡 Observations: each data frame has been converted into their respective Spatial Points and Spatial Polygons data frames.\n\n\n\n4.4 Converting the Spatial* Class Into Generic sp Format, then ppp Object Format\nThe spatstat package requires analytical data in planar point pattern (ppp) object format. As there is no direct way to convert a Spatial* classes into ppp object, we will need to convert the Spatial* classes into a Spatial object first.\n\nStep 1: Convert Spatial* classes into generic Spatial objects\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nHere is a display of the sp objects properties as shown below.\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\n💡 Observations: However, notice that the sp objects do not contain information such as, variables, names, min values and max values.\n\n\n\nStep 2: Converting the sp objects into ppp objects\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf))\n\nWarning: data contain duplicated points\n\n\nLet’s plot the ppp object to see what it looks like.\n\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\nWe can also take a quick look at the ppp object properties by using the code chunk below.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\n\n💡 Observations: Notice the warning message about duplicates. In spatial point patterns analysis, the presence of duplicates is a significant issue as the statistical methodology used is based largely on the assumption that points represent a unique location.\n\n\n\n\n4.5 Handling the duplicates\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nTo count the number of coincidence point, we will use the multiplicity() function as shown.\n\nmultiplicity(childcare_ppp)\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 128\n\n\n\n💡 Observations: The output shows that there are 338 duplicated point events.\n\nTo view the locations of these duplicate point events, we will plot childcare data accordingly.\n\ntmap_mode('view')\n\ntmap mode set to interactive viewing\n\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\ntmap_mode('plot')\n\ntmap mode set to plotting\n\n\n\n\n\n\n\n\nNote\n\n\n\n💡 How to identify duplicated points? duplicated points can be discovered by looking at the darker spots.\nThree ways to handle the duplicates:\n\nRemove the duplicates: This is the easiest way to handle the duplicates. However, it is not recommended because it will result in loss of information.\nJittering: Add a small amount of random noise to the duplicated points so they do not occupy the exact same space.\nMake each point unique by adding a unique identifier to each point as marks. This is the most recommended way to handle the duplicates. However, it is also the most tedious way to handle the duplicates.\n\n\n\nWith that said, we will use the second method to handle the duplicates. We will use the jitter() function to add a small amount of random noise to the duplicated points.\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n# Check for duplicate points in the data\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n4.6 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to convert the sg SpatialPolygon object into owinobject of spatstat.\n\nsg_owin &lt;- as.owin(sg_sf)\n\nThe output can be displayed using the plot() function\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\nand summary() function of base R\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n4.7 Combining Point Events Object and owin Object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the codes below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe ppp object outputted from combining both the point and polygon feature is shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\nNext, I plot the newly created childcareSG_ppp object as shown.\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#kernel-density-estimation",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#kernel-density-estimation",
    "title": "Hands-on Exercise 3",
    "section": "5. Kernel Density Estimation",
    "text": "5. Kernel Density Estimation\nIn this section, I will be computing the kernel density estimation (KDE) of childcare services in Singapore by using density() of the spatstat package.\nHere are the following configurations of density():\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is Gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n5.1 Compute a Kernel Density\nThe code chunk below computes a kernel density by using the\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                              kernel=\"gaussian\")\n\nThe plot() function of Base R is then used to display the kernel density derived.\n\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\n💡 Observations: The density values of the output range from 0 to 0.000035 which is way too small to comprehend!\n💡 Why? It is worth noting that the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in “number of points per square meter”.\n\nAs a side note, one can retrieve the bandwidth used to compute the KDE layer by using the code chunk below.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n298.4095 \n\n\n\n\n5.2 Re-scalling KDE values\nTo make the density values more comprehensible, we will rescale the density values from meter to kilometer using rescale().\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\nNow, we can re-run the density() function to compute the KDE map.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\n💡 Observations: Notice the output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n5.3 Working with Different Automatic Bandwidth Methods\nBesides bw.diggle(), there are other automatic bandwidth selection methods that can be used to determine the bandwidth. Such as bw.CvL(), bw.scott(), and bw.ppl().\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods\n\n bw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\n bw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.224898 1.450966 \n\n bw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\n bw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.2984095 \n\n\n\n\n\n\n\n\nNote\n\n\n\nTo use bw.diggle() or bw.ppl()?\nBaddeley et. (2016) suggested to use bw.ppl() when the pattern consists predominantly of tight clusters. While the bw.diggle() method works better when detecting a single tight cluster in the midst of random noise.\n\n\n\n# Let's compare the outputs!\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\n5.4 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is Gaussian. Nonetheless, there are 3 other options: Epanechnikov, Quartic and Dics.\nLet’s compute these three other kernel density estimations by indicating the kernel method as such.\n\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\n\n\n\n\n\n\n\n\n\n\n5.5. Fixed and Adaptive KDE\n\n5.5.1 Computing KDE by using fixed bandwidth\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\n\n\n\n5.5.2 Computing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n5.5.3 Converting KDE output into grid object\nThe result is the same, we just convert it so that it is suitable for mapping purposes\n\nkde_raster &lt;- raster(kde_childcareSG.bw)\ngridded_kde_childcareSG_bw &lt;- as(kde_raster, \"SpatialGridDataFrame\")\n\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\nStep 1) Converting gridded output into raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\n\n💡 Observations: Notice that the CRS property is NA.\n\n\n\nStep 2) Assigning projection systems\nThe code chunk below will be used to include the CRS information on kde_childcareSG_bw_raster RasterLayer.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\n\n💡 Observations: Notice that the CRS property is now completed.\n\n\n\n\n5.5.4 Visualising KDE Layer output in tmap\nFinally, we will display the KDE raster layer in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"layer\", palette=\"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\n\n\n💡 Observations: Notice that the raster values are encoded explicitly onto the raster pixel using the values in “layer” field.\n\n\n\n5.5.5 Comparing Spatial Point Patterns using KDE\nNext, I will compare the KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\nStep 1) Extracting Study Area\nThe code chunk below will be used to extract the target planning areas.\n\n# Extracting the planning areas\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nNext, let’s plot the target planning areas.\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(tm, main = \"Tampines\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\nWarning: plotting the first 10 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(jw, main = \"Jurong West\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\nStep 2) Creating owin Object\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\nStep 3) Combining Childcare Points and Study Area\nNext, we run these codes to extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale.ppp() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\nStep 4) Computing KDE\nThe code chunk below will be used to compute the KDE of these four planning area. The bw.diggle() method is used to derive the bandwidth of each planning area.\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\nStep 5) Computing Fixed Bandwidth KDE\nFor comparison purposes with fixed bandwidth KDE, we will use 250m as the bandwidth.\n\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#fixed-and-adaptive-kde",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#fixed-and-adaptive-kde",
    "title": "Hands-on Exercise 3",
    "section": "5.5. Fixed and Adaptive KDE",
    "text": "5.5. Fixed and Adaptive KDE\n\n5.5.1 Computing KDE by using fixed bandwidth\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\n\n\n\n5.5.2 Computing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n5.5.3 Converting KDE output into grid object\nThe result is the same, we just convert it so that it is suitable for mapping purposes\n\nkde_raster &lt;- raster(kde_childcareSG.bw)\ngridded_kde_childcareSG_bw &lt;- as(kde_raster, \"SpatialGridDataFrame\")\n\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\nStep 1) Converting gridded output into raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\n\n💡 Observations: Notice that the CRS property is NA.\n\n\n\nStep 2) Assigning projection systems\nThe code chunk below will be used to include the CRS information on kde_childcareSG_bw_raster RasterLayer.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\n\n💡 Observations: Notice that the CRS property is now completed.\n\n\n\n\n5.5.4 Visualising KDE Layer output in tmap\nFinally, we will display the KDE raster layer in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"layer\", palette=\"plasma\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\n\n\n💡 Observations: Notice that the raster values are encoded explicitly onto the raster pixel using the values in “layer” field.\n\n\n\n5.5.5 Comparing Spatial Point Patterns using KDE\nNext, I will compare the KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\nStep 1) Extracting Study Area\nThe code chunk below will be used to extract the target planning areas.\n\n# Extracting the planning areas\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nNext, let’s plot the target planning areas.\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(tm, main = \"Tampines\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\nWarning: plotting the first 10 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\nplot(jw, main = \"Jurong West\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\nStep 2) Creating owin Object\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\nStep 3) Combining Childcare Points and Study Area\nNext, we run these codes to extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale.ppp() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\nStep 4) Computing KDE\nThe code chunk below will be used to compute the KDE of these four planning area. The bw.diggle() method is used to derive the bandwidth of each planning area.\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\nStep 5) Computing Fixed Bandwidth KDE\nFor comparison purposes with fixed bandwidth KDE, we will use 250m as the bandwidth.\n\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#nearest-neighbour-analysis",
    "title": "Hands-on Exercise 3",
    "section": "6. Nearest Neighbour Analysis",
    "text": "6. Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\n\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n6.1 Testing spatial point patterns using Clark and Evans Test\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.55631, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n💡 Observations: The output shows that the p-value is less than 0.05. Therefore, we reject the null hypothesis and conclude that the distribution of childcare services are not randomly distributed. We can also see that the R value is less than 1. This means that the distribution of childcare services are clustered.\n\n\n\n6.2 Clark and Evans Test: Choa Chu Kang planning area\nIn the code chunk below, clarkevans.test() of spatstat is used to performs Clark-Evans test of aggregation for childcare centre in Choa Chu Kang planning area.\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.95348, p-value = 0.487\nalternative hypothesis: two-sided\n\n\n\n\n6.3 Clark and Evans Test: Tampines planning area\nIn the code chunk below, the similar test is used to analyse the spatial point patterns of childcare centre in Tampines planning area.\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.79145, p-value = 0.0001673\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-g-function",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-g-function",
    "title": "Hands-on Exercise 3",
    "section": "7. Analysing Spatial Point Process Using G-Function",
    "text": "7. Analysing Spatial Point Process Using G-Function\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n7.1 Choa Chu Kang planning area\n\n7.1.1 Computing G-function Estimation\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n7.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with G-function\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n# Plot\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n7.2 Tampines planning area\n\n7.2.1 Computing G-function Estimation\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n\n\n\n\n7.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with G-function\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n# Plot\nplot(G_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-f-function",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-f-function",
    "title": "Hands-on Exercise 3",
    "section": "8. Analysing Spatial Point Process Using F-Function",
    "text": "8. Analysing Spatial Point Process Using F-Function\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n8.1 Choa Chu Kang planning area\n\n8.1.1 Computing F-function Estimation\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\n# Computing F-function estimation \nF_CK = Fest(childcare_ck_ppp) \n\n# Plot\nplot(F_CK)\n\n\n\n\n\n\n\n\n\n\n8.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with F-function\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n# Plot\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n8.2 Tampines planning area\n\n8.2.1 Computing F-function Estimation\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\n\n\n\n8.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with F-function\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n# Plot\nplot(F_CK.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-k-function",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-k-function",
    "title": "Hands-on Exercise 3",
    "section": "9. Analysing Spatial Point Process Using K-Function",
    "text": "9. Analysing Spatial Point Process Using K-Function\nK-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n9.1 Choa Chu Kang planning area\n\n9.1.1 Computing K-function Estimation\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n9.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with F-function\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n# Plot\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n9.2 Tampines planning area\n\n9.2.1 Computing K-function Estimation\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n9.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with F-function\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n# Plot\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-l-function",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#analysing-spatial-point-process-using-l-function",
    "title": "Hands-on Exercise 3",
    "section": "10. Analysing Spatial Point Process Using L-Function",
    "text": "10. Analysing Spatial Point Process Using L-Function\nIn this section, I will be computing L-function estimation by using Lest() of spatstat package. I will also perform monta carlo simulation test using envelope() of the spatstat package.\n\n10.1 Choa Chu Kang planning area\n\n10.1.1 Computing L-Function Estimation\nFirstly, let’s compute the L-function estimation for Choa Chu Kang.\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n10.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n# Monte Carlo test with L-function\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n# Plot\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n10.2 Tampines planning area\n\n10.2.1 Computing L-Function Estimation\nNext, let’s compute the L-function estimation for Tampines.\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n10.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below will be used to perform the hypothesis testing.\n\n# Monte Carlo test with L-function\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n# Plot\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-1-installing-maptools",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-1-installing-maptools",
    "title": "In-class Exercise 3",
    "section": "Issue #1: Installing maptools",
    "text": "Issue #1: Installing maptools\nmaptools have been retired and binary have been removed from CRAN. However, we can download from Posit Public Package Manager snapshots by using this code chunk below.\n\n# You can use this but it's not encouraged since maptools has depreciated!\ninstall.packages(\"maptools\",\n                 repos = \"https://packagemanager.posit.co/cran/2023-10-13\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-2-creating-coastal-outline-data",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-2-creating-coastal-outline-data",
    "title": "In-class Exercise 3",
    "section": "Issue #2: Creating Coastal Outline Data",
    "text": "Issue #2: Creating Coastal Outline Data\nIn sf package, there are two functions that allow us to combine multiple simple features into one simple features. They are st_ combine() and st_union().\n\nst_combine() returns a single, combined geometry, with no resolved boundaries; returned geometries may well be invalid.\nIf y is missing, st_union(x) returns a single geometry with resolved boundaries, else the geometries for all unioned pairs of xi] and yfil.\n\n\n# Impmort dataset into R\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n# Derive costal outline sf tibble data.frame\nsg_sf &lt;- mpsz_sf %&gt;%\n  st_union()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-3-converting-data-to-spatialgriddataframe",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#issue-3-converting-data-to-spatialgriddataframe",
    "title": "In-class Exercise 3",
    "section": "Issue #3: Converting Data to SpatialGridDataFrame",
    "text": "Issue #3: Converting Data to SpatialGridDataFrame\nSince maptools isn’t installed in the Hands-on Exercise 3, we will need to use another method for converting the results of kde_childcareSG.bw to a Spatial Grid Data Frame.\n\n## This code won't work anymore\n# gridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\n\n## This code should work instead\n# kde_raster &lt;- raster(kde_childcareSG.bw)\n# gridded_kde_childcareSG_bw &lt;- as(kde_raster, \"SpatialGridDataFrame\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The conflict in Myanmar is not just a result of the coup but is deeply rooted in the country’s decades-old complex ethnic and political landscape, characterised by tensions between the central government and various ethnic minority groups, each with its own armed forces. The post-coup violence has exacerbated these long-standing conflicts, leading to a severe humanitarian crisis, with thousands killed, hundreds of thousands displaced, and widespread human rights abuses reported.\n\n\n\nAs such, Geospatial analytics has become a valuable tool for evaluating and comprehending the intricacies of increasing conflicts. This exercise aims to reveal the spatial and spatio-temporal distribution of armed conflict in Myanmar by leveraging spatial point pattern analysis. Additionally, it aims to gain clearer insights into the geographical and logistical patterns of violence throughout the nation.\nBy the end of this take-home exercise, I aim to complete these steps in my spatial point pattern analysis in uncovering the distribution of armed conflict in Myanmar.\n\nUsing appropriate function of sf and tidyverse packages, import and transform the downloaded armed conflict data and administrative boundary data into sf tibble data.frames.\nUsing the geospatial data sets prepared, derive quarterly KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatial Point Patterns Analysis.\nUsing the geospatial data sets prepared, derive quarterly spatio-temporal KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatio-temporal Point Patterns Analysis.\nUsing appropriate tmap functions, display the KDE and Spatio-temporal KDE layers on openstreetmap of Myanmar.\nDescribe the spatial patterns revealed by the KDE and Spatio-temporal KDE maps.\n\n\n\n\n\n\nThis Armed Conflict Location & Event Data (ACLED) is an independent, impartial, international non-profit organisation which owns an extensive database of violent conflict and protest in countries and territories around the world.\n\nFor the purpose of this exercise, I have downloaded ACLED’s data on Myanmar which includes a series of conflict events, particularly between 1 January 2021 to 30 June 2024.\n🔗 Source: ACLED\n📁 Format: comma separated values (CSV)\nAs the dataset is rather extensive, I will be performing my analysis on armed conflict events in a quarterly basis to streamline my tasks. The data included in this dataset are as follows:\nEvent Type\nACLED categorises events into various types. I will mainly be focusing on these four event types: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\n\n\n\nEvent Type\nACLED categorises events into various types. I will mainly be focusing on these four event types: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\n\nevent_id_cnty: unique ID for each conflict\nevent_type: category of event e.g. Battle, Violence Against Civilians, Protests, Explosions/Remote Violence, Strategic Developments\nsub_event_type: a more detailed classification within event type\ndisorder_type: classifies the event based on the nature of the disorder e.g. political violence, demonstrations, strategic developments[A1]\ncivilian_targeting: yes/no value, whether event involves specifically targeting civilians\nNote: when “strategic developments” are used in Event Type, it is also used in the disorder type (vice-versa)\n\n\n\nLocation and Geospatial Data\nThe database provides detailed geographic information, pinpointing the exact or approximate locations of conflict events across Myanmar. This includes cities, towns, and rural areas.\n\niso: the country code for Myanmar which uses 104 in this case\nregion: region of conflict within Myanmar\ncountry: indicates Myanmar\nadmin1, admin2, admin3: 1st, 2nd and 3rd level administration division within Myanmar e.g. states, division, sub-division\nlocation: specific geographic location or name of the place where the conflict event occurred\nlatitude: latitude of the conflict event\nlongitude: longitude of the conflict event\ngeo_precision: indicates the level of precision for the geographic coordinates provided\n\n\n\nDate and Time\nACLED records the specific dates and, where possible, times of conflict events.\n\nevent_date: date of conflict\nyear: year of conflict\ntime_precision: accuracy of the date and time information provided\n\n\n\nActors\n\nIndicate the actors involved in the conflict, such as the Tatmadaw (Myanmar’s military), ethnic armed organizations, local militias, civilian protestors, and other groups.\nactor1: primary actor involved in the conflict event. E.g. a government force, rebel group, militia, or any organised entity\nassoc_actor_1: a secondary group that is aligned with or supports the primary actor (Actor1) in the event\ninter1: an interaction code that categorises actor1, could be a government force, rebel group, military force, rioter, civilian, or other entities\ninteraction: combined description of actor1 and actor2\n\n\n\nFatalities\n\nfatalities: tracks the number of reported fatalities associated with each conflict event\n\n\n\nOthers\n\nsource: source of information for the conflict event\nsource_scale: scale of the source e.g. local, national, international\nnotes : additional comments\ntags: keywords associated with the conflict event\ntimestamp: date and time when conflict event was entered/updated in the database\n\n\n\n\n\n\n\nI will also be using a geospatial dataset from the Myanmar Information Management Unit (MIMU) in shapefile (.shp) format, specifically of the Myanmar state at the 2nd administrative level with district boundaries.\n🔗 Source: MIMU\n📁 Format: shapefile (.shp)\nMy reasoning for choosing the district boundary dataset is that we do not want to select a boundary dataset that is too broad when analysing conflict events since it might not provide sufficient insights to trends where conflict events happen. Neither do we want to analyse a geography that is too divided (e.g. Admin 3) since it can be computationally inefficient as seen in the types of boundary data below.\n\n\n\n\n\n\n\n\n\n\nAdmin 0\nAdmin 1\nAdmin 1\nAdmin 2 - To Use\nAdmin 3\n\n\n\n\nNational boundary\nMyanmar region\nRegion and sub-region\nDistrict boundary\nMyanmar township\n\n\n\n\n\n\n\n\n\n\nI have donwloaded the two data sets and organised them into my folder as follows."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#lets-set-up",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#lets-set-up",
    "title": "Take-home Exercise 1",
    "section": "2. Let’s Set Up!",
    "text": "2. Let’s Set Up!\n\n2.1 Importing Libraries into R\nTo carry out this exercise, I will be using the following R packages:\n\nsf: a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat: has a wide range of useful functions for point pattern analysis. In this take-home exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster: reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this take-home exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools: provides a set of tools for manipulating geographic data. We mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap: provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nNow, let’s install and load these packages in RStudio.\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse)\n\n\n\n2.2 Importing Data Sets into R\n\n1) Armed Conflicts Data\nNext, I will import the downloaded armed conflict data. For aspatial datasets like this, we will import into Rstudio using read_csv() function of the readr package.\n\n# Import armed conflict data\nconflict_data &lt;- read_csv(\"data/aspatial/2021-01-01-2024-06-30-Myanmar.csv\")\n\nRows: 87746 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): event_id_cnty, event_date, disorder_type, event_type, sub_event_ty...\ndbl (10): year, time_precision, inter1, interaction, iso, latitude, longitud...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe 2021-01-01-2024-06-30-Myanmar.csv dataset contains 87746 rows and 28 columns which indicates the presence of 87746 unique armed conflict events in Myanmar.\n\n\nAfter importing the dataset, we can inspect the dataset using the glimpse() function.\n\n# Inspect the conflict data\nglimpse(conflict_data)\n\nRows: 87,746\nColumns: 28\n$ event_id_cnty      &lt;chr&gt; \"MMR64313\", \"MMR64313\", \"MMR64320\", \"MMR64320\", \"MM…\n$ event_date         &lt;chr&gt; \"30 June 2024\", \"30 June 2024\", \"30 June 2024\", \"30…\n$ year               &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 202…\n$ time_precision     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ disorder_type      &lt;chr&gt; \"Political violence\", \"Political violence\", \"Politi…\n$ event_type         &lt;chr&gt; \"Battles\", \"Battles\", \"Battles\", \"Battles\", \"Battle…\n$ sub_event_type     &lt;chr&gt; \"Armed clash\", \"Armed clash\", \"Armed clash\", \"Armed…\n$ actor1             &lt;chr&gt; \"People's Defense Force - Mandalay\", \"Military Forc…\n$ assoc_actor_1      &lt;chr&gt; \"MDA - AGF: Madaya - The Authentic Genes Force; SST…\n$ inter1             &lt;dbl&gt; 3, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 2, 1, …\n$ interaction        &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 10, 13, 13, 10, 12, 12, 12,…\n$ civilian_targeting &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ iso                &lt;dbl&gt; 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 1…\n$ region             &lt;chr&gt; \"Southeast Asia\", \"Southeast Asia\", \"Southeast Asia…\n$ country            &lt;chr&gt; \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanma…\n$ admin1             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Ma…\n$ admin2             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Pyinoolwin\", \"Pyinoolwin\",…\n$ admin3             &lt;chr&gt; \"Patheingyi\", \"Patheingyi\", \"Singu\", \"Singu\", \"Thab…\n$ location           &lt;chr&gt; \"Aung Tha Pyay\", \"Aung Tha Pyay\", \"Pin Lel Gyi\", \"P…\n$ latitude           &lt;dbl&gt; 22.1504, 22.1504, 22.5752, 22.5752, 22.8800, 22.880…\n$ longitude          &lt;dbl&gt; 96.2364, 96.2364, 96.0661, 96.0661, 95.9700, 95.970…\n$ geo_precision      &lt;dbl&gt; 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, …\n$ source             &lt;chr&gt; \"Democratic Voice of Burma; Irrawaddy\", \"Democratic…\n$ source_scale       &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Na…\n$ notes              &lt;chr&gt; \"On 30 June 2024, near Aung Tha Pyay village (Pathe…\n$ fatalities         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, …\n$ tags               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ timestamp          &lt;dbl&gt; 1720552468, 1720552468, 1720552468, 1720552468, 172…\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe event_date field shows that it uses a character datatype instead of date - we will fix this later. Also, we can observe that thelongitude and langitude fields appear to be adopting the WGS84 geographic coordinate system since they are in the -180/180 and -90/90 range respectively.\n\n\n\n\n2) Myanmar Boundary Data\n\n\n\n\n\n\nObservations\n\n\n\nWhen working with Myanmar’s boundary, we need to assign the appropriate coordinate reference system. However, since Myanmar is split into two UTM - West Myanmar (crs: 32646) and East Myanmar (crs: 32647).\n\n\nHence, I will also import the administrative boundary data into a simple features tibble data.frame using st_read() of the sf package and check the number of rows returned for both CRS 32646 and 32647. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\n\nFind out conflicts count by CRS\nconflict_crs &lt;- st_as_sf(conflict_data, coords = c(\"longitude\", \"latitude\"), crs = 4326) \n\n# Count number of conflicts for CRS 32646\nconflict_data_32646 &lt;- st_transform(conflict_crs, crs = 32646)\ncount_32646 &lt;- nrow(conflict_data_32646)\n# Count number of conflicts for CRS 32647\nconflict_data_32647 &lt;- st_transform(conflict_crs, crs = 32647)\ncount_32647 &lt;- nrow(conflict_data_32647)\n\ncrs_counts &lt;- data.frame(\n  CRS = c(\"EPSG: 32646\", \"EPSG: 32647\"),\n  Conflicts_Count = c(count_32646, count_32647)\n)\n\nprint(crs_counts)\n\n\n          CRS Conflicts_Count\n1 EPSG: 32646           87746\n2 EPSG: 32647           87746\n\n\nSince there is no difference in the count, I will decide to focus on UTM zone 47N (EPSG:32647), east of Myanmar, for the purpose of this exercise. The st_transform() function below converts the CRS of the sf object to EPSG:32647.\n\n# Import boundary data\nboundary_sf &lt;- st_read(dsn = \"data/geospatial\",layer = \"mmr_polbnda_adm2_250k_mimu\") %&gt;% st_transform(crs = 32647)\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Take-home_Ex\\Take-home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nIn the code below, we can notice that the ESPG code has been updated to 32647.\n\n# Check for changes\nst_crs(boundary_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nHere, I will use the plot() function which plots the geometry of the sf object. The st_geometry() function is used to extract the geometry of the mpsz_sf object which includes the districts of Myanmar as shown below.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(boundary_sf))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-wrangling",
    "title": "Take-home Exercise 1",
    "section": "3. Data Wrangling",
    "text": "3. Data Wrangling\n\n3.1 Fixing Incorrect Datatypes\nRecall that the earlier inspection of the conflict_data tibble data frame revealed that the datatype indicated for event date is wrongly labelled as a character instead of a date format.\nAs such, let’s convert the datatype to the correct ‘date’ format as shown below.\n\n# Convert the datatype for event_date\nconflict_data$event_date &lt;- as.Date(conflict_data$event_date, format = \"%d %B %Y\")\n\n# Check for changes\nhead(conflict_data)\n\n# A tibble: 6 × 28\n  event_id_cnty event_date  year time_precision disorder_type      event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence Battles   \n2 MMR64313      2024-06-30  2024              1 Political violence Battles   \n3 MMR64320      2024-06-30  2024              1 Political violence Battles   \n4 MMR64320      2024-06-30  2024              1 Political violence Battles   \n5 MMR64321      2024-06-30  2024              1 Political violence Battles   \n6 MMR64321      2024-06-30  2024              1 Political violence Battles   \n# ℹ 22 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;\n\n\n\n\n3.2 Adding new year_quarter column\nWe will want to create a new column to indicate the specific year and quarter for each conflict event since the spatial analysis will be done later in a quarterly manner.\n\n\nExtract year and quarter\nconflict_data$year_quarter &lt;- paste0(\n  year(conflict_data$event_date), \n  \" Q\", \n  quarter(conflict_data$event_date)\n)\n\n# View the new data column\nunique(conflict_data$year_quarter)\n\n\n [1] \"2024 Q2\" \"2024 Q1\" \"2023 Q4\" \"2023 Q3\" \"2023 Q2\" \"2023 Q1\" \"2022 Q4\"\n [8] \"2022 Q3\" \"2022 Q2\" \"2022 Q1\" \"2021 Q4\" \"2021 Q3\" \"2021 Q2\" \"2021 Q1\"\n\n\n\n\n3.3 Fixing Duplicated Event ID in conflict_data Dataframe\nAs shown, there are presence of duplicates in our dataframe returned by the duplicated() function.\n\n# Check for duplicates\nany(duplicated(conflict_data))\n\n[1] TRUE\n\n\nBased on the duplicated event ID: MMR64313 for instance. We can observe the two records are of the same political violence event happening between two actors on 30/6/2024, between the People’s Defense Force and Military Forces of Myanmar. Upon further research, these two actors are opposing political parties of Myanmar’s ongoing conflict.\n\n# Inspect an instance of the duplciated event IDs\nhead(conflict_data,2)\n\n# A tibble: 2 × 29\n  event_id_cnty event_date  year time_precision disorder_type      event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence Battles   \n2 MMR64313      2024-06-30  2024              1 Political violence Battles   \n# ℹ 23 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;, year_quarter &lt;chr&gt;\n\n\n\n\n\n\n\n\nReflection\n\n\n\nShould duplicated data be removed in this analysis?\nA single event (e.g. MMR64313) can have duplicated rows with different actor1 values, typically due to counterattacks from opposing sides, leading to different data entries into the conflict_data dataset.\nHence, I will remove duplicated events found in the conflict_data dataframe as long as the rows have the same event ID indicated.\n\n\nHere, I did another check to ensure there is not more than 2 possible repeated event IDs in the first 20 rows of conflict_data.\n\n\nCheck duplicated events for first 20 rows\nduplicate_counts_first_20 &lt;- conflict_data %&gt;%\n  slice(1:20) %&gt;%            \n  group_by(event_id_cnty) %&gt;% \n  summarize(count = n()) %&gt;%  \n  filter(count &gt; 1)         \n\n# View the result\nprint(duplicate_counts_first_20)\n\n\n# A tibble: 9 × 2\n  event_id_cnty count\n  &lt;chr&gt;         &lt;int&gt;\n1 MMR64313          2\n2 MMR64320          2\n3 MMR64321          2\n4 MMR64323          2\n5 MMR64325          2\n6 MMR64326          2\n7 MMR64328          2\n8 MMR64330          2\n9 MMR64331          2\n\n\nWith that checked, I’ll remove the duplicated rows with a repeated Event ID.\n\n\nRemove duplicated rows\n# Retrieve data of duplicated rows\nmerged_duplicates &lt;- conflict_data %&gt;%\n  filter(duplicated(event_id_cnty) | duplicated(event_id_cnty, fromLast = TRUE)) %&gt;%\n  arrange(event_id_cnty) %&gt;%\n  group_by(event_id_cnty) %&gt;%\n  summarize(\n    actor2 = last(actor1),\n    assoc_actor_2 = last(assoc_actor_1)\n  )\n\nconflict_data_no_duplicates &lt;- conflict_data %&gt;%\n  filter(!duplicated(event_id_cnty))\n\n# Update conflict_data dataframe with new columns\nconflict_data &lt;- conflict_data_no_duplicates %&gt;%\n  left_join(merged_duplicates, by = \"event_id_cnty\")\n\n# View dataframe\nprint(head(conflict_data))\n\n\n# A tibble: 6 × 31\n  event_id_cnty event_date  year time_precision disorder_type         event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence    Battles   \n2 MMR64320      2024-06-30  2024              1 Political violence    Battles   \n3 MMR64321      2024-06-30  2024              1 Political violence    Battles   \n4 MMR64322      2024-06-30  2024              1 Strategic developmen… Strategic…\n5 MMR64323      2024-06-30  2024              1 Political violence    Battles   \n6 MMR64324      2024-06-30  2024              1 Strategic developmen… Strategic…\n# ℹ 25 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;, year_quarter &lt;chr&gt;, actor2 &lt;chr&gt;,\n#   assoc_actor_2 &lt;chr&gt;\n\n\nWe can observe that there are no longer any duplicated event IDs in our conflict_data data frame.\n\nany(duplicated(conflict_data))\n\n[1] FALSE\n\n\n\n\n3.4 Converting Aspatial Data to Simple Feature Format\nFor the purpose of this exercise, we will want to integrate and analyse aspatial data in a geographic context. I’ll do a check if conflict_data needs to be converted to a sf data frame - if it outputs anything else but sf, then it’s not a simple feature data frame!\n\nclass(conflict_data)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can see that conflict_data is not a sf data frame. Since a non-simple feature data frame does not have a “geometry” column, we’ll need to convert conflict_data into a simple feature data frame\n\n\nWe can convert conflict_data into a simple feature data frame by using st_as_sf() from the sf package. Addiitionally, we will also need to transform coordinate system from geographic (ESPG: 4326) to projected (ESPG: 32647) using st_transform().\n\n# Convert to simple feature format\nconflict_data_sf &lt;- st_as_sf(conflict_data, coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;% st_transform(crs = 32647)\n\n# Inspect the changes\nglimpse(conflict_data_sf)\n\nRows: 51,553\nColumns: 30\n$ event_id_cnty      &lt;chr&gt; \"MMR64313\", \"MMR64320\", \"MMR64321\", \"MMR64322\", \"MM…\n$ event_date         &lt;date&gt; 2024-06-30, 2024-06-30, 2024-06-30, 2024-06-30, 20…\n$ year               &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 202…\n$ time_precision     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ disorder_type      &lt;chr&gt; \"Political violence\", \"Political violence\", \"Politi…\n$ event_type         &lt;chr&gt; \"Battles\", \"Battles\", \"Battles\", \"Strategic develop…\n$ sub_event_type     &lt;chr&gt; \"Armed clash\", \"Armed clash\", \"Armed clash\", \"Chang…\n$ actor1             &lt;chr&gt; \"People's Defense Force - Mandalay\", \"People's Defe…\n$ assoc_actor_1      &lt;chr&gt; \"MDA - AGF: Madaya - The Authentic Genes Force; SST…\n$ inter1             &lt;dbl&gt; 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 3, 3, 3, 7, 1, …\n$ interaction        &lt;dbl&gt; 13, 13, 13, 10, 13, 10, 12, 12, 12, 12, 12, 13, 13,…\n$ civilian_targeting &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ iso                &lt;dbl&gt; 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 1…\n$ region             &lt;chr&gt; \"Southeast Asia\", \"Southeast Asia\", \"Southeast Asia…\n$ country            &lt;chr&gt; \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanma…\n$ admin1             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Sagaing\", \"Sag…\n$ admin2             &lt;chr&gt; \"Mandalay\", \"Pyinoolwin\", \"Pyinoolwin\", \"Shwebo\", \"…\n$ admin3             &lt;chr&gt; \"Patheingyi\", \"Singu\", \"Thabeikkyin\", \"Khin-U\", \"My…\n$ location           &lt;chr&gt; \"Aung Tha Pyay\", \"Pin Lel Gyi\", \"Thabeikkyin\", \"Khi…\n$ geo_precision      &lt;dbl&gt; 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, …\n$ source             &lt;chr&gt; \"Democratic Voice of Burma; Irrawaddy\", \"Irrawaddy\"…\n$ source_scale       &lt;chr&gt; \"National\", \"National\", \"National\", \"Subnational-Na…\n$ notes              &lt;chr&gt; \"On 30 June 2024, near Aung Tha Pyay village (Pathe…\n$ fatalities         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ tags               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ timestamp          &lt;dbl&gt; 1720552468, 1720552468, 1720552468, 1720552468, 172…\n$ year_quarter       &lt;chr&gt; \"2024 Q2\", \"2024 Q2\", \"2024 Q2\", \"2024 Q2\", \"2024 Q…\n$ actor2             &lt;chr&gt; \"Military Forces of Myanmar (2021-)\", \"Military For…\n$ assoc_actor_2      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Uniden…\n$ geometry           &lt;POINT [m]&gt; POINT (214961 2452068), POINT (198303.2 24994…\n\n\n\n\n\n\n\n\nObservations\n\n\n\nNotice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been removed from the data frame.\n\n\nWe can further inspect the newly created ‘geometry’ column of conflict_data_sf\n\n# Retrieve geometry column\nst_geometry(conflict_data_sf)\n\nGeometry set for 51553 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -208804.4 ymin: 1103500 xmax: 640934.5 ymax: 3042960\nProjected CRS: WGS 84 / UTM zone 47N\nFirst 5 geometries:\n\n\nPOINT (214961 2452068)\n\n\nPOINT (198303.2 2499463)\n\n\nPOINT (189105.4 2533434)\n\n\nPOINT (160913.9 2522331)\n\n\nPOINT (146213 2428487)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt consists of 51,533 features consisting of point geometric features where the underlying datum is in WGS 84 format.\n\n\nTo ensure that the coordinate system is correctly updated, we can use the st_crs() function where we observe that the ESPG code is correctly indicated as 32647.\n\n# Check CRS format\nst_crs(conflict_data_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\n3.5 Reduce Data File Size\nIn this section, I will reduce the current Myanmar armed conflict dataset as the time taken for computing the kernel density estimates can take up to 30 minutes long which is not computationally efficient.\n\n1) Remove ‘Protests’ and ‘Riots’ Event Types\nI will remove rows in the conflicts_data_sf dataset that don’t focus on the four main event types (Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians), as mentioned in the exercise brief.\n\nconflict_data_sf &lt;- conflict_data_sf %&gt;%\n  filter(!(event_type %in% c(\"Protests\", \"Riots\")))\n\nunique(conflict_data_sf$event_type)\n\n[1] \"Battles\"                    \"Strategic developments\"    \n[3] \"Violence against civilians\" \"Explosions/Remote violence\"\n\n\n\n\n2) Remove unused columns in boundary_sf\nAs seen, there are 8 columns in the simple feature data frame of boundary_sf.\n\n# Inspect first rows of data in boundary_sf\nhead(boundary_sf)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -14915.04 ymin: 1736124 xmax: 187961.7 ymax: 2051144\nProjected CRS: WGS 84 / UTM zone 47N\n  OBJECTID         ST ST_PCODE        DT   DT_PCODE      DT_MMR PCode_V\n1        1 Ayeyarwady   MMR017  Hinthada MMR017D002    ဟင်္သာတခရိုင်     9.4\n2        2 Ayeyarwady   MMR017   Labutta MMR017D004    လပွတ္တာခရိုင်     9.4\n3        3 Ayeyarwady   MMR017    Maubin MMR017D005     မအူပင်ခရိုင်     9.4\n4        4 Ayeyarwady   MMR017 Myaungmya MMR017D003 မြောင်းမြခရိုင်     9.4\n5        5 Ayeyarwady   MMR017   Pathein MMR017D001      ပုသိမ်ခရိုင်     9.4\n6        6 Ayeyarwady   MMR017    Pyapon MMR017D006     ဖျာပုံခရိုင်     9.4\n                        geometry\n1 MULTIPOLYGON (((90859.89 20...\n2 MULTIPOLYGON (((75991.51 17...\n3 MULTIPOLYGON (((115559 1928...\n4 MULTIPOLYGON (((39919.39 18...\n5 MULTIPOLYGON (((-6302.348 1...\n6 MULTIPOLYGON (((93411.72 17...\n\n\nI will remove ’DT_MMR” column as we already have the District Name in English in DT and won’t require the district names in Myanmar Language. Next, we will remove the coded versions of ST (state/region) and DT (district) columns, namely ST_PCODE and DT_PCODE. Additionally, we won’t need the PCode_V column since we will be dropping the PCODE column too.\n\nboundary_sf &lt;- boundary_sf %&gt;% dplyr::select('OBJECTID', 'ST', 'DT','geometry')\nsummary(boundary_sf)\n\n    OBJECTID          ST                 DT                     geometry \n Min.   : 1.00   Length:80          Length:80          MULTIPOLYGON :80  \n 1st Qu.:20.75   Class :character   Class :character   epsg:32647   : 0  \n Median :40.50   Mode  :character   Mode  :character   +proj=utm ...: 0  \n Mean   :40.50                                                           \n 3rd Qu.:60.25                                                           \n Max.   :80.00                                                           \n\n\n\n\n3) Remove unused columns in conflict_data\nI will also remove unnecessary columns of the conflict_data data frame that won’t be used in our spatial analysis later.\n\n\nRemove unnecessary columns\nconflict_data_sf &lt;- conflict_data_sf %&gt;%\n  select(event_id_cnty, event_date, year_quarter, disorder_type, event_type, location, geometry, fatalities)\n\nsummary(conflict_data_sf)\n\n\n event_id_cnty        event_date         year_quarter       disorder_type     \n Length:42608       Min.   :2021-01-01   Length:42608       Length:42608      \n Class :character   1st Qu.:2022-01-10   Class :character   Class :character  \n Mode  :character   Median :2022-10-13   Mode  :character   Mode  :character  \n                    Mean   :2022-10-29                                        \n                    3rd Qu.:2023-08-29                                        \n                    Max.   :2024-06-30                                        \n  event_type          location                  geometry       fatalities    \n Length:42608       Length:42608       POINT        :42608   Min.   :  0.00  \n Class :character   Class :character   epsg:32647   :    0   1st Qu.:  0.00  \n Mode  :character   Mode  :character   +proj=utm ...:    0   Median :  0.00  \n                                                             Mean   :  1.27  \n                                                             3rd Qu.:  1.00  \n                                                             Max.   :201.00  \n\n\nLet’s append conflict_data_sf with the columns of boundary_sf to assist our analysis later.\n\n# Link conflict event to its district region\nconflict_data_sf &lt;- st_join(conflict_data_sf, boundary_sf, join = st_intersects)\n\n\n\n\n3.6 Converting Simple Features Data Frame into ppp Object\nIt is important that we convert conflict_data_sf (a simple feature data frame) into a planer point pattern (ppp) object format, since the spatstat package that we’ll be using for the Spatial Point Pattern Analysis later is specifically designed for working with ppp-formated data. Additionally, I will begin with categorising the ppp objects into their unique year_quarter category.\n\n\nCreate ppp objects based on year_quarter category\n# Create an empty list to store the ppp objects\nppp_list &lt;- list()\n\n# Loop through each unique year_quarter category\nfor (yq in unique(conflict_data_sf$year_quarter)) {\n  # Subset the data for the current year_quarter\n  subset_data_sf &lt;- conflict_data_sf %&gt;% filter(year_quarter == yq)\n  \n  # Convert the subset to a ppp object\n  subset_ppp &lt;- as.ppp(subset_data_sf$geometry)\n  \n  # Add the ppp object to the list\n  ppp_list[[yq]] &lt;- subset_ppp\n}\n\n# Check list\nppp_list\n\n\n$`2024 Q2`\nPlanar point pattern: 2788 points\nwindow: rectangle = [-208804.4, 597543.7] x [1103500.1, 3026504.9] units\n\n$`2024 Q1`\nPlanar point pattern: 3186 points\nwindow: rectangle = [-207135, 591875.9] x [1245380, 3026504.9] units\n\n$`2023 Q4`\nPlanar point pattern: 3627 points\nwindow: rectangle = [-206931.7, 604775.1] x [1103500.1, 3020772.2] units\n\n$`2023 Q3`\nPlanar point pattern: 3010 points\nwindow: rectangle = [-197883.4, 518300.4] x [1103500.1, 3027041.8] units\n\n$`2023 Q2`\nPlanar point pattern: 2745 points\nwindow: rectangle = [-191261.5, 518300.4] x [1103500.1, 3006372.9] units\n\n$`2023 Q1`\nPlanar point pattern: 3101 points\nwindow: rectangle = [-199243.8, 591875.9] x [1103500.1, 3026504.9] units\n\n$`2022 Q4`\nPlanar point pattern: 3296 points\nwindow: rectangle = [-206531.5, 518300.4] x [1103500.1, 2931517.1] units\n\n$`2022 Q3`\nPlanar point pattern: 3486 points\nwindow: rectangle = [-206196.6, 568361.5] x [1103500.1, 3026504.9] units\n\n$`2022 Q2`\nPlanar point pattern: 3580 points\nwindow: rectangle = [-206931.7, 640934.5] x [1103500.1, 3026504.9] units\n\n$`2022 Q1`\nPlanar point pattern: 3563 points\nwindow: rectangle = [-204784, 591875.9] x [1103500.1, 3026504.9] units\n\n$`2021 Q4`\nPlanar point pattern: 3844 points\nwindow: rectangle = [-200024.3, 591875.9] x [1103500.1, 3042960.3] units\n\n$`2021 Q3`\nPlanar point pattern: 2754 points\nwindow: rectangle = [-193181.1, 591875.9] x [1103500.1, 3042960.3] units\n\n$`2021 Q2`\nPlanar point pattern: 2916 points\nwindow: rectangle = [-191409.1, 640934.5] x [1132472.1, 3042960.3] units\n\n$`2021 Q1`\nPlanar point pattern: 712 points\nwindow: rectangle = [-203795.3, 591875.9] x [1375186.1, 3026504.9] units\n\n\nWe can visualise the spread of conflict events across each quarter from January 2021 to June 2024 using the plot() function as shown below.\n\n\nVisualise the spread of conflicts by year_quarter\n# Ensure 'year_quarter' is a factor\nconflict_data_sf$year_quarter &lt;- as.factor(conflict_data_sf$year_quarter)\n\n# Loop through each unique year_quarter and create separate plots\nyear_quarters &lt;- unique(conflict_data_sf$year_quarter)\n\n# Set up a grid layout for multiple plots (adjust 'mfrow' as needed)\npar(mfrow = c(2,3))\npar(mar = c(0,0,1,0))\n\n# Loop through each year_quarter and plot\nfor (yq in year_quarters) {\n  subset_data_sf &lt;- conflict_data_sf[conflict_data_sf$year_quarter == yq, ]\n  conflict_data_ppp &lt;- as.ppp(subset_data_sf$geometry)\n  \n  # Plot each subset ppp object\n  plot(conflict_data_ppp, main = paste(\"Year-Quarter:\", yq))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt is noticeable that there conflict events have occured more frequently since 2021 as points plotted on the graph have gotten darker across 2021 to 2024. We can also observe the possibility of duplicated events occurring from the darker spots in the plot, in which it appears more intense in Myanmar’s central and west regions.\n\n\n\n\n3.7 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area, that is Myanmar’s boundary in this case. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to convert the boundary_data_sf simple feature data frame into an owin object of spatstat.\n\n# Convert to owin object\nmyanmar_owin &lt;- as.owin(boundary_sf)\n\n# Visualise the owin object\nplot(myanmar_owin)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom my observations, the as.owin() function converts the boundary_data_sf spatial boundary into a window object that represents the outer boundary of the spatial region and does not handle internal structures or districts we previously saw from the plot of boundary_data_sf.\n\n\nWe can also take a quick look at the owin object properties as shown. I will be converting it to a data frame for the purposes of getting a quick glimpse of the object.\n\n# Summary info of owin object\nowin_df &lt;- as.data.frame(myanmar_owin)\nprint(head(owin_df))\n\n         x       y id sign\n1 56519.39 2741919  1   -1\n2 56917.28 2741947  1   -1\n3 57000.15 2741973  1   -1\n4 57068.51 2741994  1   -1\n5 57221.44 2742142  1   -1\n6 57068.51 2741994  1   -1\n\n\n\n\n3.8 Combining ppp Object and owin Object\nIn this last step of geospatial data wrangling, I will mask all ppp object with the owin object I created earlier to put in place all conflict events within the boundary of Myanmar. Doing so can also optimise the memory usage for large datasets.\n\n\n\n\n\n\nThe ppp object outputted from combining both the point and polygon feature results in the boundary of Myanmar outlining the plot of conflict events as shown.\n\n# Set up plotting layout\nn &lt;- length(masked_ppp_list)\n\n# Plot each masked ppp object\npar(mfrow = c(2,3), mar = c(0,0,1,0))  # Adjust margins as needed\nfor (quarter in names(masked_ppp_list)) {\n  plot(masked_ppp_list[[quarter]], main = paste(\"Year Quarter:\", quarter))\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#st-order-spatial-point-patterns-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#st-order-spatial-point-patterns-analysis",
    "title": "Take-home Exercise 1",
    "section": "5. 1st Order Spatial Point Patterns Analysis",
    "text": "5. 1st Order Spatial Point Patterns Analysis\n\n5.1 Kernel Density Estimation\n\n5.1.1 Working with Fixed Bandwidth Methods\nUsing the geospatial data sets prepared, I will now perform 1st order spatial point pattern analysis by leveraging kernel density estimation (KDE) to understand the intensity of conflicts in different regions.\nI will be using a variety of fixed bandwidth methods via density() of the spatstat package, to determine the most optimal method for this analysis. Namely using bw.diggle(), bw.ppl(), bw.CvL() and bw.scott().\nSteps taken to calculate the KDE:\n\nExtract the masked ppp object for the current quarter.\nCompute the kernel density estimate by setting the signma parameters.\nPlot the kernel density estimate using plot() where “Bw” represents the optimal bandwidth\n\nFor the purposes of identifying the most optimal bandwidth method, I will create a ppp_obj using the 2021 Q1 conflict events first to assist my decision-making.\n\n# Set Up\nppp_obj = masked_ppp_list$`2021 Q1`\ncolours &lt;- colorRampPalette(c(\"midnightblue\", \"skyblue\"))(100)\n\n\n1) Using bw.diggle()\nThe bw.diggle() bandwidth is referred to as Diggle’s cross-validation bandwidth which minimises the mean-squared error (MSE) to balance between under and over-smoothing. I will use the density() function to compute the kernel density of the masked ppp objects and visualise the distribution of conflict event points by using the plot() function.\n\n# bw.diggle()\nkde_conflict_bw_diggle &lt;- density(ppp_obj,\n                           sigma=bw.diggle,\n                           edge=TRUE,\n                           kernel=\"gaussian\")\noptimal_bw_d = floor(bw.diggle(ppp_obj)[[1]]*10)/10\nplot(kde_conflict_bw_diggle, main = paste(\"BW: diggle\", \"(\",optimal_bw_d,\"m)\"), col = colours)\n\n\n\n\n2) Using bw.ppl()\nThe second bandwidth method I attempted using is bw.ppl(), This method chooses the bandwidth that minimises the likelihood cross-validation score and improving the prediction accuracy of the kernel density estimate.\n\n# bw.ppl()\nkde_conflict_bw_ppl &lt;- density(ppp_obj,\n                           sigma=bw.ppl,\n                           edge=TRUE,\n                           kernel=\"gaussian\")\noptimal_bw_p = floor(bw.ppl(ppp_obj)[[1]]*10)/10\nplot(kde_conflict_bw_ppl, main = paste(\"Bw: ppl\", \"(\",optimal_bw_p,\"m)\"), col = colours)\n\n\n\n\n\n\n\n\nReflections\n\n\n\nAs bw.ppl() tends to choose smaller bandwidths, it provide more localised density estimates which highlights finer spatial details. As such, we can see more variability and finer details in the density distribution, with more variation between high- and low-density areas.\n\n\n\n\n3) Using bw.CvL()\nThirdly, let’s explore the bandwidth method bw.CvL(), also known as Cronie and Van Lieshout cross-validation, designed to provide an optimal, adaptive bandwidth for inhomogeneous point patterns.. Similar to bw.ppl(), it aims to reduce the error measure but also aims to balance over and under-fitting based on the spatial structure of the data.\n\n# bw.CvL()\nkde_conflict_bw_CvL &lt;- density(ppp_obj,\n                           sigma=bw.CvL,\n                           edge=TRUE,\n                           kernel=\"gaussian\")\noptimal_bw_c = floor(bw.CvL(ppp_obj)[[1]]*10)/10\nplot(kde_conflict_bw_CvL, main = paste(\"Bw: CvL (\",optimal_bw_c,\"m)\"), col = colours)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe kernel density plot shows that CvL makes a good attempt in balancing between detail and smoothness, making it more suitable for capturing the overall density trends in spatial data with some local structures highlighted.\n\n\n\n\n4) Using bw.scott()\nLastly, I will explore the bw.scott() bandwidth method. This method returns separate bandwidths for the x- and y-axes which is ideal for our spatial data that contains both x and y components. I will combine these bandwidths into a single value for isotropic kernel density estimation by taking the taking the geometric mean as shown in the value returned by sigma_combined.\n\n# bw.scott()\nbw_values &lt;- bw.scott(ppp_obj)\nsigma_x &lt;- bw_values[1]\nsigma_y &lt;- bw_values[2]\nsigma_combined &lt;- sqrt(sigma_x * sigma_y)\n\nkde_conflict_bw_scott &lt;- density(ppp_obj,\n                           sigma = sigma_combined,\n                           edge = TRUE,\n                           kernel = \"gaussian\")\n\noptimal_bw_s = floor(sigma_combined*10)/10\nplot(kde_conflict_bw_scott, main = paste(\"Bw: scott\", \"(\",optimal_bw_s,\"m)\"), col = colours)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nAs shown, the geometric mean ensures equal smoothing in both x and y directions, and it largely similar to bw.Cvl(), making it a good choice for a balanced and general overview of the spatial data distribution.\n\n\n\n\nSelecting a Bandwidth Method\nBased on my research and observations of the charts below, the four methods cater to different types of data depending on how varied the densities are spread across and the granularity of conflict events. Additionally, I notice that bw.ppl() takes a significantly longer time to complete its KDE computations.\n\nbw.diggle() seems effective for homogeneous data in seeing general conflict hotspots.\nbw.ppl() for non-homogeneous data in analysing specific locations of localised conflict zones.\nbw.CvL() for non-homogeneous data in capturing both localised conflicts and the broader conflict trends.\nbw.scott() for a fast overview and aren’t focusing on small clusters or detailed variations especially when working with large datasets.\n\n\n\nPlot all bandwidth methods\npar(mfrow = c(2,2), mar = c(0,0,1,0)) \nplot(kde_conflict_bw_diggle, main = paste(\"diggle (\",optimal_bw_d,\"m)\"), col = colours)\nplot(kde_conflict_bw_ppl, main = paste(\"ppl (\",optimal_bw_p,\"m)\"), col = colours)\nplot(kde_conflict_bw_CvL, main = paste(\"CvL (\",optimal_bw_c,\"m)\"), col = colours)\nplot(kde_conflict_bw_scott, main = paste(\"scott (\",optimal_bw_s,\"m)\"), col = colours)\n\n\n\n\n💡 Decision: I decided to use bw.CvL() for computing the KDE of the masked ppp objects based on each quarter. As seen above, we get a relatively smooth density estimate that isn’t too detailed like bw.ppl() and has a bandwidth of ~10,000m less than bw.scott() which makes the density not as generalised. This bandwidth method is effective for the non-homogeneous data spread of our Myanmar conflict data and it isn’t as computationally heavy as bw.ppl().\n\n\n\nPutting Together our Fixed KDE\nNow, let us perform the KDE computation for the conflict events across all quarters using bw.CvL().\n\n# Calculate density using bw.CvL()\npar(mfrow = c(2,3), mar = c(0,0,1,0)) \nfor (quarter in names(masked_ppp_list)) {\n  ppp_obj = masked_ppp_list[[quarter]]\n  kde_conflict_bw &lt;- density(ppp_obj,\n                             sigma=bw.CvL,\n                             edge=TRUE,\n                             kernel=\"gaussian\")\n  optimal_bw = floor(bw.CvL(ppp_obj)[[1]]*10)/10\n  plot(kde_conflict_bw, main = paste(quarter, \"(Bw:\",optimal_bw,\"m)\"), col = colours)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nUsing a fixed KDE is beneficial for our quarterly analysis here as it ensures consistency in bandwidth across different time periods so we can focus on spatial distribution over time.\nDensity Values Range: The density values of the output range from 0 to 0.000000002 which can be too small to comprehend. After some research, it appears that the default unit of measurement of EPSG:32647 is in metres. As such, this causes the density values computed to be in number of points per square metre.\nBandwidth Size: A bandwidth of around 60,000 to 100,000 is considered relatively large as compared to the bandwidth returned from using bw.diggle() and bw.ppl(). Hence, this results in a smoother density estimate with less emphasis on local clusters as indicated in the generalised spatial trends.\n\n\nTo make the density values more comprehensible, we will re-scale the density values from metres to kilometres using rescale().\n\nmasked_ppp_list_km = list()\n\nfor (quarter in names(masked_ppp_list)) {\n  ppp_obj &lt;- masked_ppp_list[[quarter]]\n  ppp_obj_km &lt;- rescale(ppp_obj, 1000, \"km\")\n  masked_ppp_list_km[[quarter]] &lt;- ppp_obj_km\n}\n\nNow, we can re-run the density() function to compute the KDE map and round the numbers to their 3rd decimal place.\n\npar(mfrow = c(2,3), mar = c(0,0,1,0)) \nfor (quarter in names(masked_ppp_list_km)) {\n  ppp_obj = masked_ppp_list_km[[quarter]]\n  kde_conflict_bw &lt;- density(ppp_obj,\n                             sigma=bw.CvL,\n                             edge=TRUE,\n                             kernel=\"gaussian\")\n  optimal_bw = floor(bw.CvL(ppp_obj)[[1]]*1000)/1000\n  plot(kde_conflict_bw, main = paste(quarter, \"(Bw:\",optimal_bw,\"km)\"), col = colours)\n}\n\n\n\n\n\n💡 Notice the output image looks identical to the earlier version, the only changes are in the data values from metres to kilometres (refer to the legend).\n\n\n\n\n5.1.2 Working with Different Kernel Methods\nNow, I will experiment with a variety of kernels for the CvL bandwidth method, specifically using the 2021 Q1 conflict data to assist me. I will be using these four kernel methods, namely gaussian, epanechniko, quartic and disc.\n\n\n\n\n\n\nObservations\n\n\n\nIt is of my observation that the CvL bandwidth will automatically be defaulted to the Gaussian kernel like most KDE implementations which causes a warning message ’Bandwidth selection will be based on Gaussian kernel’. This means that only after the bandwidth is selected using the Gaussian kernel, the KDE calculation will perform using the non-gaussian kernel specified e.g. quartic / epanechniko / disc. In many cases, this still provides a reasonable estimate.\n\n\nWith that said, let us begin with setting up the ppp_obj taken from 2021 Q1 and run the density estimation for each kernel method to identify the most optimal for our dataset.\n\n# Set Up\npar(mfrow = c(2,2), mar = c(0,0,1,0)) \nppp_obj = masked_ppp_list$`2021 Q1`\n\n# Using the gaussian kernel\nkde_conflict_g &lt;- density(ppp_obj,\n                           sigma=bw.CvL,\n                           edge=TRUE,\n                           kernel=\"gaussian\")\nplot(kde_conflict_g, main=\"Gaussian Kernel\", col = colours)\n\n# Using the epanechniko kernel\nkde_conflict_e &lt;- density(ppp_obj,\n                           sigma=bw.CvL,\n                           edge=TRUE,\n                           kernel=\"epanechnikov\")\nplot(kde_conflict_e, main=\"Epanechnikov Kernel\", col = colours)\n\n# Using the quartic kernel\nkde_conflict_g &lt;- density(ppp_obj,\n                           sigma=bw.CvL,\n                           edge=TRUE,\n                           kernel=\"quartic\")\nplot(kde_conflict_g, main=\"Quartic Kernel\", col = colours)\n\n# Using the disc kernel\nkde_conflict_e &lt;- density(ppp_obj,\n                           sigma=bw.CvL,\n                           edge=TRUE,\n                           kernel=\"disc\")\nplot(kde_conflict_e, main=\"Disc Kernel\", col = colours)\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nGaussian: provides a localised density estimate over the entire spatial extent as compared to epanechnikov and quartic. It is good at highlighting variance and opposing ends of conflict intensities as shown by the wider range used in the legend.\nEpanechnikov: It is more efficient than the gaussian in terms of variance but produces a slightly rougher surface. It is also more localised than the quartic kernel, focusing on areas near each point, with a sharper boundary at the bandwidth limit.\nQuartic: Results in a good balance between smoothness and localised influence, smoother than epanechnikov but with similar properties. It appears suitable for moderate smoothing and sharper focus on local patterns.\nDisc: results in the sharpest density estimate as compared to the other three kernels as all points within a certain distance are made to have equal influence and zero influence beyond that distance.\n\nDecision: Hence, I will use the quartic kernel method to ensure a relatively smooth density estimate with emphasis on local points over distant ones.\n\n\nAs such, I run the density estimate computation using kernel = 'quartic'.\n\n# Using 'quartic' kernel\npar(mfrow = c(2,3), mar = c(0,0,1,0)) \n\nfor (quarter in names(masked_ppp_list_km)) {\n  ppp_obj = masked_ppp_list_km[[quarter]]\n  kde_conflict_bw &lt;- density(ppp_obj,\n                             sigma=bw.CvL,\n                             edge=TRUE,\n                             kernel=\"quartic\")\n  optimal_bw = floor(bw.CvL(ppp_obj)[[1]]*1000)/1000\n  plot(kde_conflict_bw, main = paste(quarter, \"(Bw:\",optimal_bw,\"km)\"), col = colours)\n}\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can see high densities of armed conflict in the central and southern regions of Myanmar but more can be uncovered from conflict data. Let’s proceed to the next section.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor all subsequent fixed KDE computations, I will assign sigma using the average of the CvL bandwidth returned from each quarter. Here’s the calculations based on the plots returned:\nAverage bandwidth size = 61.649 + 64.386 + 74.501 + 114.08 + 103.863 + 103.863 + 95.567 + 56.757 + 57.752 + 66.968 + 49.649 + 48.135 + 48.135 + 60.323) / 14 = 71.831\n\n\nLet’s recompute the Fixed KDE based on the newly calculated average bandwidth such that sigma = 71.831. I’ll store the quarterly KDE outputs into a list called kde_conflict_bw_list.\n\n# Using sigma = 71.831\npar(mfrow = c(2,3), mar = c(0,0,1,0)) \n\n# Add KDE into this list\nkde_conflict_bw_list &lt;- list()\nfor (quarter in names(masked_ppp_list_km)) {\n  ppp_obj = masked_ppp_list_km[[quarter]]\n  kde_conflict_bw &lt;- density(ppp_obj,\n                             sigma=71.831,\n                             edge=TRUE,\n                             kernel=\"quartic\")\n  optimal_bw = floor(bw.CvL(ppp_obj)[[1]]*1000)/1000\n  kde_conflict_bw_list[[quarter]] &lt;- kde_conflict_bw\n  plot(kde_conflict_bw, main = paste(quarter, \"(Bw:\",optimal_bw,\"km)\"), col = colours)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.3 Working with Adaptive KDE\n\n\n\n\n\nAs seen above, fixed bandwidths tend to oversmooth the mode of the distribution. On the contrary, the adaptive kernel estimate has the ability to reduce variability of estimates in areas with low density and increases it in areas with higher density (The Stata Journal, 2003).\nOnce again, let us use the 2021 Q1 conflict data to illustrate the difference in outputs of all three adaptive methods.\n\n\n\n\n\n\nWe can also compare the performance of each method based on the top 4 highest density states as highlighted earlier.\n\n\n\n\n\n\nComparing the three Adaptive KDE Types\nFrom the outputs above, it appears that there is no major differences between the distribution of KDE values returned across the three methods, where there is high concentration of points in a specific area. Hence, we will choose to go with Adapative Kernel method.\n\npar(mar = c(2,2,2,2),mfrow = c(3,1))\nhist(vd_adaptive_kde,main = \"Voronoi-Dirichlet Adaptive\")\nhist(adaptive_kde,main = \"Adaptive Kernel\")\nhist(nn_kde,main = \"Nearest-Neighbour Adaptive\")\n\n\nLet’s compare the results of my two selected fixed and adaptive KDEs (E.g. Magway District)\n\npar(mfrow = c(1,2), mar = c(0,0,1,0))\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_magway))\nppp_obj &lt;- as.ppp(st_geometry(conflict_magway), W = district_boundary)\nppp_obj &lt;- rescale(ppp_obj, 1000, \"km\")\n\nkde_fixed &lt;- density(ppp_obj, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\")\nplot(kde_fixed, main = paste(\"Fixed KDE (CvL | quartic)\"), col = colours)\nkde_adaptive &lt;- adaptive.density(ppp_obj, method=\"kernel\")\nplot(kde_adaptive, main = paste(\"Adaptive KDE (Kernel)\"), col=colours)\n\n\n\nWe can observe how adaptive kernels provides a more detailed picture of conflict spatial distribution but since it’s largely localised, conflict spots require more effort in identifying and can be computationally heavy for this exercise.\nAdditionally, varying bandwidth makes comparisons across regions or time periods (like quarters) more difficult because the scale of smoothing is not constant across space and time.\n\n\n\n5.1.5 Converting Gridded KDE Output into Raster\nNext, we need to convert the KDE output to KDE raster layers before it can be viewed using tmap.\nStep 1) Converting KDE to Spatial Grid Data Frame\n\nlibrary(grid)\n\n\nAttaching package: 'grid'\n\n\nThe following object is masked from 'package:spatstat.geom':\n\n    as.mask\n\nplot_list &lt;- list()\nfor (quarter in names(kde_conflict_bw_list)) {\n  ppp_obj &lt;- kde_conflict_bw_list[[quarter]]\n  gridded_ppp_obj &lt;- as(ppp_obj, \"SpatialGridDataFrame\")\n  plot_list[[quarter]] &lt;- spplot(gridded_ppp_obj, main = paste(quarter), col.regions = colours)\n}\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nplot_list_subset1 &lt;- plot_list[1:6]\nplot_list_subset2 &lt;- plot_list[7:12]\nplot_list_subset3 &lt;- plot_list[13:14]\ngrid.newpage()\ngrid.arrange(grobs = plot_list_subset1, ncol = 3, nrow = 2)\n\n\n\n\n\n\n\ngrid.newpage() \ngrid.arrange(grobs = plot_list_subset2, ncol = 3, nrow = 2)\n\n\n\n\n\n\n\ngrid.newpage()\ngrid.arrange(grobs = plot_list_subset3, ncol = 3, nrow = 2)\n\n\n\n\n\n\n\n\nStep 2) Rasterisation of Grid Outputs & Assigning Projection Systems\n\ngridded_ppp_obj_raster_list &lt;- list()\nfor (quarter in names(kde_conflict_bw_list)) {\n  gridded_ppp_obj = kde_conflict_bw_list[[quarter]]\n  gridded_ppp_obj_raster &lt;- raster(gridded_ppp_obj)\n  projection(gridded_ppp_obj_raster) &lt;- CRS(\"+init=EPSG:32647\")\n  gridded_ppp_obj_raster_list[[quarter]] &lt;- gridded_ppp_obj_raster\n}\n\n# Inspect for 2024 Q2\ngridded_ppp_obj_raster_list$`2024 Q2`\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 7.302001, 16.30032  (x, y)\nextent     : -210.0086, 724.6476, 1072.026, 3158.467  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=47 +datum=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : 7.061861e-06, 0.01847887  (min, max)\n\n\nStep 3) Plot Maps\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nplots_by_quarter &lt;- list()\nfor (quarter in names(gridded_ppp_obj_raster_list)){\n  gridded_ppp_obj_raster = gridded_ppp_obj_raster_list[[quarter]]\n  raster_plot &lt;- tm_shape(gridded_ppp_obj_raster) +\n    tm_raster(\"layer\", title=\"Density\", palette=\"Blues\") +\n    tm_layout(legend.position = c(\"left\",\"bottom\"),frame=FALSE, main.title = quarter,\n            main.title.size=1, main.title.position = \"center\", legend.text.size = 0.5,legend.title.size = 0.7) \n  plots_by_quarter[[quarter]] &lt;- raster_plot\n}\n\ntmap_arrange(plots_by_quarter[1:14], ncol=5, nrow=3)\n\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\nLegend labels were too wide. The labels have been resized to 0.3, 0.3, 0.3, 0.3, 0.3, 0.3. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nPlotting raster grid versions of KDE outputs uses discrete colour ranges which does a good job in highlighting gradual changes in conflict events across an area. Since 2021 Q2, more conflicts are seen in Southern parts of Myanmar. The density range differs for each quarter but we can see an increase in no. of armed conflicts per kilometre from 2021 Q1 to 2022 Q2, which stagnates in density and increases again in 2023 Q4.\n\n\n\n\n\n5.2 Nearest Neighbour Analysis\nOur current analyses does not reveal patterns of clustering or dispersion, to which Michael J. Crawley proposes to employ Clark-Evans test spatial randomness for its simplicity and applicability for first-order spatial analysis, which means checking for overall spatial randomness based on nearest-neighbor distances. (Crawley M. J. , 2007)\n\n\n\n\n\nClark-Evans Test\nThe test checks whether the observed point pattern of armed conflicts in Myanmar shows clustering (points are closer than expected under randomness), dispersion (points are more spread out), or randomness.\nThe test hypotheses are:\n\nHo = The distribution of armed conflicts in Myanmar are randomly distributed.\nH1= The distribution of armed conflicts in Myanmar are not randomly distributed.\nThe 95% confident interval will be used.\n\nWe will conduct the test using clarkevans.test() of statspat.\n\nfor (quarter in names(masked_ppp_list_km)) {\n  ppp_obj = masked_ppp_list_km[[quarter]]\n  print(quarter)\n  print(clarkevans.test(ppp_obj,\n                  correction=\"none\",\n                  clipregion=\"boundary_sf\",\n                  alternative=c(\"clustered\"),\n                  nsim=99))\n}\n\n[1] \"2024 Q2\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.26663, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2024 Q1\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.23563, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2023 Q4\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.21795, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2023 Q3\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.22002, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2023 Q2\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.24485, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2023 Q1\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.24365, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2022 Q4\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.22139, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2022 Q3\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.23974, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2022 Q2\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.22989, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2022 Q1\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.21976, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2021 Q4\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.21341, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2021 Q3\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.21808, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2021 Q2\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.17458, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n[1] \"2021 Q1\"\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  ppp_obj\nR = 0.24696, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFor a 95% confidence level, If the p-value &lt; 0.05, I will reject the null hypothesis of complete spatial randomness and check if data is uniform (R &gt; 1) or clustered (R &lt; 1).\nWith that said, all tests conducted across each quarter rejects the null hypothesis as p &lt; 0.05 and spatial points are found to be clustered since R &lt; 1.\n\n\n\n\n5.3 Further Data Exploration\nBy using the fixed KDE with CvL bandwidth and quartic kernel, let’s see what insights can we glean from the density of conflicts in Myanmar.\n\n5.3.1 KDE by Event Type\nFirst, let’s identify the unique event types in this dataset.\n\n# Check unique events\nunique(conflict_data_sf$event_type)\n\n[1] \"Battles\"                    \"Strategic developments\"    \n[3] \"Violence against civilians\" \"Explosions/Remote violence\"\n\n\nNow, let us analyse the kernel density estimate of each unique event type found in conflict_data_sf to identify hot and cold spots across Myanmar.\n\n\nPlot the KDE based on Event Type\n# Set Up\npar(mfrow = c(2,2), mar = c(0,0,1,0)) \n\nconflict_data_sf %&gt;%\n  group_by(event_type) %&gt;%\n  group_split() -&gt; conflict_by_event_type\n\n# Convert the sf object to owin\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_sf))\n\nkde_list &lt;- lapply(seq_along(conflict_by_event_type), function(i) {\n  data &lt;- conflict_by_event_type[[i]]\n  event_type &lt;- unique(data$event_type)\n  ppp_obj &lt;- as.ppp(st_geometry(data), W = district_boundary)\n  ppp_obj &lt;- rescale(ppp_obj, 1000, \"km\")\n  kde &lt;- density(ppp_obj,\n                 sigma=71.831,\n                 edge=TRUE,\n                 kernel=\"quartic\")\n  plot(kde, main = paste(event_type), col=colours)\n  return(kde)\n})\n\n\nWarning: 3 points were rejected as lying outside the specified window\n\n\nWarning: data contain duplicated points\nWarning: data contain duplicated points\n\n\nWarning: 1 point was rejected as lying outside the specified window\n\n\nWarning: data contain duplicated points\n\n\nWarning: 3 points were rejected as lying outside the specified window\n\n\nWarning: data contain duplicated points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can almost see an equal spread of all four event types, with battles being more dominantly found in Central Myanmar, followed by strategic developments and violence against citizens.\n\n\n\n\n5.3.2 KDE Across Top 4 States With Most Conflicts\nPreviously, we identified the top 4 states with the highest proportions of conflicts as Sagaing, Mandalay, Magway and Yangon. We can delve deeper into each state by analysing the intensity of conflicts across these states using density().\n\n\nPlot the KDE of Top 4 States\n# Set Up\npar(mfrow = c(2,2), mar = c(0,0,1,0))\n\n# Sagaing\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_sagaing))\nppp_obj &lt;- as.ppp(st_geometry(conflict_sagaing), W = district_boundary)\n\n\nWarning: data contain duplicated points\n\n\nPlot the KDE of Top 4 States\nppp_obj_sagaing &lt;- rescale(ppp_obj, 1000, \"km\")\nkde &lt;- density(ppp_obj_sagaing,\n               sigma=71.831,\n               edge=TRUE,\n               kernel=\"quartic\")\nplot(kde, main = paste(\"Sagaing\"), col=colours)\n\n# Mandalay\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_mandalay))\nppp_obj &lt;- as.ppp(st_geometry(conflict_mandalay), W = district_boundary)\n\n\nWarning: data contain duplicated points\n\n\nPlot the KDE of Top 4 States\nppp_obj_mandalay &lt;- rescale(ppp_obj, 1000, \"km\")\nkde &lt;- density(ppp_obj_mandalay,\n               sigma=71.831,\n               edge=TRUE,\n               kernel=\"quartic\")\nplot(kde, main = paste(\"Mandalay\"), col=colours)\n\n# Magway\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_magway))\nppp_obj &lt;- as.ppp(st_geometry(conflict_magway), W = district_boundary)\n\n\nWarning: data contain duplicated points\n\n\nPlot the KDE of Top 4 States\nppp_obj_magway &lt;- rescale(ppp_obj, 1000, \"km\")\nkde &lt;- density(ppp_obj_magway,\n               sigma=71.831,\n               edge=TRUE,\n               kernel=\"quartic\")\nplot(kde, main = paste(\"Magway\"), col=colours)\n\n# Yangon\ndistrict_boundary &lt;- as.owin(st_as_sfc(boundary_yangon))\nppp_obj &lt;- as.ppp(st_geometry(conflict_yangon), W = district_boundary)\n\n\nWarning: data contain duplicated points\n\n\nPlot the KDE of Top 4 States\nppp_obj_yangon &lt;- rescale(ppp_obj, 1000, \"km\")\nkde &lt;- density(ppp_obj_yangon,\n               sigma=71.831,\n               edge=TRUE,\n               kernel=\"quartic\")\nplot(kde, main = paste(\"Yangon\"), col=colours)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt’s interesting that armed conflict isn’t evenly distributed across the states though it does seem that armed conflict has inflicted the entire state of Yangon. Nonetheless, it is worth noting that Yangon is relatively smaller in size than the other three states and that will increase the density of conflict quite significantly.\n\n\n\n\n5.3.3 KDE of Top 4 States by Event Type\nIt’ll also be interesting to breakdown each top 4 state by the event type category as shown.\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe kernel density of violence against civillians is generally found to be the lowest amongst all conflict events. Additionally, all types of armed conflicts tend to occur repeatedly in the same parts of each state. E.g. conflicts regarding strategic development tend to happen in Southern part of the Sagaing state, just as it is for explosions/remote violence."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#overview",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#overview",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The conflict in Myanmar is not just a result of the coup but is deeply rooted in the country’s decades-old complex ethnic and political landscape, characterised by tensions between the central government and various ethnic minority groups, each with its own armed forces. The post-coup violence has exacerbated these long-standing conflicts, leading to a severe humanitarian crisis, with thousands killed, hundreds of thousands displaced, and widespread human rights abuses reported.\n\n\n\nAs such, Geospatial analytics has become a valuable tool for evaluating and comprehending the intricacies of increasing conflicts. This exercise aims to reveal the spatial and spatio-temporal distribution of armed conflict in Myanmar by leveraging spatial point pattern analysis. Additionally, it aims to gain clearer insights into the geographical and logistical patterns of violence throughout the nation.\nBy the end of this take-home exercise, I aim to complete these steps in my spatial point pattern analysis in uncovering the distribution of armed conflict in Myanmar.\n\nUsing appropriate function of sf and tidyverse packages, import and transform the downloaded armed conflict data and administrative boundary data into sf tibble data.frames.\nUsing the geospatial data sets prepared, derive quarterly KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatial Point Patterns Analysis.\nUsing the geospatial data sets prepared, derive quarterly spatio-temporal KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatio-temporal Point Patterns Analysis.\nUsing appropriate tmap functions, display the KDE and Spatio-temporal KDE layers on openstreetmap of Myanmar.\nDescribe the spatial patterns revealed by the KDE and Spatio-temporal KDE maps.\n\n\n\n\n\n\nThis Armed Conflict Location & Event Data (ACLED) is an independent, impartial, international non-profit organisation which owns an extensive database of violent conflict and protest in countries and territories around the world.\n\nFor the purpose of this exercise, I have downloaded ACLED’s data on Myanmar which includes a series of conflict events, particularly between 1 January 2021 to 30 June 2024.\n🔗 Source: ACLED\n📁 Format: comma separated values (CSV)\nAs the dataset is rather extensive, I will be performing my analysis on armed conflict events in a quarterly basis to streamline my tasks. The data included in this dataset are as follows:\nEvent Type\nACLED categorises events into various types. I will mainly be focusing on these four event types: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\n\n\n\nEvent Type\nACLED categorises events into various types. I will mainly be focusing on these four event types: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\n\nevent_id_cnty: unique ID for each conflict\nevent_type: category of event e.g. Battle, Violence Against Civilians, Protests, Explosions/Remote Violence, Strategic Developments\nsub_event_type: a more detailed classification within event type\ndisorder_type: classifies the event based on the nature of the disorder e.g. political violence, demonstrations, strategic developments[A1]\ncivilian_targeting: yes/no value, whether event involves specifically targeting civilians\nNote: when “strategic developments” are used in Event Type, it is also used in the disorder type (vice-versa)\n\n\n\nLocation and Geospatial Data\nThe database provides detailed geographic information, pinpointing the exact or approximate locations of conflict events across Myanmar. This includes cities, towns, and rural areas.\n\niso: the country code for Myanmar which uses 104 in this case\nregion: region of conflict within Myanmar\ncountry: indicates Myanmar\nadmin1, admin2, admin3: 1st, 2nd and 3rd level administration division within Myanmar e.g. states, division, sub-division\nlocation: specific geographic location or name of the place where the conflict event occurred\nlatitude: latitude of the conflict event\nlongitude: longitude of the conflict event\ngeo_precision: indicates the level of precision for the geographic coordinates provided\n\n\n\nDate and Time\nACLED records the specific dates and, where possible, times of conflict events.\n\nevent_date: date of conflict\nyear: year of conflict\ntime_precision: accuracy of the date and time information provided\n\n\n\nActors\n\nIndicate the actors involved in the conflict, such as the Tatmadaw (Myanmar’s military), ethnic armed organizations, local militias, civilian protestors, and other groups.\nactor1: primary actor involved in the conflict event. E.g. a government force, rebel group, militia, or any organised entity\nassoc_actor_1: a secondary group that is aligned with or supports the primary actor (Actor1) in the event\ninter1: an interaction code that categorises actor1, could be a government force, rebel group, military force, rioter, civilian, or other entities\ninteraction: combined description of actor1 and actor2\n\n\n\nFatalities\n\nfatalities: tracks the number of reported fatalities associated with each conflict event\n\n\n\nOthers\n\nsource: source of information for the conflict event\nsource_scale: scale of the source e.g. local, national, international\nnotes : additional comments\ntags: keywords associated with the conflict event\ntimestamp: date and time when conflict event was entered/updated in the database\n\n\n\n\n\n\n\nI will also be using a geospatial dataset from the Myanmar Information Management Unit (MIMU) in shapefile (.shp) format, specifically of the Myanmar state at the 2nd administrative level with district boundaries.\n🔗 Source: MIMU\n📁 Format: shapefile (.shp)\nMy reasoning for choosing the district boundary dataset is that we do not want to select a boundary dataset that is too broad when analysing conflict events since it might not provide sufficient insights to trends where conflict events happen. Neither do we want to analyse a geography that is too divided (e.g. Admin 3) since it can be computationally inefficient as seen in the types of boundary data below.\n\n\n\n\n\n\n\n\n\n\nAdmin 0\nAdmin 1\nAdmin 1\nAdmin 2 - To Use\nAdmin 3\n\n\n\n\nNational boundary\nMyanmar region\nRegion and sub-region\nDistrict boundary\nMyanmar township\n\n\n\n\n\n\n\n\n\n\nI have donwloaded the two data sets and organised them into my folder as follows."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#spatio-tempmoral-kde",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#spatio-tempmoral-kde",
    "title": "Take-home Exercise 1",
    "section": "7. Spatio-Tempmoral KDE",
    "text": "7. Spatio-Tempmoral KDE\nWe focus on the continuous time"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#overall",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#overall",
    "title": "In-class Exercise 3",
    "section": "1. Overall",
    "text": "1. Overall\n\n1.1 The research questions\nThe specific question we would like to answer is:\n\nare the locations of forest fire in Kepulauan Bangka Belitung spatial and spatio-temporally independent?\nif the answer is NO, where and when the observed forest fire locations tend to cluster?\n\n\n\n1.2 The data\nFor the purpose of this exercise, two data sets are used, they are:\n\nforestfires, a csv file provides locations of forest fire detected from the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor data. The data are downloaded from Fire Information for Resource Management System. For the purpose of this exercise, only forest fires within Kepulauan Bangka Belitung will be used.\nKepulauan_Bangka_Belitung, an ESRI shapefile showing the sub-district (i.e. kelurahan) boundary of Kepulauan Bangka Belitung. The data set was downloaded from Indonesia Geospatial portal. The original data covers the whole Indonesia. For the purpose of this exercise, only sub-districts within Kepulauan Bangka Belitung are extracted."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#installing-and-loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#installing-and-loading-the-r-packages",
    "title": "In-class Exercise 3",
    "section": "2. Installing and Loading the R packages",
    "text": "2. Installing and Loading the R packages\nFor the purpose of this study, I will be using these five R packages. They are:\n\nrgdal for importing geospatial data in GIS file format such as shapefile into R and save them as Spatial*DataFrame,\nmaptools for converting Spatial* object into ppp object,\nraster for handling raster data in R,\nspatstat for performing Spatial Point Patterns Analysis such as kcross, Lcross, etc., and\ntmap for producing cartographic quality thematic maps.\n\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#importing-and-preparing-study-area",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#importing-and-preparing-study-area",
    "title": "In-class Exercise 3",
    "section": "3. Importing and Preparing Study Area",
    "text": "3. Importing and Preparing Study Area\n\n3.1 Importing Study Area\nLet us first import the data using the st_read() function.\n\nkbb &lt;- st_read(dsn=\"data/rawdata\",\n               layer = \"Kepulauan_Bangka_Belitung\") \n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex4\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nkbb\n\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   OBJECTID         NAMOBJ      FCODE REMARK\n1     26195        Airbara BA03070040   &lt;NA&gt;\n2     26196       Airgegas BA03070040   &lt;NA&gt;\n3     26197    Arung Dalam BA03070040   &lt;NA&gt;\n4     26202 Batu Betumpang BA03070040   &lt;NA&gt;\n5     26205      Bedengung BA03070040   &lt;NA&gt;\n6     26206      Belimbing BA03070040   &lt;NA&gt;\n7     26207         Bencah BA03070040   &lt;NA&gt;\n8     26209          Berok BA03070040   &lt;NA&gt;\n9     26210         Bikang BA03070040   &lt;NA&gt;\n10    26212    Bukit Terap BA03070040   &lt;NA&gt;\n                                       METADATA SRS_ID KDBBPS KDCBPS   KDCPUM\n1  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.03\n2  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.03\n3  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.04.01\n4  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.07\n5  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.05\n6  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.04.06\n7  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.03\n8  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.04.01\n9  TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.01\n10 TASWIL1000020221227_DATA_BATAS_DESAKELURAHAN   4326   &lt;NA&gt;   &lt;NA&gt; 19.03.06\n   KDEBPS        KDEPUM KDPBPS KDPKAB KDPPUM LUASWH TIPADM      WADMKC\n1    &lt;NA&gt; 19.03.03.2008   &lt;NA&gt;  19.03     19      0      1   Air Gegas\n2    &lt;NA&gt; 19.03.03.2001   &lt;NA&gt;  19.03     19      0      1   Air Gegas\n3    &lt;NA&gt; 19.04.01.1002   &lt;NA&gt;  19.04     19      0      2        Koba\n4    &lt;NA&gt; 19.03.07.2001   &lt;NA&gt;  19.03     19      0      1  Pulaubesar\n5    &lt;NA&gt; 19.03.05.2006   &lt;NA&gt;  19.03     19      0      1      Payung\n6    &lt;NA&gt; 19.04.06.2009   &lt;NA&gt;  19.04     19      0      1 Lubuk Besar\n7    &lt;NA&gt; 19.03.03.2004   &lt;NA&gt;  19.03     19      0      1   Air Gegas\n8    &lt;NA&gt; 19.04.01.1017   &lt;NA&gt;  19.04     19      0      2        Koba\n9    &lt;NA&gt; 19.03.01.2006   &lt;NA&gt;  19.03     19      0      1     Toboali\n10   &lt;NA&gt; 19.03.06.2005   &lt;NA&gt;  19.03     19      0      1 Tukak Sadai\n           WADMKD         WADMKK                    WADMPR WIADKC WIADKK WIADPR\n1         Airbara Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n2        Airgegas Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n3     Arung Dalam  Bangka Tengah Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n4  Batu Betumpang Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n5       Bedengung Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n6       Belimbing  Bangka Tengah Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n7          Bencah Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n8           Berok  Bangka Tengah Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n9          Bikang Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n10    Bukit Terap Bangka Selatan Kepulauan Bangka Belitung   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n   WIADKD                            UUPP       LUAS      AREA\n1    &lt;NA&gt; Hasil Delineasi Batas Desa 2019  77.160034  77160034\n2    &lt;NA&gt; Hasil Delineasi Batas Desa 2019  68.445344  68445426\n3       0 Hasil Delineasi Batas Desa 2019  20.759893  20759893\n4    &lt;NA&gt; Hasil Delineasi Batas Desa 2019 138.255656 138247711\n5    &lt;NA&gt; Hasil Delineasi Batas Desa 2019  96.103135  96102987\n6    &lt;NA&gt; Hasil Delineasi Batas Desa 2019  21.356034  21356035\n7    &lt;NA&gt; Hasil Delineasi Batas Desa 2019 133.589935 133590216\n8       0 Hasil Delineasi Batas Desa 2019   3.196318   3196318\n9    &lt;NA&gt; Hasil Delineasi Batas Desa 2019  53.235589  53235592\n10   &lt;NA&gt; Hasil Delineasi Batas Desa 2019  18.038894  18038895\n                         geometry\n1  POLYGON Z ((106.4285 -2.562...\n2  POLYGON Z ((106.4589 -2.692...\n3  POLYGON Z ((106.3998 -2.478...\n4  POLYGON Z ((106.0563 -2.778...\n5  POLYGON Z ((106.2187 -2.679...\n6  POLYGON Z ((106.4636 -2.568...\n7  POLYGON Z ((106.5133 -2.724...\n8  POLYGON Z ((106.4047 -2.477...\n9  POLYGON Z ((106.522 -2.8827...\n10 POLYGON Z ((106.6278 -2.968...\n\n\nWe will need to drop the ‘z’ dimension value from the dataset as we are only working with x,y dimensions, not with height data. Hence, let’s re-read the data and perform some wrangling.\n\nkbb_sf &lt;- st_read(dsn=\"data/rawdata\", layer=\"Kepulauan_Bangka_Belitung\") %&gt;%\n  st_union() %&gt;%\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;%\n  st_transform(crs = 32748)\n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex4\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nst_as_s2(): dropping Z and/or M coordinate\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_read() reads the spatial data from the specified file.\nst_union() performs a spatial union, combining all separate geometries (e.g., polygons) into one single geometry object. This is useful if you want to treat the entire area as a single entity, rather than as individual geometries (e.g., islands or districts).\nst_zm(drop = TRUE, what = \"ZM\") removes the Z (elevation) and M (measure) dimensions, simplifying the geometry to 2D.\nst_transform(crs = 32748) reprojects the geometry to the specified coordinate reference system (CRS), EPSG:32748 (UTM zone 48S, often used for areas around Southeast Asia).\n\n\n\nLet’s inspect the newly created dataframe.\n\nkbb_sf\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 512066.8 ymin: 9655398 xmax: 705559.4 ymax: 9834006\nProjected CRS: WGS 84 / UTM zone 48S\n\n\nMULTIPOLYGON (((590979.6 9741359, 590966.1 9741...\n\n\n\n\n3.2 Converting to OWIN Layer\nNext, as.owin() is used to convert the kbb data into an own object.\n\nkbb_owin &lt;- as.owin(kbb_sf)\nkbb_owin\n\nwindow: polygonal boundary\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n\n\nNext, class() is used to confirm if the output is indeed an owin object.\n\nclass(kbb_owin)\n\n[1] \"owin\""
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#importing-and-preparing-forest-fire-data.",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#importing-and-preparing-forest-fire-data.",
    "title": "In-class Exercise 3",
    "section": "4. Importing and Preparing Forest Fire Data.",
    "text": "4. Importing and Preparing Forest Fire Data.\nNext, we will import the forest fire data (i.e. forestfires.csv) into the R environment.\n\nfire_sf &lt;- read_csv(\"data/rawdata/forestfires.csv\") %&gt;%\n  st_as_sf(coords = c(\"longitude\",\"latitude\"),\n           crs = 4326) %&gt;%\n  st_transform(crs = 32748)\n\nRows: 741 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (3): satellite, instrument, daynight\ndbl  (11): latitude, longitude, brightness, scan, track, acq_time, confidenc...\ndate  (1): acq_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSince ppp object only acce[ts a numerical or character as mark, we will use the codes below to convert the data type of acq_dae to numeric.\n\nfire_sf &lt;- fire_sf %&gt;%\n  mutate(DayofYear = yday(acq_date)) %&gt;%\n  mutate(Month_num = month(acq_date)) %&gt;%\n  mutate(Month_fac = month(acq_date, \n                         label = TRUE,\n                         abbr = FALSE))\n\nfire_sf\n\nSimple feature collection with 741 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 521564.1 ymin: 9658137 xmax: 695791 ymax: 9828767\nProjected CRS: WGS 84 / UTM zone 48S\n# A tibble: 741 × 17\n   brightness  scan track acq_date   acq_time satellite instrument confidence\n *      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;\n 1       312.   1.3   1.1 2023-01-10      629 Aqua      MODIS              67\n 2       314.   1.2   1.1 2023-01-10      629 Aqua      MODIS              70\n 3       315.   1.2   1.1 2023-01-10      629 Aqua      MODIS              71\n 4       309.   1.2   1.1 2023-01-10      629 Aqua      MODIS              54\n 5       308.   1.2   1.1 2023-01-10      629 Aqua      MODIS              33\n 6       322.   1.3   1.1 2023-01-10      629 Aqua      MODIS              72\n 7       318.   1.2   1.1 2023-01-10      629 Aqua      MODIS              71\n 8       318.   1.2   1.1 2023-01-10      629 Aqua      MODIS              75\n 9       327.   2     1.4 2023-01-12      616 Aqua      MODIS              73\n10       321.   2     1.4 2023-01-12      616 Aqua      MODIS              75\n# ℹ 731 more rows\n# ℹ 9 more variables: version &lt;dbl&gt;, bright_t31 &lt;dbl&gt;, frp &lt;dbl&gt;,\n#   daynight &lt;chr&gt;, type &lt;dbl&gt;, geometry &lt;POINT [m]&gt;, DayofYear &lt;dbl&gt;,\n#   Month_num &lt;dbl&gt;, Month_fac &lt;ord&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#visualise-the-plot",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#visualise-the-plot",
    "title": "In-class Exercise 3",
    "section": "5. Visualise the Plot",
    "text": "5. Visualise the Plot\n\n5.1 Overall Plot\nNow, I will prepare a point symbol map showing the distribution of fire points.\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf)+\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n5.2 Visuaising geographic distribution of forest fires by month\nNext, I will prepare a point symbol map showing the monthly geographic distribution of forest fires in 2023.\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf)+\n  tm_dots(size = 0.1) +\n  tm_facets(by = \"Month_fac\",\n            free.coords = FALSE,\n            drop.units = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#computing-stkde-by-month",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#computing-stkde-by-month",
    "title": "In-class Exercise 3",
    "section": "6. Computing STKDE by Month",
    "text": "6. Computing STKDE by Month\nIn this section, I will learn how to compute STKDE by using spattemp.density() of sparr package.\n\n6.1 Extracting Forest Fires by Month\nThe code below is used to remove the unwanted fields from the fire_sf simple feature data frame. This is because as.ppp() only needs the mark field and geometry field from the input of the data frame.\n\nfire_month &lt;- fire_sf %&gt;%\n  select(Month_num)\n\nhead(fire_month)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 606178.8 ymin: 9682757 xmax: 669933.6 ymax: 9703062\nProjected CRS: WGS 84 / UTM zone 48S\n# A tibble: 6 × 2\n  Month_num           geometry\n      &lt;dbl&gt;        &lt;POINT [m]&gt;\n1         1 (606178.8 9703062)\n2         1 (661410.6 9683536)\n3         1 (637808.8 9682757)\n4         1 (654882.2 9690665)\n5         1 (669933.6 9697468)\n6         1 (609133.5 9700119)\n\n\n\n\n6.2 Creating ppp objects\nThe code below is used to derive a ppp object called the fire_month from fire_month of data.frame.\n\nfire_month_ppp &lt;- as.ppp(fire_month)\nfire_month_ppp\n\nMarked planar point pattern: 741 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n\n\nThe code below is used to check the output is in the correct object class\n\nsummary(fire_month_ppp)\n\nMarked planar point pattern:  741 points\nAverage intensity 2.49258e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n                    (174200 x 170600 units)\nWindow area = 29728200000 square units\n\n\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(fire_month_ppp))\n\n[1] FALSE\n\n\n\n\n6.3 Including Owin object\nHere we combine fire_month_ppp object with the kkb_owin object into one.\n\nfire_month_owin &lt;- fire_month_ppp[kbb_owin]\nsummary(fire_month_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\nAs a good practice, plot() is used to plot ff_owin so that we can examine the correctness of the output object.\n\nplot(fire_month_owin)\n\n\n\n\n\n\n\n\n\n\n6.4 Computing Spatio-temporal KDE\nNext, spattemp.density() of sparr package is used to compute the STKDE.\n\nst_kde &lt;- spattemp.density(fire_month_owin)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 0.0304 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [1, 12]\n\nEvaluation\n  128 x 128 x 12 trivariate lattice\n  Density range: [1.233458e-27, 8.202976e-10]\n\n\n\n\n6.5 Plotting the spatio-temporal KDE object\nWe’ll use the plot() function of R base to plot the KDE between July 2023 to December 2023.\n\ntims &lt;- c(7,8,9,10,11,12)\npar(mfcol=c(1,2))\nfor(i in tims){ \n  plot(st_kde, i, \n       override.par=FALSE, \n       fix.range=TRUE, \n       main=paste(\"KDE at month\",i))\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#computing-stkde-by-day-of-year",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#computing-stkde-by-day-of-year",
    "title": "In-class Exercise 3",
    "section": "7. Computing STKDE by Day of Year",
    "text": "7. Computing STKDE by Day of Year\nNow, I will compute the STKDE of forest fires by day of year.\n\n7.1 Creating ppp object\nIn the code chunk below, DayofYear from the fire_sf data frame is selected and is included in the output ppp object.\n\nfire_yday_ppp &lt;- fire_sf %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\n\n\n\n7.2 Including Owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\nfire_yday_owin &lt;- fire_yday_ppp[kbb_owin]\nsummary(fire_yday_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   10.0   213.0   258.0   245.9   287.0   352.0 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\n\n\n7.3 Performing Spatio-Temporal KDE\nNow, I will perform a spatio-temporal kernel density estimate on the fire_yday_owin object which gives us insights into where and when fire occurrences are concentrated within the specified observation window.\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 6.3198 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [10, 352]\n\nEvaluation\n  128 x 128 x 343 trivariate lattice\n  Density range: [3.959516e-27, 2.751287e-12]\n\n\nPlotting the graph by days of the year will produce 365/366 charts.\n\n#plot(kde_yday)\n\nInstead, let us plot an animated plot to show the change in KDE across each day of the year.\n\nkde_yday$z$'10'\n\nreal-valued pixel image\n128 x 128 pixel array (ny, nx)\nenclosing rectangle: [512070, 705560] x [9655400, 9834000] units\n\n\n\nplot(kde_yday$z$'10')\n\n\n\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(spatstat)\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.98\nEnabled features: cairo, freetype, fftw, ghostscript, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fontconfig, x11\n\nlibrary(viridis)  # For color mapping\n\nLoading required package: viridisLite\n\n# Create a directory to store PNG frames\nif (!dir.exists(\"frames\")) {\n  dir.create(\"frames\")\n}\n\n# Get the unique day values from kde_yday\ndays &lt;- names(kde_yday$z)  # Assuming 'kde_yday$z' contains KDE results for each day\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- kde_yday$z[[day]]  # Access KDE result for the day\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Load magick library\nlibrary(magick)\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"frames\", full.names = TRUE, pattern = \"*.png\"))\n\n# Create animated GIF\nanimation &lt;- image_animate(image_join(frames), fps = 10)  # Adjust fps as needed\n\n# Save the animation\noutput_path &lt;- \"animated_kde_yday.gif\"\nimage_write(animation, path = output_path)\n\n# Display the GIF (optional)\nprint(animation)\n\n# A tibble: 344 × 7\n   format width height colorspace matte filesize density\n   &lt;chr&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n 1 gif      800    800 sRGB       FALSE        0 72x72  \n 2 gif      800    800 sRGB       TRUE         0 72x72  \n 3 gif      800    800 sRGB       TRUE         0 72x72  \n 4 gif      800    800 sRGB       TRUE         0 72x72  \n 5 gif      800    800 sRGB       TRUE         0 72x72  \n 6 gif      800    800 sRGB       TRUE         0 72x72  \n 7 gif      800    800 sRGB       TRUE         0 72x72  \n 8 gif      800    800 sRGB       TRUE         0 72x72  \n 9 gif      800    800 sRGB       TRUE         0 72x72  \n10 gif      800    800 sRGB       TRUE         0 72x72  \n# ℹ 334 more rows"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#exploratory-data-analysis",
    "title": "Take-home Exercise 1",
    "section": "4. Exploratory Data Analysis",
    "text": "4. Exploratory Data Analysis\n\n4.1 Identifying Districts with Highest Proportion of Conflicts\nIt’ll also be interesting to find out specific districts with the highest concentration of armed conflicts. I will first calculate the total occurrences of conflict events per district and add the column to boundary_sf.\n\n\nCount number of conflicts by districts\nconflict_count &lt;- conflict_data_sf %&gt;%\n  group_by(DT) %&gt;%\n  summarise(total_count_DT = n()) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(DT, total_count_DT)\n\n# Perform the join\nboundary_sf &lt;- boundary_sf %&gt;%\n  left_join(conflict_count, by = \"DT\")\n\n\nNext, let’s calculate the proportion of total conflicts and add it as a column into the boundary_sf dataset as proportion_DT.\n\n# Create new 'proportion_DT' column\nboundary_sf &lt;- boundary_sf %&gt;%\n  mutate(proportion_DT = total_count_DT / sum(total_count_DT))\nhead(boundary_sf[c('DT','total_count_DT','proportion_DT')])\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -14915.04 ymin: 1736124 xmax: 187961.7 ymax: 2051144\nProjected CRS: WGS 84 / UTM zone 47N\n         DT total_count_DT proportion_DT                       geometry\n1  Hinthada            160   0.003755780 MULTIPOLYGON (((90859.89 20...\n2   Labutta             51   0.001197155 MULTIPOLYGON (((75991.51 17...\n3    Maubin            118   0.002769888 MULTIPOLYGON (((115559 1928...\n4 Myaungmya             59   0.001384944 MULTIPOLYGON (((39919.39 18...\n5   Pathein            333   0.007816718 MULTIPOLYGON (((-6302.348 1...\n6    Pyapon            131   0.003075045 MULTIPOLYGON (((93411.72 17...\n\n\nAt a quick glance, we can see that central and southern parts of Myanmar have the highest proportions of armed conflict events occuring.\n\n\nSet up the points map\ndistricts_choropleth &lt;-\ntm_shape(boundary_sf) +\n  tm_fill(\"proportion_DT\",\n          n=10,\n          title=\"Proportion\",\n          style=\"equal\",\n          palette=\"Blues\") +\n  tm_borders(lwd=0.2,\n             alpha=1) +\n  tm_text(text = \"DT\", \n          size = 0.2, \n          col = \"black\",\n          fontface = \"bold\") +\n  tm_layout(main.title = \"Distribution of Conflict Points Across Districts\",\n            legend.outside=FALSE,\n            main.title.size=1)\n\n\n\n# Plot the map\ntmap_mode(\"plot\")\ntmap_arrange(districts_choropleth)\n\n\nMore specifically, we can observe that the district types are all unique for the top 3 conflict areas and mainly found in the districts of Yinmarbin, Shwebo and Pakokku which lies in the central regions of Myanmar.\n\n# Count number of conflicts by district\nconflict_count &lt;- conflict_data_sf %&gt;%\n  group_by(DT) %&gt;%\n  summarise(total_count_DT = n()) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(DT, total_count_DT)\n\n# Perform the join\nboundary_sf &lt;- boundary_sf %&gt;%\n  left_join(conflict_count, by = \"DT\")\n\nconflict_count %&gt;%\n  arrange(desc(total_count_DT)) %&gt;%\n  slice(1:10)\n\n# A tibble: 10 × 2\n   DT         total_count_DT\n   &lt;chr&gt;               &lt;int&gt;\n 1 Shwebo               2694\n 2 Pakokku              2491\n 3 Yinmarbin            1788\n 4 Monywa               1514\n 5 Kale                 1261\n 6 Pyinoolwin           1261\n 7 Muse                 1244\n 8 Loikaw               1172\n 9 Dawei                1165\n10 Sagaing              1146\n\n\n\n\n4.2 Identifying States with Highest Proportion of Conflicts\nInstead, let us also explore the top 10 states with the highest proportions of armed conflict events.\n\n\nCount number of conflicts by states\nconflict_count &lt;- conflict_data_sf %&gt;%\n  group_by(ST) %&gt;%\n  summarise(total_count_ST = n()) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(ST, total_count_ST)\n\n# Perform the join\nboundary_sf &lt;- boundary_sf %&gt;%\n  left_join(conflict_count, by = \"ST\")\n\n\nLikewise, I’ll add a new column called proportion_ST to represent the proportion based on each Myanmar state.\n\nboundary_sf &lt;- boundary_sf %&gt;%\n  mutate(proportion_ST = total_count_ST / sum(total_count_ST))\n\nhead(boundary_sf[c('ST','total_count_ST','proportion_ST')])\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -14915.04 ymin: 1736124 xmax: 187961.7 ymax: 2051144\nProjected CRS: WGS 84 / UTM zone 47N\n          ST total_count_ST proportion_ST                       geometry\n1 Ayeyarwady            852   0.003053731 MULTIPOLYGON (((90859.89 20...\n2 Ayeyarwady            852   0.003053731 MULTIPOLYGON (((75991.51 17...\n3 Ayeyarwady            852   0.003053731 MULTIPOLYGON (((115559 1928...\n4 Ayeyarwady            852   0.003053731 MULTIPOLYGON (((39919.39 18...\n5 Ayeyarwady            852   0.003053731 MULTIPOLYGON (((-6302.348 1...\n6 Ayeyarwady            852   0.003053731 MULTIPOLYGON (((93411.72 17...\n\n\nAt a quick glance, we can see that central and southern parts of Myanmar have the highest proportions of armed conflict events occurring, particularly in Sagaing, Mandalay and Magway states.\n\n\nCreate the points map\nstates_choropleth &lt;-\ntm_shape(boundary_sf) +\n  tm_fill(\"proportion_ST\",\n          n=10,\n          title=\"Proportion\",\n          style=\"equal\",\n          palette=\"Blues\") +\n  tm_borders(lwd=0.2,\n             alpha=1) +\n  tm_text(text = \"ST\", \n          size = 0.2, \n          col = \"black\",\n          fontface = \"bold\") +\n  tm_layout(main.title = \"Distribution of Conflict Points Across States\",\n            legend.outside=FALSE,\n            main.title.size=1)\n\n\n\n# Plot the map\ntmap_mode(\"plot\")\ntmap_arrange(states_choropleth)\n\n\nFor greater clarity, the top states with the most conflicts exist in Sagaing, Mandalay, Magway and Yangon states as indicated in the map above (darkest shade of blue).\n\nconflict_count %&gt;%\n  arrange(desc(total_count_ST)) %&gt;%\n  slice(1:10)\n\n# A tibble: 10 × 2\n   ST           total_count_ST\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Sagaing               11128\n 2 Magway                 4179\n 3 Mandalay               3603\n 4 Shan (North)           2938\n 5 Kachin                 2776\n 6 Yangon                 2608\n 7 Rakhine                2277\n 8 Tanintharyi            2240\n 9 Kayin                  1817\n10 Mon                    1677"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html",
    "title": "Hands-on Exercise 5",
    "section": "",
    "text": "Hands-on Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#overview",
    "title": "Hands-on Exercise 5",
    "section": "1. Overview",
    "text": "1. Overview\nIn this hands-on exercise, I will be computing spatial weights by executing the following:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#lets-set-up",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#lets-set-up",
    "title": "Hands-on Exercise 5",
    "section": "2. Let’s Set Up!",
    "text": "2. Let’s Set Up!\n\n2.1 Importing Libraries into R\nIn this hands-on exercise, we will we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in R.\n\n\nLoad the packages\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\n\n\n\n2.2 Download Data and Set Up Folders\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\nThis is the file structure for containing the data files that I have extracted."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#import-data-sets-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#import-data-sets-into-r",
    "title": "Hands-on Exercise 5",
    "section": "3. Import Data Sets into R",
    "text": "3. Import Data Sets into R\n\n3.1 Importing Geospatial Data\nFirstly, we will import the Hunan county boundary layer. This is a geospatial data set in ESRI shapefile format. The code chunk below uses st_read() of sf package.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex5\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n3.2 Importing Aspatial Data\nNext, I will import the aspatial data set. This data is a csv file containing selected Hunan’s local development indicators in 2012.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012) |&gt; \n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#visualizing-regional-development-indicator",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#visualizing-regional-development-indicator",
    "title": "Hands-on Exercise 5",
    "section": "4. Visualizing Regional Development Indicator",
    "text": "4. Visualizing Regional Development Indicator\nNow, we will use qtm() function of tmap package to create a basemap and a choropleth map showing the distribution of GDPPC 2012.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#computing-contiguity-spatial-weights",
    "title": "Hands-on Exercise 5",
    "section": "5. Computing Contiguity Spatial Weights",
    "text": "5. Computing Contiguity Spatial Weights\nIn this section, I will use the poly2nb() function of spdep package to compute contiguity spatial weights.\n\n💡 What does poly2nb() do?\nThis function builds a neighbours list based on regions with contiguous boundaries. A “queen” argument can take either TRUE or FALSE as options.\nNote: If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n\n5.1 Computing Queen contiguity based neighbors\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nWe can observe that there are 88 regions in the data set and that the average number of neighbours is 5.1. The maximum number of neighbours is 11 and the minimum is 1.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n5.2 Computing Rook contiguity based neighbors\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nWe can observe that there are 88 regions in the data set and that the average number of neighbours is 5. The maximum number of neighbours is 10 and the minimum is 1.\n\n\n5.3 Visualising the contiguity weights\nA connectivity graph takes a point in any polygon and draws a line to all its neighbors. The most common way to get the points is to use the coordinates of the centroids of the polygons.\nHowever, getting the points associated with each polygon is a little more complicated than just running st_centroid() on the sf object. We need the coordinates to be in a separate data frame for this to work. To do this, we need to use a mapping function map_dbl().\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nWe can check the first few observations to see if things are formatted correctly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\nNow, we can visualize the queen and rook spatial weights, using 3 ways.\n\nMethod 1: Plotting Queen contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n\n\n\n\nMethod 2: Plotting Rook contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nMethod 3: Plotting both Queen and Rook contiguity based neighbours maps\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#computing-distance-based-neighbours",
    "title": "Hands-on Exercise 5",
    "section": "6. Computing distance based neighbours",
    "text": "6. Computing distance based neighbours\nThis section will use the dnearneigh() function of spdep package to compute distance based neighbours.\n\n💡 What does dnearneigh() do?\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument.\nIf unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\n6.1 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n6.2 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nWe can observe that there are 88 regions, with an average of 3.681818 neighbours per region.\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\nn_comp &lt;- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\n\n6.2.1 Plotting fixed distance weight matrix\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n6.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\n\n6.3.1 Plotting distance based neighbours\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#weights-based-on-idw",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#weights-based-on-idw",
    "title": "Hands-on Exercise 5",
    "section": "7. Weights based on IDW",
    "text": "7. Weights based on IDW\nIn this section, I attempt to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#row-standardised-weights-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#row-standardised-weights-matrix",
    "title": "Hands-on Exercise 5",
    "section": "8. Row-standardised weights matrix",
    "text": "8. Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”).\nThis is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values.\nWhile this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data.\nFor this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors type:\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#application-of-spatial-weight-matrix",
    "title": "Hands-on Exercise 5",
    "section": "9. Application of Spatial Weight Matrix",
    "text": "9. Application of Spatial Weight Matrix\nNow, I will create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n9.1 Spatial lag with row-standardised weights\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecall that in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nFrom the output above, we can see that row-standardized weights are calculated by dividing each weight by the sum of the weights for each polygon. In other words, the weights are normalized so that they sum to 1. This is done to ensure that the spatial lag variable is on the same scale as the original variable.\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n9.2 Spatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nNow, let’s examine the results by using this code.\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nFrom the output above, we can see that the spatial lag GDPPC values are calculated by summing the GDPPC of the neighboring counties. We can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n9.3 Spatial window average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw().\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\n\n💡 Note: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\n\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n9.4 Spatial window sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\n\n💡 Note: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\n\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan |&gt;\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") |&gt;\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#nd-order-spatial-point-patterns-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#nd-order-spatial-point-patterns-analysis",
    "title": "Take-home Exercise 1",
    "section": "6. 2nd Order Spatial Point Patterns Analysis",
    "text": "6. 2nd Order Spatial Point Patterns Analysis\nUnlike 1st-order analysis, which studies the intensity of points (e.g., density), let’s also leverage 2nd-order analysis to examine how points are distributed relative to each other, which can offer deeper insights into the spatial interaction between events.\nI will use the K-function and L-function is to understand the spatial relationships between events, particularly focusing on whether the points exhibit clustering, uniformity, or randomness.\n\n6.1 Using K-Function Estimation\nK-function helps detect spatial patterns by comparing the observed distribution of points against a random pattern at different distances.\n\n6.1.1 Yinmarbin District\n1) Computing K-Function Estimation\nFor Yinmarbin district, let’s compute K-function estimates by using Kest() of the spatstat package.\n\n\nPrepare Dataset for Yinmarbin District\nconflict_yinmarbin = filter(conflict_data_sf, DT == \"Yinmarbin\")\nboundary_yinmarbin &lt;- filter(boundary_sf, DT == \"Yinmarbin\")\nyinmarbin_owin &lt;- as.owin(boundary_yinmarbin)\n\n# Create a combined ppp and owin object\nppp_obj &lt;- as.ppp(conflict_yinmarbin$geometry)\nmasked_ppp &lt;- ppp_obj[yinmarbin_owin]\nyinmarbin_ppp_owin &lt;- rescale(masked_ppp, 1000, \"km\")\n\n\nWe are now ready to plot the K-function.\n\nK_ck = Kest(yinmarbin_ppp_owin, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\",\n       main = paste(\"Yinmarbin District (K-Function)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nHow to interpret the plot:\n\nK-iso represents the observed or estimated K-function value calculated from the actual data\nK-pois is the theoretical K-function that represents the expected K-function\n\nWith that said…\nWe can observe how the observed line (K-iso) is constantly above the theoretical line from 2021 Q2 to 2024 Q2. This confirms that conflict points in Yinmarbin are highly clustered. In fact, it is more clustered together than expected by the null hypothesis.\nHowever, we do not have any conflict points in 2021 Q1\nNote: Since I had used the default edge = TRUE settings, edge correction will account for missing neighbours outside the boundary which helps maintain an accurate estimate of the K-function.\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e. Monta Carlo simulation test) will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of conflict events in Myanmar are randomly distributed.\nH1= The distribution of conflict events in Myanmar are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nBy using envelop(), we can get a more robust interpretation by comparing the observed K-function against a simulation envelope of K-functions generated under the null hypothesis.\n\n\n\n\n\n\nNote\n\n\n\nTo achieve a 95% confidence envelope in a K-function test with Complete Spatial Randomness, I will need to exclude the upper 2.5% and lower 2.5% of the simulated K-functions., i.e. I will need to generate at least 40 simulations where nsim = 39.\n\n\n\n# Monte Carlo test with K-function\nK_ck.csr &lt;- envelope(yinmarbin_ppp_owin, Kest, \n                     nsim = 39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, \n[29:43 remaining, estimate finish 2024-09-16 19:42:45]\n3, \n[27:46 remaining, estimate finish 2024-09-16 19:41:32]\n4, \n[27:03 remaining, estimate finish 2024-09-16 19:41:35]\n5, \n[25:53 remaining, estimate finish 2024-09-16 19:41:09]\n6, \n[25:05 remaining, estimate finish 2024-09-16 19:41:06]\n7, \n[24:32 remaining, estimate finish 2024-09-16 19:41:21]\n8, \n[24:01 remaining, estimate finish 2024-09-16 19:41:39]\n9, \n[23:11 remaining, estimate finish 2024-09-16 19:41:36]\n10, \n[22:19 remaining, estimate finish 2024-09-16 19:41:27]\n11, \n[21:31 remaining, estimate finish 2024-09-16 19:41:25]\n12, \n[20:53 remaining, estimate finish 2024-09-16 19:41:37]\n13, \n[20:04 remaining, estimate finish 2024-09-16 19:41:32]\n14, \n[19:22 remaining, estimate finish 2024-09-16 19:41:40]\n15, \n[18:34 remaining, estimate finish 2024-09-16 19:41:37]\n16, \n[17:49 remaining, estimate finish 2024-09-16 19:41:40]\n17, \n[17:00 remaining, estimate finish 2024-09-16 19:41:35]\n18, \n[16:19 remaining, estimate finish 2024-09-16 19:41:44]\n19, \n[15:32 remaining, estimate finish 2024-09-16 19:41:44]\n20, \n[14:43 remaining, estimate finish 2024-09-16 19:41:39]\n21, \n[13:57 remaining, estimate finish 2024-09-16 19:41:40]\n22, \n[13:10 remaining, estimate finish 2024-09-16 19:41:39]\n23, \n[12:22 remaining, estimate finish 2024-09-16 19:41:36]\n24, \n[11:35 remaining, estimate finish 2024-09-16 19:41:34]\n25, \n[10:49 remaining, estimate finish 2024-09-16 19:41:35]\n26, \n[10:03 remaining, estimate finish 2024-09-16 19:41:37]\n27,  [9:17 remaining] 28,  [8:31 remaining] 29,  [7:44 remaining] 30,  [6:57 remaining] 31,  [6:15 remaining] 32,  [5:32 remaining] 33,  [4:46 remaining] 34,  [3:59 remaining] 35,  [3:12 remaining] 36,  [2:26 remaining] 37,  [1:39 remaining] 38,  [49 sec remaining] \n39.\n\nDone.\n\nplot(K_ck.csr, main = paste(\"Yinmarbin District (CSR)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n\n6.1.2 Shwebo District\n1) Computing K-Function Estimation\nFor Shwebo district, let’s compute K-function estimates by using Kest() of the spatstat package.\n\n\nPrepare Dataset for Shwebo District\nconflict_shwebo = filter(conflict_data_sf, DT == \"Shwebo\")\nboundary_shwebo &lt;- filter(boundary_sf, DT == \"Shwebo\")\nshwebo_owin &lt;- as.owin(boundary_shwebo)\n\n# Create a combined ppp and owin object\nppp_obj &lt;- as.ppp(conflict_shwebo$geometry)\nmasked_ppp &lt;- ppp_obj[shwebo_owin]\nshwebo_ppp_owin &lt;- rescale(masked_ppp, 1000, \"km\")\n\n\nWe are now ready to plot the K-function.\n\nK_ck = Kest(shwebo_ppp_owin, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\",\n       main = paste(\"Shwebo District (K-Function)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test (i.e. Monta Carlo simulation test) will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of conflict events in Myanmar are randomly distributed.\nH1= The distribution of conflict events in Myanmar are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\n\n# Monte Carlo test with K-function\nK_ck.csr &lt;- envelope(shwebo_ppp_owin, Kest, \n                     nsim = 39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, \n[2:02:05 remaining, estimate finish 2024-09-16 22:02:05]\n3, \n[2:05:33 remaining, estimate finish 2024-09-16 22:09:13]\n4, \n[2:01:07 remaining, estimate finish 2024-09-16 22:08:12]\n5, \n[1:56:04 remaining, estimate finish 2024-09-16 22:06:26]\n6, \n[1:55:29 remaining, estimate finish 2024-09-16 22:09:41]\n7, \n[1:59:33 remaining, estimate finish 2024-09-16 22:18:39]\n8, \n[1:58:01 remaining, estimate finish 2024-09-16 22:21:21]\n9, \n[1:52:50 remaining, estimate finish 2024-09-16 22:19:37]\n10, \n[1:51:44 remaining, estimate finish 2024-09-16 22:23:06]\n11, \n[1:48:46 remaining, estimate finish 2024-09-16 22:24:19]\n12, \n[1:47:49 remaining, estimate finish 2024-09-16 22:28:27]\n13, \n[1:43:52 remaining, estimate finish 2024-09-16 22:28:30]\n14, \n[1:38:23 remaining, estimate finish 2024-09-16 22:26:15]\n15, \n[1:33:19 remaining, estimate finish 2024-09-16 22:24:27]\n16, \n[1:28:26 remaining, estimate finish 2024-09-16 22:22:49]\n17, \n[1:23:52 remaining, estimate finish 2024-09-16 22:21:33]\n18, \n[1:19:50 remaining, estimate finish 2024-09-16 22:21:09]\n19, \n[1:15:26 remaining, estimate finish 2024-09-16 22:20:01]\n20, \n[1:11:13 remaining, estimate finish 2024-09-16 22:19:08]\n21, \n[1:07:06 remaining, estimate finish 2024-09-16 22:18:22]\n22, \n[1:03:09 remaining, estimate finish 2024-09-16 22:17:52]\n23, \n[59:12 remaining, estimate finish 2024-09-16 22:17:18]\n24, \n[55:18 remaining, estimate finish 2024-09-16 22:16:47]\n25, \n[51:20 remaining, estimate finish 2024-09-16 22:16:03]\n26, \n[47:30 remaining, estimate finish 2024-09-16 22:15:33]\n27, \n[44:13 remaining, estimate finish 2024-09-16 22:16:44]\n28, \n[40:37 remaining, estimate finish 2024-09-16 22:17:02]\n29, \n[36:47 remaining, estimate finish 2024-09-16 22:16:30]\n30, \n[33:04 remaining, estimate finish 2024-09-16 22:16:17]\n31, \n[29:16 remaining, estimate finish 2024-09-16 22:15:43]\n32, \n[25:32 remaining, estimate finish 2024-09-16 22:15:18]\n33, \n[21:50 remaining, estimate finish 2024-09-16 22:15:02]\n34, \n[18:07 remaining, estimate finish 2024-09-16 22:14:23]\n35, \n[14:26 remaining, estimate finish 2024-09-16 22:13:51]\n36, \n[10:46 remaining, estimate finish 2024-09-16 22:13:08]\n37,  [7:08 remaining] 38,  [3:33 remaining] \n39.\n\nDone.\n\nplot(K_ck.csr, main = paste(\"Shwebo District (CSR)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n\n\n6.2 Using L-Function Estimation\nIn this section, I will be computing L-function via Lest() of spatstat package which is normalises the K-function to a linear scale for easier interpretation.\n\n6.2.1 Yinmarbin District\n1) Computing L-function Estimation\n\nL_ck = Lest(yinmarbin_ppp_owin, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(m)\",\n     main = paste(\"Yinmarbin District (L-Function)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of conflict events in Myanmar are randomly distributed.\nH1= The distribution of conflict events in Myanmar are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nI will also perform monta carlo simulation test using envelope() of the spatstat package.\n\n# Monte Carlo test with L-function\nL_ck.csr &lt;- envelope(yinmarbin_ppp_owin, Lest, \n                     nsim = 39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, \n[26:56 remaining, estimate finish 2024-09-16 22:43:13]\n3, \n[25:21 remaining, estimate finish 2024-09-16 22:42:18]\n4, \n[24:56 remaining, estimate finish 2024-09-16 22:42:38]\n5, \n[24:28 remaining, estimate finish 2024-09-16 22:42:53]\n6, \n[24:22 remaining, estimate finish 2024-09-16 22:43:36]\n7, \n[24:41 remaining, estimate finish 2024-09-16 22:44:51]\n8, \n[23:30 remaining, estimate finish 2024-09-16 22:44:21]\n9, \n[22:50 remaining, estimate finish 2024-09-16 22:44:29]\n10, \n[22:19 remaining, estimate finish 2024-09-16 22:44:48]\n11, \n[21:28 remaining, estimate finish 2024-09-16 22:44:41]\n12, \n[21:13 remaining, estimate finish 2024-09-16 22:45:25]\n13, \n[20:43 remaining, estimate finish 2024-09-16 22:45:49]\n14, \n[19:47 remaining, estimate finish 2024-09-16 22:45:37]\n15, \n[18:48 remaining, estimate finish 2024-09-16 22:45:18]\n16, \n[17:58 remaining, estimate finish 2024-09-16 22:45:15]\n17, \n[17:08 remaining, estimate finish 2024-09-16 22:45:08]\n18, \n[16:26 remaining, estimate finish 2024-09-16 22:45:17]\n19, \n[15:36 remaining, estimate finish 2024-09-16 22:45:12]\n20, \n[14:47 remaining, estimate finish 2024-09-16 22:45:08]\n21, \n[13:56 remaining, estimate finish 2024-09-16 22:44:58]\n22, \n[13:07 remaining, estimate finish 2024-09-16 22:44:53]\n23, \n[12:19 remaining, estimate finish 2024-09-16 22:44:48]\n24, \n[11:30 remaining, estimate finish 2024-09-16 22:44:40]\n25, \n[10:40 remaining, estimate finish 2024-09-16 22:44:30]\n26,  [9:56 remaining] 27,  [9:12 remaining] 28,  [8:30 remaining] 29,  [7:44 remaining] 30,  [6:56 remaining] 31,  [6:10 remaining] 32,  [5:23 remaining] 33,  [4:36 remaining] 34,  [3:49 remaining] 35,  [3:03 remaining] 36,  [2:17 remaining] 37,  [1:31 remaining] 38,  [45 sec remaining] \n39.\n\nDone.\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\",\n       main = paste(\"Yinmarbin District (CSR)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n\n6.2.2 Shwebo District\n1) Computing L-function Estimation\n\nL_ck = Lest(shwebo_ppp_owin, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(m)\",\n     main = paste(\"Shwebo District (L-Function)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n2) Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of conflict events in Myanmar are randomly distributed.\nH1= The distribution of conflict events in Myanmar are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nI will also perform monta carlo simulation test using envelope() of the spatstat package.\n\n# Monte Carlo test with L-function\nL_ck.csr &lt;- envelope(shwebo_ppp_owin, Lest, \n                     nsim = 39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, \n[1:48:44 remaining, estimate finish 2024-09-17 00:47:43]\n3, \n[1:45:47 remaining, estimate finish 2024-09-17 00:47:42]\n4, \n[1:44:19 remaining, estimate finish 2024-09-17 00:49:18]\n5, \n[1:42:10 remaining, estimate finish 2024-09-17 00:50:13]\n6, \n[1:39:51 remaining, estimate finish 2024-09-17 00:51:02]\n7, \n[1:36:04 remaining, estimate finish 2024-09-17 00:50:07]\n8, \n[1:32:40 remaining, estimate finish 2024-09-17 00:49:39]\n9, \n[1:29:28 remaining, estimate finish 2024-09-17 00:49:23]\n10, \n[1:26:25 remaining, estimate finish 2024-09-17 00:49:16]\n11, \n[1:23:13 remaining, estimate finish 2024-09-17 00:48:59]\n12, \n[1:19:49 remaining, estimate finish 2024-09-17 00:48:22]\n13, \n[1:16:41 remaining, estimate finish 2024-09-17 00:48:06]\n14, \n[1:13:34 remaining, estimate finish 2024-09-17 00:47:51]\n15, \n[1:10:30 remaining, estimate finish 2024-09-17 00:47:40]\n16, \n[1:07:38 remaining, estimate finish 2024-09-17 00:47:47]\n17, \n[1:04:40 remaining, estimate finish 2024-09-17 00:47:44]\n18, \n[1:01:53 remaining, estimate finish 2024-09-17 00:48:02]\n19, \n[59:01 remaining, estimate finish 2024-09-17 00:48:11]\n20, \n[56:06 remaining, estimate finish 2024-09-17 00:48:15]\n21, \n[53:00 remaining, estimate finish 2024-09-17 00:47:57]\n22, \n[50:06 remaining, estimate finish 2024-09-17 00:48:02]\n23, \n[47:11 remaining, estimate finish 2024-09-17 00:48:07]\n24, \n[44:20 remaining, estimate finish 2024-09-17 00:48:22]\n25, \n[41:26 remaining, estimate finish 2024-09-17 00:48:30]\n26, \n[38:32 remaining, estimate finish 2024-09-17 00:48:42]\n27, \n[35:31 remaining, estimate finish 2024-09-17 00:48:30]\n28, \n[32:34 remaining, estimate finish 2024-09-17 00:48:31]\n29, \n[29:34 remaining, estimate finish 2024-09-17 00:48:24]\n30, \n[26:37 remaining, estimate finish 2024-09-17 00:48:25]\n31, \n[23:39 remaining, estimate finish 2024-09-17 00:48:21]\n32, \n[20:39 remaining, estimate finish 2024-09-17 00:48:10]\n33, \n[17:43 remaining, estimate finish 2024-09-17 00:48:14]\n34, \n[14:44 remaining, estimate finish 2024-09-17 00:48:02]\n35, \n[11:47 remaining, estimate finish 2024-09-17 00:47:56]\n36,  [8:51 remaining] 37,  [5:55 remaining] 38,  [2:58 remaining] \n39.\n\nDone.\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\",\n       main = paste(\"Shwebo District (CSR)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#references",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#references",
    "title": "Take-home Exercise 1",
    "section": "References",
    "text": "References\nCrawley, M. J. (2007). The R Book. Wiley.\nThe Stata Journal. (2003). Adaptive kernel density estimation. Sage Journals. https://journals.sagepub.com/doi/pdf/10.1177/1536867X0300300204"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "To carry out this exercise, I will be using the following R packages:\n\nsf: a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat: has a wide range of useful functions for point pattern analysis. In this take-home exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster: reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this take-home exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools: provides a set of tools for manipulating geographic data. We mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap: provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nNow, let’s install and load these packages in RStudio.\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, tidyverse)\n\n\n\n\nNext, I will import the downloaded armed conflict data. For aspatial datasets like this, we will import into Rstudio using read_csv() function of the readr package.\n\n# Import armed conflict data\nconflict_data &lt;- read_csv(\"data/aspatial/2021-01-01-2024-06-30-Myanmar.csv\")\n\nRows: 87746 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): event_id_cnty, event_date, disorder_type, event_type, sub_event_ty...\ndbl (10): year, time_precision, inter1, interaction, iso, latitude, longitud...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe 2021-01-01-2024-06-30-Myanmar.csv dataset contains 87746 rows and 28 columns which indicates the presence of 87746 unique armed conflict events in Myanmar.\n\n\nAfter importing the dataset, we can inspect the dataset using the glimpse() function.\n\n# Inspect the conflict data\nglimpse(conflict_data)\n\nRows: 87,746\nColumns: 28\n$ event_id_cnty      &lt;chr&gt; \"MMR64313\", \"MMR64313\", \"MMR64320\", \"MMR64320\", \"MM…\n$ event_date         &lt;chr&gt; \"30 June 2024\", \"30 June 2024\", \"30 June 2024\", \"30…\n$ year               &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 202…\n$ time_precision     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ disorder_type      &lt;chr&gt; \"Political violence\", \"Political violence\", \"Politi…\n$ event_type         &lt;chr&gt; \"Battles\", \"Battles\", \"Battles\", \"Battles\", \"Battle…\n$ sub_event_type     &lt;chr&gt; \"Armed clash\", \"Armed clash\", \"Armed clash\", \"Armed…\n$ actor1             &lt;chr&gt; \"People's Defense Force - Mandalay\", \"Military Forc…\n$ assoc_actor_1      &lt;chr&gt; \"MDA - AGF: Madaya - The Authentic Genes Force; SST…\n$ inter1             &lt;dbl&gt; 3, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 2, 1, …\n$ interaction        &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 10, 13, 13, 10, 12, 12, 12,…\n$ civilian_targeting &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ iso                &lt;dbl&gt; 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 1…\n$ region             &lt;chr&gt; \"Southeast Asia\", \"Southeast Asia\", \"Southeast Asia…\n$ country            &lt;chr&gt; \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanma…\n$ admin1             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Ma…\n$ admin2             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Pyinoolwin\", \"Pyinoolwin\",…\n$ admin3             &lt;chr&gt; \"Patheingyi\", \"Patheingyi\", \"Singu\", \"Singu\", \"Thab…\n$ location           &lt;chr&gt; \"Aung Tha Pyay\", \"Aung Tha Pyay\", \"Pin Lel Gyi\", \"P…\n$ latitude           &lt;dbl&gt; 22.1504, 22.1504, 22.5752, 22.5752, 22.8800, 22.880…\n$ longitude          &lt;dbl&gt; 96.2364, 96.2364, 96.0661, 96.0661, 95.9700, 95.970…\n$ geo_precision      &lt;dbl&gt; 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, …\n$ source             &lt;chr&gt; \"Democratic Voice of Burma; Irrawaddy\", \"Democratic…\n$ source_scale       &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Na…\n$ notes              &lt;chr&gt; \"On 30 June 2024, near Aung Tha Pyay village (Pathe…\n$ fatalities         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, …\n$ tags               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ timestamp          &lt;dbl&gt; 1720552468, 1720552468, 1720552468, 1720552468, 172…\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe event_date field shows that it uses a character datatype instead of date - we will fix this later. Also, we can observe that thelongitude and langitude fields appear to be adopting the WGS84 geographic coordinate system since they are in the -180/180 and -90/90 range respectively.\n\n\nI will also import the administrative boundary data into a simple features tibble data.frame using st_read() of the sf package. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\n# Import boundary data\nboundary_sf &lt;- st_read(dsn = \"data/geospatial\",layer = \"mmr_polbnda_adm2_250k_mimu\") %&gt;% st_transform(crs = 32647)\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Take-home_Ex\\Take-home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIn the code above, the %&gt;% operator is used to pass the output of st_read() directly to the st_transform() function. Since the dataset represents the Myanmar boundary, we need to assign the appropriate coordinate reference system, which is UTM zone 47N (EPSG:32647), east of Myanmar. The st_transform() function then converts the CRS of the sf object to EPSG:32647.\n\n\nIn the code below, we can notice that the ESPG code has been updated to 32647.\n\n# Check for changes\nst_crs(boundary_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nHere, I will use the plot() function which plots the geometry of the sf object. The st_geometry() function is used to extract the geometry of the mpsz_sf object which includes the districts of Myanmar as shown below.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(boundary_sf))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#lets-set-up",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#lets-set-up",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "To carry out this exercise, I will be using the following R packages:\n\nsf: a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat: has a wide range of useful functions for point pattern analysis. In this take-home exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster: reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this take-home exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools: provides a set of tools for manipulating geographic data. We mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap: provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nNow, let’s install and load these packages in RStudio.\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, tidyverse)\n\n\n\n\nNext, I will import the downloaded armed conflict data. For aspatial datasets like this, we will import into Rstudio using read_csv() function of the readr package.\n\n# Import armed conflict data\nconflict_data &lt;- read_csv(\"data/aspatial/2021-01-01-2024-06-30-Myanmar.csv\")\n\nRows: 87746 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): event_id_cnty, event_date, disorder_type, event_type, sub_event_ty...\ndbl (10): year, time_precision, inter1, interaction, iso, latitude, longitud...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe 2021-01-01-2024-06-30-Myanmar.csv dataset contains 87746 rows and 28 columns which indicates the presence of 87746 unique armed conflict events in Myanmar.\n\n\nAfter importing the dataset, we can inspect the dataset using the glimpse() function.\n\n# Inspect the conflict data\nglimpse(conflict_data)\n\nRows: 87,746\nColumns: 28\n$ event_id_cnty      &lt;chr&gt; \"MMR64313\", \"MMR64313\", \"MMR64320\", \"MMR64320\", \"MM…\n$ event_date         &lt;chr&gt; \"30 June 2024\", \"30 June 2024\", \"30 June 2024\", \"30…\n$ year               &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 202…\n$ time_precision     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ disorder_type      &lt;chr&gt; \"Political violence\", \"Political violence\", \"Politi…\n$ event_type         &lt;chr&gt; \"Battles\", \"Battles\", \"Battles\", \"Battles\", \"Battle…\n$ sub_event_type     &lt;chr&gt; \"Armed clash\", \"Armed clash\", \"Armed clash\", \"Armed…\n$ actor1             &lt;chr&gt; \"People's Defense Force - Mandalay\", \"Military Forc…\n$ assoc_actor_1      &lt;chr&gt; \"MDA - AGF: Madaya - The Authentic Genes Force; SST…\n$ inter1             &lt;dbl&gt; 3, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 2, 1, …\n$ interaction        &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 10, 13, 13, 10, 12, 12, 12,…\n$ civilian_targeting &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ iso                &lt;dbl&gt; 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 1…\n$ region             &lt;chr&gt; \"Southeast Asia\", \"Southeast Asia\", \"Southeast Asia…\n$ country            &lt;chr&gt; \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanma…\n$ admin1             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Ma…\n$ admin2             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Pyinoolwin\", \"Pyinoolwin\",…\n$ admin3             &lt;chr&gt; \"Patheingyi\", \"Patheingyi\", \"Singu\", \"Singu\", \"Thab…\n$ location           &lt;chr&gt; \"Aung Tha Pyay\", \"Aung Tha Pyay\", \"Pin Lel Gyi\", \"P…\n$ latitude           &lt;dbl&gt; 22.1504, 22.1504, 22.5752, 22.5752, 22.8800, 22.880…\n$ longitude          &lt;dbl&gt; 96.2364, 96.2364, 96.0661, 96.0661, 95.9700, 95.970…\n$ geo_precision      &lt;dbl&gt; 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, …\n$ source             &lt;chr&gt; \"Democratic Voice of Burma; Irrawaddy\", \"Democratic…\n$ source_scale       &lt;chr&gt; \"National\", \"National\", \"National\", \"National\", \"Na…\n$ notes              &lt;chr&gt; \"On 30 June 2024, near Aung Tha Pyay village (Pathe…\n$ fatalities         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, …\n$ tags               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ timestamp          &lt;dbl&gt; 1720552468, 1720552468, 1720552468, 1720552468, 172…\n\n\n\n\n\n\n\n\nObservations\n\n\n\nThe event_date field shows that it uses a character datatype instead of date - we will fix this later. Also, we can observe that thelongitude and langitude fields appear to be adopting the WGS84 geographic coordinate system since they are in the -180/180 and -90/90 range respectively.\n\n\nI will also import the administrative boundary data into a simple features tibble data.frame using st_read() of the sf package. This function reads the shapefile data and returns an sf object that can be used for further analysis.\n\n# Import boundary data\nboundary_sf &lt;- st_read(dsn = \"data/geospatial\",layer = \"mmr_polbnda_adm2_250k_mimu\") %&gt;% st_transform(crs = 32647)\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\Take-home_Ex\\Take-home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIn the code above, the %&gt;% operator is used to pass the output of st_read() directly to the st_transform() function. Since the dataset represents the Myanmar boundary, we need to assign the appropriate coordinate reference system, which is UTM zone 47N (EPSG:32647), east of Myanmar. The st_transform() function then converts the CRS of the sf object to EPSG:32647.\n\n\nIn the code below, we can notice that the ESPG code has been updated to 32647.\n\n# Check for changes\nst_crs(boundary_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nHere, I will use the plot() function which plots the geometry of the sf object. The st_geometry() function is used to extract the geometry of the mpsz_sf object which includes the districts of Myanmar as shown below.\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(boundary_sf))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#data-wrangling",
    "title": "Take-home Exercise 1",
    "section": "3. Data Wrangling",
    "text": "3. Data Wrangling\n\n3.1 Fixing Incorrect Datatypes\nRecall that the earlier inspection of the conflict_data tibble data frame revealed that the datatype indicated for event date is wrongly labelled as a character instead of a date format.\nAs such, let’s convert the datatype to the correct ‘date’ format as shown below.\n\n# Convert the datatype for event_date\nconflict_data$event_date &lt;- as.Date(conflict_data$event_date, format = \"%d %B %Y\")\n\n# Check for changes\nhead(conflict_data)\n\n# A tibble: 6 × 28\n  event_id_cnty event_date  year time_precision disorder_type      event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence Battles   \n2 MMR64313      2024-06-30  2024              1 Political violence Battles   \n3 MMR64320      2024-06-30  2024              1 Political violence Battles   \n4 MMR64320      2024-06-30  2024              1 Political violence Battles   \n5 MMR64321      2024-06-30  2024              1 Political violence Battles   \n6 MMR64321      2024-06-30  2024              1 Political violence Battles   \n# ℹ 22 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;\n\n\n\n\n3.2 Adding new year_quarter column\nWe will want to create a new column to indicate the specific year and quarter for each conflict event since the spatial analysis will be done later in a quarterly manner.\n\n\nExtract year and quarter\nconflict_data$year_quarter &lt;- paste0(\n  year(conflict_data$event_date), \n  \" Q\", \n  quarter(conflict_data$event_date)\n)\n\n# View the new data column\nunique(conflict_data$year_quarter)\n\n\n [1] \"2024 Q2\" \"2024 Q1\" \"2023 Q4\" \"2023 Q3\" \"2023 Q2\" \"2023 Q1\" \"2022 Q4\"\n [8] \"2022 Q3\" \"2022 Q2\" \"2022 Q1\" \"2021 Q4\" \"2021 Q3\" \"2021 Q2\" \"2021 Q1\"\n\n\n\n\n3.3 Fixing Duplicated Event ID in conflict_data Dataframe\nAs shown, there are presence of duplicates in our dataframe returned by the duplicated() function.\n\n# Check for duplicates\nany(duplicated(conflict_data))\n\n[1] TRUE\n\n\nBased on the duplicated event ID: MMR64313 for instance. We can observe the two records are of the same political violence event happening between two actors on 30/6/2024, between the People’s Defense Force and Military Forces of Myanmar. Upon further research, these two actors are opposing political parties of Myanmar’s ongoing conflict.\n\n# Inspect an instance of the duplciated event IDs\nhead(conflict_data,2)\n\n# A tibble: 2 × 29\n  event_id_cnty event_date  year time_precision disorder_type      event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence Battles   \n2 MMR64313      2024-06-30  2024              1 Political violence Battles   \n# ℹ 23 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;, year_quarter &lt;chr&gt;\n\n\n\n\n\n\n\n\nReflection\n\n\n\nShould duplicated data be removed in this analysis?\nA single event (e.g. MMR64313) can have duplicated rows with different actor1 values, typically due to counterattacks from opposing sides, leading to different data entries into the conflict_data dataset.\nHence, I will remove duplicated events found in the conflict_data dataframe as long as the rows have the same event ID indicated.\n\n\nHere, I did another check to ensure there is not more than 2 possible repeated event IDs in the first 20 rows of conflict_data.\n\n\nCheck duplicated events for first 20 rows\nduplicate_counts_first_20 &lt;- conflict_data %&gt;%\n  slice(1:20) %&gt;%            \n  group_by(event_id_cnty) %&gt;% \n  summarize(count = n()) %&gt;%  \n  filter(count &gt; 1)         \n\n# View the result\nprint(duplicate_counts_first_20)\n\n\n# A tibble: 9 × 2\n  event_id_cnty count\n  &lt;chr&gt;         &lt;int&gt;\n1 MMR64313          2\n2 MMR64320          2\n3 MMR64321          2\n4 MMR64323          2\n5 MMR64325          2\n6 MMR64326          2\n7 MMR64328          2\n8 MMR64330          2\n9 MMR64331          2\n\n\nWith that checked, I’ll remove the duplicated rows with a repeated Event ID.\n\n\nRemove duplicated rows\n# Retrieve data of duplicated rows\nmerged_duplicates &lt;- conflict_data %&gt;%\n  filter(duplicated(event_id_cnty) | duplicated(event_id_cnty, fromLast = TRUE)) %&gt;%\n  arrange(event_id_cnty) %&gt;%\n  group_by(event_id_cnty) %&gt;%\n  summarize(\n    actor2 = last(actor1),\n    assoc_actor_2 = last(assoc_actor_1)\n  )\n\nconflict_data_no_duplicates &lt;- conflict_data %&gt;%\n  filter(!duplicated(event_id_cnty))\n\n# Update conflict_data dataframe with new columns\nconflict_data &lt;- conflict_data_no_duplicates %&gt;%\n  left_join(merged_duplicates, by = \"event_id_cnty\")\n\n# View dataframe\nprint(head(conflict_data))\n\n\n# A tibble: 6 × 31\n  event_id_cnty event_date  year time_precision disorder_type         event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence    Battles   \n2 MMR64320      2024-06-30  2024              1 Political violence    Battles   \n3 MMR64321      2024-06-30  2024              1 Political violence    Battles   \n4 MMR64322      2024-06-30  2024              1 Strategic developmen… Strategic…\n5 MMR64323      2024-06-30  2024              1 Political violence    Battles   \n6 MMR64324      2024-06-30  2024              1 Strategic developmen… Strategic…\n# ℹ 25 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;,\n#   region &lt;chr&gt;, country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;,\n#   location &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, geo_precision &lt;dbl&gt;,\n#   source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;, fatalities &lt;dbl&gt;,\n#   tags &lt;chr&gt;, timestamp &lt;dbl&gt;, year_quarter &lt;chr&gt;, actor2 &lt;chr&gt;,\n#   assoc_actor_2 &lt;chr&gt;\n\n\nWe can observe that there are no longer any duplicated event IDs in our conflict_data data frame.\n\nany(duplicated(conflict_data))\n\n[1] FALSE\n\n\n\n\n3.4 Converting Aspatial Data to Simple Feature Format\nFor the purpose of this exercise, we will want to integrate and analyse aspatial data in a geographic context. I’ll do a check if conflict_data needs to be converted to a sf data frame - if it outputs anything else but sf, then it’s not a simple feature data frame!\n\nclass(conflict_data)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n\n\n\n\n\n\nObservations\n\n\n\nWe can see that conflict_data is not a sf data frame. Since a non-simple feature data frame does not have a “geometry” column, we’ll need to convert conflict_data into a simple feature data frame\n\n\nWe can convert conflict_data into a simple feature data frame by using st_as_sf() from the sf package. Addiitionally, we will also need to transform coordinate system from geographic (ESPG: 4326) to projected (ESPG: 32647) using st_transform().\n\n# Convert to simple feature format\nconflict_data_sf &lt;- st_as_sf(conflict_data, coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;% st_transform(crs = 32647)\n\n# Inspect the changes\nglimpse(conflict_data_sf)\n\nRows: 51,553\nColumns: 30\n$ event_id_cnty      &lt;chr&gt; \"MMR64313\", \"MMR64320\", \"MMR64321\", \"MMR64322\", \"MM…\n$ event_date         &lt;date&gt; 2024-06-30, 2024-06-30, 2024-06-30, 2024-06-30, 20…\n$ year               &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 202…\n$ time_precision     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ disorder_type      &lt;chr&gt; \"Political violence\", \"Political violence\", \"Politi…\n$ event_type         &lt;chr&gt; \"Battles\", \"Battles\", \"Battles\", \"Strategic develop…\n$ sub_event_type     &lt;chr&gt; \"Armed clash\", \"Armed clash\", \"Armed clash\", \"Chang…\n$ actor1             &lt;chr&gt; \"People's Defense Force - Mandalay\", \"People's Defe…\n$ assoc_actor_1      &lt;chr&gt; \"MDA - AGF: Madaya - The Authentic Genes Force; SST…\n$ inter1             &lt;dbl&gt; 3, 3, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 3, 3, 3, 7, 1, …\n$ interaction        &lt;dbl&gt; 13, 13, 13, 10, 13, 10, 12, 12, 12, 12, 12, 13, 13,…\n$ civilian_targeting &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ iso                &lt;dbl&gt; 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 1…\n$ region             &lt;chr&gt; \"Southeast Asia\", \"Southeast Asia\", \"Southeast Asia…\n$ country            &lt;chr&gt; \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanmar\", \"Myanma…\n$ admin1             &lt;chr&gt; \"Mandalay\", \"Mandalay\", \"Mandalay\", \"Sagaing\", \"Sag…\n$ admin2             &lt;chr&gt; \"Mandalay\", \"Pyinoolwin\", \"Pyinoolwin\", \"Shwebo\", \"…\n$ admin3             &lt;chr&gt; \"Patheingyi\", \"Singu\", \"Thabeikkyin\", \"Khin-U\", \"My…\n$ location           &lt;chr&gt; \"Aung Tha Pyay\", \"Pin Lel Gyi\", \"Thabeikkyin\", \"Khi…\n$ geo_precision      &lt;dbl&gt; 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, …\n$ source             &lt;chr&gt; \"Democratic Voice of Burma; Irrawaddy\", \"Irrawaddy\"…\n$ source_scale       &lt;chr&gt; \"National\", \"National\", \"National\", \"Subnational-Na…\n$ notes              &lt;chr&gt; \"On 30 June 2024, near Aung Tha Pyay village (Pathe…\n$ fatalities         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ tags               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ timestamp          &lt;dbl&gt; 1720552468, 1720552468, 1720552468, 1720552468, 172…\n$ year_quarter       &lt;chr&gt; \"2024 Q2\", \"2024 Q2\", \"2024 Q2\", \"2024 Q2\", \"2024 Q…\n$ actor2             &lt;chr&gt; \"Military Forces of Myanmar (2021-)\", \"Military For…\n$ assoc_actor_2      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Uniden…\n$ geometry           &lt;POINT [m]&gt; POINT (214961 2452068), POINT (198303.2 24994…\n\n\n\n\n\n\n\n\nObservations\n\n\n\nNotice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been removed from the data frame.\n\n\nWe can further inspect the newly created ‘geometry’ column of conflict_data_sf\n\n# Retrieve geometry column\nst_geometry(conflict_data_sf)\n\nGeometry set for 51553 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -208804.4 ymin: 1103500 xmax: 640934.5 ymax: 3042960\nProjected CRS: WGS 84 / UTM zone 47N\nFirst 5 geometries:\n\n\nPOINT (214961 2452068)\n\n\nPOINT (198303.2 2499463)\n\n\nPOINT (189105.4 2533434)\n\n\nPOINT (160913.9 2522331)\n\n\nPOINT (146213 2428487)\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt consists of 51,533 features consisting of point geometric features where the underlying datum is in WGS 84 format.\n\n\nTo ensure that the coordinate system is correctly updated, we can use the st_crs() function where we observe that the ESPG code is correctly indicated as 32647.\n\n# Check CRS format\nst_crs(conflict_data_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\n3.5 Reduce Data File Size\nIn this section, I will reduce the current Myanmar armed conflict dataset as the time taken for computing the kernel density estimates can take up to 30 minutes long which is not computationally efficient.\n\n1) Remove ‘Protests’ and ‘Riots’ Event Types\nI will remove rows in the conflicts_data_sf dataset that don’t focus on the four main event types (Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians), as mentioned in the exercise brief.\n\nconflict_data_sf &lt;- conflict_data_sf %&gt;%\n  filter(!(event_type %in% c(\"Protests\", \"Riots\")))\n\nunique(conflict_data_sf$event_type)\n\n[1] \"Battles\"                    \"Strategic developments\"    \n[3] \"Violence against civilians\" \"Explosions/Remote violence\"\n\n\n\n\n2) Remove unused columns in boundary_sf\nAs seen, there are 8 columns in the simple feature data frame of boundary_sf.\n\n# Inspect first rows of data in boundary_sf\nhead(boundary_sf)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -14915.04 ymin: 1736124 xmax: 187961.7 ymax: 2051144\nProjected CRS: WGS 84 / UTM zone 47N\n  OBJECTID         ST ST_PCODE        DT   DT_PCODE      DT_MMR PCode_V\n1        1 Ayeyarwady   MMR017  Hinthada MMR017D002    ဟင်္သာတခရိုင်     9.4\n2        2 Ayeyarwady   MMR017   Labutta MMR017D004    လပွတ္တာခရိုင်     9.4\n3        3 Ayeyarwady   MMR017    Maubin MMR017D005     မအူပင်ခရိုင်     9.4\n4        4 Ayeyarwady   MMR017 Myaungmya MMR017D003 မြောင်းမြခရိုင်     9.4\n5        5 Ayeyarwady   MMR017   Pathein MMR017D001      ပုသိမ်ခရိုင်     9.4\n6        6 Ayeyarwady   MMR017    Pyapon MMR017D006     ဖျာပုံခရိုင်     9.4\n                        geometry\n1 MULTIPOLYGON (((90859.89 20...\n2 MULTIPOLYGON (((75991.51 17...\n3 MULTIPOLYGON (((115559 1928...\n4 MULTIPOLYGON (((39919.39 18...\n5 MULTIPOLYGON (((-6302.348 1...\n6 MULTIPOLYGON (((93411.72 17...\n\n\nI will remove ’DT_MMR” column as we already have the District Name in English in DT and won’t require the district names in Myanmar Language. Next, we will remove the coded versions of ST (state/region) and DT (district) columns, namely ST_PCODE and DT_PCODE. Additionally, we won’t need the PCode_V column since we will be dropping the PCODE column too.\n\nboundary_sf &lt;- boundary_sf %&gt;% dplyr::select('OBJECTID', 'ST', 'DT','geometry')\nsummary(boundary_sf)\n\n    OBJECTID          ST                 DT                     geometry \n Min.   : 1.00   Length:80          Length:80          MULTIPOLYGON :80  \n 1st Qu.:20.75   Class :character   Class :character   epsg:32647   : 0  \n Median :40.50   Mode  :character   Mode  :character   +proj=utm ...: 0  \n Mean   :40.50                                                           \n 3rd Qu.:60.25                                                           \n Max.   :80.00                                                           \n\n\n\n\n3) Remove unused columns in conflict_data\nI will also remove unnecessary columns of the conflict_data data frame that won’t be used in our spatial analysis later.\n\n\nRemove unnecessary columns\nconflict_data_sf &lt;- conflict_data_sf %&gt;%\n  select(event_id_cnty, event_date, year_quarter, disorder_type, event_type, location, geometry, fatalities)\n\nsummary(conflict_data_sf)\n\n\n event_id_cnty        event_date         year_quarter       disorder_type     \n Length:42608       Min.   :2021-01-01   Length:42608       Length:42608      \n Class :character   1st Qu.:2022-01-10   Class :character   Class :character  \n Mode  :character   Median :2022-10-13   Mode  :character   Mode  :character  \n                    Mean   :2022-10-29                                        \n                    3rd Qu.:2023-08-29                                        \n                    Max.   :2024-06-30                                        \n  event_type          location                  geometry       fatalities    \n Length:42608       Length:42608       POINT        :42608   Min.   :  0.00  \n Class :character   Class :character   epsg:32647   :    0   1st Qu.:  0.00  \n Mode  :character   Mode  :character   +proj=utm ...:    0   Median :  0.00  \n                                                             Mean   :  1.27  \n                                                             3rd Qu.:  1.00  \n                                                             Max.   :201.00  \n\n\nLet’s append conflict_data_sf with the columns of boundary_sf to assist our analysis later.\n\n# Link conflict event to its district region\nconflict_data_sf &lt;- st_join(conflict_data_sf, boundary_sf, join = st_intersects)\n\n\n\n\n3.6 Converting Simple Features Data Frame into ppp Object\nIt is important that we convert conflict_data_sf (a simple feature data frame) into a planer point pattern (ppp) object format, since the spatstat package that we’ll be using for the Spatial Point Pattern Analysis later is specifically designed for working with ppp-formated data. Additionally, I will begin with categorising the ppp objects into their unique year_quarter category.\n\n\nCreate ppp objects based on year_quarter category\n# Create an empty list to store the ppp objects\nppp_list &lt;- list()\n\n# Loop through each unique year_quarter category\nfor (yq in unique(conflict_data_sf$year_quarter)) {\n  # Subset the data for the current year_quarter\n  subset_data_sf &lt;- conflict_data_sf %&gt;% filter(year_quarter == yq)\n  \n  # Convert the subset to a ppp object\n  subset_ppp &lt;- as.ppp(subset_data_sf$geometry)\n  \n  # Add the ppp object to the list\n  ppp_list[[yq]] &lt;- subset_ppp\n}\n\n# Check list\nppp_list\n\n\n$`2024 Q2`\nPlanar point pattern: 2788 points\nwindow: rectangle = [-208804.4, 597543.7] x [1103500.1, 3026504.9] units\n\n$`2024 Q1`\nPlanar point pattern: 3186 points\nwindow: rectangle = [-207135, 591875.9] x [1245380, 3026504.9] units\n\n$`2023 Q4`\nPlanar point pattern: 3627 points\nwindow: rectangle = [-206931.7, 604775.1] x [1103500.1, 3020772.2] units\n\n$`2023 Q3`\nPlanar point pattern: 3010 points\nwindow: rectangle = [-197883.4, 518300.4] x [1103500.1, 3027041.8] units\n\n$`2023 Q2`\nPlanar point pattern: 2745 points\nwindow: rectangle = [-191261.5, 518300.4] x [1103500.1, 3006372.9] units\n\n$`2023 Q1`\nPlanar point pattern: 3101 points\nwindow: rectangle = [-199243.8, 591875.9] x [1103500.1, 3026504.9] units\n\n$`2022 Q4`\nPlanar point pattern: 3296 points\nwindow: rectangle = [-206531.5, 518300.4] x [1103500.1, 2931517.1] units\n\n$`2022 Q3`\nPlanar point pattern: 3486 points\nwindow: rectangle = [-206196.6, 568361.5] x [1103500.1, 3026504.9] units\n\n$`2022 Q2`\nPlanar point pattern: 3580 points\nwindow: rectangle = [-206931.7, 640934.5] x [1103500.1, 3026504.9] units\n\n$`2022 Q1`\nPlanar point pattern: 3563 points\nwindow: rectangle = [-204784, 591875.9] x [1103500.1, 3026504.9] units\n\n$`2021 Q4`\nPlanar point pattern: 3844 points\nwindow: rectangle = [-200024.3, 591875.9] x [1103500.1, 3042960.3] units\n\n$`2021 Q3`\nPlanar point pattern: 2754 points\nwindow: rectangle = [-193181.1, 591875.9] x [1103500.1, 3042960.3] units\n\n$`2021 Q2`\nPlanar point pattern: 2916 points\nwindow: rectangle = [-191409.1, 640934.5] x [1132472.1, 3042960.3] units\n\n$`2021 Q1`\nPlanar point pattern: 712 points\nwindow: rectangle = [-203795.3, 591875.9] x [1375186.1, 3026504.9] units\n\n\nWe can visualise the spread of conflict events across each quarter from January 2021 to June 2024 using the plot() function as shown below.\n\n\nVisualise the spread of conflicts by year_quarter\n# Ensure 'year_quarter' is a factor\nconflict_data_sf$year_quarter &lt;- as.factor(conflict_data_sf$year_quarter)\n\n# Loop through each unique year_quarter and create separate plots\nyear_quarters &lt;- unique(conflict_data_sf$year_quarter)\n\n# Set up a grid layout for multiple plots (adjust 'mfrow' as needed)\npar(mfrow = c(2,3))\npar(mar = c(0,0,1,0))\n\n# Loop through each year_quarter and plot\nfor (yq in year_quarters) {\n  subset_data_sf &lt;- conflict_data_sf[conflict_data_sf$year_quarter == yq, ]\n  conflict_data_ppp &lt;- as.ppp(subset_data_sf$geometry)\n  \n  # Plot each subset ppp object\n  plot(conflict_data_ppp, main = paste(\"Year-Quarter:\", yq))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIt is noticeable that there conflict events have occured more frequently since 2021 as points plotted on the graph have gotten darker across 2021 to 2024. We can also observe the possibility of duplicated events occurring from the darker spots in the plot, in which it appears more intense in Myanmar’s central and west regions.\n\n\n\n\n3.7 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area, that is Myanmar’s boundary in this case. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to convert the boundary_data_sf simple feature data frame into an owin object of spatstat.\n\n# Convert to owin object\nmyanmar_owin &lt;- as.owin(boundary_sf)\n\n# Visualise the owin object\nplot(myanmar_owin)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nFrom my observations, the as.owin() function converts the boundary_data_sf spatial boundary into a window object that represents the outer boundary of the spatial region and does not handle internal structures or districts we previously saw from the plot of boundary_data_sf.\n\n\nWe can also take a quick look at the owin object properties as shown. I will be converting it to a data frame for the purposes of getting a quick glimpse of the object.\n\n# Summary info of owin object\nowin_df &lt;- as.data.frame(myanmar_owin)\nprint(head(owin_df))\n\n         x       y id sign\n1 56519.39 2741919  1   -1\n2 56917.28 2741947  1   -1\n3 57000.15 2741973  1   -1\n4 57068.51 2741994  1   -1\n5 57221.44 2742142  1   -1\n6 57068.51 2741994  1   -1\n\n\n\n\n3.8 Combining ppp Object and owin Object\nIn this last step of geospatial data wrangling, I will mask all ppp object with the owin object I created earlier to put in place all conflict events within the boundary of Myanmar. Doing so can also optimise the memory usage for large datasets.\n\n\n\n\n\n\nThe ppp object outputted from combining both the point and polygon feature results in the boundary of Myanmar outlining the plot of conflict events as shown.\n\n# Set up plotting layout\nn &lt;- length(masked_ppp_list)\n\n# Plot each masked ppp object\npar(mfrow = c(2,3), mar = c(0,0,1,0))  # Adjust margins as needed\nfor (quarter in names(masked_ppp_list)) {\n  plot(masked_ppp_list[[quarter]], main = paste(\"Year Quarter:\", quarter))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmasked_ppp_list_km = list()\n\nfor (quarter in names(masked_ppp_list)) {\n  ppp_obj &lt;- masked_ppp_list[[quarter]]\n  ppp_obj_km &lt;- rescale(ppp_obj, 1000, \"km\")\n  masked_ppp_list_km[[quarter]] &lt;- ppp_obj_km\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#spatio-temporal-kde",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#spatio-temporal-kde",
    "title": "Take-home Exercise 1",
    "section": "7. Spatio-Temporal KDE",
    "text": "7. Spatio-Temporal KDE\n\n\nSet up DayofYear variable per quarter\nQ2_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2024 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2024 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2024 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2023 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2023 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2022 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2022 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ4_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q4\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ3_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q3\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ2_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q2\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\nQ1_2021 &lt;- conflict_data_sf %&gt;%\n  filter(year_quarter == \"2021 Q1\") %&gt;%\n  mutate(DayofYear = yday(event_date))\n\n\n\n7.1 Creating ppp object\nIn the code chunk below, DayofYear from the fire_sf data frame is selected and is included in the output ppp object.\n\n\nCreate ppp object per quarter\nQ2_2024_ppp &lt;- Q2_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2024_ppp &lt;- Q1_2024 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2023_ppp &lt;- Q4_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2023_ppp &lt;- Q3_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2023_ppp &lt;- Q2_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2023_ppp &lt;- Q1_2023 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2022_ppp &lt;- Q4_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2022_ppp &lt;- Q3_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2022_ppp &lt;- Q2_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2022_ppp &lt;- Q1_2022 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ4_2021_ppp &lt;- Q4_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ3_2021_ppp &lt;- Q3_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ2_2021_ppp &lt;- Q2_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\nQ1_2021_ppp &lt;- Q1_2021 %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\n\n7.2 Combining ppp with owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\n\nMask the ppp object with owin object\nQ2_2024_owin &lt;- Q2_2024_ppp[myanmar_owin]\n\nQ1_2024_owin &lt;- Q1_2024_ppp[myanmar_owin]\n\nQ4_2023_owin &lt;- Q4_2023_ppp[myanmar_owin]\n\nQ3_2023_owin &lt;- Q3_2023_ppp[myanmar_owin]\n\nQ2_2023_owin &lt;- Q2_2023_ppp[myanmar_owin]\n\nQ1_2023_owin &lt;- Q1_2023_ppp[myanmar_owin]\n\nQ4_2022_owin &lt;- Q4_2022_ppp[myanmar_owin]\n\nQ3_2022_owin &lt;- Q3_2022_ppp[myanmar_owin]\n\nQ2_2022_owin &lt;- Q2_2022_ppp[myanmar_owin]\n\nQ1_2022_owin &lt;- Q1_2022_ppp[myanmar_owin]\n\nQ4_2021_owin &lt;- Q4_2021_ppp[myanmar_owin]\n\nQ3_2021_owin &lt;- Q3_2021_ppp[myanmar_owin]\n\nQ2_2021_owin &lt;- Q2_2021_ppp[myanmar_owin]\n\nQ1_2021_owin &lt;- Q1_2021_ppp[myanmar_owin]\n\n\nNow, I will perform a spatio-temporal kernel density estimate on the owin object which gives us insights into where and when conflict event occurrences are concentrated within the specified observation window.\n\n\nPerform spatial temporal KDE per quarter\nQ2_2024_stkde &lt;- spattemp.density(Q2_2024_owin)\n\nQ1_2024_stkde &lt;- spattemp.density(Q1_2024_owin)\n\nQ4_2023_stkde &lt;- spattemp.density(Q4_2023_owin)\n\nQ3_2023_stkde &lt;- spattemp.density(Q3_2023_owin)\n\nQ2_2023_stkde &lt;- spattemp.density(Q2_2023_owin)\n\nQ1_2023_stkde &lt;- spattemp.density(Q1_2023_owin)\n\nQ4_2022_stkde &lt;- spattemp.density(Q4_2022_owin)\n\nQ3_2022_stkde &lt;- spattemp.density(Q3_2022_owin)\n\nQ2_2022_stkde &lt;- spattemp.density(Q2_2022_owin)\n\nQ1_2022_stkde &lt;- spattemp.density(Q1_2022_owin)\n\nQ4_2021_stkde &lt;- spattemp.density(Q4_2021_owin)\n\nQ3_2021_stkde &lt;- spattemp.density(Q3_2021_owin)\n\nQ2_2021_stkde &lt;- spattemp.density(Q2_2021_owin)\n\nQ1_2021_stkde &lt;- spattemp.density(Q1_2021_owin)\n\n\n\n\nSTKDE of 2024 Q2\n# Load necessary libraries\nlibrary(spatstat)\nlibrary(magick)\nlibrary(viridis)\n\n# Create a directory to store PNG frames\nif (!dir.exists(\"2024_Q2_frames\")) {\n  dir.create(\"2024_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2024_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2024_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2024_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2024 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2024_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2024_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2024 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"2024_Q1_frames\")) {\n  dir.create(\"2024_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2024_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2024_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2024_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2024 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2024_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2024_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q4\n# Create a directory to store PNG frames\nif (!dir.exists(\"2023_Q4_frames\")) {\n  dir.create(\"2023_Q4_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q4_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q4_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2023_Q4_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q4 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2023_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2023_Q4_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q3\n# Create a directory to store PNG frames\nif (!dir.exists(\"2023_Q3_frames\")) {\n  dir.create(\"2023_Q3_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q3_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q3_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2023_Q3_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q3 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2023_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2023_Q3_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q2\n# Create a directory to store PNG frames\nif (!dir.exists(\"2023_Q2_frames\")) {\n  dir.create(\"2023_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2023_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2023_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2023_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2023 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"2023_Q1_frames\")) {\n  dir.create(\"2023_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2023_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2023_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2023_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2023 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2023_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2023_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q4\n# Create a directory to store PNG frames\nif (!dir.exists(\"2022_Q4_frames\")) {\n  dir.create(\"2022_Q4_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q4_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q4_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2022_Q4_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q4 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2022_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2022_Q4_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q3\n# Create a directory to store PNG frames\nif (!dir.exists(\"2022_Q3_frames\")) {\n  dir.create(\"2022_Q3_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q3_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q3_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2022_Q3_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q3 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2022_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2022_Q3_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q2\n# Create a directory to store PNG frames\nif (!dir.exists(\"2022_Q2_frames\")) {\n  dir.create(\"2022_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2022_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2022_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2022_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2022 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"2022_Q1_frames\")) {\n  dir.create(\"2022_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2022_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2022_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2022_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2022_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2022_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q4\n# Create a directory to store PNG frames\nif (!dir.exists(\"2021_Q4_frames\")) {\n  dir.create(\"2021_Q4_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q4_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q4_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2021_Q4_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2021 Q4 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2021_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2021_Q4_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q3\n# Create a directory to store PNG frames\nif (!dir.exists(\"2021_Q3_frames\")) {\n  dir.create(\"2021_Q3_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q3_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q3_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2021_Q3_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2021 Q3 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2021_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2021_Q3_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q2\n# Create a directory to store PNG frames\nif (!dir.exists(\"2021_Q2_frames\")) {\n  dir.create(\"2021_Q2_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q2_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q2_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2021_Q2_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2021 Q2 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2021_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2021_Q2_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\n\n\nSTKDE of 2021 Q1\n# Create a directory to store PNG frames\nif (!dir.exists(\"2021_Q1_frames\")) {\n  dir.create(\"2021_Q1_frames\")\n}\n\n# Get the unique day values\ndays &lt;- names(Q1_2021_stkde$z)\n\n# Loop through each day and save the plot\nfor (day in days) {\n  kde_result &lt;- Q1_2021_stkde$z[[day]]\n  \n  # Create PNG filename\n  png_filename &lt;- file.path(\"2021_Q1_frames\", sprintf(\"frame_%s.png\", day))\n  \n  # Save the plot as PNG\n  png(filename = png_filename, width = 800, height = 800)\n  plot(kde_result, main = paste(\"2022 Q1 - Day\", day), col = viridis::viridis(100))\n  dev.off()\n}\n\n# Read all PNG files from the frames directory\nframes &lt;- image_read(list.files(\"2021_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10)\noutput_path &lt;- \"2021_Q1_stkde.gif\"\nimage_write(animation, path = output_path)\n\n\nLet’s plot our animated spatio-temporal KDE outputs now.\n\nlibrary(spatstat)\nlibrary(magick)\nlibrary(viridis)\n\n# 2024 Q2\nframes &lt;- image_read(list.files(\"2024_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2024 Q1\nframes &lt;- image_read(list.files(\"2024_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n# 2023 Q4\nframes &lt;- image_read(list.files(\"2023_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2023 Q3\nframes &lt;- image_read(list.files(\"2023_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2023 Q2\nframes &lt;- image_read(list.files(\"2023_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2023 Q1\nframes &lt;- image_read(list.files(\"2023_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n# 2022 Q4\nframes &lt;- image_read(list.files(\"2022_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2022 Q3\nframes &lt;- image_read(list.files(\"2022_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2022 Q2\nframes &lt;- image_read(list.files(\"2022_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2022 Q1\nframes &lt;- image_read(list.files(\"2022_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n# 2021 Q4\nframes &lt;- image_read(list.files(\"2021_Q4_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2021 Q3\nframes &lt;- image_read(list.files(\"2021_Q3_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2021 Q2\nframes &lt;- image_read(list.files(\"2021_Q2_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n# 2021 Q1\nframes &lt;- image_read(list.files(\"2021_Q1_frames\", full.names = TRUE, pattern = \"*.png\"))\nanimation &lt;- image_animate(image_join(frames), fps = 10) \nanimation\n\n\n\n\n\n\n\nObservations"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#references",
    "href": "Take-home_Ex/Take-home_Ex1/Spatio-temporal_Ex1.html#references",
    "title": "Take-home Exercise 1",
    "section": "References",
    "text": "References\nCrawley, M. J. (2007). The R Book. Wiley.\nThe Stata Journal. (2003). Adaptive kernel density estimation. Sage Journals. https://journals.sagepub.com/doi/pdf/10.1177/1536867X0300300204"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#spatio-temporal-kde",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#spatio-temporal-kde",
    "title": "Take-home Exercise 1",
    "section": "7. Spatio-Temporal KDE",
    "text": "7. Spatio-Temporal KDE\nWe focus on the continuous time"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "In-class Exercise"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#install-required-libraries",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#install-required-libraries",
    "title": "In-class Exercise 5",
    "section": "1.1 Install Required Libraries",
    "text": "1.1 Install Required Libraries\nWe will first want to install the GWModel package from CRAN\n\ninstall.packages(\"GWmodel\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#importing-libraries-into-r",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#importing-libraries-into-r",
    "title": "In-class Exercise 5",
    "section": "1.2 Importing Libraries into R",
    "text": "1.2 Importing Libraries into R\nIn this in-class exercise, sf, spdep, tmap, tidyverse, knitr and GWmodel will be used.\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr, GWmodel)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-datasets",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-datasets",
    "title": "In-class Exercise 5",
    "section": "1.3 Preparing the Datasets",
    "text": "1.3 Preparing the Datasets\nI will be using the Hunan dataset used in the Hands-on Exercise 5 spatial weights and applications.\n\n1.3.1 Importing Geospatial Data\nFirstly, we will import the Hunan county boundary layer. This is a geospatial data set in ESRI shapefile format. The code chunk below uses st_read() of sf package.\n\nhunan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\SamanthaxFoo\\IS415-GAA\\In-class_Ex\\In-class_Ex5\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n1.3.2 Importing Aspatial Data\nNext, I will import the aspatial data set. This data is a csv file containing selected Hunan’s local development indicators in 2012.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n1.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan_sf &lt;- left_join(hunan_sf, hunan2012) %&gt;%\n  select(1:3, 7, 15, 16, 31, 32)\n\nJoining with `by = join_by(County)`"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#mapping-gdppc",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#mapping-gdppc",
    "title": "In-class Exercise 5",
    "section": "2. Mapping GDPPC",
    "text": "2. Mapping GDPPC\nNow, we will use qtm() function of tmap package to create a basemap and a choropleth map showing the distribution of GDPPC 2012.\n\nbasemap &lt;- tm_shape(hunan_sf) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan_sf, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#determine-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#determine-adaptive-bandwidth",
    "title": "In-class Exercise 5",
    "section": "4.1 Determine adaptive bandwidth",
    "text": "4.1 Determine adaptive bandwidth\n\n1) Using Cross-Validation\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = TRUE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nAdaptive bandwidth: 62 CV score: 15515442343 \nAdaptive bandwidth: 46 CV score: 14937956887 \nAdaptive bandwidth: 36 CV score: 14408561608 \nAdaptive bandwidth: 29 CV score: 14198527496 \nAdaptive bandwidth: 26 CV score: 13898800611 \nAdaptive bandwidth: 22 CV score: 13662299974 \nAdaptive bandwidth: 22 CV score: 13662299974 \n\n\n\n\n2) Using AIC\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = TRUE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1923.156 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1920.469 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 1917.324 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 1916.661 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1914.897 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-geographically-wieghted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-geographically-wieghted-summary-statistics",
    "title": "In-class Exercise 5",
    "section": "4.2 Computing geographically wieghted summary statistics",
    "text": "4.2 Computing geographically wieghted summary statistics\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-output-data",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-output-data",
    "title": "In-class Exercise 5",
    "section": "4.3 Preparing the output data",
    "text": "4.3 Preparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#visualising-geographically-weighted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#visualising-geographically-weighted-summary-statistics",
    "title": "In-class Exercise 5",
    "section": "4.4 Visualising geographically weighted summary statistics",
    "text": "4.4 Visualising geographically weighted summary statistics\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically weightted mean\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.text.size = 1,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#determine-fixed-bandwidth",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#determine-fixed-bandwidth",
    "title": "In-class Exercise 5",
    "section": "5.1 Determine fixed bandwidth",
    "text": "5.1 Determine fixed bandwidth\n\n5.1.1 Cross-Validation\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nFixed bandwidth: 357.4897 CV score: 16265191728 \nFixed bandwidth: 220.985 CV score: 14954930931 \nFixed bandwidth: 136.6204 CV score: 14134185837 \nFixed bandwidth: 84.48025 CV score: 13693362460 \nFixed bandwidth: 52.25585 CV score: Inf \nFixed bandwidth: 104.396 CV score: 13891052305 \nFixed bandwidth: 72.17162 CV score: 13577893677 \nFixed bandwidth: 64.56447 CV score: 14681160609 \nFixed bandwidth: 76.8731 CV score: 13444716890 \nFixed bandwidth: 79.77877 CV score: 13503296834 \nFixed bandwidth: 75.07729 CV score: 13452450771 \nFixed bandwidth: 77.98296 CV score: 13457916138 \nFixed bandwidth: 76.18716 CV score: 13442911302 \nFixed bandwidth: 75.76323 CV score: 13444600639 \nFixed bandwidth: 76.44916 CV score: 13442994078 \nFixed bandwidth: 76.02523 CV score: 13443285248 \nFixed bandwidth: 76.28724 CV score: 13442844774 \nFixed bandwidth: 76.34909 CV score: 13442864995 \nFixed bandwidth: 76.24901 CV score: 13442855596 \nFixed bandwidth: 76.31086 CV score: 13442847019 \nFixed bandwidth: 76.27264 CV score: 13442846793 \nFixed bandwidth: 76.29626 CV score: 13442844829 \nFixed bandwidth: 76.28166 CV score: 13442845238 \nFixed bandwidth: 76.29068 CV score: 13442844678 \nFixed bandwidth: 76.29281 CV score: 13442844691 \nFixed bandwidth: 76.28937 CV score: 13442844698 \nFixed bandwidth: 76.2915 CV score: 13442844676 \nFixed bandwidth: 76.292 CV score: 13442844679 \nFixed bandwidth: 76.29119 CV score: 13442844676 \nFixed bandwidth: 76.29099 CV score: 13442844676 \nFixed bandwidth: 76.29131 CV score: 13442844676 \nFixed bandwidth: 76.29138 CV score: 13442844676 \nFixed bandwidth: 76.29126 CV score: 13442844676 \nFixed bandwidth: 76.29123 CV score: 13442844676 \n\n\n\n\n5.1.2 AIC\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nFixed bandwidth: 357.4897 AICc value: 1927.631 \nFixed bandwidth: 220.985 AICc value: 1921.547 \nFixed bandwidth: 136.6204 AICc value: 1919.993 \nFixed bandwidth: 84.48025 AICc value: 1940.603 \nFixed bandwidth: 168.8448 AICc value: 1919.457 \nFixed bandwidth: 188.7606 AICc value: 1920.007 \nFixed bandwidth: 156.5362 AICc value: 1919.41 \nFixed bandwidth: 148.929 AICc value: 1919.527 \nFixed bandwidth: 161.2377 AICc value: 1919.392 \nFixed bandwidth: 164.1433 AICc value: 1919.403 \nFixed bandwidth: 159.4419 AICc value: 1919.393 \nFixed bandwidth: 162.3475 AICc value: 1919.394 \nFixed bandwidth: 160.5517 AICc value: 1919.391"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-fixed-bandwidth",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-fixed-bandwidth",
    "title": "In-class Exercise 5",
    "section": "5.2 Computing Fixed Bandwidth",
    "text": "5.2 Computing Fixed Bandwidth\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = FALSE,\n               longlat = T)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-output-data-1",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#preparing-the-output-data-1",
    "title": "In-class Exercise 5",
    "section": "5.3 Preparing the output data",
    "text": "5.3 Preparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#visualising-geographically-weighted-summary-statistics-1",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#visualising-geographically-weighted-summary-statistics-1",
    "title": "In-class Exercise 5",
    "section": "5.4 Visualising geographically weighted summary statistics",
    "text": "5.4 Visualising geographically weighted summary statistics\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically weightted mean\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.text.size = 1,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-geographically-weightted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html#computing-geographically-weightted-summary-statistics",
    "title": "In-class Exercise 5",
    "section": "4.2 Computing geographically weightted summary statistics",
    "text": "4.2 Computing geographically weightted summary statistics\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)"
  }
]